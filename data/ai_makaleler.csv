title,abstract
The Governance of Physical Artificial Intelligence,"Physical artificial intelligence can prove to be one of the most important
challenges of the artificial intelligence. The governance of physical
artificial intelligence would define its responsible intelligent application in
the society."
"Does an artificial intelligence perform market manipulation with its own
  discretion? -- A genetic algorithm learns in an artificial market simulation","Who should be charged with responsibility for an artificial intelligence
performing market manipulation have been discussed. In this study, I
constructed an artificial intelligence using a genetic algorithm that learns in
an artificial market simulation, and investigated whether the artificial
intelligence discovers market manipulation through learning with an artificial
market simulation despite a builder of artificial intelligence has no intention
of market manipulation. As a result, the artificial intelligence discovered
market manipulation as an optimal investment strategy. This result suggests
necessity of regulation, such as obligating builders of artificial intelligence
to prevent artificial intelligence from performing market manipulation."
Impact of Artificial Intelligence on Economic Theory,"Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory."
The case for psychometric artificial general intelligence,"A short review of the literature on measurement and detection of artificial
general intelligence is made. Proposed benchmarks and tests for artificial
general intelligence are critically evaluated against multiple criteria. Based
on the findings, the most promising approaches are identified and some useful
directions for future work are proposed."
"Proceedings of the Thirteenth Conference on Uncertainty in Artificial
  Intelligence (1997)","This is the Proceedings of the Thirteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Providence, RI, August 1-3, 1997"
"Proceedings of the Ninth Conference on Uncertainty in Artificial
  Intelligence (1993)","This is the Proceedings of the Ninth Conference on Uncertainty in Artificial
Intelligence, which was held in Washington, DC, July 9-11, 1993"
"Proceedings of the Second Conference on Uncertainty in Artificial
  Intelligence (1986)","This is the Proceedings of the Second Conference on Uncertainty in Artificial
Intelligence, which was held in Philadelphia, PA, August 8-10, 1986"
Artificial Intelligence in Humans,"In this paper, I put forward that in many instances, thinking mechanisms are
equivalent to artificial intelligence modules programmed into the human mind."
"AAAI FSS-18: Artificial Intelligence in Government and Public Sector
  Proceedings","Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, October 18-20, 2018"
"Watershed of Artificial Intelligence: Human Intelligence, Machine
  Intelligence, and Biological Intelligence","This article reviews the ""Once learning"" mechanism that was proposed 23 years
ago and the subsequent successes of ""One-shot learning"" in image classification
and ""You Only Look Once - YOLO"" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning."
"Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence","Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems."
Comprehensible Artificial Intelligence on Knowledge Graphs: A survey,"Artificial Intelligence applications gradually move outside the safe walls of
research labs and invade our daily lives. This is also true for Machine
Learning methods on Knowledge Graphs, which has led to a steady increase in
their application since the beginning of the 21st century. However, in many
applications, users require an explanation of the Artificial Intelligences
decision. This led to increased demand for Comprehensible Artificial
Intelligence. Knowledge Graphs epitomize fertile soil for Comprehensible
Artificial Intelligence, due to their ability to display connected data, i.e.
knowledge, in a human- as well as machine-readable way. This survey gives a
short history to Comprehensible Artificial Intelligence on Knowledge Graphs.
Furthermore, we contribute by arguing that the concept Explainable Artificial
Intelligence is overloaded and overlapping with Interpretable Machine Learning.
By introducing the parent concept Comprehensible Artificial Intelligence, we
provide a clear-cut distinction of both concepts while accounting for their
similarities. Thus, we provide in this survey a case for Comprehensible
Artificial Intelligence on Knowledge Graphs consisting of Interpretable Machine
Learning on Knowledge Graphs and Explainable Artificial Intelligence on
Knowledge Graphs. This leads to the introduction of a novel taxonomy for
Comprehensible Artificial Intelligence on Knowledge Graphs. In addition, a
comprehensive overview of the research on Comprehensible Artificial
Intelligence on Knowledge Graphs is presented and put into the context of the
taxonomy. Finally, research gaps in the field of Comprehensible Artificial
Intelligence on Knowledge Graphs are identified for future research."
Human $\neq$ AGI,"Terms Artificial General Intelligence (AGI) and Human-Level Artificial
Intelligence (HLAI) have been used interchangeably to refer to the Holy Grail
of Artificial Intelligence (AI) research, creation of a machine capable of
achieving goals in a wide range of environments. However, widespread implicit
assumption of equivalence between capabilities of AGI and HLAI appears to be
unjustified, as humans are not general intelligences. In this paper, we will
prove this distinction."
"Artificial Intelligence Technology analysis using Artificial
  Intelligence patent through Deep Learning model and vector space model","Thanks to rapid development of artificial intelligence technology in recent
years, the current artificial intelligence technology is contributing to many
part of society. Education, environment, medical care, military, tourism,
economy, politics, etc. are having a very large impact on society as a whole.
For example, in the field of education, there is an artificial intelligence
tutoring system that automatically assigns tutors based on student's level. In
the field of economics, there are quantitative investment methods that
automatically analyze large amounts of data to find investment laws to create
investment models or predict changes in financial markets. As such, artificial
intelligence technology is being used in various fields. So, it is very
important to know exactly what factors have an important influence on each
field of artificial intelligence technology and how the relationship between
each field is connected. Therefore, it is necessary to analyze artificial
intelligence technology in each field. In this paper, we analyze patent
documents related to artificial intelligence technology. We propose a method
for keyword analysis within factors using artificial intelligence patent data
sets for artificial intelligence technology analysis. This is a model that
relies on feature engineering based on deep learning model named KeyBERT, and
using vector space model. A case study of collecting and analyzing artificial
intelligence patent data was conducted to show how the proposed model can be
applied to real world problems."
Three IQs of AI Systems and their Testing Methods,"The rapid development of artificial intelligence has brought the artificial
intelligence threat theory as well as the problem about how to evaluate the
intelligence level of intelligent products. Both need to find a quantitative
method to evaluate the intelligence level of intelligence systems, including
human intelligence. Based on the standard intelligence system and the extended
Von Neumann architecture, this paper proposes General IQ, Service IQ and Value
IQ evaluation methods for intelligence systems, depending on different
evaluation purposes. Among them, the General IQ of intelligence systems is to
answer the question of whether the artificial intelligence can surpass the
human intelligence, which is reflected in putting the intelligence systems on
an equal status and conducting the unified evaluation. The Service IQ and Value
IQ of intelligence systems are used to answer the question of how the
intelligent products can better serve the human, reflecting the intelligence
and required cost of each intelligence system as a product in the process of
serving human."
"Examining correlation between trust and transparency with explainable
  artificial intelligence","Trust between humans and artificial intelligence(AI) is an issue which has
implications in many fields of human computer interaction. The current issue
with artificial intelligence is a lack of transparency into its decision
making, and literature shows that increasing transparency increases trust.
Explainable artificial intelligence has the ability to increase transparency of
AI, which could potentially increase trust for humans. This paper attempts to
use the task of predicting yelp review star ratings with assistance from an
explainable and non explainable artificial intelligence to see if trust is
increased with increased transparency. Results show that for these tasks,
explainable artificial intelligence provided significant increase in trust as a
measure of influence."
"The Artificial Scientist: Logicist, Emergentist, and Universalist
  Approaches to Artificial General Intelligence","We attempt to define what is necessary to construct an Artificial Scientist,
explore and evaluate several approaches to artificial general intelligence
(AGI) which may facilitate this, conclude that a unified or hybrid approach is
necessary and explore two theories that satisfy this requirement to some
degree."
"Proceedings of the Twenty-Seventh Conference on Uncertainty in
  Artificial Intelligence (2011)","This is the Proceedings of the Twenty-Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Barcelona, Spain, July 14 - 17 2011."
"Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial
  Intelligence (2010)","This is the Proceedings of the Twenty-Sixth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA, July 8 - 11
2010."
"Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial
  Intelligence (2009)","This is the Proceedings of the Twenty-Fifth Conference on Uncertainty in
Artificial Intelligence, which was held in Montreal, QC, Canada, June 18 - 21
2009."
"Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial
  Intelligence (2008)","This is the Proceedings of the Twenty-Fourth Conference on Uncertainty in
Artificial Intelligence, which was held in Helsinki, Finland, July 9 - 12 2008."
"Proceedings of the Twenty-First Conference on Uncertainty in Artificial
  Intelligence (2005)","This is the Proceedings of the Twenty-First Conference on Uncertainty in
Artificial Intelligence, which was held in Edinburgh, Scotland July 26 - 29
2005."
"Proceedings of the Twenty-Second Conference on Uncertainty in Artificial
  Intelligence (2006)","This is the Proceedings of the Twenty-Second Conference on Uncertainty in
Artificial Intelligence, which was held in Cambridge, MA, July 13 - 16 2006."
"Proceedings of the Twentieth Conference on Uncertainty in Artificial
  Intelligence (2004)","This is the Proceedings of the Twentieth Conference on Uncertainty in
Artificial Intelligence, which was held in Banff, Canada, July 7 - 11 2004."
"Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial
  Intelligence (2012)","This is the Proceedings of the Twenty-Eighth Conference on Uncertainty in
Artificial Intelligence, which was held on Catalina Island, CA August 14-18
2012."
"Proceedings of the Nineteenth Conference on Uncertainty in Artificial
  Intelligence (2003)","This is the Proceedings of the Nineteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Acapulco, Mexico, August 7-10 2003"
"Proceedings of the Seventeenth Conference on Uncertainty in Artificial
  Intelligence (2001)","This is the Proceedings of the Seventeenth Conference on Uncertainty in
Artificial Intelligence, which was held in Seattle, WA, August 2-5 2001"
"Proceedings of the Eighteenth Conference on Uncertainty in Artificial
  Intelligence (2002)","This is the Proceedings of the Eighteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Alberta, Canada, August 1-4 2002"
"Proceedings of the Sixteenth Conference on Uncertainty in Artificial
  Intelligence (2000)","This is the Proceedings of the Sixteenth Conference on Uncertainty in
Artificial Intelligence, which was held in San Francisco, CA, June 30 - July 3,
2000"
"Proceedings of the Fifteenth Conference on Uncertainty in Artificial
  Intelligence (1999)","This is the Proceedings of the Fifteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Stockholm Sweden, July 30 - August
1, 1999"
"Proceedings of the Fourteenth Conference on Uncertainty in Artificial
  Intelligence (1998)","This is the Proceedings of the Fourteenth Conference on Uncertainty in
Artificial Intelligence, which was held in Madison, WI, July 24-26, 1998"
"Proceedings of the Twelfth Conference on Uncertainty in Artificial
  Intelligence (1996)","This is the Proceedings of the Twelfth Conference on Uncertainty in
Artificial Intelligence, which was held in Portland, OR, August 1-4, 1996"
"Proceedings of the Eleventh Conference on Uncertainty in Artificial
  Intelligence (1995)","This is the Proceedings of the Eleventh Conference on Uncertainty in
Artificial Intelligence, which was held in Montreal, QU, August 18-20, 1995"
"Proceedings of the Tenth Conference on Uncertainty in Artificial
  Intelligence (1994)","This is the Proceedings of the Tenth Conference on Uncertainty in Artificial
Intelligence, which was held in Seattle, WA, July 29-31, 1994"
"Proceedings of the Eighth Conference on Uncertainty in Artificial
  Intelligence (1992)","This is the Proceedings of the Eighth Conference on Uncertainty in Artificial
Intelligence, which was held in Stanford, CA, July 17-19, 1992"
"Proceedings of the Seventh Conference on Uncertainty in Artificial
  Intelligence (1991)","This is the Proceedings of the Seventh Conference on Uncertainty in
Artificial Intelligence, which was held in Los Angeles, CA, July 13-15, 1991"
"Proceedings of the Sixth Conference on Uncertainty in Artificial
  Intelligence (1990)","This is the Proceedings of the Sixth Conference on Uncertainty in Artificial
Intelligence, which was held in Cambridge, MA, Jul 27 - Jul 29, 1990"
"Proceedings of the Fifth Conference on Uncertainty in Artificial
  Intelligence (1989)","This is the Proceedings of the Fifth Conference on Uncertainty in Artificial
Intelligence, which was held in Windsor, ON, August 18-20, 1989"
"Proceedings of the Fourth Conference on Uncertainty in Artificial
  Intelligence (1988)","This is the Proceedings of the Fourth Conference on Uncertainty in Artificial
Intelligence, which was held in Minneapolis, MN, July 10-12, 1988"
"Proceedings of the Third Conference on Uncertainty in Artificial
  Intelligence (1987)","This is the Proceedings of the Third Conference on Uncertainty in Artificial
Intelligence, which was held in Seattle, WA, July 10-12, 1987"
"Proceedings of the First Conference on Uncertainty in Artificial
  Intelligence (1985)","This is the Proceedings of the First Conference on Uncertainty in Artificial
Intelligence, which was held in Los Angeles, CA, July 10-12, 1985"
"Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial
  Intelligence (2013)","This is the Proceedings of the Twenty-Ninth Conference on Uncertainty in
Artificial Intelligence, which was held in Bellevue, WA, August 11-15, 2013"
Philosophy in the Face of Artificial Intelligence,"In this article, I discuss how the AI community views concerns about the
emergence of superintelligent AI and related philosophical issues."
"Advances in Artificial Intelligence Require Progress Across all of
  Computer Science","Advances in Artificial Intelligence require progress across all of computer
science."
"AAAI FSS-19: Artificial Intelligence in Government and Public Sector
  Proceedings","Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Arlington, Virginia, USA, November 7-8, 2019"
"AAAI FSS-20: Artificial Intelligence in Government and Public Sector
  Proceedings","Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Washington, DC, USA, November 13-14, 2020"
"AAAI FSS-21: Artificial Intelligence in Government and Public Sector
  Proceedings","Proceedings of the AAAI Fall Symposium on Artificial Intelligence in
Government and Public Sector, Washington, DC, USA, November 4-6, 2021"
"Can transformative AI shape a new age for our civilization?: Navigating
  between speculation and reality","Artificial Intelligence is widely regarded as a transformative force with the
potential to redefine numerous sectors of human civilization. While Artificial
Intelligence has evolved from speculative fiction to a pivotal element of
technological progress, its role as a truly transformative agent, or
transformative Artificial Intelligence, remains a subject of debate. This work
explores the historical precedents of technological breakthroughs, examining
whether Artificial Intelligence can achieve a comparable impact, and it delves
into various ethical frameworks that shape the perception and development of
Artificial Intelligence. Additionally, it considers the societal, technical,
and regulatory challenges that must be addressed for Artificial Intelligence to
become a catalyst for global change. We also examine not only the strategies
and methodologies that could lead to transformative Artificial Intelligence but
also the barriers that could ultimately make these goals unattainable. We end
with a critical inquiry into whether reaching a transformative Artificial
Intelligence might compel humanity to adopt an entirely new ethical approach,
tailored to the complexities of advanced Artificial Intelligence. By addressing
the ethical, social, and scientific dimensions of Artificial Intelligence's
development, this work contributes to the broader discourse on the long-term
implications of Artificial Intelligence and its capacity to drive civilization
toward a new era of progress or, conversely, exacerbate existing inequalities
and risks."
"Intelligence of Astronomical Optical Telescope: Present Status and
  Future Perspectives","Artificial intelligence technology has been widely used in astronomy, and new
artificial intelligence technologies and application scenarios are constantly
emerging. There have been a large number of papers reviewing the application of
artificial intelligence technology in astronomy. However, relevant articles
seldom mention telescope intelligence separately, and it is difficult to
understand the current development status and research hotspots of telescope
intelligence from these papers. This paper combines the development history of
artificial intelligence technology and the difficulties of critical
technologies of telescopes, comprehensively introduces the development and
research hotspots of telescope intelligence, then conducts statistical analysis
on various research directions of telescope intelligence and defines the
research directions' merits. All kinds of research directions are evaluated,
and the research trend of each telescope's intelligence is pointed out.
Finally, according to the advantages of artificial intelligence technology and
the development trend of telescopes, future research hotspots of telescope
intelligence are given."
AAAI-2019 Workshop on Games and Simulations for Artificial Intelligence,"This volume represents the accepted submissions from the AAAI-2019 Workshop
on Games and Simulations for Artificial Intelligence held on January 29, 2019
in Honolulu, Hawaii, USA. https://www.gamesim.ai"
From Statistical Relational to Neuro-Symbolic Artificial Intelligence,"Neuro-symbolic and statistical relational artificial intelligence both
integrate frameworks for learning with logical reasoning. This survey
identifies several parallels across seven different dimensions between these
two fields. These cannot only be used to characterize and position
neuro-symbolic artificial intelligence approaches but also to identify a number
of directions for further research."
Games for Artificial Intelligence Research: A Review and Perspectives,"Games have been the perfect test-beds for artificial intelligence research
for the characteristics that widely exist in real-world scenarios. Learning and
optimisation, decision making in dynamic and uncertain environments, game
theory, planning and scheduling, design and education are common research areas
shared between games and real-world problems. Numerous open-source games or
game-based environments have been implemented for studying artificial
intelligence. In addition to single- or multi-player, collaborative or
adversarial games, there has also been growing interest in implementing
platforms for creative design in recent years. Those platforms provide ideal
benchmarks for exploring and comparing artificial intelligence ideas and
techniques. This paper reviews the games and game-based platforms for
artificial intelligence research, provides guidance on matching particular
types of artificial intelligence with suitable games for testing and matching
particular needs in games with suitable artificial intelligence techniques,
discusses the research trend induced by the evolution of those games and
platforms, and gives an outlook."
A Study on Artificial Intelligence IQ and Standard Intelligent Model,"Currently, potential threats of artificial intelligence (AI) to human have
triggered a large controversy in society, behind which, the nature of the issue
is whether the artificial intelligence (AI) system can be evaluated
quantitatively. This article analyzes and evaluates the challenges that the AI
development level is facing, and proposes that the evaluation methods for the
human intelligence test and the AI system are not uniform; and the key reason
for which is that none of the models can uniformly describe the AI system and
the beings like human. Aiming at this problem, a standard intelligent system
model is established in this study to describe the AI system and the beings
like human uniformly. Based on the model, the article makes an abstract
mathematical description, and builds the standard intelligent machine
mathematical model; expands the Von Neumann architecture and proposes the
Liufeng - Shiyong architecture; gives the definition of the artificial
intelligence IQ, and establishes the artificial intelligence scale and the
evaluation method; conduct the test on 50 search engines and three human
subjects at different ages across the world, and finally obtains the ranking of
the absolute IQ and deviation IQ ranking for artificial intelligence IQ 2014."
"Hybrid Systems Knowledge Representation Using Modelling Environment
  System Techniques Artificial Intelligence","Knowledge-based or Artificial Intelligence techniques are used increasingly
as alternatives to more classical techniques to model ENVIRONMENTAL SYSTEMS.
Use of Artificial Intelligence (AI) in environmental modelling has increased
with recognition of its potential. In this paper we examine the DIFFERENT
TECHNIQUES of Artificial intelligence with profound examples of human
perception, learning and reasoning to solve complex problems. However with the
increase of complexity better methods are required. Keeping in view of the
above some researchers introduced the idea of hybrid mechanism in which two or
more methods can be combined which seems to be a positive effort for creating a
more complex; advanced and intelligent system which has the capability to in-
cooperate human decisions thus driving the landscape changes."
"A Finite-Time Technological Singularity Model With Artificial
  Intelligence Self-Improvement","Recent advances in the development of artificial intelligence, technological
progress acceleration, long-term trends of macroeconomic dynamics increase the
relevance of technological singularity hypothesis. In this paper, we build a
model of finite-time technological singularity assuming that artificial
intelligence will replace humans for artificial intelligence engineers after
some point in time when it is developed enough. This model implies the
following: let A be the level of development of artificial intelligence. Then,
the moment of technological singularity n is defined as the point in time where
artificial intelligence development function approaches infinity. Thus, it
happens in finite time. Although infinite level of development of artificial
intelligence cannot be reached practically, this approximation is useful for
several reasons, firstly because it allows modeling a phase transition or a
change of regime. In the model, intelligence growth function appears to be
hyperbolic function under relatively broad conditions which we list and
compare. Subsequently, we also add a stochastic term (Brownian motion) to the
model and investigate the changes in its behavior. The results can be applied
for the modeling of dynamics of various processes characterized by
multiplicative growth."
Creativity and Artificial Intelligence: A Digital Art Perspective,"This paper describes the application of artificial intelligence to the
creation of digital art. AI is a computational paradigm that codifies
intelligence into machines. There are generally three types of artificial
intelligence and these are machine learning, evolutionary programming and soft
computing. Machine learning is the statistical approach to building intelligent
systems. Evolutionary programming is the use of natural evolutionary systems to
design intelligent machines. Some of the evolutionary programming systems
include genetic algorithm which is inspired by the principles of evolution and
swarm optimization which is inspired by the swarming of birds, fish, ants etc.
Soft computing includes techniques such as agent based modelling and fuzzy
logic. Opportunities on the applications of these to digital art are explored."
"Foundations of Intelligence in Natural and Artificial Systems: A
  Workshop Report","In March of 2021, the Santa Fe Institute hosted a workshop as part of its
Foundations of Intelligence in Natural and Artificial Systems project. This
project seeks to advance the field of artificial intelligence by promoting
interdisciplinary research on the nature of intelligence. During the workshop,
speakers from diverse disciplines gathered to develop a taxonomy of
intelligence, articulating their own understanding of intelligence and how
their research has furthered that understanding. In this report, we summarize
the insights offered by each speaker and identify the themes that emerged
during the talks and subsequent discussions."
The limit of human intelligence,"In 1998, Fields medalist Stephen Smale [S. Smale, Mathematical problems for
the next century, The mathematical Intelligencer, 20(2) (1998), 7-15] proposed
his famous eighteen problems to the mathematicians of this century. The
statement of his eighteenth problem is very simple but very important. He asked
""What are the limits of intelligence, both artificial and human?"". In this
paper, we prove that human intelligence is limitless. Moreover, we provide
justifications to state that artificial intelligence has limitations. Thus,
human intelligence will always remain superior to artificial intelligence.
Moreover, we provide justifications to conclude the limitations of artificial
intelligence."
"Improving Global Weather and Ocean Wave Forecast with Large Artificial
  Intelligence Models","The rapid advancement of artificial intelligence technologies, particularly
in recent years, has led to the emergence of several large parameter artificial
intelligence weather forecast models. These models represent a significant
breakthrough, overcoming the limitations of traditional numerical weather
prediction models and indicating the emergence of profound potential tools for
atmosphere-ocean forecasts. This study explores the evolution of these advanced
artificial intelligence forecast models, and based on the identified
commonalities, proposes the ""Three Large Rules"" to measure their development.
We discuss the potential of artificial intelligence in revolutionizing
numerical weather prediction, and briefly outlining the underlying reasons for
its great potential. While acknowledging the high accuracy, computational
efficiency, and ease of deployment of large artificial intelligence forecast
models, we also emphasize the irreplaceable values of traditional numerical
forecasts and explore the challenges in the future development of large-scale
artificial intelligence atmosphere-ocean forecast models. We believe that the
optimal future of atmosphere-ocean weather forecast lies in achieving a
seamless integration of artificial intelligence and traditional numerical
models. Such a synthesis is anticipated to offer a more advanced and reliable
approach for improved atmosphere-ocean forecasts. Additionally, we illustrate
how forecasters can adapt and leverage the advanced artificial intelligence
model through an example by building a large artificial intelligence model for
global ocean wave forecast."
On a Functional Definition of Intelligence,"Without an agreed-upon definition of intelligence, asking ""is this system
intelligent?"""" is an untestable question. This lack of consensus hinders
research, and public perception, on Artificial Intelligence (AI), particularly
since the rise of generative- and large-language models. Most work on precisely
capturing what we mean by ""intelligence"" has come from the fields of
philosophy, psychology, and cognitive science. Because these perspectives are
intrinsically linked to intelligence as it is demonstrated by natural
creatures, we argue such fields cannot, and will not, provide a sufficiently
rigorous definition that can be applied to artificial means. Thus, we present
an argument for a purely functional, black-box definition of intelligence,
distinct from how that intelligence is actually achieved; focusing on the
""what"", rather than the ""how"". To achieve this, we first distinguish other
related concepts (sentience, sensation, agency, etc.) from the notion of
intelligence, particularly identifying how these concepts pertain to artificial
intelligent systems. As a result, we achieve a formal definition of
intelligence that is conceptually testable from only external observation, that
suggests intelligence is a continuous variable. We conclude by identifying
challenges that still remain towards quantifiable measurement. This work
provides a useful perspective for both the development of AI, and for public
perception of the capabilities and risks of AI."
Tests of Machine Intelligence,"Although the definition and measurement of intelligence is clearly of
fundamental importance to the field of artificial intelligence, no general
survey of definitions and tests of machine intelligence exists. Indeed few
researchers are even aware of alternatives to the Turing test and its many
derivatives. In this paper we fill this gap by providing a short survey of the
many tests of machine intelligence that have been proposed."
An Approximation of the Universal Intelligence Measure,"The Universal Intelligence Measure is a recently proposed formal definition
of intelligence. It is mathematically specified, extremely general, and
captures the essence of many informal definitions of intelligence. It is based
on Hutter's Universal Artificial Intelligence theory, an extension of Ray
Solomonoff's pioneering work on universal induction. Since the Universal
Intelligence Measure is only asymptotically computable, building a practical
intelligence test from it is not straightforward. This paper studies the
practical issues involved in developing a real-world UIM-based performance
metric. Based on our investigation, we develop a prototype implementation which
we use to evaluate a number of different artificial agents."
Decision under Uncertainty,"We derive axiomatically the probability function that should be used to make
decisions given any form of underlying uncertainty."
The Book of Why: Review,"This is a review of ""The Book of Why"", by Judea Pearl."
On a measure of intelligence,"The Fall 2024 Logic in Computer Science column of the Bulletin of EATCS is a
little discussion on intelligence, measuring intelligence, and related issues,
provoked by a fascinating must-read article ``On the measure of intelligence''
by Fran\c{c}ois Chollet. The discussion includes a modicum of critique of the
article."
"Bridging the Gap between Artificial Intelligence and Artificial General
  Intelligence: A Ten Commandment Framework for Human-Like Intelligence","The field of artificial intelligence has seen explosive growth and
exponential success. The last phase of development showcased deep learnings
ability to solve a variety of difficult problems across a multitude of domains.
Many of these networks met and exceeded human benchmarks by becoming experts in
the domains in which they are trained. Though the successes of artificial
intelligence have begun to overshadow its failures, there is still much that
separates current artificial intelligence tools from becoming the exceptional
general learners that humans are. In this paper, we identify the ten
commandments upon which human intelligence is systematically and hierarchically
built. We believe these commandments work collectively to serve as the
essential ingredients that lead to the emergence of higher-order cognition and
intelligence. This paper discusses a computational framework that could house
these ten commandments and suggests new architectural modifications that could
lead to the development of smarter, more explainable, and generalizable
artificial systems inspired by a neuromorphic approach."
"Techniques for Adversarial Examples Threatening the Safety of Artificial
  Intelligence Based Systems","Artificial intelligence is known as the most effective technological field
for rapid developments shaping the future of the world. Even today, it is
possible to see intense use of intelligence systems in all fields of the life.
Although advantages of the Artificial Intelligence are widely observed, there
is also a dark side employing efforts to design hacking oriented techniques
against Artificial Intelligence. Thanks to such techniques, it is possible to
trick intelligent systems causing directed results for unsuccessful outputs.
That is critical for also cyber wars of the future as it is predicted that the
wars will be done unmanned, autonomous intelligent systems. Moving from the
explanations, objective of this study is to provide information regarding
adversarial examples threatening the Artificial Intelligence and focus on
details of some techniques, which are used for creating adversarial examples.
Adversarial examples are known as training data, which can trick a Machine
Learning technique to learn incorrectly about the target problem and cause an
unsuccessful or maliciously directed intelligent system at the end. The study
enables the readers to learn enough about details of recent techniques for
creating adversarial examples."
Action Selection Properties in a Software Simulated Agent,"This article analyses the properties of the Internal Behaviour network, an
action selection mechanism previously proposed by the authors, with the aid of
a simulation developed for such ends. A brief review of the Internal Behaviour
network is followed by the explanation of the implementation of the simulation.
Then, experiments are presented and discussed analysing the properties of the
action selection in the proposed model."
"A Novel Method for Developing Robotics via Artificial Intelligence and
  Internet of Things","This paper describe about a new methodology for developing and improving the
robotics field via artificial intelligence and internet of things. Now a day,
we can say Artificial Intelligence take the world into robotics. Almost all
industries use robots for lot of works. They are use co-operative robots to
make different kind of works. But there was some problem to make robot for
multi tasks. So there was a necessary new methodology to made multi tasking
robots. It will be done only by artificial intelligence and internet of things."
"Modular Belief Updates and Confusion about Measures of Certainty in
  Artificial Intelligence Research","Over the last decade, there has been growing interest in the use or measures
or change in belief for reasoning with uncertainty in artificial intelligence
research. An important characteristic of several methodologies that reason with
changes in belief or belief updates, is a property that we term modularity. We
call updates that satisfy this property modular updates. Whereas probabilistic
measures of belief update - which satisfy the modularity property were first
discovered in the nineteenth century, knowledge and discussion of these
quantities remains obscure in artificial intelligence research. We define
modular updates and discuss their inappropriate use in two influential expert
systems."
Stovepiping and Malicious Software: A Critical Review of AGI Containment,"Awareness of the possible impacts associated with artificial intelligence has
risen in proportion to progress in the field. While there are tremendous
benefits to society, many argue that there are just as many, if not more,
concerns related to advanced forms of artificial intelligence. Accordingly,
research into methods to develop artificial intelligence safely is increasingly
important. In this paper, we provide an overview of one such safety paradigm:
containment with a critical lens aimed toward generative adversarial networks
and potentially malicious artificial intelligence. Additionally, we illuminate
the potential for a developmental blindspot in the stovepiping of containment
mechanisms."
Human-Centered Artificial Intelligence and Machine Learning,"Humans are increasingly coming into contact with artificial intelligence and
machine learning systems. Human-centered artificial intelligence is a
perspective on AI and ML that algorithms must be designed with awareness that
they are part of a larger system consisting of humans. We lay forth an argument
that human-centered artificial intelligence can be broken down into two
aspects: (1) AI systems that understand humans from a sociocultural
perspective, and (2) AI systems that help humans understand them. We further
argue that issues of social responsibility such as fairness, accountability,
interpretability, and transparency."
"Machine Learning in Artificial Intelligence: Towards a Common
  Understanding","The application of ""machine learning"" and ""artificial intelligence"" has
become popular within the last decade. Both terms are frequently used in
science and media, sometimes interchangeably, sometimes with different
meanings. In this work, we aim to clarify the relationship between these terms
and, in particular, to specify the contribution of machine learning to
artificial intelligence. We review relevant literature and present a conceptual
framework which clarifies the role of machine learning to build (artificial)
intelligent agents. Hence, we seek to provide more terminological clarity and a
starting point for (interdisciplinary) discussions and future research."
"Deep Learning, Natural Language Processing, and Explainable Artificial
  Intelligence in the Biomedical Domain","In this article, we first give an introduction to artificial intelligence and
its applications in biology and medicine in Section 1. Deep learning methods
are then described in Section 2. We narrow down the focus of the study on
textual data in Section 3, where natural language processing and its
applications in the biomedical domain are described. In Section 4, we give an
introduction to explainable artificial intelligence and discuss the importance
of explainability of artificial intelligence systems, especially in the
biomedical domain."
"A Cybersecurity Risk Analysis Framework for Systems with Artificial
  Intelligence Components","The introduction of the European Union Artificial Intelligence Act, the NIST
Artificial Intelligence Risk Management Framework, and related norms demands a
better understanding and implementation of novel risk analysis approaches to
evaluate systems with Artificial Intelligence components. This paper provides a
cybersecurity risk analysis framework that can help assessing such systems. We
use an illustrative example concerning automated driving systems."
"Artificial intelligence-based blockchain-driven financial default
  prediction","With the rapid development of technology, blockchain and artificial
intelligence technology are playing a huge role in all walks of life. In the
financial sector, blockchain solves many security problems in data storage and
management in traditional systems with its advantages of decentralization and
security. And artificial intelligence has huge advantages in financial
forecasting and risk management through its powerful algorithmic modeling
capabilities. In financial default prediction using blockchain and artificial
intelligence technology is a very powerful application. Blockchain technology
guarantees the credibility of data and consistency on all nodes, and machine
learning builds a high-level default prediction model through detailed analysis
of big data. This study offers financial institutions new thoughts on financial
technology in terms of credit risk mitigation and financial system
stabilization."
"Quantifying Natural and Artificial Intelligence in Robots and Natural
  Systems with an Algorithmic Behavioural Test","One of the most important aims of the fields of robotics, artificial
intelligence and artificial life is the design and construction of systems and
machines as versatile and as reliable as living organisms at performing high
level human-like tasks. But how are we to evaluate artificial systems if we are
not certain how to measure these capacities in living systems, let alone how to
define life or intelligence? Here I survey a concrete metric towards measuring
abstract properties of natural and artificial systems, such as the ability to
react to the environment and to control one's own behaviour."
Seeding the Singularity for A.I,"The singularity refers to an idea that once a machine having an artificial
intelligence surpassing the human intelligence capacity is created, it will
trigger explosive technological and intelligence growth. I propose to test the
hypothesis that machine intelligence capacity can grow autonomously starting
with an intelligence comparable to that of bacteria - microbial intelligence.
The goal will be to demonstrate that rapid growth in intelligence capacity can
be realized at all in artificial computing systems. I propose the following
three properties that may allow an artificial intelligence to exhibit a steady
growth in its intelligence capacity: (i) learning with the ability to modify
itself when exposed to more data, (ii) acquiring new functionalities (skills),
and (iii) expanding or replicating itself. The algorithms must demonstrate a
rapid growth in skills of dataprocessing and analysis and gain qualitatively
different functionalities, at least until the current computing technology
supports their scalable development. The existing algorithms that already
encompass some of these or similar properties, as well as missing abilities
that must yet be implemented, will be reviewed in this work. Future
computational tests could support or oppose the hypothesis that artificial
intelligence can potentially grow to the level of superintelligence which
overcomes the limitations in hardware by producing necessary processing
resources or by changing the physical realization of computation from using
chip circuits to using quantum computing principles."
"Physical Artificial Intelligence: The Concept Expansion of
  Next-Generation Artificial Intelligence","Artificial Intelligence has been a growth catalyst to our society and is
cosidered across all idustries as a fundamental technology. However, its
development has been limited to the signal processing domain that relies on the
generated and collected data from other sensors. In recent research, concepts
of Digital Artificial Intelligence and Physicial Artifical Intelligence have
emerged and this can be considered a big step in the theoretical development of
Artifical Intelligence. In this paper we explore the concept of Physicial
Artifical Intelligence and propose two subdomains: Integrated Physicial
Artifical Intelligence and Distributed Physicial Artifical Intelligence. The
paper will also examine the trend and governance of Physicial Artifical
Intelligence."
Can Artificial Intelligence Do Everything That We Can?,"In this article, I discuss what AI can and cannot yet do, and the
implications for humanity."
Artificial Intelligence and Economic Theories,"The advent of artificial intelligence has changed many disciplines such as
engineering, social science and economics. Artificial intelligence is a
computational technique which is inspired by natural intelligence such as the
swarming of birds, the working of the brain and the pathfinding of the ants.
These techniques have impact on economic theories. This book studies the impact
of artificial intelligence on economic theories, a subject that has not been
extensively studied. The theories that are considered are: demand and supply,
asymmetrical information, pricing, rational choice, rational expectation, game
theory, efficient market hypotheses, mechanism design, prospect, bounded
rationality, portfolio theory, rational counterfactual and causality. The
benefit of this book is that it evaluates existing theories of economics and
update them based on the developments in artificial intelligence field."
Human-in-the-loop Artificial Intelligence,"Little by little, newspapers are revealing the bright future that Artificial
Intelligence (AI) is building. Intelligent machines will help everywhere.
However, this bright future has a dark side: a dramatic job market contraction
before its unpredictable transformation. Hence, in a near future, large numbers
of job seekers will need financial support while catching up with these novel
unpredictable jobs. This possible job market crisis has an antidote inside. In
fact, the rise of AI is sustained by the biggest knowledge theft of the recent
years. Learning AI machines are extracting knowledge from unaware skilled or
unskilled workers by analyzing their interactions. By passionately doing their
jobs, these workers are digging their own graves.
  In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI)
as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward
aware and unaware knowledge producers with a different scheme: decisions of AI
systems generating revenues will repay the legitimate owners of the knowledge
used for taking those decisions. As modern Robin Hoods, HIT-AI researchers
should fight for a fairer Artificial Intelligence that gives back what it
steals."
"Use of Artificial Intelligence Techniques / Applications in Cyber
  Defense","Nowadays, considering the speed of the processes and the amount of data used
in cyber defense, it cannot be expected to have an effective defense by using
only human power without the help of automation systems. However, for the
effective defense against dynamically evolving attacks on networks, it is
difficult to develop software with conventional fixed algorithms. This can be
achieved by using artificial intelligence methods that provide flexibility and
learning capability. The likelihood of developing cyber defense capabilities
through increased intelligence of defense systems is quite high. Given the
problems associated with cyber defense in real life, it is clear that many
cyber defense problems can be successfully solved only when artificial
intelligence methods are used. In this article, the current artificial
intelligence practices and techniques are reviewed and the use and importance
of artificial intelligence in cyber defense systems is mentioned. The aim of
this article is to be able to explain the use of these methods in the field of
cyber defense with current examples by considering and analyzing the artificial
intelligence technologies and methodologies that are currently being developed
and integrating them with the role and adaptation of the technology and
methodology in the defense of cyberspace."
"An Introductory Review of Spiking Neural Network and Artificial Neural
  Network: From Biological Intelligence to Artificial Intelligence","Recently, stemming from the rapid development of artificial intelligence,
which has gained expansive success in pattern recognition, robotics, and
bioinformatics, neuroscience is also gaining tremendous progress. A kind of
spiking neural network with biological interpretability is gradually receiving
wide attention, and this kind of neural network is also regarded as one of the
directions toward general artificial intelligence. This review introduces the
following sections, the biological background of spiking neurons and the
theoretical basis, different neuronal models, the connectivity of neural
circuits, the mainstream neural network learning mechanisms and network
architectures, etc. This review hopes to attract different researchers and
advance the development of brain-inspired intelligence and artificial
intelligence."
Hybrid Super Intelligence and Polymetric Analysis,"The problem of possible applications Polymetric Analysis for the resolution
problems of artificial Intelligence is discussed. As example the hybrid super
intelligence system by N. Moiseev type was selected. The bond between
polymetric analysis and hybrid super intelligence system was shown. In
operational sense polymetric analysis is more general system. Therefore main
principles of Moiseev concept may be unify with the help of polymetric
analysis. Main peculiarities of this unification are analyzed."
"What is Meant by AGI? On the Definition of Artificial General
  Intelligence","This paper aims to establish a consensus on AGI's definition. General
intelligence refers to the adaptation to open environments according to certain
principles using limited resources. It emphasizes that adaptation or learning
is an indispensable property of intelligence, and places the controversial part
within the principles of intelligence, which can be described from different
perspectives."
What Does Artificial Life Tell Us About Death?,Short philosophical essay
A Formal Measure of Machine Intelligence,"A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this measure formally captures the concept of machine intelligence in the
broadest reasonable sense."
"Computational Narrative Intelligence: A Human-Centered Goal for
  Artificial Intelligence","Narrative intelligence is the ability to craft, tell, understand, and respond
affectively to stories. We argue that instilling artificial intelligences with
computational narrative intelligence affords a number of applications
beneficial to humans. We lay out some of the machine learning challenges
necessary to solve to achieve computational narrative intelligence. Finally, we
argue that computational narrative is a practical step towards machine
enculturation, the teaching of sociocultural values to machines."
Understanding Human Intelligence through Human Limitations,"Recent progress in artificial intelligence provides the opportunity to ask
the question of what is unique about human intelligence, but with a new
comparison class. I argue that we can understand human intelligence, and the
ways in which it may differ from artificial intelligence, by considering the
characteristics of the kind of computational problems that human minds have to
solve. I claim that these problems acquire their structure from three
fundamental limitations that apply to human beings: limited time, limited
computation, and limited communication. From these limitations we can derive
many of the properties we associate with human intelligence, such as rapid
learning, the ability to break down problems into parts, and the capacity for
cumulative cultural evolution."
"Intelligence as information processing: brains, swarms, and computers","There is no agreed definition of intelligence, so it is problematic to simply
ask whether brains, swarms, computers, or other systems are intelligent or not.
To compare the potential intelligence exhibited by different cognitive systems,
I use the common approach used by artificial intelligence and artificial life:
Instead of studying the substrate of systems, let us focus on their
organization. This organization can be measured with information. Thus, I apply
an informationist epistemology to describe cognitive systems, including brains
and computers. This allows me to frame the usefulness and limitations of the
brain-computer analogy in different contexts. I also use this perspective to
discuss the evolution and ecology of intelligence."
Accelerating Entrepreneurial Decision-Making Through Hybrid Intelligence,"Accelerating Entrepreneurial Decision-Making Through Hybrid Intelligence
DESIGN PARADIGMS AND PRINCIPLES FOR DECISIONAL GUIDANCE IN ENTREPRENEURSHIP"
"Integrating Generative Artificial Intelligence in Intelligent Vehicle
  Systems","This paper aims to serve as a comprehensive guide for researchers and
practitioners, offering insights into the current state, potential
applications, and future research directions for generative artificial
intelligence and foundation models within the context of intelligent vehicles.
As the automotive industry progressively integrates AI, generative artificial
intelligence technologies hold the potential to revolutionize user
interactions, delivering more immersive, intuitive, and personalised in-car
experiences. We provide an overview of current applications of generative
artificial intelligence in the automotive domain, emphasizing speech, audio,
vision, and multimodal interactions. We subsequently outline critical future
research areas, including domain adaptability, alignment, multimodal
integration and others, as well as, address the challenges and risks associated
with ethics. By fostering collaboration and addressing these research areas,
generative artificial intelligence can unlock its full potential, transforming
the driving experience and shaping the future of intelligent vehicles."
Measuring the intelligence of an idealized mechanical knowing agent,"We define a notion of the intelligence level of an idealized mechanical
knowing agent. This is motivated by efforts within artificial intelligence
research to define real-number intelligence levels of complicated intelligent
systems. Our agents are more idealized, which allows us to define a much
simpler measure of intelligence level for them. In short, we define the
intelligence level of a mechanical knowing agent to be the supremum of the
computable ordinals that have codes the agent knows to be codes of computable
ordinals. We prove that if one agent knows certain things about another agent,
then the former necessarily has a higher intelligence level than the latter.
This allows our intelligence notion to serve as a stepping stone to obtain
results which, by themselves, are not stated in terms of our intelligence
notion (results of potential interest even to readers totally skeptical that
our notion correctly captures intelligence). As an application, we argue that
these results comprise evidence against the possibility of intelligence
explosion (that is, the notion that sufficiently intelligent machines will
eventually be capable of designing even more intelligent machines, which can
then design even more intelligent machines, and so on)."
"Intelligent behavior depends on the ecological niche: Scaling up AI to
  human-like intelligence in socio-cultural environments","This paper outlines a perspective on the future of AI, discussing directions
for machines models of human-like intelligence. We explain how developmental
and evolutionary theories of human cognition should further inform artificial
intelligence. We emphasize the role of ecological niches in sculpting
intelligent behavior, and in particular that human intelligence was
fundamentally shaped to adapt to a constantly changing socio-cultural
environment. We argue that a major limit of current work in AI is that it is
missing this perspective, both theoretically and experimentally. Finally, we
discuss the promising approach of developmental artificial intelligence,
modeling infant development through multi-scale interaction between
intrinsically motivated learning, embodiment and a fastly changing
socio-cultural environment. This paper takes the form of an interview of
Pierre-Yves Oudeyer by Mandred Eppe, organized within the context of a KI -
K{\""{u}}nstliche Intelligenz special issue in developmental robotics."
Artificial Intelligence for Interstellar Travel,"The large distances involved in interstellar travel require a high degree of
spacecraft autonomy, realized by artificial intelligence. The breadth of tasks
artificial intelligence could perform on such spacecraft involves maintenance,
data collection, designing and constructing an infrastructure using in-situ
resources. Despite its importance, existing publications on artificial
intelligence and interstellar travel are limited to cursory descriptions where
little detail is given about the nature of the artificial intelligence. This
article explores the role of artificial intelligence for interstellar travel by
compiling use cases, exploring capabilities, and proposing typologies, system
and mission architectures. Estimations for the required intelligence level for
specific types of interstellar probes are given, along with potential system
and mission architectures, covering those proposed in the literature but also
presenting novel ones. Finally, a generic design for interstellar probes with
an AI payload is proposed. Given current levels of increase in computational
power, a spacecraft with a similar computational power as the human brain would
have a mass from dozens to hundreds of tons in a 2050-2060 timeframe. Given
that the advent of the first interstellar missions and artificial general
intelligence are estimated to be by the mid-21st century, a more in-depth
exploration of the relationship between the two should be attempted, focusing
on neglected areas such as protecting the artificial intelligence payload from
radiation in interstellar space and the role of artificial intelligence in
self-replication."
"Measuring Intelligence and Growth Rate: Variations on Hibbard's
  Intelligence Measure","In 2011, Hibbard suggested an intelligence measure for agents who compete in
an adversarial sequence prediction game. We argue that Hibbard's idea should
actually be considered as two separate ideas: first, that the intelligence of
such agents can be measured based on the growth rates of the runtimes of the
competitors that they defeat; and second, one specific (somewhat arbitrary)
method for measuring said growth rates. Whereas Hibbard's intelligence measure
is based on the latter growth-rate-measuring method, we survey other methods
for measuring function growth rates, and exhibit the resulting Hibbard-like
intelligence measures and taxonomies. Of particular interest, we obtain
intelligence taxonomies based on Big-O and Big-Theta notation systems, which
taxonomies are novel in that they challenge conventional notions of what an
intelligence measure should look like. We discuss how intelligence measurement
of sequence predictors can indirectly serve as intelligence measurement for
agents with Artificial General Intelligence (AGIs)."
Cognitive Architecture for Co-Evolutionary Hybrid Intelligence,"This paper questions the feasibility of a strong (general) data-centric
artificial intelligence (AI). The disadvantages of this type of intelligence
are discussed. As an alternative, the concept of co-evolutionary hybrid
intelligence is proposed. It is based on the cognitive interoperability of man
and machine. An analysis of existing approaches to the construction of
cognitive architectures is given. An architecture seamlessly incorporates a
human into the loop of intelligent problem solving is considered. The article
is organized as follows. The first part contains a critique of data-centric
intelligent systems. The reasons why it is impossible to create a strong
artificial intelligence based on this type of intelligence are indicated. The
second part briefly presents the concept of co-evolutionary hybrid intelligence
and shows its advantages. The third part gives an overview and analysis of
existing cognitive architectures. It is concluded that many do not consider
humans part of the intelligent data processing process. The next part discusses
the cognitive architecture for co-evolutionary hybrid intelligence, providing
integration with humans. It finishes with general conclusions about the
feasibility of developing intelligent systems with humans in the
problem-solving loop."
"Psychology of Artificial Intelligence: Epistemological Markers of the
  Cognitive Analysis of Neural Networks","What is the ""nature"" of the cognitive processes and contents of an artificial
neural network? In other words, how does an artificial intelligence
fundamentally ""think,"" and in what form does its knowledge reside? The
psychology of artificial intelligence, as predicted by Asimov (1950), aims to
study this AI probing and explainability-sensitive matter. This study requires
a neuronal level of cognitive granularity, so as not to be limited solely to
the secondary macro-cognitive results (such as cognitive and cultural biases)
of synthetic neural cognition. A prerequisite for examining the latter is to
clarify some epistemological milestones regarding the cognitive status we can
attribute to its phenomenology."
A Definition of Artificial Intelligence,"In this paper we offer a formal definition of Artificial Intelligence and
this directly gives us an algorithm for construction of this object. Really,
this algorithm is useless due to the combinatory explosion.
  The main innovation in our definition is that it does not include the
knowledge as a part of the intelligence. So according to our definition a newly
born baby also is an Intellect. Here we differs with Turing's definition which
suggests that an Intellect is a person with knowledge gained through the years."
Guidelines for Artificial Intelligence Containment,"With almost daily improvements in capabilities of artificial intelligence it
is more important than ever to develop safety software for use by the AI
research community. Building on our previous work on AI Containment Problem we
propose a number of guidelines which should help AI safety researchers to
develop reliable sandboxing software for intelligent programs of all levels.
Such safety container software will make it possible to study and analyze
intelligent artificial agent while maintaining certain level of safety against
information leakage, social engineering attacks and cyberattacks from within
the container."
Physics Enhanced Artificial Intelligence,"We propose that intelligently combining models from the domains of Artificial
Intelligence or Machine Learning with Physical and Expert models will yield a
more ""trustworthy"" model than any one model from a single domain, given a
complex and narrow enough problem. Based on mean-variance portfolio theory and
bias-variance trade-off analysis, we prove combining models from various
domains produces a model that has lower risk, increasing user trust. We call
such combined models - physics enhanced artificial intelligence (PEAI), and
suggest use cases for PEAI."
GeoAI in Social Science,"GeoAI, or geospatial artificial intelligence, is an exciting new area that
leverages artificial intelligence (AI), geospatial big data, and massive
computing power to solve problems with high automation and intelligence. This
paper reviews the progress of AI in social science research, highlighting
important advancements in using GeoAI to fill critical data and knowledge gaps.
It also discusses the importance of breaking down data silos, accelerating
convergence among GeoAI research methods, as well as moving GeoAI beyond
geospatial benefits."
LightHouse: A Survey of AGI Hallucination,"With the development of artificial intelligence, large-scale models have
become increasingly intelligent. However, numerous studies indicate that
hallucinations within these large models are a bottleneck hindering the
development of AI research. In the pursuit of achieving strong artificial
intelligence, a significant volume of research effort is being invested in the
AGI (Artificial General Intelligence) hallucination research. Previous
explorations have been conducted in researching hallucinations within LLMs
(Large Language Models). As for multimodal AGI, research on hallucinations is
still in an early stage. To further the progress of research in the domain of
hallucinatory phenomena, we present a bird's eye view of hallucinations in AGI,
summarizing the current work on AGI hallucinations and proposing some
directions for future research."
Ethical Considerations in Artificial Intelligence Courses,"The recent surge in interest in ethics in artificial intelligence may leave
many educators wondering how to address moral, ethical, and philosophical
issues in their AI courses. As instructors we want to develop curriculum that
not only prepares students to be artificial intelligence practitioners, but
also to understand the moral, ethical, and philosophical impacts that
artificial intelligence will have on society. In this article we provide
practical case studies and links to resources for use by AI educators. We also
provide concrete suggestions on how to integrate AI ethics into a general
artificial intelligence course and how to teach a stand-alone artificial
intelligence ethics course."
"Explanation in Artificial Intelligence: Insights from the Social
  Sciences","There has been a recent resurgence in the area of explainable artificial
intelligence as researchers and practitioners seek to make their algorithms
more understandable. Much of this research is focused on explicitly explaining
decisions or actions to a human observer, and it should not be controversial to
say that looking at how humans explain to each other can serve as a useful
starting point for explanation in artificial intelligence. However, it is fair
to say that most work in explainable artificial intelligence uses only the
researchers' intuition of what constitutes a `good' explanation. There exists
vast and valuable bodies of research in philosophy, psychology, and cognitive
science of how people define, generate, select, evaluate, and present
explanations, which argues that people employ certain cognitive biases and
social expectations towards the explanation process. This paper argues that the
field of explainable artificial intelligence should build on this existing
research, and reviews relevant papers from philosophy, cognitive
psychology/science, and social psychology, which study these topics. It draws
out some important findings, and discusses ways that these can be infused with
work on explainable artificial intelligence."
Artificial Intelligence and Medicine: A literature review,"In practically every industry today, artificial intelligence is one of the
most effective ways for machines to assist humans. Since its inception, a large
number of researchers throughout the globe have been pioneering the application
of artificial intelligence in medicine. Although artificial intelligence may
seem to be a 21st-century concept, Alan Turing pioneered the first foundation
concept in the 1940s. Artificial intelligence in medicine has a huge variety of
applications that researchers are continually exploring. The tremendous
increase in computer and human resources has hastened progress in the 21st
century, and it will continue to do so for many years to come. This review of
the literature will highlight the emerging field of artificial intelligence in
medicine and its current level of development."
"B-SMART: A Reference Architecture for Artificially Intelligent Autonomic
  Smart Buildings","The pervasive application of artificial intelligence and machine learning
algorithms is transforming many industries and aspects of the human experience.
One very important industry trend is the move to convert existing human
dwellings to smart buildings, and to create new smart buildings. Smart
buildings aim to mitigate climate change by reducing energy consumption and
associated carbon emissions. To accomplish this, they leverage artificial
intelligence, big data, and machine learning algorithms to learn and optimize
system performance. These fields of research are currently very rapidly
evolving and advancing, but there has been very little guidance to help
engineers and architects working on smart buildings apply artificial
intelligence algorithms and technologies in a systematic and effective manner.
In this paper we present B-SMART: the first reference architecture for
autonomic smart buildings. B-SMART facilitates the application of artificial
intelligence techniques and technologies to smart buildings by decoupling
conceptually distinct layers of functionality and organizing them into an
autonomic control loop. We also present a case study illustrating how B-SMART
can be applied to accelerate the introduction of artificial intelligence into
an existing smart building."
"Improving content marketing processes with the approaches by artificial
  intelligence","Content marketing is todays one of the most remarkable approaches in the
context of marketing processes of companies. Value of this kind of marketing
has improved in time, thanks to the latest developments regarding to computer
and communication technologies. Nowadays, especially social media based
platforms have a great importance on enabling companies to design multimedia
oriented, interactive content. But on the other hand, there is still something
more to do for improved content marketing approaches. In this context,
objective of this study is to focus on intelligent content marketing, which can
be done by using artificial intelligence. Artificial Intelligence is todays one
of the most remarkable research fields and it can be used easily as
multidisciplinary. So, this study has aimed to discuss about its potential on
improving content marketing. In detail, the study has enabled readers to
improve their awareness about the intersection point of content marketing and
artificial intelligence. Furthermore, the authors have introduced some example
models of intelligent content marketing, which can be achieved by using current
Web technologies and artificial intelligence techniques."
"Dislocated Accountabilities in the AI Supply Chain: Modularity and
  Developers' Notions of Responsibility","Responsible artificial intelligence guidelines ask engineers to consider how
their systems might harm. However, contemporary artificial intelligence systems
are built by composing many preexisting software modules that pass through many
hands before becoming a finished product or service. How does this shape
responsible artificial intelligence practice? In interviews with 27 artificial
intelligence engineers across industry, open source, and academia, our
participants often did not see the questions posed in responsible artificial
intelligence guidelines to be within their agency, capability, or
responsibility to address. We use Suchman's ""located accountability"" to show
how responsible artificial intelligence labor is currently organized and to
explore how it could be done differently. We identify cross-cutting social
logics, like modularizability, scale, reputation, and customer orientation,
that organize which responsible artificial intelligence actions do take place
and which are relegated to low status staff or believed to be the work of the
next or previous person in the imagined ""supply chain."" We argue that current
responsible artificial intelligence interventions, like ethics checklists and
guidelines that assume panoptical knowledge and control over systems, could be
improved by taking a located accountability approach, recognizing where
relations and obligations might intertwine inside and outside of this supply
chain."
Latest Trends in Artificial Intelligence Technology: A Scoping Review,"Artificial intelligence is more ubiquitous in multiple domains. Smartphones,
social media platforms, search engines, and autonomous vehicles are just a few
examples of applications that utilize artificial intelligence technologies to
enhance their performance. This study carries out a scoping review of the
current state-of-the-art artificial intelligence technologies following the
PRISMA framework. The goal was to find the most advanced technologies used in
different domains of artificial intelligence technology research. Three
recognized journals were used from artificial intelligence and machine learning
domain: Journal of Artificial Intelligence Research, Journal of Machine
Learning Research, and Machine Learning, and articles published in 2022 were
observed. Certain qualifications were laid for the technological solutions: the
technology must be tested against comparable solutions, commonly approved or
otherwise well justified datasets must be used while applying, and results must
show improvements against comparable solutions. One of the most important parts
of the technology development appeared to be how to process and exploit the
data gathered from multiple sources. The data can be highly unstructured and
the technological solution should be able to utilize the data with minimum
manual work from humans. The results of this review indicate that creating
labeled datasets is very laborious, and solutions exploiting unsupervised or
semi-supervised learning technologies are more and more researched. The
learning algorithms should be able to be updated efficiently, and predictions
should be interpretable. Using artificial intelligence technologies in
real-world applications, safety and explainable predictions are mandatory to
consider before mass adoption can occur."
A Framework for Searching for General Artificial Intelligence,"There is a significant lack of unified approaches to building generally
intelligent machines. The majority of current artificial intelligence research
operates within a very narrow field of focus, frequently without considering
the importance of the 'big picture'. In this document, we seek to describe and
unify principles that guide the basis of our development of general artificial
intelligence. These principles revolve around the idea that intelligence is a
tool for searching for general solutions to problems. We define intelligence as
the ability to acquire skills that narrow this search, diversify it and help
steer it to more promising areas. We also provide suggestions for studying,
measuring, and testing the various skills and abilities that a human-level
intelligent machine needs to acquire. The document aims to be both
implementation agnostic, and to provide an analytic, systematic, and scalable
way to generate hypotheses that we believe are needed to meet the necessary
conditions in the search for general artificial intelligence. We believe that
such a framework is an important stepping stone for bringing together
definitions, highlighting open problems, connecting researchers willing to
collaborate, and for unifying the arguably most significant search of this
century."
Brain Intelligence: Go Beyond Artificial Intelligence,"Artificial intelligence (AI) is an important technology that supports daily
social life and economic activities. It contributes greatly to the sustainable
growth of Japan's economy and solves various social problems. In recent years,
AI has attracted attention as a key for growth in developed countries such as
Europe and the United States and developing countries such as China and India.
The attention has been focused mainly on developing new artificial intelligence
information communication technology (ICT) and robot technology (RT). Although
recently developed AI technology certainly excels in extracting certain
patterns, there are many limitations. Most ICT models are overly dependent on
big data, lack a self-idea function, and are complicated. In this paper, rather
than merely developing next-generation artificial intelligence technology, we
aim to develop a new concept of general-purpose intelligence cognition
technology called Beyond AI. Specifically, we plan to develop an intelligent
learning model called Brain Intelligence (BI) that generates new ideas about
events without having experienced them by using artificial life with an imagine
function. We will also conduct demonstrations of the developed BI intelligence
learning model on automatic driving, precision medical care, and industrial
robots."
Intelligence Quotient and Intelligence Grade of Artificial Intelligence,"Although artificial intelligence is currently one of the most interesting
areas in scientific research, the potential threats posed by emerging AI
systems remain a source of persistent controversy. To address the issue of AI
threat, this study proposes a standard intelligence model that unifies AI and
human characteristics in terms of four aspects of knowledge, i.e., input,
output, mastery, and creation. Using this model, we observe three challenges,
namely, expanding of the von Neumann architecture; testing and ranking the
intelligence quotient of naturally and artificially intelligent systems,
including humans, Google, Bing, Baidu, and Siri; and finally, the dividing of
artificially intelligent systems into seven grades from robots to Google Brain.
Based on this, we conclude that AlphaGo belongs to the third grade."
"Evaluation of Multidisciplinary Effects of Artificial Intelligence with
  Optimization Perspective","Artificial Intelligence has an important place in the scientific community as
a result of its successful outputs in terms of different fields. In time, the
field of Artificial Intelligence has been divided into many sub-fields because
of increasing number of different solution approaches, methods, and techniques.
Machine Learning has the most remarkable role with its functions to learn from
samples from the environment. On the other hand, intelligent optimization done
by inspiring from nature and swarms had its own unique scientific literature,
with effective solutions provided for optimization problems from different
fields. Because intelligent optimization can be applied in different fields
effectively, this study aims to provide a general discussion on
multidisciplinary effects of Artificial Intelligence by considering its
optimization oriented solutions. The study briefly focuses on background of the
intelligent optimization briefly and then gives application examples of
intelligent optimization from a multidisciplinary perspective."
"Ancilia: Scalable Intelligent Video Surveillance for the Artificial
  Intelligence of Things","With the advancement of vision-based artificial intelligence, the
proliferation of the Internet of Things connected cameras, and the increasing
societal need for rapid and equitable security, the demand for accurate
real-time intelligent surveillance has never been higher. This article presents
Ancilia, an end-to-end scalable, intelligent video surveillance system for the
Artificial Intelligence of Things. Ancilia brings state-of-the-art artificial
intelligence to real-world surveillance applications while respecting ethical
concerns and performing high-level cognitive tasks in real-time. Ancilia aims
to revolutionize the surveillance landscape, to bring more effective,
intelligent, and equitable security to the field, resulting in safer and more
secure communities without requiring people to compromise their right to
privacy."
Building Safer AGI by introducing Artificial Stupidity,"Artificial Intelligence (AI) achieved super-human performance in a broad
variety of domains. We say that an AI is made Artificially Stupid on a task
when some limitations are deliberately introduced to match a human's ability to
do the task. An Artificial General Intelligence (AGI) can be made safer by
limiting its computing power and memory, or by introducing Artificial Stupidity
on certain tasks. We survey human intellectual limits and give recommendations
for which limits to implement in order to build a safe AGI."
Swarm Intelligence,"Biologically inspired computing is an area of computer science which uses the
advantageous properties of biological systems. It is the amalgamation of
computational intelligence and collective intelligence. Biologically inspired
mechanisms have already proved successful in achieving major advances in a wide
range of problems in computing and communication systems. The consortium of
bio-inspired computing are artificial neural networks, evolutionary algorithms,
swarm intelligence, artificial immune systems, fractal geometry, DNA computing
and quantum computing, etc. This article gives an introduction to swarm
intelligence."
"Artificial Intelligence in Intelligent Tutoring Robots: A Systematic
  Review and Design Guidelines","This study provides a systematic review of the recent advances in designing
the intelligent tutoring robot (ITR), and summarises the status quo of applying
artificial intelligence (AI) techniques. We first analyse the environment of
the ITR and propose a relationship model for describing interactions of ITR
with the students, the social milieu and the curriculum. Then, we transform the
relationship model into the perception-planning-action model for exploring what
AI techniques are suitable to be applied in the ITR. This article provides
insights on promoting human-robot teaching-learning process and AI-assisted
educational techniques, illustrating the design guidelines and future research
perspectives in intelligent tutoring robots."
"Artificial Intelligence Quotient (AIQ): A Novel Framework for Measuring
  Human-AI Collaborative Intelligence","As artificial intelligence becomes increasingly integrated into professional
and personal domains, traditional metrics of human intelligence require
reconceptualization. This paper introduces the Artificial Intelligence Quotient
(AIQ), a novel measurement framework designed to assess an individual's
capacity to effectively collaborate with and leverage AI systems, particularly
Large Language Models (LLMs). Building upon established cognitive assessment
methodologies and contemporary AI interaction research, we present a
comprehensive framework for quantifying human-AI collaborative intelligence.
This work addresses the growing need for standardized evaluation of
AI-augmented cognitive capabilities in educational and professional contexts."
"Future Trends for Human-AI Collaboration: A Comprehensive Taxonomy of
  AI/AGI Using Multiple Intelligences and Learning Styles","This article discusses some trends and concepts in developing new generation
of future Artificial General Intelligence (AGI) systems which relate to complex
facets and different types of human intelligence, especially social, emotional,
attentional and ethical intelligence. We describe various aspects of multiple
human intelligences and learning styles, which may impact on a variety of AI
problem domains. Using the concept of 'multiple intelligences' rather than a
single type of intelligence, we categorize and provide working definitions of
various AGI depending on their cognitive skills or capacities. Future AI
systems will be able not only to communicate with human users and each other,
but also to efficiently exchange knowledge and wisdom with abilities of
cooperation, collaboration and even co-creating something new and valuable and
have meta-learning capacities. Multi-agent systems such as these can be used to
solve problems that would be difficult to solve by any individual intelligent
agent.
  Key words: Artificial General Intelligence (AGI), multiple intelligences,
learning styles, physical intelligence, emotional intelligence, social
intelligence, attentional intelligence, moral-ethical intelligence, responsible
decision making, creative-innovative intelligence, cognitive functions,
meta-learning of AI systems."
Hybrid Reasoning and the Future of Iconic Representations,"We give a brief overview of the main characteristics of diagrammatic
reasoning, analyze a case of human reasoning in a mastermind game, and explain
why hybrid representation systems (HRS) are particularly attractive and
promising for Artificial General Intelligence and Computer Science in general."
Diverse Consequences of Algorithmic Probability,"We reminisce and discuss applications of algorithmic probability to a wide
range of problems in artificial intelligence, philosophy and technological
society. We propose that Solomonoff has effectively axiomatized the field of
artificial intelligence, therefore establishing it as a rigorous scientific
discipline. We also relate to our own work in incremental machine learning and
philosophy of complexity."
"Proceedings of the Twenty-Third Conference on Uncertainty in Artificial
  Intelligence (2007)","This is the Proceedings of the Twenty-Third Conference on Uncertainty in
Artificial Intelligence, which was held in Vancouver, British Columbia, July 19
- 22 2007."
A Backwards View for Assessment,"Much artificial intelligence research focuses on the problem of deducing the
validity of unobservable propositions or hypotheses from observable evidence.!
Many of the knowledge representation techniques designed for this problem
encode the relationship between evidence and hypothesis in a directed manner.
Moreover, the direction in which evidence is stored is typically from evidence
to hypothesis."
"Foundations of Probability Theory for AI - The Application of
  Algorithmic Probability to Problems in Artificial Intelligence","This paper covers two topics: first an introduction to Algorithmic Complexity
Theory: how it defines probability, some of its characteristic properties and
past successful applications. Second, we apply it to problems in A.I. - where
it promises to give near optimum search procedures for two very broad classes
of problems."
Multilayered Model of Speech,"Human speech is the most important part of General Artificial Intelligence
and subject of much research. The hypothesis proposed in this article provides
explanation of difficulties that modern science tackles in the field of human
brain simulation. The hypothesis is based on the author's conviction that the
brain of any given person has different ability to process and store
information. Therefore, the approaches that are currently used to create
General Artificial Intelligence have to be altered."
"Proceedings of the Artificial Intelligence for Cyber Security (AICS)
  Workshop 2019","This volume represents the proceedings of the Artificial Intelligence for
Cyber Security (AICS) Workshop 2019, held on January 27, 2019 in Honolulu,
Hawaii."
The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence,"Recent research in artificial intelligence and machine learning has largely
emphasized general-purpose learning and ever-larger training sets and more and
more compute. In contrast, I propose a hybrid, knowledge-driven,
reasoning-based approach, centered around cognitive models, that could provide
the substrate for a richer, more robust AI than is currently possible."
Artificial Intelligence in Minimally Invasive Interventional Treatment,"Minimally invasive image guided treatment procedures often employ advanced
image processing algorithms. The recent developments of artificial intelligence
algorithms harbor potential to further enhance this domain. In this article we
explore several application areas within the minimally invasive treatment space
and discuss the deployment of artificial intelligence within these areas."
2021 Drexel Society of Artificial Intelligence Research Conference,"The 2021 Drexel Society of Artificial Intelligence Research Conference
highlights papers focused on a broad set of papers in machine learning. This
was our organizations' first annual conference. It was conducted virtually via
Zoom. The highlights are currently posted on YouTube."
"Understanding the Impact of Artificial Intelligence in Academic Writing:
  Metadata to the Rescue","This column advocates for including artificial intelligence (AI)-specific
metadata on those academic papers that are written with the help of AI in an
attempt to analyze the use of such tools for disseminating research."
"On the influence of intelligence in (social) intelligence testing
  environments","This paper analyses the influence of including agents of different degrees of
intelligence in a multiagent system. The goal is to better understand how we
can develop intelligence tests that can evaluate social intelligence. We
analyse several reinforcement algorithms in several contexts of cooperation and
competition. Our experimental setting is inspired by the recently developed
Darwin-Wallace distribution."
The Embeddings World and Artificial General Intelligence,"From early days, a key and controversial question inside the artificial
intelligence community was whether Artificial General Intelligence (AGI) is
achievable. AGI is the ability of machines and computer programs to achieve
human-level intelligence and do all tasks that a human being can. While there
exist a number of systems in the literature claiming they realize AGI, several
other researchers argue that it is impossible to achieve it. In this paper, we
take a different view to the problem. First, we discuss that in order to
realize AGI, along with building intelligent machines and programs, an
intelligent world should also be constructed which is on the one hand, an
accurate approximation of our world and on the other hand, a significant part
of reasoning of intelligent machines is already embedded in this world. Then we
discuss that AGI is not a product or algorithm, rather it is a continuous
process which will become more and more mature over time (like human
civilization and wisdom). Then, we argue that pre-trained embeddings play a key
role in building this intelligent world and as a result, realizing AGI. We
discuss how pre-trained embeddings facilitate achieving several characteristics
of human-level intelligence, such as embodiment, common sense knowledge,
unconscious knowledge and continuality of learning, by machines."
Artificial and Biological Intelligence,"This article considers evidence from physical and biological sciences to show
machines are deficient compared to biological systems at incorporating
intelligence. Machines fall short on two counts: firstly, unlike brains,
machines do not self-organize in a recursive manner; secondly, machines are
based on classical logic, whereas Nature's intelligence may depend on quantum
mechanics."
Consider ethical and social challenges in smart grid research,"Artificial Intelligence and Machine Learning are increasingly seen as key
technologies for building more decentralised and resilient energy grids, but
researchers must consider the ethical and social implications of their use"
Planning with Brain-inspired AI,"This article surveys engineering and neuroscientific models of planning as a
cognitive function, which is regarded as a typical function of fluid
intelligence in the discussion of general intelligence. It aims to present
existing planning models as references for realizing the planning function in
brain-inspired AI or artificial general intelligence (AGI). It also proposes
themes for the research and development of brain-inspired AI from the viewpoint
of tasks and architecture."
"Artificial Intelligence in the Service of Entrepreneurial Finance:
  Knowledge Structure and the Foundational Algorithmic Paradigm","While the application of Artificial Intelligence in Finance has a long
tradition, its potential in Entrepreneurship has been intensively explored only
recently. In this context, Entrepreneurial Finance is a particularly fertile
ground for future Artificial Intelligence proliferation. To support the latter,
the study provides a bibliometric review of Artificial Intelligence
applications in (1) entrepreneurial finance literature, and (2) corporate
finance literature with implications for Entrepreneurship. Rigorous search and
screening procedures of the scientific database Web of Science Core Collection
resulted in the identification of 1890 relevant journal articles subjected to
analysis. The bibliometric analysis gives a rich insight into the knowledge
field's conceptual, intellectual, and social structure, indicating nascent and
underdeveloped research directions. As far as we were able to identify, this is
the first study to map and bibliometrically analyze the academic field
concerning the relationship between Artificial Intelligence, Entrepreneurship,
and Finance, and the first review that deals with Artificial Intelligence
methods in Entrepreneurship. According to the results, Artificial Neural
Network, Deep Neural Network and Support Vector Machine are highly represented
in almost all identified topic niches. At the same time, applying Topic
Modeling, Fuzzy Neural Network and Growing Hierarchical Self-organizing Map is
quite rare. As an element of the research, and before final remarks, the
article deals as well with a discussion of certain gaps in the relationship
between Computer Science and Economics. These gaps do represent problems in the
application of Artificial Intelligence in Economic Science. As a way to at
least in part remedy this situation, the foundational paradigm and the bespoke
demonstration of the Monte Carlo randomized algorithm are presented."
Students' interest in knowledge acquisition in Artificial Intelligence,"Some students' expectations and points of view related to the Artificial
Intelligence course are explored and analyzed in this study. We anonymous
collected answers from 58 undergraduate students out of 200 enrolled in the
Computer Science specialization. The answers were analysed and interpreted
using thematic analysis to find out their interests and attractive and
unattractive aspects related to the Artificial Intelligence study topic. We
concluded that students are interested in Artificial Intelligence due to its
trendiness, applicability, their passion and interest in the subject, the
potential for future growth, and high salaries. However, the students'
expectations were mainly related to achieving medium knowledge in the
Artificial Intelligence field, and men seem to be more interested in acquiring
high-level skills than women. The most common part that wasn't enjoyed by the
students was the mathematical aspect used in Artificial Intelligence. Some of
them (a small group) were also aware of the Artificial Intelligence potential
which could be used in an unethical manner for negative purposes. Our study
also provides a short comparison to the Databases course, in which students
were not that passionate or interested in achieving medium knowledge, their
interest was related to DB usage and basic information."
"A Classification of Artificial Intelligence Systems for Mathematics
  Education","This chapter provides an overview of the different Artificial Intelligence
(AI) systems that are being used in contemporary digital tools for Mathematics
Education (ME). It is aimed at researchers in AI and Machine Learning (ML), for
whom we shed some light on the specific technologies that are being used in
educational applications; and at researchers in ME, for whom we clarify: i)
what the possibilities of the current AI technologies are, ii) what is still
out of reach and iii) what is to be expected in the near future. We start our
analysis by establishing a high-level taxonomy of AI tools that are found as
components in digital ME applications. Then, we describe in detail how these AI
tools, and in particular ML, are being used in two key applications,
specifically AI-based calculators and intelligent tutoring systems. We finish
the chapter with a discussion about student modeling systems and their
relationship to artificial general intelligence."
Human in the AI loop via xAI and Active Learning for Visual Inspection,"Industrial revolutions have historically disrupted manufacturing by
introducing automation into production. Increasing automation reshapes the role
of the human worker. Advances in robotics and artificial intelligence open new
frontiers of human-machine collaboration. Such collaboration can be realized
considering two sub-fields of artificial intelligence: active learning and
explainable artificial intelligence. Active learning aims to devise strategies
that help obtain data that allows machine learning algorithms to learn better.
On the other hand, explainable artificial intelligence aims to make the machine
learning models intelligible to the human person. The present work first
describes Industry 5.0, human-machine collaboration, and state-of-the-art
regarding quality inspection, emphasizing visual inspection. Then it outlines
how human-machine collaboration could be realized and enhanced in visual
inspection. Finally, some of the results obtained in the EU H2020 STAR project
regarding visual inspection are shared, considering artificial intelligence,
human digital twins, and cybersecurity."
Artificial Intelligence: A Child's Play,"We discuss the objectives of any endeavor in creating artificial
intelligence, AI, and provide a possible alternative. Intelligence might be an
unintended consequence of curiosity left to roam free, best exemplified by a
frolicking infant. This suggests that our attempts at AI could have been
misguided. What we actually need to strive for can be termed artificial
curiosity, AC, and intelligence happens as a consequence of those efforts. For
this unintentional yet welcome aftereffect to set in a foundational list of
guiding principles needs to be present. We start with the intuition for this
line of reasoning and formalize it with a series of definitions, assumptions,
ingredients, models and iterative improvements that will be necessary to make
the incubation of intelligence a reality. Our discussion provides conceptual
modifications to the Turing Test and to Searle's Chinese room argument. We
discuss the future implications for society as AI becomes an integral part of
life.
  We provide a road-map for creating intelligence with the technical parts
relegated to the appendix so that the article is accessible to a wide audience.
The central techniques in our formal approach to creating intelligence draw
upon tools and concepts widely used in physics, cognitive science, psychology,
evolutionary biology, statistics, linguistics, communication systems, pattern
recognition, marketing, economics, finance, information science and
computational theory highlighting that solutions for creating artificial
intelligence have to transcend the artificial barriers between various fields
and be highly multi-disciplinary."
"Man and Machine: Questions of Risk, Trust and Accountability in Today's
  AI Technology","Artificial Intelligence began as a field probing some of the most fundamental
questions of science - the nature of intelligence and the design of intelligent
artifacts. But it has grown into a discipline that is deeply entwined with
commerce and society. Today's AI technology, such as expert systems and
intelligent assistants, pose some difficult questions of risk, trust and
accountability. In this paper, we present these concerns, examining them in the
context of historical developments that have shaped the nature and direction of
AI research. We also suggest the exploration and further development of two
paradigms, human intelligence-machine cooperation, and a sociological view of
intelligence, which might help address some of these concerns."
"Controlled Language and Baby Turing Test for General Conversational
  Intelligence","General conversational intelligence appears to be an important part of
artificial general intelligence. Respectively, it requires accessible measures
of the intelligence quality and controllable ways of its achievement, ideally -
having the linguistic and semantic models represented in a reasonable way. Our
work is suggesting to use Baby Turing Test approach to extend the classic
Turing Test for conversational intelligence and controlled language based on
semantic graph representation extensible for arbitrary subject domain. We
describe how the two can be used together to build a general-purpose
conversational system such as an intelligent assistant for online media and
social network data processing."
Frontiers in Collective Intelligence: A Workshop Report,"In August of 2021, the Santa Fe Institute hosted a workshop on collective
intelligence as part of its Foundations of Intelligence project. This project
seeks to advance the field of artificial intelligence by promoting
interdisciplinary research on the nature of intelligence. The workshop brought
together computer scientists, biologists, philosophers, social scientists, and
others to share their insights about how intelligence can emerge from
interactions among multiple agents--whether those agents be machines, animals,
or human beings. In this report, we summarize each of the talks and the
subsequent discussions. We also draw out a number of key themes and identify
important frontiers for future research."
Blockchain Intelligence: When Blockchain Meets Artificial Intelligence,"Blockchain is gaining extensive attention due to its provision of secure and
decentralized resource sharing manner. However, the incumbent blockchain
systems also suffer from a number of challenges in operational maintenance,
quality assurance of smart contracts and malicious behaviour detection of
blockchain data. The recent advances in artificial intelligence bring the
opportunities in overcoming the above challenges. The integration of blockchain
with artificial intelligence can be beneficial to enhance current blockchain
systems. This article presents an introduction of the convergence of blockchain
and artificial intelligence (namely blockchain intelligence). This article also
gives a case study to further demonstrate the feasibility of blockchain
intelligence and point out the future directions."
"Synergetic Learning Systems: Concept, Architecture, and Algorithms","Drawing on the idea that brain development is a Darwinian process of
``evolution + selection'' and the idea that the current state is a local
equilibrium state of many bodies with self-organization and evolution processes
driven by the temperature and gravity in our universe, in this work, we
describe an artificial intelligence system called the ``Synergetic Learning
Systems''. The system is composed of two or more subsystems (models, agents or
virtual bodies), and it is an open complex giant system. Inspired by natural
intelligence, the system achieves intelligent information processing and
decision-making in a given environment through cooperative/competitive
synergetic learning. The intelligence evolved by the natural law of ``it is not
the strongest of the species that survives, but the one most responsive to
change,'' while an artificial intelligence system should adopt the law of
``human selection'' in the evolution process. Therefore, we expect that the
proposed system architecture can also be adapted in human-machine synergy or
multi-agent synergetic systems. It is also expected that under our design
criteria, the proposed system will eventually achieve artificial general
intelligence through long term coevolution."
"Bio-inspired AI: Integrating Biological Complexity into Artificial
  Intelligence","The pursuit of creating artificial intelligence (AI) mirrors our longstanding
fascination with understanding our own intelligence. From the myths of Talos to
Aristotelian logic and Heron's inventions, we have sought to replicate the
marvels of the mind. While recent advances in AI hold promise, singular
approaches often fall short in capturing the essence of intelligence. This
paper explores how fundamental principles from biological
computation--particularly context-dependent, hierarchical information
processing, trial-and-error heuristics, and multi-scale organization--can guide
the design of truly intelligent systems. By examining the nuanced mechanisms of
biological intelligence, such as top-down causality and adaptive interaction
with the environment, we aim to illuminate potential limitations in artificial
constructs. Our goal is to provide a framework inspired by biological systems
for designing more adaptable and robust artificial intelligent systems."
Measurements of collective machine intelligence,"Independent from the still ongoing research in measuring individual
intelligence, we anticipate and provide a framework for measuring collective
intelligence. Collective intelligence refers to the idea that several
individuals can collaborate in order to achieve high levels of intelligence. We
present thus some ideas on how the intelligence of a group can be measured and
simulate such tests. We will however focus here on groups of artificial
intelligence agents (i.e., machines). We will explore how a group of agents is
able to choose the appropriate problem and to specialize for a variety of
tasks. This is a feature which is an important contributor to the increase of
intelligence in a group (apart from the addition of more agents and the
improvement due to common decision making). Our results reveal some interesting
results about how (collective) intelligence can be modeled, about how
collective intelligence tests can be designed and about the underlying dynamics
of collective intelligence. As it will be useful for our simulations, we
provide also some improvements of the threshold allocation model originally
used in the area of swarm intelligence but further generalized here."
The Role of Artificial Intelligence Technologies in Crisis Response,"Crisis response poses many of the most difficult information technology in
crisis management. It requires information and communication-intensive efforts,
utilized for reducing uncertainty, calculating and comparing costs and
benefits, and managing resources in a fashion beyond those regularly available
to handle routine problems. In this paper, we explore the benefits of
artificial intelligence technologies in crisis response. This paper discusses
the role of artificial intelligence technologies; namely, robotics, ontology
and semantic web, and multi-agent systems in crisis response."
A Systematic Approach to Artificial Agents,"Agents and agent systems are becoming more and more important in the
development of a variety of fields such as ubiquitous computing, ambient
intelligence, autonomous computing, intelligent systems and intelligent
robotics. The need for improvement of our basic knowledge on agents is very
essential. We take a systematic approach and present extended classification of
artificial agents which can be useful for understanding of what artificial
agents are and what they can be in the future. The aim of this classification
is to give us insights in what kind of agents can be created and what type of
problems demand a specific kind of agents for their solution."
Comparison between the two definitions of AI,"Two different definitions of the Artificial Intelligence concept have been
proposed in papers [1] and [2]. The first definition is informal. It says that
any program that is cleverer than a human being, is acknowledged as Artificial
Intelligence. The second definition is formal because it avoids reference to
the concept of human being. The readers of papers [1] and [2] might be left
with the impression that both definitions are equivalent and the definition in
[2] is simply a formal version of that in [1]. This paper will compare both
definitions of Artificial Intelligence and, hopefully, will bring a better
understanding of the concept."
Probability Judgement in Artificial Intelligence,"This paper is concerned with two theories of probability judgment: the
Bayesian theory and the theory of belief functions. It illustrates these
theories with some simple examples and discusses some of the issues that arise
when we try to implement them in expert systems. The Bayesian theory is well
known; its main ideas go back to the work of Thomas Bayes (1702-1761). The
theory of belief functions, often called the Dempster-Shafer theory in the
artificial intelligence community, is less well known, but it has even older
antecedents; belief-function arguments appear in the work of George Hooper
(16401723) and James Bernoulli (1654-1705). For elementary expositions of the
theory of belief functions, see Shafer (1976, 1985)."
AI Methods in Algorithmic Composition: A Comprehensive Survey,"Algorithmic composition is the partial or total automation of the process of
music composition by using computers. Since the 1950s, different computational
techniques related to Artificial Intelligence have been used for algorithmic
composition, including grammatical representations, probabilistic methods,
neural networks, symbolic rule-based systems, constraint programming and
evolutionary algorithms. This survey aims to be a comprehensive account of
research on algorithmic composition, presenting a thorough view of the field
for researchers in Artificial Intelligence."
ICON Challenge on Algorithm Selection,We present the results of the ICON Challenge on Algorithm Selection.
A Python Engine for Teaching Artificial Intelligence in Games,"Computer games play an important role in our society and motivate people to
learn computer science. Since artificial intelligence is integral to most
games, they can also be used to teach artificial intelligence. We introduce the
Game AI Game Engine (GAIGE), a Python game engine specifically designed to
teach about how AI is used in computer games. A progression of seven
assignments builds toward a complete, working Multi-User Battle Arena (MOBA)
game. We describe the engine, the assignments, and our experiences using it in
a class on Game Artificial Intelligence."
Rational Choice and Artificial Intelligence,"The theory of rational choice assumes that when people make decisions they do
so in order to maximize their utility. In order to achieve this goal they ought
to use all the information available and consider all the choices available to
choose an optimal choice. This paper investigates what happens when decisions
are made by artificially intelligent machines in the market rather than human
beings. Firstly, the expectations of the future are more consistent if they are
made by an artificially intelligent machine and the decisions are more rational
and thus marketplace becomes more rational."
A Reputation System for Artificial Societies,"One approach to achieving artificial general intelligence (AGI) is through
the emergence of complex structures and dynamic properties arising from
decentralized networks of interacting artificial intelligence (AI) agents.
Understanding the principles of consensus in societies and finding ways to make
consensus more reliable becomes critically important as connectivity and
interaction speed increase in modern distributed systems of hybrid collective
intelligences, which include both humans and computer systems. We propose a new
form of reputation-based consensus with greater resistance to reputation gaming
than current systems have. We discuss options for its implementation, and
provide initial practical results."
Bias Amplification in Artificial Intelligence Systems,"As Artificial Intelligence (AI) technologies proliferate, concern has
centered around the long-term dangers of job loss or threats of machines
causing harm to humans. All of this concern, however, detracts from the more
pertinent and already existing threats posed by AI today: its ability to
amplify bias found in training datasets, and swiftly impact marginalized
populations at scale. Government and public sector institutions have a
responsibility to citizens to establish a dialogue with technology developers
and release thoughtful policy around data standards to ensure diverse
representation in datasets to prevent bias amplification and ensure that AI
systems are built with inclusion in mind."
"Using Scratch to Teach Undergraduate Students' Skills on Artificial
  Intelligence","This paper presents a educational workshop in Scratch that is proposed for
the active participation of undergraduate students in contexts of Artificial
Intelligence. The main objective of the activity is to demystify the complexity
of Artificial Intelligence and its algorithms. For this purpose, students must
realize simple exercises of clustering and two neural networks, in Scratch. The
detailed methodology to get that is presented in the article."
"Implications of Quantum Computing for Artificial Intelligence alignment
  research","We explain some key features of quantum computing via three heuristics and
apply them to argue that a deep understanding of quantum computing is unlikely
to be helpful to address current bottlenecks in Artificial Intelligence
Alignment. Our argument relies on the claims that Quantum Computing leads to
compute overhang instead of algorithmic overhang, and that the difficulties
associated with the measurement of quantum states do not invalidate any major
assumptions of current Artificial Intelligence Alignment research agendas. We
also discuss tripwiring, adversarial blinding, informed oversight and side
effects as possible exceptions."
Artificial Intelligence Approaches,"Artificial Intelligence (AI) has received tremendous attention from academia,
industry, and the general public in recent years. The integration of geography
and AI, or GeoAI, provides novel approaches for addressing a variety of
problems in the natural environment and our human society. This entry briefly
reviews the recent development of AI with a focus on machine learning and deep
learning approaches. We discuss the integration of AI with geography and
particularly geographic information science, and present a number of GeoAI
applications and possible future directions."
"Imagine All the People: Citizen Science, Artificial Intelligence, and
  Computational Research","Machine learning, artificial intelligence, and deep learning have advanced
significantly over the past decade. Nonetheless, humans possess unique
abilities such as creativity, intuition, context and abstraction, analytic
problem solving, and detecting unusual events. To successfully tackle pressing
scientific and societal challenges, we need the complementary capabilities of
both humans and machines. The Federal Government could accelerate its
priorities on multiple fronts through judicious integration of citizen science
and crowdsourcing with artificial intelligence (AI), Internet of Things (IoT),
and cloud strategies."
"MHealth: An Artificial Intelligence Oriented Mobile Application for
  Personal Healthcare Support","Main objective of this study is to introduce an expert system-based mHealth
application that takes Artificial Intelligence support by considering
previously introduced solutions from the literature and employing possible
requirements for a better solution. Thanks to that research study, a mobile
software system having Artificial Intelligence support and providing dynamic
support against the common health problems in daily life was designed-developed
and it was evaluated via survey and diagnosis-based evaluation tasks.
Evaluation tasks indicated positive outcomes for the mHealth system."
Symbol Emergence and The Solutions to Any Task,"The following defines intent, an arbitrary task and its solutions, and then
argues that an agent which always constructs what is called an Intensional
Solution would qualify as artificial general intelligence. We then explain how
natural language may emerge and be acquired by such an agent, conferring the
ability to model the intent of other individuals labouring under similar
compulsions, because an abstract symbol system and the solution to a task are
one and the same."
Certifiable Artificial Intelligence Through Data Fusion,"This paper reviews and proposes concerns in adopting, fielding, and
maintaining artificial intelligence (AI) systems. While the AI community has
made rapid progress, there are challenges in certifying AI systems. Using
procedures from design and operational test and evaluation, there are
opportunities towards determining performance bounds to manage expectations of
intended use. A notional use case is presented with image data fusion to
support AI object recognition certifiability considering precision versus
distance."
"Proceedings of the Robust Artificial Intelligence System Assurance
  (RAISA) Workshop 2022","The Robust Artificial Intelligence System Assurance (RAISA) workshop will
focus on research, development and application of robust artificial
intelligence (AI) and machine learning (ML) systems. Rather than studying
robustness with respect to particular ML algorithms, our approach will be to
explore robustness assurance at the system architecture level, during both
development and deployment, and within the human-machine teaming context. While
the research community is converging on robust solutions for individual AI
models in specific scenarios, the problem of evaluating and assuring the
robustness of an AI system across its entire life cycle is much more complex.
Moreover, the operational context in which AI systems are deployed necessitates
consideration of robustness and its relation to principles of fairness,
privacy, and explainability."
CLAS12 Track Reconstruction with Artificial Intelligence,"In this article we describe the implementation of Artificial Intelligence
models in track reconstruction software for the CLAS12 detector at Jefferson
Lab. The Artificial Intelligence based approach resulted in improved track
reconstruction efficiency in high luminosity experimental conditions. The track
reconstruction efficiency increased by $10-12\%$ for single particle, and
statistics in multi-particle physics reactions increased by $15\%-35\%$
depending on the number of particles in the reaction. The implementation of
artificial intelligence in the workflow also resulted in a speedup of the
tracking by $35\%$."
State of the Art in Artificial Intelligence applied to the Legal Domain,"While Artificial Intelligence applied to the legal domain is a topic with
origins in the last century, recent advances in Artificial Intelligence are
posed to revolutionize it. This work presents an overview and contextualizes
the main advances on the field of Natural Language Processing and how these
advances have been used to further the state of the art in legal text analysis."
Witgenstein's influence on artificial intelligence,"We examine how much of the contemporary progress in artificial intelligence
(and, specifically, in natural language processing), can be, more or less
directly, traced back to the seminal work and ideas of the Austrian-British
philosopher Ludwig Wittgenstein, with particular focus on his late views.
Discussing Wittgenstein's original theses will give us the chance to survey the
state of artificial intelligence, and comment on both its strengths and
weaknesses. A similar text appeared first in Spanish as a chapter of CENTENARIO
DEL SILENCIO (2021), a book celebrating 100 years since the publication of the
Tractatus."
AI for Agile development: a Meta-Analysis,"This study explores the benefits and challenges of integrating Artificial
Intelligence with Agile software development methodologies, focusing on
improving continuous integration and delivery. A systematic literature review
and longitudinal meta-analysis of the retrieved studies was conducted to
analyse the role of Artificial Intelligence and it's future applications within
Agile software development. The review helped identify critical challenges,
such as the need for specialised socio-technical expertise. While Artificial
Intelligence holds promise for improved software development practices, further
research is needed to better understand its impact on processes and
practitioners, and to address the indirect challenges associated with its
implementation."
Artificial intelligence and the limits of the humanities,"The complexity of cultures in the modern world is now beyond human
comprehension. Cognitive sciences cast doubts on the traditional explanations
based on mental models. The core subjects in humanities may lose their
importance. Humanities have to adapt to the digital age. New, interdisciplinary
branches of humanities emerge. Instant access to information will be replaced
by instant access to knowledge. Understanding the cognitive limitations of
humans and the opportunities opened by the development of artificial
intelligence and interdisciplinary research necessary to address global
challenges is the key to the revitalization of humanities. Artificial
intelligence will radically change humanities, from art to political sciences
and philosophy, making these disciplines attractive to students and enabling
them to go beyond current limitations."
Spontaneous Theory of Mind for Artificial Intelligence,"Existing approaches to Theory of Mind (ToM) in Artificial Intelligence (AI)
overemphasize prompted, or cue-based, ToM, which may limit our collective
ability to develop Artificial Social Intelligence (ASI). Drawing from research
in computer science, cognitive science, and related disciplines, we contrast
prompted ToM with what we call spontaneous ToM -- reasoning about others'
mental states that is grounded in unintentional, possibly uncontrollable
cognitive functions. We argue for a principled approach to studying and
developing AI ToM and suggest that a robust, or general, ASI will respond to
prompts \textit{and} spontaneously engage in social reasoning."
"Distributed Artificial Intelligence as a Means to Achieve
  Self-X-Functions for Increasing Resilience: the First Steps","Using sensors as a means to achieve self-awareness and artificial
intelligence for decision-making, may be a way to make complex systems
self-adaptive, autonomous and resilient. Investigating the combination of
distributed artificial intelligence methods and bio-inspired robotics can
provide results that will be helpful for implementing autonomy of such robots
and other complex systems. In this paper, we describe Distributed Artificial
Intelligence application area, the most common examples of continuum robots and
provide a description of our first steps towards implementing distributed
control."
A real-time Artificial Intelligence system for learning Sign Language,"A primary challenge for the deaf and hearing-impaired community stems from
the communication gap with the hearing society, which can greatly impact their
daily lives and result in social exclusion. To foster inclusivity in society,
our endeavor focuses on developing a cost-effective, resource-efficient, and
open technology based on Artificial Intelligence, designed to assist people in
learning and using Sign Language for communication. The analysis presented in
this research paper intends to enrich the recent academic scientific literature
on Sign Language solutions based on Artificial Intelligence, with a particular
focus on American Sign Language (ASL). This research has yielded promising
preliminary results and serves as a basis for further development."
"Scalable Artificial Intelligence for Science: Perspectives, Methods and
  Exemplars","In a post-ChatGPT world, this paper explores the potential of leveraging
scalable artificial intelligence for scientific discovery. We propose that
scaling up artificial intelligence on high-performance computing platforms is
essential to address such complex problems. This perspective focuses on
scientific use cases like cognitive simulations, large language models for
scientific inquiry, medical image analysis, and physics-informed approaches.
The study outlines the methodologies needed to address such challenges at scale
on supercomputers or the cloud and provides exemplars of such approaches
applied to solve a variety of scientific problems."
The Artificial Intelligence Disclosure (AID) Framework: An Introduction,"As the use of Generative Artificial Intelligence tools have grown in higher
education and research, there have been increasing calls for transparency and
granularity around the use and attribution of the use of these tools. Thus far,
this need has been met via the recommended inclusion of a note, with little to
no guidance on what the note itself should include. This has been identified as
a problem to the use of AI in academic and research contexts. This article
introduces The Artificial Intelligence Disclosure (AID) Framework, a standard,
comprehensive, and detailed framework meant to inform the development and
writing of GenAI disclosure for education and research."
Development of REGAI: Rubric Enabled Generative Artificial Intelligence,"This paper presents and evaluates a new retrieval augmented generation (RAG)
and large language model (LLM)-based artificial intelligence (AI) technique:
rubric enabled generative artificial intelligence (REGAI). REGAI uses rubrics,
which can be created manually or automatically by the system, to enhance the
performance of LLMs for evaluation purposes. REGAI improves on the performance
of both classical LLMs and RAG-based LLM techniques. This paper describes
REGAI, presents data regarding its performance and discusses several possible
application areas for the technology."
What is an intelligent system?,"The concept of intelligent system has emerged in information technology as a
type of system derived from successful applications of artificial intelligence.
The goal of this paper is to give a general description of an intelligent
system, which integrates previous approaches and takes into account recent
advances in artificial intelligence. The paper describes an intelligent system
in a generic way, identifying its main properties and functional components.
The presented description follows a pragmatic approach to be used in an
engineering context as a general framework to analyze and build intelligent
systems. Its generality and its use is illustrated with real-world system
examples and related with artificial intelligence methods."
Simulation of Human and Artificial Emotion (SHArE),"The framework for Simulation of Human and Artificial Emotion (SHArE)
describes the architecture of emotion in terms of parameters transferable
between psychology, neuroscience, and artificial intelligence. These parameters
can be defined as abstract concepts or granularized down to the voltage levels
of individual neurons. This model enables emotional trajectory design for
humans which may lead to novel therapeutic solutions for various mental health
concerns. For artificial intelligence, this work provides a compact notation
which can be applied to neural networks as a means to observe the emotions and
motivations of machines."
Intelligent Autonomous Things on the Battlefield,"Numerous, artificially intelligent, networked things will populate the
battlefield of the future, operating in close collaboration with human
warfighters, and fighting as teams in highly adversarial environments. This
chapter explores the characteristics, capabilities and intelli-gence required
of such a network of intelligent things and humans - Internet of Battle Things
(IOBT). The IOBT will experience unique challenges that are not yet well
addressed by the current generation of AI and machine learning."
What can the brain teach us about building artificial intelligence?,"This paper is the preprint of an invited commentary on Lake et al's
Behavioral and Brain Sciences article titled ""Building machines that learn and
think like people"". Lake et al's paper offers a timely critique on the recent
accomplishments in artificial intelligence from the vantage point of human
intelligence, and provides insightful suggestions about research directions for
building more human-like intelligence. Since we agree with most of the points
raised in that paper, we will offer a few points that are complementary."
Co-evolutionary hybrid intelligence,"Artificial intelligence is one of the drivers of modern technological
development. The current approach to the development of intelligent systems is
data-centric. It has several limitations: it is fundamentally impossible to
collect data for modeling complex objects and processes; training neural
networks requires huge computational and energy resources; solutions are not
explainable. The article discusses an alternative approach to the development
of artificial intelligence systems based on human-machine hybridization and
their co-evolution."
"The Emerging Artificial Intelligence Protocol for Hierarchical
  Information Network","The recent development of artificial intelligence enables a machine to
achieve a human level of intelligence. Problem-solving and decision-making are
two mental abilities to measure human intelligence. Many scholars have proposed
different models. However, there is a gap in establishing an AI-oriented
hierarchical model with a multilevel abstraction. This study proposes a novel
model known as the emerged AI protocol that consists of seven distinct layers
capable of providing an optimal and explainable solution for a given problem."
Quantization of Games: Towards Quantum Artificial Intelligence,"On grounds of the discussed material, we reason about possible future
development of quantum game theory and its impact on information processing and
the emerging information society. The idea of quantum artificial intelligence
is explained."
Identifying Independencies in Causal Graphs with Feedback,"We show that the d -separation criterion constitutes a valid test for
conditional independence relationships that are induced by feedback systems
involving discrete variables."
Constructing Lower Probabilities,"An elaboration of Dempster's method of constructing belief functions suggests
a broadly applicable strategy for constructing lower probabilities under a
variety of evidentiary constraints."
The Assumptions Behind Dempster's Rule,"This paper examines the concept of a combination rule for belief functions.
It is shown that two fairly simple and apparently reasonable assumptions
determine Dempster's rule, giving a new justification for it."
"The Nature of the Unnormalized Beliefs Encountered in the Transferable
  Belief Model","Within the transferable belief model, positive basic belief masses can be
allocated to the empty set, leading to unnormalized belief functions. The
nature of these unnormalized beliefs is analyzed."
Coefficients of Relations for Probabilistic Reasoning,"Definitions and notations with historical references are given for some
numerical coefficients commonly used to quantify relations among collections of
objects for the purpose of expressing approximate knowledge and probabilistic
reasoning."
"Evolution towards Smart Optical Networking: Where Artificial
  Intelligence (AI) meets the World of Photonics","Smart optical networks are the next evolution of programmable networking and
programmable automation of optical networks, with human-in-the-loop network
control and management. The paper discusses this evolution and the role of
Artificial Intelligence (AI)."
"Explainable artificial intelligence (XAI), the goodness criteria and the
  grasp-ability test","This paper introduces the ""grasp-ability test"" as a ""goodness"" criteria by
which to compare which explanation is more or less meaningful than others for
users to understand the automated algorithmic data processing."
"KI, Philosophie, Logik","This is a short (and personal) introduction in German to the connections
between artificial intelligence, philosophy, and logic, and to the author's
work.
  Dies ist eine kurze (und persoenliche) Einfuehrung in die Zusammenhaenge
zwischen Kuenstlicher Intelligenz, Philosophie, und Logik, und in die Arbeiten
des Autors."
"Super forecasting the technological singularity risks from artificial
  intelligence","The article forecasts emerging cyber-risks from the integration of AI in
cybersecurity."
Responsible AI Research Needs Impact Statements Too,"All types of research, development, and policy work can have unintended,
adverse consequences - work in responsible artificial intelligence (RAI),
ethical AI, or ethics in AI is no exception."
"Report on Candidate Computational Indicators for Conscious Valenced
  Experience","This report enlists 13 functional conditions cashed out in computational
terms that have been argued to be constituent of conscious valenced experience.
These are extracted from existing empirical and theoretical literature on,
among others, animal sentience, medical disorders, anaesthetics, philosophy,
evolution, neuroscience, and artificial intelligence."
Synthetic Data and Health Privacy,"This Viewpoint discusses generative artificial intelligence and safeguarding
privacy by using synthetic data as a substitute for private health data."
"To Root Artificial Intelligence Deeply in Basic Science for a New
  Generation of AI","One of the ambitions of artificial intelligence is to root artificial
intelligence deeply in basic science while developing brain-inspired artificial
intelligence platforms that will promote new scientific discoveries. The
challenges are essential to push artificial intelligence theory and applied
technologies research forward. This paper presents the grand challenges of
artificial intelligence research for the next 20 years which include:~(i) to
explore the working mechanism of the human brain on the basis of understanding
brain science, neuroscience, cognitive science, psychology and data science;
(ii) how is the electrical signal transmitted by the human brain? What is the
coordination mechanism between brain neural electrical signals and human
activities? (iii)~to root brain-computer interface~(BCI) and brain-muscle
interface~(BMI) technologies deeply in science on human behaviour; (iv)~making
research on knowledge-driven visual commonsense reasoning~(VCR), develop a new
inference engine for cognitive network recognition~(CNR); (v)~to develop
high-precision, multi-modal intelligent perceptrons; (vi)~investigating
intelligent reasoning and fast decision-making systems based on knowledge
graph~(KG). We believe that the frontier theory innovation of AI,
knowledge-driven modeling methodologies for commonsense reasoning,
revolutionary innovation and breakthroughs of the novel algorithms and new
technologies in AI, and developing responsible AI should be the main research
strategies of AI scientists in the future."
Quantum Operation of Affective Artificial Intelligence,"The review analyzes the fundamental principles which Artificial Intelligence
should be based on in order to imitate the realistic process of taking
decisions by humans experiencing emotions. Two approaches are compared, one
based on quantum theory and the other employing classical terms. Both these
approaches have a number of similarities, being principally probabilistic. The
analogies between quantum measurements under intrinsic noise and affective
decision making are elucidated. It is shown that cognitive processes have many
features that are formally similar to quantum measurements. This, however, in
no way means that for the imitation of human decision making Affective
Artificial Intelligence has necessarily to rely on the functioning of quantum
systems. Appreciating the common features between quantum measurements and
decision making helps for the formulation of an axiomatic approach employing
only classical notions. Artificial Intelligence, following this approach,
operates similarly to humans, by taking into account the utility of the
considered alternatives as well as their emotional attractiveness. Affective
Artificial Intelligence, whose operation takes account of the cognition-emotion
duality, avoids numerous behavioural paradoxes of traditional decision making.
A society of intelligent agents, interacting through the repeated multistep
exchange of information, forms a network accomplishing dynamic decision making.
The considered intelligent networks can characterize the operation of either a
human society of affective decision makers, or the brain composed of neurons,
or a typical probabilistic network of an artificial intelligence."
"Digital twin brain: a bridge between biological intelligence and
  artificial intelligence","In recent years, advances in neuroscience and artificial intelligence have
paved the way for unprecedented opportunities for understanding the complexity
of the brain and its emulation by computational systems. Cutting-edge
advancements in neuroscience research have revealed the intricate relationship
between brain structure and function, while the success of artificial neural
networks highlights the importance of network architecture. Now is the time to
bring them together to better unravel how intelligence emerges from the brain's
multiscale repositories. In this review, we propose the Digital Twin Brain
(DTB) as a transformative platform that bridges the gap between biological and
artificial intelligence. It consists of three core elements: the brain
structure that is fundamental to the twinning process, bottom-layer models to
generate brain functions, and its wide spectrum of applications. Crucially,
brain atlases provide a vital constraint, preserving the brain's network
organization within the DTB. Furthermore, we highlight open questions that
invite joint efforts from interdisciplinary fields and emphasize the
far-reaching implications of the DTB. The DTB can offer unprecedented insights
into the emergence of intelligence and neurological disorders, which holds
tremendous promise for advancing our understanding of both biological and
artificial intelligence, and ultimately propelling the development of
artificial general intelligence and facilitating precision mental healthcare."
One Decade of Universal Artificial Intelligence,"The first decade of this century has seen the nascency of the first
mathematical theory of general artificial intelligence. This theory of
Universal Artificial Intelligence (UAI) has made significant contributions to
many theoretical, philosophical, and practical AI questions. In a series of
papers culminating in book (Hutter, 2005), an exciting sound and complete
mathematical model for a super intelligent agent (AIXI) has been developed and
rigorously analyzed. While nowadays most AI researchers avoid discussing
intelligence, the award-winning PhD thesis (Legg, 2008) provided the
philosophical embedding and investigated the UAI-based universal measure of
rational intelligence, which is formal, objective and non-anthropocentric.
Recently, effective approximations of AIXI have been derived and experimentally
investigated in JAIR paper (Veness et al. 2011). This practical breakthrough
has resulted in some impressive applications, finally muting earlier critique
that UAI is only a theory. For the first time, without providing any domain
knowledge, the same agent is able to self-adapt to a diverse range of
interactive environments. For instance, AIXI is able to learn from scratch to
play TicTacToe, Pacman, Kuhn Poker, and other games by trial and error, without
even providing the rules of the games.
  These achievements give new hope that the grand goal of Artificial General
Intelligence is not elusive.
  This article provides an informal overview of UAI in context. It attempts to
gently introduce a very theoretical, formal, and mathematical subject, and
discusses philosophical and technical ingredients, traits of intelligence, some
social questions, and the past and future of UAI."
"A mathematical framework of intelligence and consciousness based on
  Riemannian Geometry","Understanding intelligence is a central pursuit in neuroscience, cognitive
science, and artificial intelligence. Intelligence encompasses learning,
problem-solving, creativity, and even consciousness. Recent advancements in
geometric analysis have revealed new insights into high-dimensional information
representation and organisation, exposing intrinsic data structures and dynamic
processes within neural and artificial systems. However, a comprehensive
framework that unifies the static and dynamic aspects of intelligence is still
lacking. This manuscript proposes a mathematical framework based on Riemannian
geometry to describe the structure and dynamics of intelligence and
consciousness. Intelligence elements are conceptualised as tokens embedded in a
high-dimensional space. The learned token embeddings capture the
interconnections of tokens across various scenarios and tasks, forming
manifolds in the intelligence space. Thought flow is depicted as the sequential
activation of tokens along geodesics within these manifolds. During the
navigation of geodesics, consciousness, as a self-referential process,
perceives the thought flow, evaluates it against predictions, and provides
feedback through prediction errors, adjusting the geodesic: non-zero prediction
errors, such as learning, lead to the restructuring of the curved manifolds,
thus changing the geodesic of thought flow. This dynamic interaction integrates
new information, evolves the geometry and facilitates learning. The geometry of
intelligence guides consciousness, and consciousness structures the geometry of
intelligence. By integrating geometric concepts, this proposed theory offers a
unified, mathematically framework for describing the structure and dynamics of
intelligence and consciousness. Applicable to biological and artificial
intelligence, this framework may pave the way for future research and empirical
validation."
Artificial Intelligence and Asymmetric Information Theory,"When human agents come together to make decisions, it is often the case that
one human agent has more information than the other. This phenomenon is called
information asymmetry and this distorts the market. Often if one human agent
intends to manipulate a decision in its favor the human agent can signal wrong
or right information. Alternatively, one human agent can screen for information
to reduce the impact of asymmetric information on decisions. With the advent of
artificial intelligence, signaling and screening have been made easier. This
paper studies the impact of artificial intelligence on the theory of asymmetric
information. It is surmised that artificial intelligent agents reduce the
degree of information asymmetry and thus the market where these agents are
deployed become more efficient. It is also postulated that the more artificial
intelligent agents there are deployed in the market the less is the volume of
trades in the market. This is because for many trades to happen the asymmetry
of information on goods and services to be traded should exist, creating a
sense of arbitrage."
"Designing Artificial Cognitive Architectures: Brain Inspired or
  Biologically Inspired?","Artificial Neural Networks (ANNs) were devised as a tool for Artificial
Intelligence design implementations. However, it was soon became obvious that
they are unable to fulfill their duties. The fully autonomous way of ANNs
working, precluded from any human intervention or supervision, deprived of any
theoretical underpinning, leads to a strange state of affairs, when ANN
designers cannot explain why and how they achieve their amazing and remarkable
results. Therefore, contemporary Artificial Intelligence R&D looks more like a
Modern Alchemy enterprise rather than a respected scientific or technological
undertaking. On the other hand, modern biological science posits that
intelligence can be distinguished not only in human brains. Intelligence today
is considered as a fundamental property of each and every living being.
Therefore, lower simplified forms of natural intelligence are more suitable for
investigation and further replication in artificial cognitive architectures."
"Surveying the reach and maturity of machine learning and artificial
  intelligence in astronomy","Machine learning (automated processes that learn by example in order to
classify, predict, discover or generate new data) and artificial intelligence
(methods by which a computer makes decisions or discoveries that would usually
require human intelligence) are now firmly established in astronomy. Every
week, new applications of machine learning and artificial intelligence are
added to a growing corpus of work. Random forests, support vector machines, and
neural networks (artificial, deep, and convolutional) are now having a genuine
impact for applications as diverse as discovering extrasolar planets, transient
objects, quasars, and gravitationally-lensed systems, forecasting solar
activity, and distinguishing between signals and instrumental effects in
gravitational wave astronomy. This review surveys contemporary, published
literature on machine learning and artificial intelligence in astronomy and
astrophysics. Applications span seven main categories of activity:
classification, regression, clustering, forecasting, generation, discovery, and
the development of new scientific insight. These categories form the basis of a
hierarchy of maturity, as the use of machine learning and artificial
intelligence emerges, progresses or becomes established."
"A Review on Explainable Artificial Intelligence for Healthcare: Why,
  How, and When?","Artificial intelligence (AI) models are increasingly finding applications in
the field of medicine. Concerns have been raised about the explainability of
the decisions that are made by these AI models. In this article, we give a
systematic analysis of explainable artificial intelligence (XAI), with a
primary focus on models that are currently being used in the field of
healthcare. The literature search is conducted following the preferred
reporting items for systematic reviews and meta-analyses (PRISMA) standards for
relevant work published from 1 January 2012 to 02 February 2022. The review
analyzes the prevailing trends in XAI and lays out the major directions in
which research is headed. We investigate the why, how, and when of the uses of
these XAI models and their implications. We present a comprehensive examination
of XAI methodologies as well as an explanation of how a trustworthy AI can be
derived from describing AI models for healthcare fields. The discussion of this
work will contribute to the formalization of the XAI field."
"Augmented Computational Design: Methodical Application of Artificial
  Intelligence in Generative Design","This chapter presents methodological reflections on the necessity and utility
of artificial intelligence in generative design. Specifically, the chapter
discusses how generative design processes can be augmented by AI to deliver in
terms of a few outcomes of interest or performance indicators while dealing
with hundreds or thousands of small decisions. The core of the
performance-based generative design paradigm is about making statistical or
simulation-driven associations between these choices and consequences for
mapping and navigating such a complex decision space. This chapter will discuss
promising directions in Artificial Intelligence for augmenting decision-making
processes in architectural design for mapping and navigating complex design
spaces."
"Tipta uzmanlik sinavinda (tus) buyuk dil modelleri insanlardan daha mi
  basarili?","The potential of artificial intelligence in medical education and assessment
has been made evident by recent developments in natural language processing and
artificial intelligence. Medical questions can now be successfully answered by
artificial intelligence algorithms. It can help medical practitioners. This
study evaluates the performance of three different artificial intelligence
models in answering Turkish medical questions in the 2021 1st Term Medical
Specialization Examination (MSE). MSE consists of a total of 240 questions
across clinical (CMST) and basic (BMST) medical sciences. According to the
results in CMST, it was concluded that Gemini correctly answered 82 questions,
ChatGPT-4 answered 105 questions and ChatGPT-4o answered 117 questions. In
BMST, Gemini and ChatGPT-4 answered 93 questions and ChatGPT-4o answered 107
questions correctly according to the answer key. ChatGPT-4o outperformed the
candidate with the highest scores of 113 and 106 according to CMST and BMST
respectively. This study highlights the importance of the potential of
artificial intelligence in medical education and assessment. It demonstrates
that advanced models can achieve high accuracy and contextual understanding,
demonstrating their potential role in medical education and evaluation."
"Will artificial intelligence accelerate or delay the race between
  nuclear energy technology budgeting and net-zero emissions?","This study explores the impact of nuclear energy technology budgeting and
artificial intelligence on carbon dioxide (CO2) emissions in 20 OECD economies.
Unlike previous research that relied on conventional panel techniques, we
utilize the Method of Moment Quantile Regression panel data estimation
techniques. This approach provides quantile-specific insights while addressing
issues of endogeneity and heteroscedasticity, resulting in a more nuanced and
robust understanding of complex relationships. A novel aspect of this research
work is introducing the moderating effect of artificial intelligence on the
relationship between nuclear energy and CO2 emissions. The results found that
the direct impact of artificial intelligence on CO2 emissions is significant,
while the effect of nuclear energy technology budgeting is not. Additionally,
artificial intelligence moderates the relationship between nuclear energy
technology budgeting and CO2 emissions, aiding nuclear energy in reducing
carbon emissions across OECD countries. Our findings indicate that
transitioning to a low-carbon future is achievable by replacing fossil fuel
energy sources with increased integration of artificial intelligence to promote
nuclear energy technologies. This study demonstrates that energy innovations
can serve as effective climate-resilience strategies to mitigate the impacts of
climate change."
"Artificial Intelligence for Software Architecture: Literature Review and
  the Road Ahead","This paper presents a forward-looking vision for artificial
intelligence-driven software architecture that addresses longstanding
challenges in design and evolution. Although artificial intelligence has
achieved notable success in software engineering, its explicit application to
software architecture remains under-explored. Traditional practices, heavily
reliant on expert knowledge and complex trade-off reasoning, tend to be manual
and error-prone, thereby compromising system quality and maintainability.
Building on recent advances, we examine how artificial intelligence can
automate architectural design, support quantitative trade-off analyses, and
continuously update architectural documentation. Our approach combines a
systematic review of state-of-the-art applications with insights from industry
practitioners. The resulting roadmap outlines 14 current artificial
intelligence contributions to software architecture, identifies six artificial
intelligence-specific challenges in supporting architectural tasks, and reveals
six avenues for future improvement, charting a course for future research and
practical implementations."
"The Trap of Presumed Equivalence: Artificial General Intelligence Should
  Not Be Assessed on the Scale of Human Intelligence","A traditional approach to assessing emerging intelligence in the theory of
intelligent systems is based on the similarity, ""imitation"" of human-like
actions and behaviors, benchmarking the performance of intelligent systems on
the scale of human cognitive skills. In this work we attempt to outline the
shortcomings of this line of thought, which is based on the implicit
presumption of the equivalence and compatibility of the originating and
emergent intelligences. We provide arguments to the point that under some
natural assumptions, developing intelligent systems will be able to form their
own intents and objectives. Then, the difference in the rate of progress of
natural and artificial systems that was noted on multiple occasions in the
discourse on artificial intelligence can lead to the scenario of a progressive
divergence of the intelligences, in their cognitive abilities, functions and
resources, values, ethical frameworks, worldviews, intents and existential
objectives: the scenario of the AGI evolutionary gap. We discuss evolutionary
processes that can guide the development of emergent intelligent systems and
attempt to identify the starting point of the progressive divergence scenario."
Naive Artificial Intelligence,"In the cognitive sciences, it is common to distinguish between crystal
intelligence, the ability to utilize knowledge acquired through past learning
or experience and fluid intelligence, the ability to solve novel problems
without relying on prior knowledge. Using this cognitive distinction between
the two types of intelligence, extensively-trained deep networks that can play
chess or Go exhibit crystal but not fluid intelligence. In humans, fluid
intelligence is typically studied and quantified using intelligence tests.
Previous studies have shown that deep networks can solve some forms of
intelligence tests, but only after extensive training. Here we present a
computational model that solves intelligence tests without any prior training.
This ability is based on continual inductive reasoning, and is implemented by
deep unsupervised latent-prediction networks. Our work demonstrates the
potential fluid intelligence of deep networks. Finally, we propose that the
computational principles underlying our approach can be used to model fluid
intelligence in the cognitive sciences."
"Intelligent Escape of Robotic Systems: A Survey of Methodologies,
  Applications, and Challenges","Intelligent escape is an interdisciplinary field that employs artificial
intelligence (AI) techniques to enable robots with the capacity to
intelligently react to potential dangers in dynamic, intricate, and
unpredictable scenarios. As the emphasis on safety becomes increasingly
paramount and advancements in robotic technologies continue to advance, a wide
range of intelligent escape methodologies has been developed in recent years.
This paper presents a comprehensive survey of state-of-the-art research work on
intelligent escape of robotic systems. Four main methods of intelligent escape
are reviewed, including planning-based methodologies, partitioning-based
methodologies, learning-based methodologies, and bio-inspired methodologies.
The strengths and limitations of existing methods are summarized. In addition,
potential applications of intelligent escape are discussed in various domains,
such as search and rescue, evacuation, military security, and healthcare. In an
effort to develop new approaches to intelligent escape, this survey identifies
current research challenges and provides insights into future research trends
in intelligent escape."
How Intelligent is your Intelligent Robot?,"How intelligent is robot A compared with robot B? And how intelligent are
robots A and B compared with animals (or plants) X and Y? These are both
interesting and deeply challenging questions. In this paper we address the
question ""how intelligent is your intelligent robot?"" by proposing that
embodied intelligence emerges from the interaction and integration of four
different and distinct kinds of intelligence. We then suggest a simple
diagrammatic representation on which these kinds of intelligence are shown as
four axes in a star diagram. A crude qualitative comparison of the intelligence
graphs of animals and robots both exposes and helps to explain the chronic
intelligence deficit of intelligent robots. Finally we examine the options for
determining numerical values for the four kinds of intelligence in an effort to
move toward a quantifiable intelligence vector."
Realizing Intelligence,"Order exists in the world. The intelligence process enables us to realize
that order, to some extent. We provide a high level description of intelligence
using simple definitions, basic building blocks, a conceptual framework and
general hierarchy. This perspective includes multiple levels of abstraction
occurring in space and in time. The resulting model offers simple, useful ways
to help realize the essence of intelligence."
A primer on Answer Set Programming,"A introduction to the syntax and Semantics of Answer Set Programming intended
as an handout to [under]graduate students taking Artificial Intlligence or
Logic Programming classes."
"Towards a Hierarchical Model of Consciousness, Intelligence, Mind and
  Body",This article is taken out.
Agent Models of Political Interactions,"Looks at state interactions from an agent based AI perspective to see state
interactions as an example of emergent intelligent behavior. Exposes basic
principles of game theory."
Memory Based Machine Intelligence Techniques in VLSI hardware,"We briefly introduce the memory based approaches to emulate machine
intelligence in VLSI hardware, describing the challenges and advantages.
Implementation of artificial intelligence techniques in VLSI hardware is a
practical and difficult problem. Deep architectures, hierarchical temporal
memories and memory networks are some of the contemporary approaches in this
area of research. The techniques attempt to emulate low level intelligence
tasks and aim at providing scalable solutions to high level intelligence
problems such as sparse coding and contextual processing."
"Quantitative Analysis of Whether Machine Intelligence Can Surpass Human
  Intelligence","Whether the machine intelligence can surpass the human intelligence is a
controversial issue. On the basis of traditional IQ, this article presents the
Universal IQ test method suitable for both the machine intelligence and the
human intelligence. With the method, machine and human intelligences were
divided into 4 major categories and 15 subcategories. A total of 50 search
engines across the world and 150 persons at different ages were subject to the
relevant test. And then, the Universal IQ ranking list of 2014 for the test
objects was obtained."
Introduction to intelligent computing unit 1,"This brief note highlights some basic concepts required toward understanding
the evolution of machine learning and deep learning models. The note starts
with an overview of artificial intelligence and its relationship to biological
neuron that ultimately led to the evolution of todays intelligent models."
"Design of a dynamic and self adapting system, supported with artificial
  intelligence, machine learning and real time intelligence for predictive
  cyber risk analytics in extreme environments, cyber risk in the colonisation
  of Mars","Multiple governmental agencies and private organisations have made
commitments for the colonisation of Mars. Such colonisation requires complex
systems and infrastructure that could be very costly to repair or replace in
cases of cyber attacks. This paper surveys deep learning algorithms, IoT cyber
security and risk models, and established mathematical formulas to identify the
best approach for developing a dynamic and self adapting system for predictive
cyber risk analytics supported with Artificial Intelligence and Machine
Learning and real time intelligence in edge computing. The paper presents a new
mathematical approach for integrating concepts for cognition engine design,
edge computing and Artificial Intelligence and Machine Learning to automate
anomaly detection. This engine instigates a step change by applying Artificial
Intelligence and Machine Learning embedded at the edge of IoT networks, to
deliver safe and functional real time intelligence for predictive cyber risk
analytics. This will enhance capacities for risk analytics and assists in the
creation of a comprehensive and systematic understanding of the opportunities
and threats that arise when edge computing nodes are deployed, and when
Artificial Intelligence and Machine Learning technologies are migrated to the
periphery of the internet and into local IoT networks."
A Theory of Intelligences,"Intelligence is a human construct to represent the ability to achieve goals.
Given this wide berth, intelligence has been defined countless times, studied
in a variety of ways and represented using numerous measures. Understanding
intelligence ultimately requires theory and quantification, both of which have
proved elusive. I develop a framework -- the Theory of Intelligences (TIS) --
that applies across all systems from physics, to biology, humans and AI. TIS
likens intelligence to a calculus, differentiating, correlating and integrating
information. Intelligence operates at many levels and scales and TIS distils
these into a parsimonious macroscopic framework centered on solving, planning
and their optimization to accomplish goals. Notably, intelligence can be
expressed in informational units or in units relative to goal difficulty, the
latter defined as complexity relative to system (individual or benchmarked)
ability. I present general equations for intelligence and its components, and a
simple expression for the evolution of intelligence traits. The measures
developed here could serve to gauge different facets of intelligence for any
step-wise transformation of information. I argue that proxies such as
environment, technology, society and collectives are essential to a general
theory of intelligence and to possible evolutionary transitions in
intelligence, particularly in humans. I conclude with testable predictions of
TIS and offer several speculations."
Universal Intelligence: A Definition of Machine Intelligence,"A fundamental problem in artificial intelligence is that nobody really knows
what intelligence is. The problem is especially acute when we need to consider
artificial systems which are significantly different to humans. In this paper
we approach this problem in the following way: We take a number of well known
informal definitions of human intelligence that have been given by experts, and
extract their essential features. These are then mathematically formalised to
produce a general measure of intelligence for arbitrary machines. We believe
that this equation formally captures the concept of machine intelligence in the
broadest reasonable sense. We then show how this formal definition is related
to the theory of universal optimal learning agents. Finally, we survey the many
other tests and definitions of intelligence that have been proposed for
machines."
Measuring Intelligence through Games,"Artificial general intelligence (AGI) refers to research aimed at tackling
the full problem of artificial intelligence, that is, create truly intelligent
agents. This sets it apart from most AI research which aims at solving
relatively narrow domains, such as character recognition, motion planning, or
increasing player satisfaction in games. But how do we know when an agent is
truly intelligent? A common point of reference in the AGI community is Legg and
Hutter's formal definition of universal intelligence, which has the appeal of
simplicity and generality but is unfortunately incomputable. Games of various
kinds are commonly used as benchmarks for ""narrow"" AI research, as they are
considered to have many important properties. We argue that many of these
properties carry over to the testing of general intelligence as well. We then
sketch how such testing could practically be carried out. The central part of
this sketch is an extension of universal intelligence to deal with finite time,
and the use of sampling of the space of games expressed in a suitably biased
game description language."
Extended Intelligence,"We argue that intelligence, construed as the disposition to perform tasks
successfully, is a property of systems composed of agents and their contexts.
This is the thesis of extended intelligence. We argue that the performance of
an agent will generally not be preserved if its context is allowed to vary.
Hence, this disposition is not possessed by an agent alone, but is rather
possessed by the system consisting of an agent and its context, which we dub an
agent-in-context. An agent's context may include an environment, other agents,
cultural artifacts (like language, technology), or all of these, as is
typically the case for humans and artificial intelligence systems, as well as
many non-human animals. In virtue of the thesis of extended intelligence, we
contend that intelligence is context-bound, task-particular and incommensurable
among agents. Our thesis carries strong implications for how intelligence is
analyzed in the context of both psychology and artificial intelligence."
"Intelligence via ultrafilters: structural properties of some
  intelligence comparators of deterministic Legg-Hutter agents","Legg and Hutter, as well as subsequent authors, considered intelligent agents
through the lens of interaction with reward-giving environments, attempting to
assign numeric intelligence measures to such agents, with the guiding principle
that a more intelligent agent should gain higher rewards from environments in
some aggregate sense. In this paper, we consider a related question: rather
than measure numeric intelligence of one Legg- Hutter agent, how can we compare
the relative intelligence of two Legg-Hutter agents? We propose an elegant
answer based on the following insight: we can view Legg-Hutter agents as
candidates in an election, whose voters are environments, letting each
environment vote (via its rewards) which agent (if either) is more intelligent.
This leads to an abstract family of comparators simple enough that we can prove
some structural theorems about them. It is an open question whether these
structural theorems apply to more practical intelligence measures."
Detecting Qualia in Natural and Artificial Agents,"The Hard Problem of consciousness has been dismissed as an illusion. By
showing that computers are capable of experiencing, we show that they are at
least rudimentarily conscious with potential to eventually reach
superconsciousness. The main contribution of the paper is a test for confirming
certain subjective experiences in a tested agent. We follow with analysis of
benefits and problems with conscious machines and implications of such
capability on future of computing, machine rights and artificial intelligence
safety."
Human Indignity: From Legal AI Personhood to Selfish Memes,"It is possible to rely on current corporate law to grant legal personhood to
Artificially Intelligent (AI) agents. In this paper, after introducing pathways
to AI personhood, we analyze consequences of such AI empowerment on human
dignity, human safety and AI rights. We emphasize possibility of creating
selfish memes and legal system hacking in the context of artificial entities.
Finally, we consider some potential solutions for addressing described
problems."
Autonomous Visual Navigation A Biologically Inspired Approach,"Inspired by the navigational behavior observed in the animal kingdom and
especially the navigational behavior of the ants, we attempt to simulate it in
an artificial environment by implementing different kinds of biomimetic
algorithms."
Avoiding Undesired Choices Using Intelligent Adaptive Systems,"We propose a number of heuristics that can be used for identifying when
intransitive choice behaviour is likely to occur in choice situations. We also
suggest two methods for avoiding undesired choice behaviour, namely transparent
communication and adaptive choice-set generation. We believe that these two
ways can contribute to the avoidance of decision biases in choice situations
that may often be regretted."
Seq2Seq AI Chatbot with Attention Mechanism,"Intelligent Conversational Agent development using Artificial Intelligence or
Machine Learning technique is an interesting problem in the field of Natural
Language Processing. With the rise of deep learning, these models were quickly
replaced by end to end trainable neural networks."
Ethical Considerations for AI Researchers,"Use of artificial intelligence is growing and expanding into applications
that impact people's lives. People trust their technology without really
understanding it or its limitations. There is the potential for harm and we are
already seeing examples of that in the world. AI researchers have an obligation
to consider the impact of intelligent applications they work on. While the
ethics of AI is not clear-cut, there are guidelines we can consider to minimize
the harm we might introduce."
The Next Decade of Telecommunications Artificial Intelligence,"It has been an exciting journey since the mobile communications and
artificial intelligence were conceived 37 years and 64 years ago. While both
fields evolved independently and profoundly changed communications and
computing industries, the rapid convergence of 5G and deep learning is
beginning to significantly transform the core communication infrastructure,
network management and vertical applications. The paper first outlines the
individual roadmaps of mobile communications and artificial intelligence in the
early stage, with a concentration to review the era from 3G to 5G when AI and
mobile communications started to converge. With regard to telecommunications
artificial intelligence, the paper further introduces in detail the progress of
artificial intelligence in the ecosystem of mobile communications. The paper
then summarizes the classifications of AI in telecom ecosystems along with its
evolution paths specified by various international telecommunications
standardization bodies. Towards the next decade, the paper forecasts the
prospective roadmap of telecommunications artificial intelligence. In line with
3GPP and ITU-R timeline of 5G & 6G, the paper further explores the network
intelligence following 3GPP and ORAN routes respectively, experience and
intention driven network management and operation, network AI signalling
system, intelligent middle-office based BSS, intelligent customer experience
management and policy control driven by BSS and OSS convergence, evolution from
SLA to ELA, and intelligent private network for verticals. The paper is
concluded with the vision that AI will reshape the future B5G or 6G landscape
and we need pivot our R&D, standardizations, and ecosystem to fully take the
unprecedented opportunities."
Object-Process Methodology for Intelligent System Development,"Development of the new artificial systems with unique characteristics is very
challenging task. In this paper the application of the hybrid super
intelligence concept with object-process methodology to develop unique
high-performance computational systems is considered. The methodological
approach how to design new intelligent components for existing high-performance
computing development systems is proposed on the example of system requirements
creation for ""MicroAI"" and ""Artificial Electronic"" systems."
"A Cyber Science Based Ontology for Artificial General Intelligence
  Containment","The development of artificial general intelligence is considered by many to
be inevitable. What such intelligence does after becoming aware is not so
certain. To that end, research suggests that the likelihood of artificial
general intelligence becoming hostile to humans is significant enough to
warrant inquiry into methods to limit such potential. Thus, containment of
artificial general intelligence is a timely and meaningful research topic.
While there is limited research exploring possible containment strategies, such
work is bounded by the underlying field the strategies draw upon. Accordingly,
we set out to construct an ontology to describe necessary elements in any
future containment technology. Using existing academic literature, we developed
a single domain ontology containing five levels, 32 codes, and 32 associated
descriptors. Further, we constructed ontology diagrams to demonstrate intended
relationships. We then identified humans, AGI, and the cyber world as novel
agent objects necessary for future containment activities. Collectively, the
work addresses three critical gaps: (a) identifying and arranging fundamental
constructs; (b) situating AGI containment within cyber science; and (c)
developing scientific rigor within the field."
"Research on Artificial Intelligence Ethics Based on the Evolution of
  Population Knowledge Base","The unclear development direction of human society is a deep reason for that
it is difficult to form a uniform ethical standard for human society and
artificial intelligence. Since the 21st century, the latest advances in the
Internet, brain science and artificial intelligence have brought new
inspiration to the research on the development direction of human society.
Through the study of the Internet brain model, AI IQ evaluation, and the
evolution of the brain, this paper proposes that the evolution of population
knowledge base is the key for judging the development direction of human
society, thereby discussing the standards and norms for the construction of
artificial intelligence ethics."
Contrastive Explanation: A Structural-Model Approach,"This paper presents a model of contrastive explanation using structural
casual models. The topic of causal explanation in artificial intelligence has
gathered interest in recent years as researchers and practitioners aim to
increase trust and understanding of intelligent decision-making. While
different sub-fields of artificial intelligence have looked into this problem
with a sub-field-specific view, there are few models that aim to capture
explanation more generally. One general model is based on structural causal
models. It defines an explanation as a fact that, if found to be true, would
constitute an actual cause of a specific event. However, research in philosophy
and social sciences shows that explanations are contrastive: that is, when
people ask for an explanation of an event -- the fact -- they (sometimes
implicitly) are asking for an explanation relative to some contrast case; that
is, ""Why P rather than Q?"". In this paper, we extend the structural causal
model approach to define two complementary notions of contrastive explanation,
and demonstrate them on two classical problems in artificial intelligence:
classification and planning. We believe that this model can help researchers in
subfields of artificial intelligence to better understand contrastive
explanation."
"Knowledge-based XAI through CBR: There is more to explanations than
  models can tell","The underlying hypothesis of knowledge-based explainable artificial
intelligence is the data required for data-centric artificial intelligence
agents (e.g., neural networks) are less diverse in contents than the data
required to explain the decisions of such agents to humans. The idea is that a
classifier can attain high accuracy using data that express a phenomenon from
one perspective whereas the audience of explanations can entail multiple
stakeholders and span diverse perspectives. We hence propose to use domain
knowledge to complement the data used by agents. We formulate knowledge-based
explainable artificial intelligence as a supervised data classification problem
aligned with the CBR methodology. In this formulation, the inputs are case
problems composed of both the inputs and outputs of the data-centric agent and
case solutions, the outputs, are explanation categories obtained from domain
knowledge and subject matter experts. This formulation does not typically lead
to an accurate classification, preventing the selection of the correct
explanation category. Knowledge-based explainable artificial intelligence
extends the data in this formulation by adding features aligned with domain
knowledge that can increase accuracy when selecting explanation categories."
"Creative Problem Solving in Artificially Intelligent Agents: A Survey
  and Framework","Creative Problem Solving (CPS) is a sub-area within Artificial Intelligence
(AI) that focuses on methods for solving off-nominal, or anomalous problems in
autonomous systems. Despite many advancements in planning and learning,
resolving novel problems or adapting existing knowledge to a new context,
especially in cases where the environment may change in unpredictable ways post
deployment, remains a limiting factor in the safe and useful integration of
intelligent systems. The emergence of increasingly autonomous systems dictates
the necessity for AI agents to deal with environmental uncertainty through
creativity. To stimulate further research in CPS, we present a definition and a
framework of CPS, which we adopt to categorize existing AI methods in this
field. Our framework consists of four main components of a CPS problem, namely,
1) problem formulation, 2) knowledge representation, 3) method of knowledge
manipulation, and 4) method of evaluation. We conclude our survey with open
research questions, and suggested directions for the future."
"Brain in a Vat: On Missing Pieces Towards Artificial General
  Intelligence in Large Language Models","In this perspective paper, we first comprehensively review existing
evaluations of Large Language Models (LLMs) using both standardized tests and
ability-oriented benchmarks. We pinpoint several problems with current
evaluation methods that tend to overstate the capabilities of LLMs. We then
articulate what artificial general intelligence should encompass beyond the
capabilities of LLMs. We propose four characteristics of generally intelligent
agents: 1) they can perform unlimited tasks; 2) they can generate new tasks
within a context; 3) they operate based on a value system that underpins task
generation; and 4) they have a world model reflecting reality, which shapes
their interaction with the world. Building on this viewpoint, we highlight the
missing pieces in artificial general intelligence, that is, the unity of
knowing and acting. We argue that active engagement with objects in the real
world delivers more robust signals for forming conceptual representations.
Additionally, knowledge acquisition isn't solely reliant on passive input but
requires repeated trials and errors. We conclude by outlining promising future
research directions in the field of artificial general intelligence."
Construction Grammar and Artificial Intelligence,"In this chapter, we argue that it is highly beneficial for the contemporary
construction grammarian to have a thorough understanding of the strong
relationship between the research fields of construction grammar and artificial
intelligence. We start by unravelling the historical links between the two
fields, showing that their relationship is rooted in a common attitude towards
human communication and language. We then discuss the first direction of
influence, focussing in particular on how insights and techniques from the
field of artificial intelligence play an important role in operationalising,
validating and scaling constructionist approaches to language. We then proceed
to the second direction of influence, highlighting the relevance of
construction grammar insights and analyses to the artificial intelligence
endeavour of building truly intelligent agents. We support our case with a
variety of illustrative examples and conclude that the further elaboration of
this relationship will play a key role in shaping the future of the field of
construction grammar."
"A Review of Findings from Neuroscience and Cognitive Psychology as
  Possible Inspiration for the Path to Artificial General Intelligence","This review aims to contribute to the quest for artificial general
intelligence by examining neuroscience and cognitive psychology methods for
potential inspiration. Despite the impressive advancements achieved by deep
learning models in various domains, they still have shortcomings in abstract
reasoning and causal understanding. Such capabilities should be ultimately
integrated into artificial intelligence systems in order to surpass data-driven
limitations and support decision making in a way more similar to human
intelligence. This work is a vertical review that attempts a wide-ranging
exploration of brain function, spanning from lower-level biological neurons,
spiking neural networks, and neuronal ensembles to higher-level concepts such
as brain anatomy, vector symbolic architectures, cognitive and categorization
models, and cognitive architectures. The hope is that these concepts may offer
insights for solutions in artificial general intelligence."
US-China perspectives on extreme AI risks and global governance,"The United States and China will play an important role in navigating safety
and security challenges relating to advanced artificial intelligence. We sought
to better understand how experts in each country describe safety and security
threats from advanced artificial intelligence, extreme risks from AI, and the
potential for international cooperation. Specifically, we compiled
publicly-available statements from major technical and policy leaders in both
the United States and China. We focused our analysis on advanced forms of
artificial intelligence, such as artificial general intelligence (AGI), that
may have the most significant impacts on national and global security. Experts
in both countries expressed concern about risks from AGI, risks from
intelligence explosions, and risks from AI systems that escape human control.
Both countries have also launched early efforts designed to promote
international cooperation around safety standards and risk management
practices. Notably, our findings only reflect information from publicly
available sources. Nonetheless, our findings can inform policymakers and
researchers about the state of AI discourse in the US and China. We hope such
work can contribute to policy discussions around advanced AI, its global
security threats, and potential international dialogues or agreements to
mitigate such threats."
"The Adoption of Artificial Intelligence in Different Network Security
  Concepts","The obstacles of each security system combined with the increase of
cyber-attacks, negatively affect the effectiveness of network security
management and rise the activities to be taken by the security staff and
network administrators. So, there is a growing need for the automated auditing
and intelligent reporting strategies for reliable network security with as less
model complexity as possible. Newly, artificial intelligence has been
effectively applied to various network security issues, and numerous studies
have been conducted that utilize various artificial intelligence techniques for
the purposes of encryption and secure communication, in addition to using
artificial intelligence to perform a large number of data encryption operations
in record time. The aim of the study is to present and discuss the most
prominent methods of artificial intelligence recently used in the field of
network security including user authentication, Key exchanging,
encryption/decryption, data integrity and intrusion detection system."
Ambiente de Planejamento Ip,"In this work we investigate the systems that implements algorithms for the
planning problem in Artificial Intelligence, called planners, with especial
attention to the planners based on the plan graph. We analyze the problem of
comparing the performance of the different algorithms and we propose an
environment for the development and analysis of planners."
Artificial Intelligence Approaches To UCAV Autonomy,"This paper covers a number of approaches that leverage Artificial
Intelligence algorithms and techniques to aid Unmanned Combat Aerial Vehicle
(UCAV) autonomy. An analysis of current approaches to autonomous control is
provided followed by an exploration of how these techniques can be extended and
enriched with AI techniques including Artificial Neural Networks (ANN),
Ensembling and Reinforcement Learning (RL) to evolve control strategies for
UCAVs."
Responsible Autonomy,"As intelligent systems are increasingly making decisions that directly affect
society, perhaps the most important upcoming research direction in AI is to
rethink the ethical implications of their actions. Means are needed to
integrate moral, societal and legal values with technological developments in
AI, both during the design process as well as part of the deliberation
algorithms employed by these systems. In this paper, we describe leading ethics
theories and propose alternative ways to ensure ethical behavior by artificial
systems. Given that ethics are dependent on the socio-cultural context and are
often only implicit in deliberation processes, methodologies are needed to
elicit the values held by designers and stakeholders, and to make these
explicit leading to better understanding and trust on artificial autonomous
systems."
Hacia los Comits de tica en Inteligencia Artificial,"The goal of Artificial Intelligence based systems is to take decisions that
have an effect in their environment and impact society. This points out to the
necessity of mechanism that regulate the impact of this type of system in
society. For this reason, it is priority to create the rules and specialized
organizations that can oversight the following of such rules, particularly that
human rights precepts at local and international level. This work proposes the
creation, at the universities, of Ethical Committees or Commissions specialized
on Artificial Intelligence that would be in charge of define the principles and
will guarantee the following of good practices in the field Artificial
Intelligence."
Neuro-Symbolic Artificial Intelligence: Current Trends,"Neuro-Symbolic Artificial Intelligence -- the combination of symbolic methods
with methods that are based on artificial neural networks -- has a
long-standing history. In this article, we provide a structured overview of
current trends, by means of categorizing recent publications from key
conferences. The article is meant to serve as a convenient starting point for
research on the general topic."
"Using a Collated Cybersecurity Dataset for Machine Learning and
  Artificial Intelligence","Artificial Intelligence (AI) and Machine Learning (ML) algorithms can support
the span of indicator-level, e.g. anomaly detection, to behavioral level cyber
security modeling and inference. This contribution is based on a dataset named
BRON which is amalgamated from public threat and vulnerability behavioral
sources. We demonstrate how BRON can support prediction of related threat
techniques and attack patterns. We also discuss other AI and ML uses of BRON to
exploit its behavioral knowledge."
"KIX: A Knowledge and Interaction-Centric Metacognitive Framework for
  Task Generalization","People aptly exhibit general intelligence behaviors in solving a variety of
tasks with flexibility and ability to adapt to novel situations by reusing and
applying high-level knowledge acquired over time. But artificial agents are
more like specialists, lacking such generalist behaviors. Artificial agents
will require understanding and exploiting critical structured knowledge
representations. We present a metacognitive generalization framework,
Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects
leveraging type space facilitate the learning of transferable interaction
concepts and generalization. It is a natural way of integrating knowledge into
reinforcement learning and is promising to act as an enabler for autonomous and
generalist behaviors in artificial intelligence systems."
Considerations upon the Machine Learning Technologies,"Artificial intelligence offers superior techniques and methods by which
problems from diverse domains may find an optimal solution. The Machine
Learning technologies refer to the domain of artificial intelligence aiming to
develop the techniques allowing the computers to ""learn"". Some systems based on
Machine Learning technologies tend to eliminate the necessity of the human
intelligence while the others adopt a man-machine collaborative approach."
"Algorithmic Randomness as Foundation of Inductive Reasoning and
  Artificial Intelligence","This article is a brief personal account of the past, present, and future of
algorithmic randomness, emphasizing its role in inductive inference and
artificial intelligence. It is written for a general audience interested in
science and philosophy. Intuitively, randomness is a lack of order or
predictability. If randomness is the opposite of determinism, then algorithmic
randomness is the opposite of computability. Besides many other things, these
concepts have been used to quantify Ockham's razor, solve the induction
problem, and define intelligence."
Hybrid Systems for Knowledge Representation in Artificial Intelligence,"There are few knowledge representation (KR) techniques available for
efficiently representing knowledge. However, with the increase in complexity,
better methods are needed. Some researchers came up with hybrid mechanisms by
combining two or more methods. In an effort to construct an intelligent
computer system, a primary consideration is to represent large amounts of
knowledge in a way that allows effective use and efficiently organizing
information to facilitate making the recommended inferences. There are merits
and demerits of combinations, and standardized method of KR is needed. In this
paper, various hybrid schemes of KR were explored at length and details
presented."
"Types of Cognition and its Implications for future High-Level Cognitive
  Machines","This work summarizes part of current knowledge on High-level Cognitive
process and its relation with biological hardware. Thus, it is possible to
identify some paradoxes which could impact the development of future
technologies and artificial intelligence: we may make a High-level Cognitive
Machine, sacrificing the principal attribute of a machine, its accuracy."
Intelligent Physiotherapy Through Procedural Content Generation,"This paper describes an avenue for artificial and computational intelligence
techniques applied within games research to be deployed for purposes of
physical therapy. We provide an overview of prototypical research focussed on
the application of motion sensor input devices and virtual reality equipment
for rehabilitation of motor impairment an issue typical of patient's of
traumatic brain injuries. We highlight how advances in procedural content
generation and player modelling can stimulate development in this area by
improving quality of rehabilitation programmes and measuring patient
performance."
Artificial Intelligence-Defined 5G Radio Access Networks,"Massive multiple-input multiple-output antenna systems, millimeter wave
communications, and ultra-dense networks have been widely perceived as the
three key enablers that facilitate the development and deployment of 5G
systems. This article discusses the intelligent agent in 5G base station which
combines sensing, learning, understanding and optimizing to facilitate these
enablers. We present a flexible, rapidly deployable, and cross-layer artificial
intelligence (AI)-based framework to enable the imminent and future demands on
5G and beyond infrastructure. We present example AI-enabled 5G use cases that
accommodate important 5G-specific capabilities and discuss the value of AI for
enabling beyond 5G network evolution."
"Artificial Intelligence-based Clinical Decision Support for COVID-19 --
  Where Art Thou?","The COVID-19 crisis has brought about new clinical questions, new workflows,
and accelerated distributed healthcare needs. While artificial intelligence
(AI)-based clinical decision support seemed to have matured, the application of
AI-based tools for COVID-19 has been limited to date. In this perspective
piece, we identify opportunities and requirements for AI-based clinical
decision support systems and highlight challenges that impact ""AI readiness""
for rapidly emergent healthcare challenges."
"A clarification of misconceptions, myths and desired status of
  artificial intelligence","The field artificial intelligence (AI) has been founded over 65 years ago.
Starting with great hopes and ambitious goals the field progressed though
various stages of popularity and received recently a revival in the form of
deep neural networks. Some problems of AI are that so far neither
'intelligence' nor the goals of AI are formally defined causing confusion when
comparing AI to other fields. In this paper, we present a perspective on the
desired and current status of AI in relation to machine learning and statistics
and clarify common misconceptions and myths. Our discussion is intended to
uncurtain the veil of vagueness surrounding AI to see its true countenance."
Meta-learning in natural and artificial intelligence,"Meta-learning, or learning to learn, has gained renewed interest in recent
years within the artificial intelligence community. However, meta-learning is
incredibly prevalent within nature, has deep roots in cognitive science and
psychology, and is currently studied in various forms within neuroscience. The
aim of this review is to recast previous lines of research in the study of
biological intelligence within the lens of meta-learning, placing these works
into a common framework. More recent points of interaction between AI and
neuroscience will be discussed, as well as interesting new directions that
arise under this perspective."
AI Techniques for Software Requirements Prioritization,"Aspects such as limited resources, frequently changing market demands, and
different technical restrictions regarding the implementation of software
requirements (features) often demand for the prioritization of requirements.
The task of prioritization is the ranking and selection of requirements that
should be included in future software releases. In this context, an intelligent
prioritization decision support is extremely important. The prioritization
approaches discussed in this paper are based on different Artificial
Intelligence (AI) techniques that can help to improve the overall quality of
requirements prioritization processes"
Is Complexity Important for Philosophy of Mind?,"Computational complexity has often been ignored in philosophy of mind, in
philosophical artificial intelligence studies. The purpose of this paper is
threefold. First and foremost, to show the importance of complexity rather than
computability in philosophical and AI problems. Second, to rephrase the notion
of computability in terms of solvability, i.e. treating computability as
non-sufficient for establishing intelligence. The Church-Turing thesis is
therefore revisited and rephrased in order to capture the ontological
background of spatial and temporal complexity. Third, to emphasize ontological
differences between different time complexities, which seem to provide a solid
base towards better understanding of artificial intelligence in general."
The Paradigm Shifts in Artificial Intelligence,"Kuhn's framework of scientific progress (Kuhn, 1962) provides a useful
framing of the paradigm shifts that have occurred in Artificial Intelligence
over the last 60 years. The framework is also useful in understanding what is
arguably a new paradigm shift in AI, signaled by the emergence of large
pre-trained systems such as GPT-3, on which conversational agents such as
ChatGPT are based. Such systems make intelligence a commoditized general
purpose technology that is configurable to applications. In this paper, I
summarize the forces that led to the rise and fall of each paradigm, and
discuss the pressing issues and risks associated with the current paradigm
shift in AI."
Artificial Intelligence Approaches for Energy Efficiency: A Review,"United Nations set Sustainable Development Goals and this paper focuses on
7th (Affordable and Clean Energy), 9th (Industries, Innovation and
Infrastructure), and 13th (Climate Action) goals. Climate change is a major
concern in our society; for this reason, a current global objective is to
reduce energy waste. This work summarizes all main approaches towards energy
efficiency using Artificial Intelligence with a particular focus on multi-agent
systems to create smart buildings. It mentions the tight relationship between
AI, especially IoT, and Big Data. It explains the application of AI to anomaly
detection in smart buildings and a possible classification of Intelligent
Energy Management Systems: Direct and Indirect. Finally, some drawbacks of AI
approaches and some possible future research focuses are proposed."
Theory of Cognitive Relativity: A Promising Paradigm for True AI,"The rise of deep learning has brought artificial intelligence (AI) to the
forefront. The ultimate goal of AI is to realize machines with human mind and
consciousness, but existing achievements mainly simulate intelligent behavior
on computer platforms. These achievements all belong to weak AI rather than
strong AI. How to achieve strong AI is not known yet in the field of
intelligence science. Currently, this field is calling for a new paradigm,
especially Theory of Cognitive Relativity (TCR). The TCR aims to summarize a
simple and elegant set of first principles about the nature of intelligence, at
least including the Principle of World's Relativity and the Principle of
Symbol's Relativity. The Principle of World's Relativity states that the
subjective world an intelligent agent can observe is strongly constrained by
the way it perceives the objective world. The Principle of Symbol's Relativity
states that an intelligent agent can use any physical symbol system to express
what it observes in its subjective world. The two principles are derived from
scientific facts and life experience. Thought experiments show that they are
important to understand high-level intelligence and necessary to establish a
scientific theory of mind and consciousness. Rather than brain-like
intelligence, the TCR indeed advocates a promising change in direction to
realize true AI, i.e. artificial general intelligence or artificial
consciousness, particularly different from humans' and animals'. Furthermore, a
TCR creed has been presented and extended to reveal the secrets of
consciousness and to guide realization of conscious machines. In the sense that
true AI could be diversely implemented in a brain-different way, the TCR would
probably drive an intelligence revolution in combination with some additional
first principles."
Ontology in Hybrid Intelligence: a concise literature review,"In a context of constant evolution and proliferation of AI technology,Hybrid
Intelligence is gaining popularity to refer a balanced coexistence between
human and artificial intelligence. The term has been extensively used in the
past two decades to define models of intelligence involving more than one
technology. This paper aims to provide (i) a concise and focused overview of
the adoption of Ontology in the broad context of Hybrid Intelligence regardless
of its definition and (ii) a critical discussion on the possible role of
Ontology to reduce the gap between human and artificial intelligence within
hybrid intelligent systems. Beside the typical benefits provided by an
effective use of ontologies, at a conceptual level, the conducted analysis has
pointed out a significant contribution of Ontology to improve quality and
accuracy, as well as a more specific role to enable extended interoperability,
system engineering and explainable/transparent systems. Additionally, an
application-oriented analysis has shown a significant role in present systems
(70+% of the cases) and, potentially, in future systems. However, despite the
relatively consistent number of papers on the topic, a proper holistic
discussion on the establishment of the next generation of hybrid-intelligent
environments with a balanced co-existence of human and artificial intelligence
is fundamentally missed in literature. Last but not the least, there is
currently a relatively low explicit focus on automatic reasoning and inference
in hybrid intelligent systems."
"Artificial Cognitively-inspired Generation of the Notion of Topological
  Group in the Context of Artificial Mathematical Intelligence","The new computational paradigm of conceptual computation has been introduced
in the research program of Artificial Mathematical Intelligence. We provide the
explicit artificial generation (or conceptual computation) for the fundamental
mathematical notion of topological groups. Specifically, we start with two
basic notions belonging to topology and abstract algebra, and we describe
recursively formal specifications in the Common Algebraic Specification
Language (CASL). The notion of conceptual blending between such conceptual
spaces can be materialized computationally in the Heterogeneous Tool Set
(HETS). The fundamental notion of topological groups is explicitly generated
through three different artificial specifications based on conceptual blending
and conceptual identification, starting with the concepts of continuous
functions and mathematical groups (described with minimal set-theoretical
conditions). This constitutes in additional heuristic evidence for the third
pillar of Artificial Mathematical Intelligence."
The relationship between Biological and Artificial Intelligence,"Intelligence can be defined as a predominantly human ability to accomplish
tasks that are generally hard for computers and animals. Artificial
Intelligence [AI] is a field attempting to accomplish such tasks with
computers. AI is becoming increasingly widespread, as are claims of its
relationship with Biological Intelligence. Often these claims are made to imply
higher chances of a given technology succeeding, working on the assumption that
AI systems which mimic the mechanisms of Biological Intelligence should be more
successful.
  In this article I will discuss the similarities and differences between AI
and the extent of our knowledge about the mechanisms of intelligence in
biology, especially within humans. I will also explore the validity of the
assumption that biomimicry in AI systems aids their advancement, and I will
argue that existing similarity to biological systems in the way Artificial
Neural Networks [ANNs] tackle tasks is due to design decisions, rather than
inherent similarity of underlying mechanisms. This article is aimed at people
who understand the basics of AI (especially ANNs), and would like to be better
able to evaluate the often wild claims about the value of biomimicry in AI."
A Survey of Hybrid Human-Artificial Intelligence for Social Computing,"Along with the development of modern computing technology and social
sciences, both theoretical research and practical applications of social
computing have been continuously extended. In particular with the boom of
artificial intelligence (AI), social computing is significantly influenced by
AI. However, the conventional technologies of AI have drawbacks in dealing with
more complicated and dynamic problems. Such deficiency can be rectified by
hybrid human-artificial intelligence (H-AI) which integrates both human
intelligence and AI into one unity, forming a new enhanced intelligence. H-AI
in dealing with social problems shows the advantages that AI can not surpass.
This paper firstly introduces the concept of H-AI. AI is the intelligence in
the transition stage of H-AI, so the latest research progresses of AI in social
computing are reviewed. Secondly, it summarizes typical challenges faced by AI
in social computing, and makes it possible to introduce H-AI to solve these
challenges. Finally, the paper proposes a holistic framework of social
computing combining with H-AI, which consists of four layers: object layer,
base layer, analysis layer, and application layer. It represents H-AI has
significant advantages over AI in solving social problems."
Towards Measuring Ethicality of an Intelligent Assistive System,"Artificial intelligence (AI) based assistive systems, so called intelligent
assistive technology (IAT) are becoming increasingly ubiquitous by each day.
IAT helps people in improving their quality of life by providing intelligent
assistance based on the provided data. A few examples of such IATs include
self-driving cars, robot assistants and smart-health management solutions.
However, the presence of such autonomous entities poses ethical challenges
concerning the stakeholders involved in using these systems. There is a lack of
research when it comes to analysing how such IAT adheres to provided ethical
regulations due to ethical, logistic and cost issues associated with such an
analysis. In the light of the above-mentioned problem statement and issues, we
present a method to measure the ethicality of an assistive system. To perform
this task, we utilised our simulation tool that focuses on modelling navigation
and assistance of Persons with Dementia (PwD) in indoor environments. By
utilising this tool, we analyse how well different assistive strategies adhere
to provided ethical regulations such as autonomy, justice and beneficence of
the stakeholders."
"Artificial Human Intelligence: The role of Humans in the Development of
  Next Generation AI","Human intelligence, the most evident and accessible form of source of
reasoning, hosted by biological hardware, has evolved and been refined over
thousands of years, positioning itself today to create new artificial forms and
preparing to self--design their evolutionary path forward. Beginning with the
advent of foundation models, the rate at which human and artificial
intelligence interact with each other has exceeded any anticipated quantitative
figures. The close engagement led both bits of intelligence to be impacted in
various ways, which naturally resulted in complex confluences that warrant
close scrutiny. In the sequel, using a novel taxonomy, we shall explore the
interplay between human and machine intelligence, focusing on the crucial role
humans play in developing ethical, responsible, and robust intelligent systems.
We briefly delve into various aspects of implementation inspired by the
mechanisms underlying neuroscience and human cognition. In addition, we propose
future perspectives, capitalizing on the advantages of symbiotic designs to
suggest a human-centered direction for next-generation developments, focusing
on the augmentation role of AI. We finalize this evolving document with some
thoughts and open questions yet to be addressed by the broader community."
"AGITB: A Signal-Level Benchmark for Evaluating Artificial General
  Intelligence","Despite remarkable progress in machine learning, current AI systems continue
to fall short of true human-like intelligence. While Large Language Models
(LLMs) excel in pattern recognition and response generation, they lack genuine
understanding - an essential hallmark of Artificial General Intelligence (AGI).
Existing AGI evaluation methods fail to offer a practical, gradual, and
informative metric. This paper introduces the Artificial General Intelligence
Test Bed (AGITB), comprising twelve rigorous tests that form a
signal-processing-level foundation for the potential emergence of cognitive
capabilities. AGITB evaluates intelligence through a model's ability to predict
binary signals across time without relying on symbolic representations or
pretraining. Unlike high-level tests grounded in language or perception, AGITB
focuses on core computational invariants reflective of biological intelligence,
such as determinism, sensitivity, and generalisation. The test bed assumes no
prior bias, operates independently of semantic meaning, and ensures
unsolvability through brute force or memorization. While humans pass AGITB by
design, no current AI system has met its criteria, making AGITB a compelling
benchmark for guiding and recognizing progress toward AGI."
"Edge Intelligence: Paving the Last Mile of Artificial Intelligence with
  Edge Computing","With the breakthroughs in deep learning, the recent years have witnessed a
booming of artificial intelligence (AI) applications and services, spanning
from personal assistant to recommendation systems to video/audio surveillance.
More recently, with the proliferation of mobile computing and
Internet-of-Things (IoT), billions of mobile and IoT devices are connected to
the Internet, generating zillions Bytes of data at the network edge. Driving by
this trend, there is an urgent need to push the AI frontiers to the network
edge so as to fully unleash the potential of the edge big data. To meet this
demand, edge computing, an emerging paradigm that pushes computing tasks and
services from the network core to the network edge, has been widely recognized
as a promising solution. The resulted new inter-discipline, edge AI or edge
intelligence, is beginning to receive a tremendous amount of interest. However,
research on edge intelligence is still in its infancy stage, and a dedicated
venue for exchanging the recent advances of edge intelligence is highly desired
by both the computer system and artificial intelligence communities. To this
end, we conduct a comprehensive survey of the recent research efforts on edge
intelligence. Specifically, we first review the background and motivation for
artificial intelligence running at the network edge. We then provide an
overview of the overarching architectures, frameworks and emerging key
technologies for deep learning model towards training/inference at the network
edge. Finally, we discuss future research opportunities on edge intelligence.
We believe that this survey will elicit escalating attentions, stimulate
fruitful discussions and inspire further research ideas on edge intelligence."
"From Statistical Relational to Neurosymbolic Artificial Intelligence: a
  Survey","This survey explores the integration of learning and reasoning in two
different fields of artificial intelligence: neurosymbolic and statistical
relational artificial intelligence. Neurosymbolic artificial intelligence
(NeSy) studies the integration of symbolic reasoning and neural networks, while
statistical relational artificial intelligence (StarAI) focuses on integrating
logic with probabilistic graphical models. This survey identifies seven shared
dimensions between these two subfields of AI. These dimensions can be used to
characterize different NeSy and StarAI systems. They are concerned with (1) the
approach to logical inference, whether model or proof-based; (2) the syntax of
the used logical theories; (3) the logical semantics of the systems and their
extensions to facilitate learning; (4) the scope of learning, encompassing
either parameter or structure learning; (5) the presence of symbolic and
subsymbolic representations; (6) the degree to which systems capture the
original logic, probabilistic, and neural paradigms; and (7) the classes of
learning tasks the systems are applied to. By positioning various NeSy and
StarAI systems along these dimensions and pointing out similarities and
differences between them, this survey contributes fundamental concepts for
understanding the integration of learning and reasoning."
"Explainable Artificial Intelligence Applications in Cyber Security:
  State-of-the-Art in Research","This survey presents a comprehensive review of current literature on
Explainable Artificial Intelligence (XAI) methods for cyber security
applications. Due to the rapid development of Internet-connected systems and
Artificial Intelligence in recent years, Artificial Intelligence including
Machine Learning (ML) and Deep Learning (DL) has been widely utilized in the
fields of cyber security including intrusion detection, malware detection, and
spam filtering. However, although Artificial Intelligence-based approaches for
the detection and defense of cyber attacks and threats are more advanced and
efficient compared to the conventional signature-based and rule-based cyber
security strategies, most ML-based techniques and DL-based techniques are
deployed in the black-box manner, meaning that security experts and customers
are unable to explain how such procedures reach particular conclusions. The
deficiencies of transparency and interpretability of existing Artificial
Intelligence techniques would decrease human users' confidence in the models
utilized for the defense against cyber attacks, especially in current
situations where cyber attacks become increasingly diverse and complicated.
Therefore, it is essential to apply XAI in the establishment of cyber security
models to create more explainable models while maintaining high accuracy and
allowing human users to comprehend, trust, and manage the next generation of
cyber defense mechanisms. Although there are papers reviewing Artificial
Intelligence applications in cyber security areas and the vast literature on
applying XAI in many fields including healthcare, financial services, and
criminal justice, the surprising fact is that there are currently no survey
research articles that concentrate on XAI applications in cyber security."
Ethics and Artificial Intelligence Adoption,"In recent years, we have witnessed a marked development and growth in
Artificial Intelligence. The growth of the data volume generated by sensors and
machines, combined with the information flow resulting from the user actions on
the Internet, with high investments of the governments and the companies in
this area, provided the practice and developed the algorithms of the Artificial
Intelligence However, the people, in general, started to feel a particular fear
regarding the security and privacy of their data and the theme of the
Artificial Intelligence Ethics began to be discussed more regularly. The
investigation aim of this work is to understand the possibility of adopting
Artificial Intelligence nowadays in our society, having, as a mandatory
assumption, Ethics and respect towards data and people's privacy. With that
purpose in mind, a model has been created, mainly supported by the theories
that were used to create the model. The suggested model has been tested and
validated through Structural equation modeling based on data taken back from
the respondents' answers to the questionnaire online: 237 answers, mainly from
the Investigation Technologies area. The results obtained enabled the
validation of seven of the nine investigation hypotheses of the proposed model.
It was impossible to confirm any association between the Social Influence
construct and the variables of Behavioral Intention and the Use of Artificial
Intelligence. The aim of this work was accomplished once the investigation
theme was validated and proved that it is possible to adopt Artificial
Intelligence in our society, using the Attitude Towards Ethical Behavioral
construct as the mainstay of the model."
"LegalScore: Development of a Benchmark for Evaluating AI Models in Legal
  Career Exams in Brazil","This research introduces LegalScore, a specialized index for assessing how
generative artificial intelligence models perform in a selected range of career
exams that require a legal background in Brazil. The index evaluates fourteen
different types of artificial intelligence models' performance, from
proprietary to open-source models, in answering objective questions applied to
these exams. The research uncovers the response of the models when applying
English-trained large language models to Brazilian legal contexts, leading us
to reflect on the importance and the need for Brazil-specific training data in
generative artificial intelligence models. Performance analysis shows that
while proprietary and most known models achieved better results overall, local
and smaller models indicated promising performances due to their Brazilian
context alignment in training. By establishing an evaluation framework with
metrics including accuracy, confidence intervals, and normalized scoring,
LegalScore enables systematic assessment of artificial intelligence performance
in legal examinations in Brazil. While the study demonstrates artificial
intelligence's potential value for exam preparation and question development,
it concludes that significant improvements are needed before AI can match human
performance in advanced legal assessments. The benchmark creates a foundation
for continued research, highlighting the importance of local adaptation in
artificial intelligence development."
"Intelligence Sequencing and the Path-Dependence of Intelligence
  Evolution: AGI-First vs. DCI-First as Irreversible Attractors","The trajectory of intelligence evolution is often framed around the emergence
of artificial general intelligence (AGI) and its alignment with human values.
This paper challenges that framing by introducing the concept of intelligence
sequencing: the idea that the order in which AGI and decentralized collective
intelligence (DCI) emerge determines the long-term attractor basin of
intelligence. Using insights from dynamical systems, evolutionary game theory,
and network models, it argues that intelligence follows a path-dependent,
irreversible trajectory. Once development enters a centralized (AGI-first) or
decentralized (DCI-first) regime, transitions become structurally infeasible
due to feedback loops and resource lock-in. Intelligence attractors are modeled
in functional state space as the co-navigation of conceptual and adaptive
fitness spaces. Early-phase structuring constrains later dynamics, much like
renormalization in physics. This has major implications for AI safety:
traditional alignment assumes AGI will emerge and must be controlled after the
fact, but this paper argues that intelligence sequencing is more foundational.
If AGI-first architectures dominate before DCI reaches critical mass,
hierarchical monopolization and existential risk become locked in. If DCI-first
emerges, intelligence stabilizes around decentralized cooperative equilibrium.
The paper further explores whether intelligence structurally biases itself
toward an attractor based on its self-modeling method -- externally imposed
axioms (favoring AGI) vs. recursive internal visualization (favoring DCI).
Finally, it proposes methods to test this theory via simulations, historical
lock-in case studies, and intelligence network analysis. The findings suggest
that intelligence sequencing is a civilizational tipping point: determining
whether the future is shaped by unbounded competition or unbounded cooperation."
"Compression, The Fermi Paradox and Artificial Super-Intelligence","The following briefly discusses possible difficulties in communication with
and control of an AGI (artificial general intelligence), building upon an
explanation of The Fermi Paradox and preceding work on symbol emergence and
artificial general intelligence. The latter suggests that to infer what someone
means, an agent constructs a rationale for the observed behaviour of others.
Communication then requires two agents labour under similar compulsions and
have similar experiences (construct similar solutions to similar tasks). Any
non-human intelligence may construct solutions such that any rationale for
their behaviour (and thus the meaning of their signals) is outside the scope of
what a human is inclined to notice or comprehend. Further, the more compressed
a signal, the closer it will appear to random noise. Another intelligence may
possess the ability to compress information to the extent that, to us, their
signals would appear indistinguishable from noise (an explanation for The Fermi
Paradox). To facilitate predictive accuracy an AGI would tend to more
compressed representations of the world, making any rationale for their
behaviour more difficult to comprehend for the same reason. Communication with
and control of an AGI may subsequently necessitate not only human-like
compulsions and experiences, but imposed cognitive impairment."
"Lateralization in Agents' Decision Making: Evidence of Benefits/Costs
  from Artificial Intelligence","Lateralization is ubiquitous in vertebrate brains which, as well as its role
in locomotion, is considered an important factor in biological intelligence.
Lateralization has been associated with both poor and good performance. It has
been hypothesized that lateralization has benefits that may counterbalance its
costs. Given that lateralization is ubiquitous, it likely has advantages that
can benefit artificial intelligence. In turn, lateralized artificial
intelligent systems can be used as tools to advance the understanding of
lateralization in biological intelligence. Recently lateralization has been
incorporated into artificially intelligent systems to solve complex problems in
computer vision and navigation domains. Here we describe and test two novel
lateralized artificial intelligent systems that simultaneously represent and
address given problems at constituent and holistic levels. The experimental
results demonstrate that the lateralized systems outperformed state-of-the-art
non-lateralized systems in resolving complex problems. The advantages arise
from the abilities, (i) to represent an input signal at both the constituent
level and holistic level simultaneously, such that the most appropriate
viewpoint controls the system; (ii) to avoid extraneous computations by
generating excite and inhibit signals. The computational costs associated with
the lateralized AI systems are either less than the conventional AI systems or
countered by providing better solutions."
"Artificial Collective Intelligence Engineering: a Survey of Concepts and
  Perspectives","Collectiveness is an important property of many systems--both natural and
artificial. By exploiting a large number of individuals, it is often possible
to produce effects that go far beyond the capabilities of the smartest
individuals, or even to produce intelligent collective behaviour out of
not-so-intelligent individuals. Indeed, collective intelligence, namely the
capability of a group to act collectively in a seemingly intelligent way, is
increasingly often a design goal of engineered computational systems--motivated
by recent techno-scientific trends like the Internet of Things, swarm robotics,
and crowd computing, just to name a few. For several years, the collective
intelligence observed in natural and artificial systems has served as a source
of inspiration for engineering ideas, models, and mechanisms. Today, artificial
and computational collective intelligence are recognised research topics,
spanning various techniques, kinds of target systems, and application domains.
However, there is still a lot of fragmentation in the research panorama of the
topic within computer science, and the verticality of most communities and
contributions makes it difficult to extract the core underlying ideas and
frames of reference. The challenge is to identify, place in a common structure,
and ultimately connect the different areas and methods addressing intelligent
collectives. To address this gap, this paper considers a set of broad scoping
questions providing a map of collective intelligence research, mostly by the
point of view of computer scientists and engineers. Accordingly, it covers
preliminary notions, fundamental concepts, and the main research perspectives,
identifying opportunities and challenges for researchers on artificial and
computational collective intelligence engineering."
Artificial Intelligence and its Role in Near Future,"AI technology has a long history which is actively and constantly changing
and growing. It focuses on intelligent agents, which contain devices that
perceive the environment and based on which takes actions in order to maximize
goal success chances. In this paper, we will explain the modern AI basics and
various representative applications of AI. In the context of the modern
digitalized world, AI is the property of machines, computer programs, and
systems to perform the intellectual and creative functions of a person,
independently find ways to solve problems, be able to draw conclusions and make
decisions. Most artificial intelligence systems have the ability to learn,
which allows people to improve their performance over time. The recent research
on AI tools, including machine learning, deep learning and predictive analysis
intended toward increasing the planning, learning, reasoning, thinking and
action taking ability. Based on which, the proposed research intends towards
exploring on how the human intelligence differs from the artificial
intelligence. Moreover, we critically analyze what AI of today is capable of
doing, why it still cannot reach human intelligence and what are the open
challenges existing in front of AI to reach and outperform human level of
intelligence. Furthermore, it will explore the future predictions for
artificial intelligence and based on which potential solution will be
recommended to solve it within next decades."
"The Entropy of Artificial Intelligence and a Case Study of AlphaZero
  from Shannon's Perspective","The recently released AlphaZero algorithm achieves superhuman performance in
the games of chess, shogi and Go, which raises two open questions. Firstly, as
there is a finite number of possibilities in the game, is there a quantifiable
intelligence measurement for evaluating intelligent systems, e.g. AlphaZero?
Secondly, AlphaZero introduces sophisticated reinforcement learning and
self-play to efficiently encode the possible states, is there a simple
information-theoretic model to represent the learning process and offer more
insights in fostering strong AI systems?
  This paper explores the above two questions by proposing a simple variance of
Shannon's communication model, the concept of intelligence entropy and the
Unified Intelligence-Communication Model is proposed, which provide an
information-theoretic metric for investigating the intelligence level and also
provide an bound for intelligent agents in the form of Shannon's capacity,
namely, the intelligence capacity. This paper then applies the concept and
model to AlphaZero as a case study and explains the learning process of
intelligent agent as turbo-like iterative decoding, so that the learning
performance of AlphaZero may be quantitatively evaluated. Finally, conclusions
are provided along with theoretical and practical remarks."
An Artificial Neural Network Functionalized by Evolution,"The topology of artificial neural networks has a significant effect on their
performance. Characterizing efficient topology is a field of promising research
in Artificial Intelligence. However, it is not a trivial task and it is mainly
experimented on through convolutional neural networks. We propose a hybrid
model which combines the tensor calculus of feed-forward neural networks with
Pseudo-Darwinian mechanisms. This allows for finding topologies that are well
adapted for elaboration of strategies, control problems or pattern recognition
tasks. In particular, the model can provide adapted topologies at early
evolutionary stages, and 'structural convergence', which can found applications
in robotics, big-data and artificial life."
"The Linguistic Blind Spot of Value-Aligned Agency, Natural and
  Artificial","The value-alignment problem for artificial intelligence (AI) asks how we can
ensure that the 'values' (i.e., objective functions) of artificial systems are
aligned with the values of humanity. In this paper, I argue that linguistic
communication (natural language) is a necessary condition for robust value
alignment. I discuss the consequences that the truth of this claim would have
for research programmes that attempt to ensure value alignment for AI systems;
or, more loftily, designing robustly beneficial or ethical artificial agents."
A Collection of Definitions of Intelligence,"This paper is a survey of a large number of informal definitions of
``intelligence'' that the authors have collected over the years. Naturally,
compiling a complete list would be impossible as many definitions of
intelligence are buried deep inside articles and books. Nevertheless, the
70-odd definitions presented here are, to the authors' knowledge, the largest
and most well referenced collection there is."
"A framework: Cluster detection and multidimensional visualization of
  automated data mining using intelligent agents","Data Mining techniques plays a vital role like extraction of required
knowledge, finding unsuspected information to make strategic decision in a
novel way which in term understandable by domain experts. A generalized frame
work is proposed by considering non - domain experts during mining process for
better understanding, making better decision and better finding new patters in
case of selecting suitable data mining techniques based on the user profile by
means of intelligent agents. KEYWORDS: Data Mining Techniques, Intelligent
Agents, User Profile, Multidimensional Visualization, Knowledge Discovery."
The Lovelace 2.0 Test of Artificial Creativity and Intelligence,"Observing that the creation of certain types of artistic artifacts
necessitate intelligence, we present the Lovelace 2.0 Test of creativity as an
alternative to the Turing Test as a means of determining whether an agent is
intelligent. The Lovelace 2.0 Test builds off prior tests of creativity and
additionally provides a means of directly comparing the relative intelligence
of different agents."
"Intelligent Biohybrid Neurotechnologies: Are They Really What They
  Claim?","In the era of intelligent biohybrid neurotechnologies for brain repair, new
fanciful terms are appearing in the scientific dictionary to define what has so
far been unimaginable. As the emerging neurotechnologies are becoming
increasingly polyhedral and sophisticated, should we talk about evolution and
rank the intelligence of these devices?"
"Design and development of a software system for swarm intelligence based
  research studies","This paper introduce a software system including widely-used Swarm
Intelligence algorithms or approaches to be used for the related scientific
research studies associated with the subject area. The programmatic
infrastructure of the system allows working on a fast, easy-to-use, interactive
platform to perform Swarm Intelligence based studies in a more effective,
efficient and accurate way. In this sense, the system employs all of the
necessary controls for the algorithms and it ensures an interactive platform on
which computer users can perform studies on a wide spectrum of solution
approaches associated with simple and also more advanced problems."
A Survey of Question Answering for Math and Science Problem,"Turing test was long considered the measure for artificial intelligence. But
with the advances in AI, it has proved to be insufficient measure. We can now
aim to mea- sure machine intelligence like we measure human intelligence. One
of the widely accepted measure of intelligence is standardized math and science
test. In this paper, we explore the progress we have made towards the goal of
making a machine smart enough to pass the standardized test. We see the
challenges and opportunities posed by the domain, and note that we are quite
some ways from actually making a system as smart as a even a middle school
scholar."
"Challenges and Characteristics of Intelligent Autonomy for Internet of
  Battle Things in Highly Adversarial Environments","Numerous, artificially intelligent, networked things will populate the
battlefield of the future, operating in close collaboration with human
warfighters, and fighting as teams in highly adversarial environments. This
paper explores the characteristics, capabilities and intelligence required of
such a network of intelligent things and humans - Internet of Battle Things
(IOBT). It will experience unique challenges that are not yet well addressed by
the current generation of AI and machine learning."
"Wireless 2.0: Towards an Intelligent Radio Environment Empowered by
  Reconfigurable Meta-Surfaces and Artificial Intelligence","We introduce ""Wireless 2.0"": The future generation of wireless communication
networks, where the radio environment becomes controllable, programmable, and
intelligent by leveraging the emerging technologies of reconfigurable
metasurfaces and artificial intelligence (AI). This paper, in particular, puts
the emphasis on AI-based computational methods and commence with an overview of
the concept of intelligent radio environments based on reconfigurable
meta-surfaces. Later we elaborate on data management aspects, the requirements
of supervised learning by examples, and the paradigm of reinforcement learning
(RL) to learn by acting. Finally, we highlight numerous open challenges and
research directions."
Symmetry as an Organizing Principle for Geometric Intelligence,"The exploration of geometrical patterns stimulates imagination and encourages
abstract reasoning which is a distinctive feature of human intelligence. In
cognitive science, Gestalt principles such as symmetry have often explained
significant aspects of human perception. We present a computational technique
for building artificial intelligence (AI) agents that use symmetry as the
organizing principle for addressing Dehaene's test of geometric intelligence
\cite{dehaene2006core}. The performance of our model is on par with extant AI
models of problem solving on the Dehaene's test and seems correlated with some
elements of human behavior on the same test."
"CTI4AI: Threat Intelligence Generation and Sharing after Red Teaming AI
  Models","As the practicality of Artificial Intelligence (AI) and Machine Learning (ML)
based techniques grow, there is an ever increasing threat of adversarial
attacks. There is a need to red team this ecosystem to identify system
vulnerabilities, potential threats, characterize properties that will enhance
system robustness, and encourage the creation of effective defenses. A
secondary need is to share this AI security threat intelligence between
different stakeholders like, model developers, users, and AI/ML security
professionals. In this paper, we create and describe a prototype system CTI4AI,
to overcome the need to methodically identify and share AI/ML specific
vulnerabilities and threat intelligence."
Ambient Technology & Intelligence,"Ambient intelligence refers to technological enhanced electronic environments
which are both responsive and sensitive to the presence of people within their
environment. Environments that are integrated with ambient intelligence tends
to adapt to the needs of individuals within the environment in an unobtrusive
manner in such a way as to enhance everyday life thereby making interaction
with technology extremely seamless and well integrated. This capability was
made possible because it is a concept that combines several key technologies
such as IoT (Internet of Things) technology, sensor technology, AI (Artificial
Intelligence), and advanced human-computer interaction all embedded and
integrated together with the environment."
Open Problems in Universal Induction & Intelligence,"Specialized intelligent systems can be found everywhere: finger print,
handwriting, speech, and face recognition, spam filtering, chess and other game
programs, robots, et al. This decade the first presumably complete mathematical
theory of artificial intelligence based on universal
induction-prediction-decision-action has been proposed. This
information-theoretic approach solidifies the foundations of inductive
inference and artificial intelligence. Getting the foundations right usually
marks a significant progress and maturing of a field. The theory provides a
gold standard and guidance for researchers working on intelligent algorithms.
The roots of universal induction have been laid exactly half-a-century ago and
the roots of universal intelligence exactly one decade ago. So it is timely to
take stock of what has been achieved and what remains to be done. Since there
are already good recent surveys, I describe the state-of-the-art only in
passing and refer the reader to the literature. This article concentrates on
the open problems in universal induction and its extension to universal
intelligence."
"Research on emotionally intelligent dialogue generation based on
  automatic dialogue system","Automated dialogue systems are important applications of artificial
intelligence, and traditional systems struggle to understand user emotions and
provide empathetic feedback. This study integrates emotional intelligence
technology into automated dialogue systems and creates a dialogue generation
model with emotional intelligence through deep learning and natural language
processing techniques. The model can detect and understand a wide range of
emotions and specific pain signals in real time, enabling the system to provide
empathetic interaction. By integrating the results of the study ""Can artificial
intelligence detect pain and express pain empathy?"", the model's ability to
understand the subtle elements of pain empathy has been enhanced, setting
higher standards for emotional intelligence dialogue systems. The project aims
to provide theoretical understanding and practical suggestions to integrate
advanced emotional intelligence capabilities into dialogue systems, thereby
improving user experience and interaction quality."
An Entropy-based Measure of Intelligence Degree of System Structures,"In this paper, we investigate how to measure the intelligence of systems
under specific structures. Two indicators are adopted to characterize the
intelligence of a given structure, namely the function diversity of the
structure, and the ability to generate order under specific environments. A
measure of intelligence degree is proposed, with which the intelligence degree
of several basic structures is calculated. It is shown that some structures are
indeed ""smarter"" than the others under the proposed measure. The results add a
possible way of revealing the evolution mechanism of natural life and
constructing life-like structures with high intelligence degree."
"Integration of cognitive tasks into artificial general intelligence test
  for large models","During the evolution of large models, performance evaluation is necessarily
performed to assess their capabilities and ensure safety before practical
application. However, current model evaluations mainly rely on specific tasks
and datasets, lacking a united framework for assessing the multidimensional
intelligence of large models. In this perspective, we advocate for a
comprehensive framework of cognitive science-inspired artificial general
intelligence (AGI) tests, aimed at fulfilling the testing needs of large models
with enhanced capabilities. The cognitive science-inspired AGI tests encompass
the full spectrum of intelligence facets, including crystallized intelligence,
fluid intelligence, social intelligence, and embodied intelligence. To assess
the multidimensional intelligence of large models, the AGI tests consist of a
battery of well-designed cognitive tests adopted from human intelligence tests,
and then naturally encapsulates into an immersive virtual community. We propose
increasing the complexity of AGI testing tasks commensurate with advancements
in large models and emphasizing the necessity for the interpretation of test
results to avoid false negatives and false positives. We believe that cognitive
science-inspired AGI tests will effectively guide the targeted improvement of
large models in specific dimensions of intelligence and accelerate the
integration of large models into human society."
Intelligence Primer,"Intelligence is a fundamental part of all living things, as well as the
foundation for Artificial Intelligence. In this primer we explore the ideas
associated with intelligence and, by doing so, understand the implications and
constraints and potentially outline the capabilities of future systems.
Artificial Intelligence, in the form of Machine Learning, has already had a
significant impact on our lives. As an exploration, we journey into different
parts of intelligence that appear essential. We hope that people find this
helpful in determining the future. Also, during the exploration, we hope to
create new thought-provoking questions. Intelligence is not a single weighable
quantity but a subject that spans Biology, Physics, Philosophy, Cognitive
Science, Neuroscience, Psychology, and Computer Science. The historian Yuval
Noah Harari pointed out that engineers and scientists in the future will have
to broaden their understandings to include disciplines such as Psychology,
Philosophy, and Ethics. Fiction writers have long portrayed engineers and
scientists as deficient in these areas. Today, in modern society, the emergence
of Artificial Intelligence and legal requirements act as forcing functions to
push these broader subjects into the foreground. We start with an introduction
to intelligence and move quickly to more profound thoughts and ideas. We call
this a Life, the Universe, and Everything primer, after the famous science
fiction book by Douglas Adams. Forty-two may be the correct answer, but what
are the questions?"
Genes in Intelligent Agents,"The genes in nature give the lives on earth the current biological
intelligence through transmission and accumulation over billions of years.
Inspired by the biological intelligence, artificial intelligence (AI) has
devoted to building the machine intelligence. Although it has achieved thriving
successes, the machine intelligence still lags far behind the biological
intelligence. The reason may lie in that animals are born with some
intelligence encoded in their genes, but machines lack such intelligence and
learn from scratch. Inspired by the genes of animals, we define the ``genes''
of machines named as the ``learngenes'' and propose the Genetic Reinforcement
Learning (GRL). GRL is a computational framework that simulates the evolution
of organisms in reinforcement learning (RL) and leverages the learngenes to
learn and evolve the intelligence agents. Leveraging GRL, we first show that
the learngenes take the form of the fragments of the agents' neural networks
and can be inherited across generations. Second, we validate that the
learngenes can transfer ancestral experience to the agents and bring them
instincts and strong learning abilities. Third, we justify the Lamarckian
inheritance of the intelligent agents and the continuous evolution of the
learngenes. Overall, the learngenes have taken the machine intelligence one
more step toward the biological intelligence."
PAMS: Platform for Artificial Market Simulations,"This paper presents a new artificial market simulation platform, PAMS:
Platform for Artificial Market Simulations. PAMS is developed as a Python-based
simulator that is easily integrated with deep learning and enabling various
simulation that requires easy users' modification. In this paper, we
demonstrate PAMS effectiveness through a study using agents predicting future
prices by deep learning."
The Computational Theory of Intelligence: Information Entropy,"This paper presents an information theoretic approach to the concept of
intelligence in the computational sense. We introduce a probabilistic framework
from which computational intelligence is shown to be an entropy minimizing
process at the local level. Using this new scheme, we develop a simple data
driven clustering example and discuss its applications."
"A Model for Combination of External and Internal Stimuli in the Action
  Selection of an Autonomous Agent","This paper proposes a model for combination of external and internal stimuli
for the action selection in an autonomous agent, based in an action selection
mechanism previously proposed by the authors. This combination model includes
additive and multiplicative elements, which allows to incorporate new
properties, which enhance the action selection. A given parameter a, which is
part of the proposed model, allows to regulate the degree of dependence of the
observed external behaviour from the internal states of the entity."
Artificial Intelligence Techniques for Steam Generator Modelling,"This paper investigates the use of different Artificial Intelligence methods
to predict the values of several continuous variables from a Steam Generator.
The objective was to determine how the different artificial intelligence
methods performed in making predictions on the given dataset. The artificial
intelligence methods evaluated were Neural Networks, Support Vector Machines,
and Adaptive Neuro-Fuzzy Inference Systems. The types of neural networks
investigated were Multi-Layer Perceptions, and Radial Basis Function. Bayesian
and committee techniques were applied to these neural networks. Each of the AI
methods considered was simulated in Matlab. The results of the simulations
showed that all the AI methods were capable of predicting the Steam Generator
data reasonably accurately. However, the Adaptive Neuro-Fuzzy Inference system
out performed the other methods in terms of accuracy and ease of
implementation, while still achieving a fast execution time as well as a
reasonable training time."
"Analysis of Microarray Data using Artificial Intelligence Based
  Techniques","Microarray is one of the essential technologies used by the biologist to
measure genome-wide expression levels of genes in a particular organism under
some particular conditions or stimuli. As microarrays technologies have become
more prevalent, the challenges of analyzing these data for getting better
insight about biological processes have essentially increased. Due to
availability of artificial intelligence based sophisticated computational
techniques, such as artificial neural networks, fuzzy logic, genetic
algorithms, and many other nature-inspired algorithms, it is possible to
analyse microarray gene expression data in more better way. Here, we reviewed
artificial intelligence based techniques for the analysis of microarray gene
expression data. Further, challenges in the field and future work direction
have also been suggested."
"Blue Sky Ideas in Artificial Intelligence Education from the EAAI 2017
  New and Future AI Educator Program","The 7th Symposium on Educational Advances in Artificial Intelligence
(EAAI'17, co-chaired by Sven Koenig and Eric Eaton) launched the EAAI New and
Future AI Educator Program to support the training of early-career university
faculty, secondary school faculty, and future educators (PhD candidates or
postdocs who intend a career in academia). As part of the program, awardees
were asked to address one of the following ""blue sky"" questions:
  * How could/should Artificial Intelligence (AI) courses incorporate ethics
into the curriculum?
  * How could we teach AI topics at an early undergraduate or a secondary
school level?
  * AI has the potential for broad impact to numerous disciplines. How could we
make AI education more interdisciplinary, specifically to benefit
non-engineering fields?
  This paper is a collection of their responses, intended to help motivate
discussion around these issues in AI education."
Artificial Intelligence Based Malware Analysis,"Artificial intelligence methods have often been applied to perform specific
functions or tasks in the cyber-defense realm. However, as adversary methods
become more complex and difficult to divine, piecemeal efforts to understand
cyber-attacks, and malware-based attacks in particular, are not providing
sufficient means for malware analysts to understand the past, present and
future characteristics of malware.
  In this paper, we present the Malware Analysis and Attributed using Genetic
Information (MAAGI) system. The underlying idea behind the MAAGI system is that
there are strong similarities between malware behavior and biological organism
behavior, and applying biologically inspired methods to corpora of malware can
help analysts better understand the ecosystem of malware attacks. Due to the
sophistication of the malware and the analysis, the MAAGI system relies heavily
on artificial intelligence techniques to provide this capability. It has
already yielded promising results over its development life, and will hopefully
inspire more integration between the artificial intelligence and cyber--defense
communities."
Knowledge Transfer Between Artificial Intelligence Systems,"We consider the fundamental question: how a legacy ""student"" Artificial
Intelligent (AI) system could learn from a legacy ""teacher"" AI system or a
human expert without complete re-training and, most importantly, without
requiring significant computational resources. Here ""learning"" is understood as
an ability of one system to mimic responses of the other and vice-versa. We
call such learning an Artificial Intelligence knowledge transfer. We show that
if internal variables of the ""student"" Artificial Intelligent system have the
structure of an $n$-dimensional topological vector space and $n$ is
sufficiently high then, with probability close to one, the required knowledge
transfer can be implemented by simple cascades of linear functionals. In
particular, for $n$ sufficiently large, with probability close to one, the
""student"" system can successfully and non-iteratively learn $k\ll n$ new
examples from the ""teacher"" (or correct the same number of mistakes) at the
cost of two additional inner products. The concept is illustrated with an
example of knowledge transfer from a pre-trained convolutional neural network
to a simple linear classifier with HOG features."
"Narrow Artificial Intelligence with Machine Learning for Real-Time
  Estimation of a Mobile Agents Location Using Hidden Markov Models","We propose to use a supervised machine learning technique to track the
location of a mobile agent in real time. Hidden Markov Models are used to build
artificial intelligence that estimates the unknown position of a mobile target
moving in a defined environment. This narrow artificial intelligence performs
two distinct tasks. First, it provides real-time estimation of the mobile
agent's position using the forward algorithm. Second, it uses the Baum-Welch
algorithm as a statistical learning tool to gain knowledge of the mobile
target. Finally, an experimental environment is proposed, namely a video game
that we use to test our artificial intelligence. We present statistical and
graphical results to illustrate the efficiency of our method."
"A Century Long Commitment to Assessing Artificial Intelligence and its
  Impact on Society","In September 2016, Stanford's ""One Hundred Year Study on Artificial
Intelligence"" project (AI100) issued the first report of its planned long-term
periodic assessment of artificial intelligence (AI) and its impact on society.
The report, entitled ""Artificial Intelligence and Life in 2030,"" examines eight
domains of typical urban settings on which AI is likely to have impact over the
coming years: transportation, home and service robots, healthcare, education,
public safety and security, low-resource communities, employment and workplace,
and entertainment. It aims to provide the general public with a scientifically
and technologically accurate portrayal of the current state of AI and its
potential and to help guide decisions in industry and governments, as well as
to inform research and development in the field. This article by the chair of
the 2016 Study Panel and the inaugural chair of the AI100 Standing Committee
describes the origins of this ambitious longitudinal study, discusses the
framing of the inaugural report, and presents the report's main findings. It
concludes with a brief description of the AI100 project's ongoing efforts and
planned next steps."
Building Ethics into Artificial Intelligence,"As artificial intelligence (AI) systems become increasingly ubiquitous, the
topic of AI governance for ethical decision-making by AI has captured public
imagination. Within the AI research community, this topic remains less familiar
to many researchers. In this paper, we complement existing surveys, which
largely focused on the psychological, social and legal discussions of the
topic, with an analysis of recent advances in technical solutions for AI
governance. By reviewing publications in leading AI conferences including AAAI,
AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four
areas: 1) exploring ethical dilemmas; 2) individual ethical decision
frameworks; 3) collective ethical decision frameworks; and 4) ethics in
human-AI interactions. We highlight the intuitions and key techniques used in
each approach, and discuss promising future research directions towards
successful integration of ethical AI systems into human societies."
Linking Artificial Intelligence Principles,"Artificial Intelligence principles define social and ethical considerations
to develop future AI. They come from research institutes, government
organizations and industries. All versions of AI principles are with different
considerations covering different perspectives and making different emphasis.
None of them can be considered as complete and can cover the rest AI principle
proposals. Here we introduce LAIP, an effort and platform for linking and
analyzing different Artificial Intelligence Principles. We want to explicitly
establish the common topics and links among AI Principles proposed by different
organizations and investigate on their uniqueness. Based on these efforts, for
the long-term future of AI, instead of directly adopting any of the AI
principles, we argue for the necessity of incorporating various AI Principles
into a comprehensive framework and focusing on how they can interact and
complete each other."
"Does the ""Artificial Intelligence Clinician"" learn optimal treatment
  strategies for sepsis in intensive care?","From 2017 to 2018 the number of scientific publications found via PubMed
search using the keyword ""Machine Learning"" increased by 46% (4,317 to 6,307).
The results of studies involving machine learning, artificial intelligence
(AI), and big data have captured the attention of healthcare practitioners,
healthcare managers, and the public at a time when Western medicine grapples
with unmitigated cost increases and public demands for accountability. The
complexity involved in healthcare applications of machine learning and the size
of the associated data sets has afforded many researchers an uncontested
opportunity to satisfy these demands with relatively little oversight. In a
recent Nature Medicine article, ""The Artificial Intelligence Clinician learns
optimal treatment strategies for sepsis in intensive care,"" Komorowski and his
coauthors propose methods to train an artificial intelligence clinician to
treat sepsis patients with vasopressors and IV fluids. In this post, we will
closely examine the claims laid out in this paper. In particular, we will study
the individual treatment profiles suggested by their AI Clinician to gain
insight into how their AI Clinician intends to treat patients on an individual
level."
"Artificial Intelligence : from Research to Application ; the Upper-Rhine
  Artificial Intelligence Symposium (UR-AI 2019)","The TriRhenaTech alliance universities and their partners presented their
competences in the field of artificial intelligence and their cross-border
cooperations with the industry at the tri-national conference 'Artificial
Intelligence : from Research to Application' on March 13th, 2019 in Offenburg.
The TriRhenaTech alliance is a network of universities in the Upper Rhine
Trinational Metropolitan Region comprising of the German universities of
applied sciences in Furtwangen, Kaiserslautern, Karlsruhe, and Offenburg, the
Baden-Wuerttemberg Cooperative State University Loerrach, the French university
network Alsace Tech (comprised of 14 'grandes \'ecoles' in the fields of
engineering, architecture and management) and the University of Applied
Sciences and Arts Northwestern Switzerland. The alliance's common goal is to
reinforce the transfer of knowledge, research, and technology, as well as the
cross-border mobility of students."
"Survey of Artificial Intelligence for Card Games and Its Application to
  the Swiss Game Jass","In the last decades we have witnessed the success of applications of
Artificial Intelligence to playing games. In this work we address the
challenging field of games with hidden information and card games in
particular. Jass is a very popular card game in Switzerland and is closely
connected with Swiss culture. To the best of our knowledge, performances of
Artificial Intelligence agents in the game of Jass do not outperform top
players yet. Our contribution to the community is two-fold. First, we provide
an overview of the current state-of-the-art of Artificial Intelligence methods
for card games in general. Second, we discuss their application to the use-case
of the Swiss card game Jass. This paper aims to be an entry point for both
seasoned researchers and new practitioners who want to join in the Jass
challenge."
The Task Analysis Cell Assembly Perspective,"An entirely novel synthesis combines the applied cognitive psychology of a
task analytic approach with a neural cell assembly perspective that models both
brain and mind function during task performance; similar cell assemblies could
be implemented as an artificially intelligent neural network. A simplified cell
assembly model is introduced and this leads to several new representational
formats that, in combination, are demonstrated as suitable for analysing tasks.
The advantages of using neural models are exposed and compared with previous
research that has used symbolic artificial intelligence production systems,
which make no attempt to model neurophysiology. For cognitive scientists, the
approach provides an easy and practical introduction to thinking about brains,
minds and artificial intelligence in terms of cell assemblies. In the future,
subsequent developments have the potential to lead to a new, general theory of
psychology and neurophysiology, supported by cell assembly based artificial
intelligences."
The Transformative Potential of Artificial Intelligence,"The terms 'human-level artificial intelligence' and 'artificial general
intelligence' are widely used to refer to the possibility of advanced
artificial intelligence (AI) with potentially extreme impacts on society. These
terms are poorly defined and do not necessarily indicate what is most important
with respect to future societal impacts. We suggest that the term
'transformative AI' is a helpful alternative, reflecting the possibility that
advanced AI systems could have very large impacts on society without reaching
human-level cognitive abilities. To be most useful, however, more analysis of
what it means for AI to be 'transformative' is needed. In this paper, we
propose three different levels on which AI might be said to be transformative,
associated with different levels of societal change. We suggest that these
distinctions would improve conversations between policy makers and decision
makers concerning the mid- to long-term impacts of advances in AI. Further, we
feel this would have a positive effect on strategic foresight efforts involving
advanced AI, which we expect to illuminate paths to alternative futures. We
conclude with a discussion of the benefits of our new framework and by
highlighting directions for future work in this area."
"Improved Explanatory Efficacy on Human Affect and Workload through
  Interactive Process in Artificial Intelligence","Despite recent advances in the field of explainable artificial intelligence
systems, a concrete quantitative measure for evaluating the usability of such
systems is nonexistent. Ensuring the success of an explanatory interface in
interacting with users requires a cyclic, symbiotic relationship between human
and artificial intelligence. We, therefore, propose explanatory efficacy, a
novel metric for evaluating the strength of the cyclic relationship the
interface exhibits. Furthermore, in a user study, we evaluated the perceived
affect and workload and recorded the EEG signals of our participants as they
interacted with our custom-built, iterative explanatory interface to build
personalized recommendation systems. We found that systems for perceptually
driven iterative tasks with greater explanatory efficacy are characterized by
statistically significant hemispheric differences in neural signals with 62.4%
accuracy, indicating the feasibility of neural correlates as a measure of
explanatory efficacy. These findings are beneficial for researchers who aim to
study the circular ecosystem of the human-artificial intelligence partnership."
"The Holy Grail of Quantum Artificial Intelligence: Major Challenges in
  Accelerating the Machine Learning Pipeline","We discuss the synergetic connection between quantum computing and artificial
intelligence. After surveying current approaches to quantum artificial
intelligence and relating them to a formal model for machine learning
processes, we deduce four major challenges for the future of quantum artificial
intelligence: (i) Replace iterative training with faster quantum algorithms,
(ii) distill the experience of larger amounts of data into the training
process, (iii) allow quantum and classical components to be easily combined and
exchanged, and (iv) build tools to thoroughly analyze whether observed benefits
really stem from quantum properties of the algorithm."
"Enterprise AI Canvas -- Integrating Artificial Intelligence into
  Business","Artificial Intelligence (AI) and Machine Learning have enormous potential to
transform businesses and disrupt entire industry sectors. However, companies
wishing to integrate algorithmic decisions into their face multiple challenges:
They have to identify use-cases in which artificial intelligence can create
value, as well as decisions that can be supported or executed automatically.
Furthermore, the organization will need to be transformed to be able to
integrate AI based systems into their human work-force. Furthermore, the more
technical aspects of the underlying machine learning model have to be discussed
in terms of how they impact the various units of a business: Where do the
relevant data come from, which constraints have to be considered, how is the
quality of the data and the prediction evaluated?
  The Enterprise AI canvas is designed to bring Data Scientist and business
expert together to discuss and define all relevant aspects which need to be
clarified in order to integrate AI based systems into a digital enterprise. It
consists of two parts where part one focuses on the business view and
organizational aspects, whereas part two focuses on the underlying machine
learning model and the data it uses."
"SoK: Exploring the State of the Art and the Future Potential of
  Artificial Intelligence in Digital Forensic Investigation","Multi-year digital forensic backlogs have become commonplace in law
enforcement agencies throughout the globe. Digital forensic investigators are
overloaded with the volume of cases requiring their expertise compounded by the
volume of data to be processed. Artificial intelligence is often seen as the
solution to many big data problems. This paper summarises existing artificial
intelligence based tools and approaches in digital forensics. Automated
evidence processing leveraging artificial intelligence based techniques shows
great promise in expediting the digital forensic analysis process while
increasing case processing capacities. For each application of artificial
intelligence highlighted, a number of current challenges and future potential
impact is discussed."
"Reasons, Values, Stakeholders: A Philosophical Framework for Explainable
  Artificial Intelligence","The societal and ethical implications of the use of opaque artificial
intelligence systems for consequential decisions, such as welfare allocation
and criminal justice, have generated a lively debate among multiple stakeholder
groups, including computer scientists, ethicists, social scientists, policy
makers, and end users. However, the lack of a common language or a
multi-dimensional framework to appropriately bridge the technical, epistemic,
and normative aspects of this debate prevents the discussion from being as
productive as it could be. Drawing on the philosophical literature on the
nature and value of explanations, this paper offers a multi-faceted framework
that brings more conceptual precision to the present debate by (1) identifying
the types of explanations that are most pertinent to artificial intelligence
predictions, (2) recognizing the relevance and importance of social and ethical
values for the evaluation of these explanations, and (3) demonstrating the
importance of these explanations for incorporating a diversified approach to
improving the design of truthful algorithmic ecosystems. The proposed
philosophical framework thus lays the groundwork for establishing a pertinent
connection between the technical and ethical aspects of artificial intelligence
systems."
Archives and AI: An Overview of Current Debates and Future Perspectives,"The digital transformation is turning archives, both old and new, into data.
As a consequence, automation in the form of artificial intelligence techniques
is increasingly applied both to scale traditional recordkeeping activities, and
to experiment with novel ways to capture, organise and access records. We
survey recent developments at the intersection of Artificial Intelligence and
archival thinking and practice. Our overview of this growing body of literature
is organised through the lenses of the Records Continuum model. We find four
broad themes in the literature on archives and artificial intelligence:
theoretical and professional considerations, the automation of recordkeeping
processes, organising and accessing archives, and novel forms of digital
archives. We conclude by underlining emerging trends and directions for future
work, which include the application of recordkeeping principles to the very
data and processes which power modern artificial intelligence, and a more
structural, yet critically-aware, integration of artificial intelligence into
archival systems and practice."
"Artificial intelligence for Sustainable Energy: A Contextual Topic
  Modeling and Content Analysis","Parallel to the rising debates over sustainable energy and artificial
intelligence solutions, the world is currently discussing the ethics of
artificial intelligence and its possible negative effects on society and the
environment. In these arguments, sustainable AI is proposed, which aims at
advancing the pathway toward sustainability, such as sustainable energy. In
this paper, we offered a novel contextual topic modeling combining LDA, BERT,
and Clustering. We then combined these computational analyses with content
analysis of related scientific publications to identify the main scholarly
topics, sub-themes, and cross-topic themes within scientific research on
sustainable AI in energy. Our research identified eight dominant topics
including sustainable buildings, AI-based DSSs for urban water management,
climate artificial intelligence, Agriculture 4, the convergence of AI with IoT,
AI-based evaluation of renewable technologies, smart campus and engineering
education, and AI-based optimization. We then recommended 14 potential future
research strands based on the observed theoretical gaps. Theoretically, this
analysis contributes to the existing literature on sustainable AI and
sustainable energy, and practically, it intends to act as a general guide for
energy engineers and scientists, AI scientists, and social scientists to widen
their knowledge of sustainability in AI and energy convergence research."
"Women, artificial intelligence, and key positions in collaboration
  networks: Towards a more equal scientific ecosystem","Scientific collaboration in almost every discipline is mainly driven by the
need of sharing knowledge, expertise, and pooled resources. Science is becoming
more complex which has encouraged scientists to involve more in collaborative
research projects in order to better address the challenges. As a highly
interdisciplinary field with a rapidly evolving scientific landscape,
artificial intelligence calls for researchers with special profiles covering a
diverse set of skills and expertise. Understanding gender aspects of scientific
collaboration is of paramount importance, especially in a field such as
artificial intelligence that has been attracting large investments. Using
social network analysis, natural language processing, and machine learning and
focusing on artificial intelligence publications for the period from 2000 to
2019, in this work, we comprehensively investigated the effects of several
driving factors on acquiring key positions in scientific collaboration networks
through a gender lens. It was found that, regardless of gender, scientific
performance in terms of quantity and impact plays a crucial in possessing the
""social researcher"" in the network. However, subtle differences were observed
between female and male researchers in acquiring the ""local influencer"" role."
"Artificial intelligence system based on multi-value classification of
  fully connected neural network for construction management","This study is devoted to solving the problem to determine the professional
adaptive capabilities of construction management staff using artificial
intelligence systems.It is proposed Fully Connected Feed-Forward Neural Network
architecture and performed empirical modeling to create a Data Set. Model of
artificial intelligence system allows evaluating the processes in an Fully
Connected Feed-Forward Neural Network during the execution of multi-value
classification of professional areas. A method has been developed for the
training process of a machine learning model, which reflects the internal
connections between the components of an artificial intelligence system that
allow it to learn from training data. To train the neural network, a data set
of 35 input parameters and 29 output parameters was used; the amount of data in
the set is 936 data lines. Neural network training occurred in the proportion
of 10% and 90%, respectively. Results of this study research can be used to
further improve the knowledge and skills necessary for successful professional
realization."
"Diagnosis of Paratuberculosis in Histopathological Images Based on
  Explainable Artificial Intelligence and Deep Learning","Artificial intelligence holds great promise in medical imaging, especially
histopathological imaging. However, artificial intelligence algorithms cannot
fully explain the thought processes during decision-making. This situation has
brought the problem of explainability, i.e., the black box problem, of
artificial intelligence applications to the agenda: an algorithm simply
responds without stating the reasons for the given images. To overcome the
problem and improve the explainability, explainable artificial intelligence
(XAI) has come to the fore, and piqued the interest of many researchers.
Against this backdrop, this study examines a new and original dataset using the
deep learning algorithm, and visualizes the output with gradient-weighted class
activation mapping (Grad-CAM), one of the XAI applications. Afterwards, a
detailed questionnaire survey was conducted with the pathologists on these
images. Both the decision-making processes and the explanations were verified,
and the accuracy of the output was tested. The research results greatly help
pathologists in the diagnosis of paratuberculosis."
"Advancing the cybersecurity of the healthcare system with
  self-optimising and self-adaptative artificial intelligence (part 2)","This article advances the knowledge on teaching and training new artificial
intelligence algorithms, for securing, preparing, and adapting the healthcare
system to cope with future pandemics. The core objective is to develop a
concept healthcare system supported by autonomous artificial intelligence that
can use edge health devices with real-time data. The article constructs two
case scenarios for applying cybersecurity with autonomous artificial
intelligence for (1) self-optimising predictive cyber risk analytics of
failures in healthcare systems during a Disease X event (i.e., undefined future
pandemic), and (2) self-adaptive forecasting of medical production and supply
chain bottlenecks during future pandemics. To construct the two testing
scenarios, the article uses the case of Covid-19 to synthesise data for the
algorithms i.e., for optimising and securing digital healthcare systems in
anticipation of disease X. The testing scenarios are built to tackle the
logistical challenges and disruption of complex production and supply chains
for vaccine distribution with optimisation algorithms."
"GeoAI at ACM SIGSPATIAL: The New Frontier of Geospatial Artificial
  Intelligence Research","Geospatial Artificial Intelligence (GeoAI) is an interdisciplinary field
enjoying tremendous adoption. However, the efficient design and implementation
of GeoAI systems face many open challenges. This is mainly due to the lack of
non-standardized approaches to artificial intelligence tool development,
inadequate platforms, and a lack of multidisciplinary engagements, which all
motivate domain experts to seek a shared stage with scientists and engineers to
solve problems of significant impact on society. Since its inception in 2017,
the GeoAI series of workshops has been co-located with the Association for
Computing Machinery International Conference on Advances in Geographic
Information Systems. The workshop series has fostered a nexus for
geoscientists, computer scientists, engineers, entrepreneurs, and
decision-makers, from academia, industry, and government to engage in
artificial intelligence, spatiotemporal data computing, and geospatial data
science research, motivated by various challenges. In this article, we revisit
and discuss the state of GeoAI open research directions, the recent
developments, and an emerging agenda calling for a continued cross-disciplinary
community engagement."
"From Robots to Books: An Introduction to Smart Applications of AI in
  Education (AIEd)","The world around us has undergone a radical transformation due to rapid
technological advancement in recent decades. The industry of the future
generation is evolving, and artificial intelligence is the following change in
the making popularly known as Industry 4.0. Indeed, experts predict that
artificial intelligence(AI) will be the main force behind the following
significant virtual shift in the way we stay, converse, study, live,
communicate and conduct business. All facets of our social connection are being
transformed by this growing technology. One of the newest areas of educational
technology is Artificial Intelligence in the field of Education(AIEd).This
study emphasizes the different applications of artificial intelligence in
education from both an industrial and academic standpoint. It highlights the
most recent contextualized learning novel transformative evaluations and
advancements in sophisticated tutoring systems. It analyses the AIEd's ethical
component and the influence of the transition on people, particularly students
and instructors as well. Finally, this article touches on AIEd's potential
future research and practices. The goal of this study is to introduce the
present-day applications to its intended audience."
"Reproducibility of Machine Learning: Terminology, Recommendations and
  Open Issues","Reproducibility is one of the core dimensions that concur to deliver
Trustworthy Artificial Intelligence. Broadly speaking, reproducibility can be
defined as the possibility to reproduce the same or a similar experiment or
method, thereby obtaining the same or similar results as the original
scientists. It is an essential ingredient of the scientific method and crucial
for gaining trust in relevant claims. A reproducibility crisis has been
recently acknowledged by scientists and this seems to affect even more
Artificial Intelligence and Machine Learning, due to the complexity of the
models at the core of their recent successes. Notwithstanding the recent debate
on Artificial Intelligence reproducibility, its practical implementation is
still insufficient, also because many technical issues are overlooked. In this
survey, we critically review the current literature on the topic and highlight
the open issues. Our contribution is three-fold. We propose a concise
terminological review of the terms coming into play. We collect and systematize
existing recommendations for achieving reproducibility, putting forth the means
to comply with them. We identify key elements often overlooked in modern
Machine Learning and provide novel recommendations for them. We further
specialize these for two critical application domains, namely the biomedical
and physical artificial intelligence fields."
"Multi label classification of Artificial Intelligence related patents
  using Modified D2SBERT and Sentence Attention mechanism","Patent classification is an essential task in patent information management
and patent knowledge mining. It is very important to classify patents related
to artificial intelligence, which is the biggest topic these days. However,
artificial intelligence-related patents are very difficult to classify because
it is a mixture of complex technologies and legal terms. Moreover, due to the
unsatisfactory performance of current algorithms, it is still mostly done
manually, wasting a lot of time and money. Therefore, we present a method for
classifying artificial intelligence-related patents published by the USPTO
using natural language processing technique and deep learning methodology. We
use deformed BERT and sentence attention overcome the limitations of BERT. Our
experiment result is highest performance compared to other deep learning
methods."
"Hybrid Classic-Quantum Computing for Staging of Invasive Ductal
  Carcinoma of Breast","Despite the great current relevance of Artificial Intelligence, and the
extraordinary innovations that this discipline has brought to many fields
-among which, without a doubt, medicine is found-, experts in medical
applications of Artificial Intelligence are looking for new alternatives to
solve problems for which current Artificial Intelligence programs do not
provide with optimal solutions. For this, one promising option could be the use
of the concepts and ideas of Quantum Mechanics, for the construction of
quantum-based Artificial Intelligence systems. From a hybrid classical-quantum
perspective, this article deals with the application of quantum computing
techniques for the staging of Invasive Ductal Carcinoma of the breast. It
includes: (1) a general explanation of a classical, and well-established,
approach for medical reasoning, (2) a description of the clinical problem, (3)
a conceptual model for staging invasive ductal carcinoma, (4) some basic
notions about Quantum Rule-Based Systems, (5) a step-by-step explanation of the
proposed approach for quantum staging of the invasive ductal carcinoma, and (6)
the results obtained after running the quantum system on a significant number
of use cases. A detailed discussion is also provided at the end of this paper."
A Review on Building Blocks of Decentralized Artificial Intelligence,"Artificial intelligence is transforming our lives, and technological progress
and transfer from the academic and theoretical sphere to the real world are
accelerating yearly. But during that progress and transition, several open
problems and questions need to be addressed for the field to develop ethically,
such as digital privacy, ownership, and control. These are some of the reasons
why the currently most popular approaches of artificial intelligence, i.e.,
centralized AI (CEAI), are questionable, with other directions also being
widely explored, such as decentralized artificial intelligence (DEAI), to solve
some of the most reaching problems. This paper provides a systematic literature
review (SLR) of existing work in the field of DEAI, presenting the findings of
71 identified studies. The paper's primary focus is identifying the building
blocks of DEAI solutions and networks, tackling the DEAI analysis from a
bottom-up approach. In the end, future directions of research and open problems
are proposed."
"Fake Artificial Intelligence Generated Contents (FAIGC): A Survey of
  Theories, Detection Methods, and Opportunities","In recent years, generative artificial intelligence models, represented by
Large Language Models (LLMs) and Diffusion Models (DMs), have revolutionized
content production methods. These artificial intelligence-generated content
(AIGC) have become deeply embedded in various aspects of daily life and work.
However, these technologies have also led to the emergence of Fake Artificial
Intelligence Generated Content (FAIGC), posing new challenges in distinguishing
genuine information. It is crucial to recognize that AIGC technology is akin to
a double-edged sword; its potent generative capabilities, while beneficial,
also pose risks for the creation and dissemination of FAIGC. In this survey, We
propose a new taxonomy that provides a more comprehensive breakdown of the
space of FAIGC methods today. Next, we explore the modalities and generative
technologies of FAIGC. We introduce FAIGC detection methods and summarize the
related benchmark from various perspectives. Finally, we discuss outstanding
challenges and promising areas for future research."
"Development of an Adaptive Multi-Domain Artificial Intelligence System
  Built using Machine Learning and Expert Systems Technologies","Producing an artificial general intelligence (AGI) has been an elusive goal
in artificial intelligence (AI) research for some time. An AGI would have the
capability, like a human, to be exposed to a new problem domain, learn about it
and then use reasoning processes to make decisions. While AI techniques have
been used across a wide variety of problem domains, an AGI would require an AI
that could reason beyond its programming and training. This paper presents a
small step towards producing an AGI. It describes a mechanism for an AI to
learn about and develop reasoning pathways to make decisions in an a priori
unknown domain. It combines a classical AI technique, the expert system, with a
its modern adaptation - the gradient descent trained expert system (GDTES) -
and utilizes generative artificial intelligence (GAI) to create a network and
training data set for this system. These can be created from available sources
or may draw upon knowledge incorporated in a GAI's own pre-trained model. The
learning process in GDTES is used to optimize the AI's decision-making. While
this approach does not meet the standards that many have defined for an AGI, it
provides a somewhat similar capability, albeit one which requires a learning
process before use."
"The Transformation Risk-Benefit Model of Artificial Intelligence:
  Balancing Risks and Benefits Through Practical Solutions and Use Cases","This paper summarizes the most cogent advantages and risks associated with
Artificial Intelligence from an in-depth review of the literature. Then the
authors synthesize the salient risk-related models currently being used in AI,
technology and business-related scenarios. Next, in view of an updated context
of AI along with theories and models reviewed and expanded constructs, the
writers propose a new framework called ""The Transformation Risk-Benefit Model
of Artificial Intelligence"" to address the increasing fears and levels of AI
risk. Using the model characteristics, the article emphasizes practical and
innovative solutions where benefits outweigh risks and three use cases in
healthcare, climate change/environment and cyber security to illustrate unique
interplay of principles, dimensions and processes of this powerful AI
transformational model."
"How scanning probe microscopy can be supported by Artificial
  Intelligence and quantum computing","We focus on the potential possibilities for supporting Scanning Probe
Microscopy measurements, emphasizing the application of Artificial
Intelligence, especially Machine Learning as well as quantum computing. It
turned out that Artificial Intelligence can be helpful in the experimental
processes automation in routine operations, the algorithmic search for good
sample regions, and shed light on the structure property relationships. Thus,
it contributes to increasing the efficiency and accuracy of optical nanoscopy
scanning probes. Moreover, the combination of Artificial Intelligence based
algorithms and quantum computing may have a huge potential to increase the
practical application of Scanning Probe Microscopy. The limitations were also
discussed. Finally, we outline a research path for the improvement of the
proposed approach."
Elements Of Legislation For Artificial Intelligence Systems,"The significant part of the operational context for autonomous company
management systems is the regulatory and legal environment in which
corporations operate. In order to create a dedicated operational context for
autonomous artificial intelligence systems, the wording of local regulatory
documents can be simultaneously presented in two versions: for use by people
and for use by autonomous systems. In this case, the artificial intelligence
system will get a well-defined operational context that allows such a system to
perform functions within the required standards. Local regulations that provide
basis for the joint work of individuals and autonomous artificial intelligence
systems can form the grounds for the relevant legislation governing the
development and implementation of autonomous systems."
"Action is the primary key: a categorical framework for episode
  description and logical reasoning","This research presents a computational framework for describing and
recognizing episodes and for logical reasoning. This framework, named
cognitive-logs, consists of a set of relational and graph databases.
Cognitive-logs record knowledge, particularly in episodes that consist of
""actions"" represented by verbs in natural languages and ""participants"" who
perform the actions. These objects are connected by arrows (morphisms) that
link each action to its participant and link cause to effect. Operations based
on category theory enable comparisons between episodes and deductive
inferences, including abstractions of stories. One of the goals of this study
is to develop a database-driven artificial intelligence. This artificial
intelligence thinks like a human but possesses the accuracy and rigour of a
machine. The vast capacities of databases (up to petabyte scales in current
technologies) enable the artificial intelligence to store a greater volume of
knowledge than neural-network based artificial intelligences. Cognitive-logs
serve as a model of human cognition and designed with references to cognitive
linguistics. Cognitive-logs also have the potential to model various human mind
activities."
"Fundamentals of legislation for autonomous artificial intelligence
  systems","The article proposes a method for forming a dedicated operational context in
course of development and implementation of autonomous corporate management
systems based on example of autonomous systems for a board of directors. The
significant part of the operational context for autonomous company management
systems is the regulatory and legal environment within which corporations
operate. In order to create a special operational context for autonomous
artificial intelligence systems, the wording of local regulatory documents can
be simultaneously presented in two versions: for use by people and for use by
autonomous systems. In this case, the artificial intelligence system will get a
well-defined operational context that allows such a system to perform functions
within the required standards. Local regulations that provide for the specifics
of the joint work of individuals and autonomous artificial intelligence systems
can create the basis of the relevant legislation governing the development and
implementation of autonomous systems."
"Redefining Finance: The Influence of Artificial Intelligence (AI) and
  Machine Learning (ML)","With rapid transformation of technologies, the fusion of Artificial
Intelligence (AI) and Machine Learning (ML) in finance is disrupting the entire
ecosystem and operations which were followed for decades. The current landscape
is where decisions are increasingly data-driven by financial institutions with
an appetite for automation while mitigating risks. The segments of financial
institutions which are getting heavily influenced are retail banking, wealth
management, corporate banking & payment ecosystem. The solution ranges from
onboarding the customers all the way fraud detection & prevention to enhancing
the customer services. Financial Institutes are leap frogging with integration
of Artificial Intelligence and Machine Learning in mainstream applications and
enhancing operational efficiency through advanced predictive analytics,
extending personalized customer experiences, and automation to minimize risk
with fraud detection techniques. However, with Adoption of AI & ML, it is
imperative that the financial institute also needs to address ethical and
regulatory challenges, by putting in place robust governance frameworks and
responsible AI practices."
"Artificial intelligence in creating, representing or expressing an
  immersive soundscape","In today's tech-driven world, significant advancements in artificial
intelligence and virtual reality have emerged. These developments drive
research into exploring their intersection in the realm of soundscape. Not only
do these technologies raise questions about how they will revolutionize the way
we design and create soundscapes, but they also draw significant inquiries into
their impact on human perception, understanding, and expression of auditory
environments. This paper aims to review and discuss the latest applications of
artificial intelligence in this domain. It explores how artificial intelligence
can be utilized to create a virtual reality immersive soundscape, exploiting
its ability to recognize complex patterns in various forms of data. This
includes translating between different modalities such as text, sounds, and
animations as well as predicting and generating data across these domains. It
addresses questions surrounding artificial intelligence's capacity to predict,
detect, and comprehend soundscape data, ultimately aiming to bridge the gap
between sound and other forms of human-readable data. 1."
"Enaction-Based Artificial Intelligence: Toward Coevolution with Humans
  in the Loop","This article deals with the links between the enaction paradigm and
artificial intelligence. Enaction is considered a metaphor for artificial
intelligence, as a number of the notions which it deals with are deemed
incompatible with the phenomenal field of the virtual. After explaining this
stance, we shall review previous works regarding this issue in terms of
artifical life and robotics. We shall focus on the lack of recognition of
co-evolution at the heart of these approaches. We propose to explicitly
integrate the evolution of the environment into our approach in order to refine
the ontogenesis of the artificial system, and to compare it with the enaction
paradigm. The growing complexity of the ontogenetic mechanisms to be activated
can therefore be compensated by an interactive guidance system emanating from
the environment. This proposition does not however resolve that of the
relevance of the meaning created by the machine (sense-making). Such
reflections lead us to integrate human interaction into this environment in
order to construct relevant meaning in terms of participative artificial
intelligence. This raises a number of questions with regards to setting up an
enactive interaction. The article concludes by exploring a number of issues,
thereby enabling us to associate current approaches with the principles of
morphogenesis, guidance, the phenomenology of interactions and the use of
minimal enactive interfaces in setting up experiments which will deal with the
problem of artificial intelligence in a variety of enaction-based ways."
Predictive Maintenance -- Bridging Artificial Intelligence and IoT,"This paper highlights the trends in the field of predictive maintenance with
the use of machine learning. With the continuous development of the Fourth
Industrial Revolution, through IoT, the technologies that use artificial
intelligence are evolving. As a result, industries have been using these
technologies to optimize their production. Through scientific research
conducted for this paper, conclusions were drawn about the trends in Predictive
Maintenance applications with the use of machine learning bridging Artificial
Intelligence and IoT. These trends are related to the types of industries in
which Predictive Maintenance was applied, the models of artificial intelligence
were implemented, mainly of machine learning and the types of sensors that are
applied through the IoT to the applications. Six sectors were presented and the
production sector was dominant as it accounted for 54.54% of total
publications. In terms of artificial intelligence models, the most prevalent
among ten were the Artificial Neural Networks, Support Vector Machine and
Random Forest with 27.84%, 17.72% and 13.92% respectively. Finally, twelve
categories of sensors emerged, of which the most widely used were the sensors
of temperature and vibration with percentages of 60.71% and 46.42%
correspondingly."
"Internet of Intelligence: A Survey on the Enabling Technologies,
  Applications, and Challenges","The Internet of intelligence is conceived as an emerging networking paradigm,
which will make intelligence as easy to obtain as information. This paper
provides an overview of the Internet of intelligence, focusing on motivations,
architecture, enabling technologies, applications, and existing challenges.
This can provide a good foundation for those who are interested to gain
insights into the concept of the Internet of intelligence and the key enablers
of this emerging networking paradigm. Specifically, this paper starts by
investigating the evolution of networking paradigms and artificial intelligence
(AI), based on which we present the motivations of the Internet of intelligence
by demonstrating that networking needs intelligence and intelligence needs
networking. We then present the layered architecture to characterize the
Internet of intelligence systems and discuss the enabling technologies of each
layer. Moreover, we discuss the critical applications and their integration
with the Internet of intelligence paradigm. Finally, some technical challenges
and open issues are summarized to fully exploit the benefits of the Internet of
intelligence."
A Contradiction-Centered Model for the Emergence of Swarm Intelligence,"The phenomenon of emergence of swarm intelligence exists widely in nature and
human society. People have been exploring the root cause of emergence of swarm
intelligence and trying to establish general theories and models for emergence
of swarm intelligence. However, the existing theories or models do not grasp
the essence of swarm intelligence, so they lack generality and are difficult to
explain various phenomena of emergence of swarm intelligence. In this paper, a
contradiction-centered model for the emergence of swarm intelligence is
proposed, in which the internal contradictions of individuals determine their
behavior and properties, individuals are related and interact within the swarm
because of competing and occupying environmental resources, interactions and
swarm potential affect the internal contradictions of individuals and their
distribution in the swarm, and the swarm intelligence is manifested as the
specific distribution of individual contradictions. This model completely
explains the conditions, dynamics, pathways, formations and processes of the
emergence of swarm intelligence. In order to verify the validity of this model,
several swarm intelligence systems are implemented and analyzed in this paper.
The experimental results show that the model has good generality and can be
used to describe the emergence of various swarm intelligence."
Cooperative Artificial Intelligence,"In the future, artificial learning agents are likely to become increasingly
widespread in our society. They will interact with both other learning agents
and humans in a variety of complex settings including social dilemmas. We argue
that there is a need for research on the intersection between game theory and
artificial intelligence, with the goal of achieving cooperative artificial
intelligence that can navigate social dilemmas well. We consider the problem of
how an external agent can promote cooperation between artificial learners by
distributing additional rewards and punishments based on observing the actions
of the learners. We propose a rule for automatically learning how to create the
right incentives by considering the anticipated parameter updates of each
agent. Using this learning rule leads to cooperation with high social welfare
in matrix games in which the agents would otherwise learn to defect with high
probability. We show that the resulting cooperative outcome is stable in
certain games even if the planning agent is turned off after a given number of
episodes, while other games require ongoing intervention to maintain mutual
cooperation. Finally, we reflect on what the goals of multi-agent reinforcement
learning should be in the first place, and discuss the necessary building
blocks towards the goal of building cooperative AI."
"Self-mediated exploration in artificial intelligence inspired by
  cognitive psychology","Exploration of the physical environment is an indispensable precursor to data
acquisition and enables knowledge generation via analytical or direct trialing.
Artificial Intelligence lacks the exploratory capabilities of even the most
underdeveloped organisms, hindering its autonomy and adaptability. Supported by
cognitive psychology, this works links human behavior and artificial agents to
endorse self-development. In accordance with reported data, paradigms of
epistemic and achievement emotion are embedded to machine-learning methodology
contingent on their impact when decision making. A study is subsequently
designed to mirror previous human trials, which artificial agents are made to
undergo repeatedly towards convergence. Results demonstrate causality, learned
by the vast majority of agents, between their internal states and exploration
to match those reported for human counterparts. The ramifications of these
findings are pondered for both research into human cognition and betterment of
artificial intelligence."
"Towards an Autonomous Surface Vehicle Prototype for Artificial
  Intelligence Applications of Water Quality Monitoring","The use of Autonomous Surface Vehicles, equipped with water quality sensors
and artificial vision systems, allows for a smart and adaptive deployment in
water resources environmental monitoring. This paper presents a real
implementation of a vehicle prototype that to address the use of Artificial
Intelligence algorithms and enhanced sensing techniques for water quality
monitoring. The vehicle is fully equipped with high-quality sensors to measure
water quality parameters and water depth. Furthermore, by means of a
stereo-camera, it also can detect and locate macro-plastics in real
environments by means of deep visual models, such as YOLOv5. In this paper,
experimental results, carried out in Lago Mayor (Sevilla), has been presented
as proof of the capabilities of the proposed architecture. The overall system,
and the early results obtained, are expected to provide a solid example of a
real platform useful for the water resource monitoring task, and to serve as a
real case scenario for deploying Artificial Intelligence algorithms, such as
path planning, artificial vision, etc."
"Towards an Intelligent Database System Founded on the SP Theory of
  Computing and Cognition","The SP theory of computing and cognition, described in previous publications,
is an attractive model for intelligent databases because it provides a simple
but versatile format for different kinds of knowledge, it has capabilities in
artificial intelligence, and it can also function like established database
models when that is required.
  This paper describes how the SP model can emulate other models used in
database applications and compares the SP model with those other models. The
artificial intelligence capabilities of the SP model are reviewed and its
relationship with other artificial intelligence systems is described. Also
considered are ways in which current prototypes may be translated into an
'industrial strength' working system."
"Faith in the Algorithm, Part 1: Beyond the Turing Test","Since the Turing test was first proposed by Alan Turing in 1950, the primary
goal of artificial intelligence has been predicated on the ability for
computers to imitate human behavior. However, the majority of uses for the
computer can be said to fall outside the domain of human abilities and it is
exactly outside of this domain where computers have demonstrated their greatest
contribution to intelligence. Another goal for artificial intelligence is one
that is not predicated on human mimicry, but instead, on human amplification.
This article surveys various systems that contribute to the advancement of
human and social intelligence."
"An existing, ecologically-successful genus of collectively intelligent
  artificial creatures","People sometimes worry about the Singularity [Vinge, 1993; Kurzweil, 2005],
or about the world being taken over by artificially intelligent robots. I
believe the risks of these are very small. However, few people recognize that
we already share our world with artificial creatures that participate as
intelligent agents in our society: corporations. Our planet is inhabited by two
distinct kinds of intelligent beings --- individual humans and corporate
entities --- whose natures and interests are intimately linked. To co-exist
well, we need to find ways to define the rights and responsibilities of both
individual humans and corporate entities, and to find ways to ensure that
corporate entities behave as responsible members of society."
Death and Suicide in Universal Artificial Intelligence,"Reinforcement learning (RL) is a general paradigm for studying intelligent
behaviour, with applications ranging from artificial intelligence to psychology
and economics. AIXI is a universal solution to the RL problem; it can learn any
computable environment. A technical subtlety of AIXI is that it is defined
using a mixture over semimeasures that need not sum to 1, rather than over
proper probability measures. In this work we argue that the shortfall of a
semimeasure can naturally be interpreted as the agent's estimate of the
probability of its death. We formally define death for generally intelligent
agents like AIXI, and prove a number of related theorems about their behaviour.
Notable discoveries include that agent behaviour can change radically under
positive linear transformations of the reward signal (from suicidal to
dogmatically self-preserving), and that the agent's posterior belief that it
will survive increases over time."
"The Algonauts Project: A Platform for Communication between the Sciences
  of Biological and Artificial Intelligence","In the last decade, artificial intelligence (AI) models inspired by the brain
have made unprecedented progress in performing real-world perceptual tasks like
object classification and speech recognition. Recently, researchers of natural
intelligence have begun using those AI models to explore how the brain performs
such tasks. These developments suggest that future progress will benefit from
increased interaction between disciplines. Here we introduce the Algonauts
Project as a structured and quantitative communication channel for
interdisciplinary interaction between natural and artificial intelligence
researchers. The project's core is an open challenge with a quantitative
benchmark whose goal is to account for brain data through computational models.
This project has the potential to provide better models of natural intelligence
and to gather findings that advance AI. The 2019 Algonauts Project focuses on
benchmarking computational models predicting human brain activity when people
look at pictures of objects. The 2019 edition of the Algonauts Project is
available online: http://algonauts.csail.mit.edu/."
Asymptotically Unambitious Artificial General Intelligence,"General intelligence, the ability to solve arbitrary solvable problems, is
supposed by many to be artificially constructible. Narrow intelligence, the
ability to solve a given particularly difficult problem, has seen impressive
recent development. Notable examples include self-driving cars, Go engines,
image classifiers, and translators. Artificial General Intelligence (AGI)
presents dangers that narrow intelligence does not: if something smarter than
us across every domain were indifferent to our concerns, it would be an
existential threat to humanity, just as we threaten many species despite no ill
will. Even the theory of how to maintain the alignment of an AGI's goals with
our own has proven highly elusive. We present the first algorithm we are aware
of for asymptotically unambitious AGI, where ""unambitiousness"" includes not
seeking arbitrary power. Thus, we identify an exception to the Instrumental
Convergence Thesis, which is roughly that by default, an AGI would seek power,
including over us."
Machine learning and deep learning,"Today, intelligent systems that offer artificial intelligence capabilities
often rely on machine learning. Machine learning describes the capacity of
systems to learn from problem-specific training data to automate the process of
analytical model building and solve associated tasks. Deep learning is a
machine learning concept based on artificial neural networks. For many
applications, deep learning models outperform shallow machine learning models
and traditional data analysis approaches. In this article, we summarize the
fundamentals of machine learning and deep learning to generate a broader
understanding of the methodical underpinning of current intelligent systems. In
particular, we provide a conceptual distinction between relevant terms and
concepts, explain the process of automated analytical model building through
machine learning and deep learning, and discuss the challenges that arise when
implementing such intelligent systems in the field of electronic markets and
networked business. These naturally go beyond technological aspects and
highlight issues in human-machine interaction and artificial intelligence
servitization."
"Reward is not enough: can we liberate AI from the reinforcement learning
  paradigm?","I present arguments against the hypothesis put forward by Silver, Singh,
Precup, and Sutton (
https://www.sciencedirect.com/science/article/pii/S0004370221000862 ) : reward
maximization is not enough to explain many activities associated with natural
and artificial intelligence including knowledge, learning, perception, social
intelligence, evolution, language, generalisation and imitation. I show such
reductio ad lucrum has its intellectual origins in the political economy of
Homo economicus and substantially overlaps with the radical version of
behaviourism. I show why the reinforcement learning paradigm, despite its
demonstrable usefulness in some practical application, is an incomplete
framework for intelligence -- natural and artificial. Complexities of
intelligent behaviour are not simply second-order complications on top of
reward maximisation. This fact has profound implications for the development of
practically usable, smart, safe and robust artificially intelligent agents."
"A Survey on Artificial Intelligence for Music Generation: Agents,
  Domains and Perspectives","Music is one of the Gardner's intelligences in his theory of multiple
intelligences. How humans perceive and understand music is still being studied
and is crucial to develop artificial intelligence models that imitate such
processes. Music generation with Artificial Intelligence is an emerging field
that is gaining much attention in the recent years. In this paper, we describe
how humans compose music and how new AI systems could imitate such process by
comparing past and recent advances in the field with music composition
techniques. To understand how AI models and algorithms generate music and the
potential applications that might appear in the future, we explore, analyze and
describe the agents that take part of the music generation process: the
datasets, models, interfaces, the users and the generated music. We mention
possible applications that might benefit from this field and we also propose
new trends and future research directions that could be explored in the future."
Suffering Toasters -- A New Self-Awareness Test for AI,"A widely accepted definition of intelligence in the context of Artificial
Intelligence (AI) still eludes us. Due to our exceedingly rapid development of
AI paradigms, architectures, and tools, the prospect of naturally arising AI
consciousness seems more likely than ever. In this paper, we claim that all
current intelligence tests are insufficient to point to the existence or lack
of intelligence \textbf{as humans intuitively perceive it}. We draw from ideas
in the philosophy of science, psychology, and other areas of research to
provide a clearer definition of the problems of artificial intelligence,
self-awareness, and agency. We furthermore propose a new heuristic approach to
test for artificial self-awareness and outline a possible implementation.
Finally, we discuss some of the questions that arise from this new heuristic,
be they philosophical or implementation-oriented."
"Artificial General Intelligence, Existential Risk, and Human Risk
  Perception","Artificial general intelligence (AGI) does not yet exist, but given the pace
of technological development in artificial intelligence, it is projected to
reach human-level intelligence within roughly the next two decades. After that,
many experts expect it to far surpass human intelligence and to do so rapidly.
The prospect of superintelligent AGI poses an existential risk to humans
because there is no reliable method for ensuring that AGI goals stay aligned
with human goals. Drawing on publicly available forecaster and opinion data,
the author examines how experts and non-experts perceive risk from AGI. The
findings indicate that the perceived risk of a world catastrophe or extinction
from AGI is greater than for other existential risks. The increase in perceived
risk over the last year is also steeper for AGI than for other existential
threats (e.g., nuclear war or human-caused climate change). That AGI is a
pressing existential risk is something on which experts and non-experts agree,
but the basis for such agreement currently remains obscure."
"Advancing Explainable AI Toward Human-Like Intelligence: Forging the
  Path to Artificial Brain","The intersection of Artificial Intelligence (AI) and neuroscience in
Explainable AI (XAI) is pivotal for enhancing transparency and interpretability
in complex decision-making processes. This paper explores the evolution of XAI
methodologies, ranging from feature-based to human-centric approaches, and
delves into their applications in diverse domains, including healthcare and
finance. The challenges in achieving explainability in generative models,
ensuring responsible AI practices, and addressing ethical implications are
discussed. The paper further investigates the potential convergence of XAI with
cognitive sciences, the development of emotionally intelligent AI, and the
quest for Human-Like Intelligence (HLI) in AI systems. As AI progresses towards
Artificial General Intelligence (AGI), considerations of consciousness, ethics,
and societal impact become paramount. The ongoing pursuit of deciphering the
mysteries of the brain with AI and the quest for HLI represent transformative
endeavors, bridging technical advancements with multidisciplinary explorations
of human cognition."
What the F*ck Is Artificial General Intelligence?,"Artificial general intelligence (AGI) is an established field of research.
Yet Melanie Mitchell and others have questioned if the term still has meaning.
AGI has been subject to so much hype and speculation it has become something of
a Rorschach test. Mitchell points out that the debate will only be settled
through long term, scientific investigation. To that end here is a short,
accessible and provocative overview of AGI. I compare definitions of
intelligence, settling on intelligence in terms of adaptation and AGI as an
artificial scientist. Taking my queue from Sutton's Bitter Lesson I describe
two foundational tools used to build adaptive systems: search and
approximation. I compare pros, cons, hybrids and architectures like o3,
AlphaGo, AERA, NARS and Hyperon. I then discuss overall meta-approaches to
making systems behave more intelligently. I divide them into scale-maxing,
simp-maxing, w-maxing based on the Bitter Lesson, Ockham's and Bennett's
Razors. These maximise resources, simplicity of form, and the weakness of
constraints on functionality. I discuss examples including AIXI, the free
energy principle and The Embiggening of language models. I conclude that though
scale-maxed approximation dominates, AGI will be a fusion of tools and
meta-approaches. The Embiggening was enabled by improvements in hardware. Now
the bottlenecks are sample and energy efficiency."
Relating Blindsight and AI: A Review,"Processes occurring in brains, a.k.a. biological neural networks, can and
have been modeled within artificial neural network architectures. Due to this,
we have conducted a review of research on the phenomenon of blindsight in an
attempt to generate ideas for artificial intelligence models. Blindsight can be
considered as a diminished form of visual experience. If we assume that
artificial networks have no form of visual experience, then deficits caused by
blindsight give us insights into the processes occurring within visual
experience that we can incorporate into artificial neural networks. This
article has been structured into three parts. Section 2 is a review of
blindsight research, looking specifically at the errors occurring during this
condition compared to normal vision. Section 3 identifies overall patterns from
Section 2 to generate insights for computational models of vision. Section 4
demonstrates the utility of examining biological research to inform artificial
intelligence research by examining computation models of visual attention
relevant to one of the insights generated in Section 3. The research covered in
Section 4 shows that incorporating one of our insights into computational
vision does benefit those models. Future research will be required to determine
whether our other insights are as valuable."
Dynamic Cognition Applied to Value Learning in Artificial Intelligence,"Experts in Artificial Intelligence (AI) development predict that advances in
the development of intelligent systems and agents will reshape vital areas in
our society. Nevertheless, if such an advance isn't done with prudence, it can
result in negative outcomes for humanity. For this reason, several researchers
in the area are trying to develop a robust, beneficial, and safe concept of
artificial intelligence. Currently, several of the open problems in the field
of AI research arise from the difficulty of avoiding unwanted behaviors of
intelligent agents, and at the same time specifying what we want such systems
to do. It is of utmost importance that artificial intelligent agents have their
values aligned with human values, given the fact that we cannot expect an AI to
develop our moral preferences simply because of its intelligence, as discussed
in the Orthogonality Thesis. Perhaps this difficulty comes from the way we are
addressing the problem of expressing objectives, values, and ends, using
representational cognitive methods. A solution to this problem would be the
dynamic cognitive approach proposed by Dreyfus, whose phenomenological
philosophy defends that the human experience of being-in-the-world cannot be
represented by the symbolic or connectionist cognitive methods. A possible
approach to this problem would be to use theoretical models such as SED
(situated embodied dynamics) to address the values learning problem in AI."
"Examples of Artificial Perceptions in Optical Character Recognition and
  Iris Recognition","This paper assumes the hypothesis that human learning is perception based,
and consequently, the learning process and perceptions should not be
represented and investigated independently or modeled in different simulation
spaces. In order to keep the analogy between the artificial and human learning,
the former is assumed here as being based on the artificial perception. Hence,
instead of choosing to apply or develop a Computational Theory of (human)
Perceptions, we choose to mirror the human perceptions in a numeric
(computational) space as artificial perceptions and to analyze the
interdependence between artificial learning and artificial perception in the
same numeric space, using one of the simplest tools of Artificial Intelligence
and Soft Computing, namely the perceptrons. As practical applications, we
choose to work around two examples: Optical Character Recognition and Iris
Recognition. In both cases a simple Turing test shows that artificial
perceptions of the difference between two characters and between two irides are
fuzzy, whereas the corresponding human perceptions are, in fact, crisp."
Intelligence in Artificial Intelligence,"The elusive quest for intelligence in artificial intelligence prompts us to
consider that instituting human-level intelligence in systems may be (still) in
the realm of utopia. In about a quarter century, we have witnessed the winter
of AI (1990) being transformed and transported to the zenith of tabloid fodder
about AI (2015). The discussion at hand is about the elements that constitute
the canonical idea of intelligence. The delivery of intelligence as a
pay-per-use-service, popping out of an app or from a shrink-wrapped software
defined point solution, is in contrast to the bio-inspired view of intelligence
as an outcome, perhaps formed from a tapestry of events, cross-pollinated by
instances, each with its own microcosm of experiences and learning, which may
not be discrete all-or-none functions but continuous, over space and time. The
enterprise world may not require, aspire or desire such an engaged solution to
improve its services for enabling digital transformation through the deployment
of digital twins, for example. One might ask whether the ""work-flow on
steroids"" version of decision support may suffice for intelligence? Are we
harking back to the era of rule based expert systems? The image conjured by the
publicity machines offers deep solutions with human-level AI and preposterous
claims about capturing the ""brain in a box"" by 2020. Even emulating insects may
be difficult in terms of real progress. Perhaps we can try to focus on worms
(Caenorhabditis elegans) which may be better suited for what business needs to
quench its thirst for so-called intelligence in AI."
"The SP Theory of Intelligence as a Foundation for the Development of a
  General, Human-Level Thinking Machine","This paper summarises how the ""SP theory of intelligence"" and its realisation
in the ""SP computer model"" simplifies and integrates concepts across artificial
intelligence and related areas, and thus provides a promising foundation for
the development of a general, human-level thinking machine, in accordance with
the main goal of research in artificial general intelligence.
  The key to this simplification and integration is the powerful concept of
""multiple alignment"", borrowed and adapted from bioinformatics. This concept
has the potential to be the ""double helix"" of intelligence, with as much
significance for human-level intelligence as has DNA for biological sciences.
  Strengths of the SP system include: versatility in the representation of
diverse kinds of knowledge; versatility in aspects of intelligence (including:
strengths in unsupervised learning; the processing of natural language; pattern
recognition at multiple levels of abstraction that is robust in the face of
errors in data; several kinds of reasoning (including: one-step `deductive'
reasoning; chains of reasoning; abductive reasoning; reasoning with
probabilistic networks and trees; reasoning with 'rules'; nonmonotonic
reasoning and reasoning with default values; Bayesian reasoning with
'explaining away'; and more); planning; problem solving; and more); seamless
integration of diverse kinds of knowledge and diverse aspects of intelligence
in any combination; and potential for application in several areas (including:
helping to solve nine problems with big data; helping to develop human-level
intelligence in autonomous robots; serving as a database with intelligence and
with versatility in the representation and integration of several forms of
knowledge; serving as a vehicle for medical knowledge and as an aid to medical
diagnosis; and several more)."
Do Artificial Intelligence Systems Understand?,"Are intelligent machines really intelligent? Is the underlying philosophical
concept of intelligence satisfactory for describing how the present systems
work? Is understanding a necessary and sufficient condition for intelligence?
If a machine could understand, should we attribute subjectivity to it? This
paper addresses the problem of deciding whether the so-called ""intelligent
machines"" are capable of understanding, instead of merely processing signs. It
deals with the relationship between syntaxis and semantics. The main thesis
concerns the inevitability of semantics for any discussion about the
possibility of building conscious machines, condensed into the following two
tenets: ""If a machine is capable of understanding (in the strong sense), then
it must be capable of combining rules and intuitions""; ""If semantics cannot be
reduced to syntaxis, then a machine cannot understand."" Our conclusion states
that it is not necessary to attribute understanding to a machine in order to
explain its exhibited ""intelligent"" behavior; a merely syntactic and
mechanistic approach to intelligence as a task-solving tool suffices to justify
the range of operations that it can display in the current state of
technological development."
Defining and Explorting the Intelligence Space,"Intelligence is a difficult concept to define, despite many attempts at doing
so. Rather than trying to settle on a single definition, this article
introduces a broad perspective on what intelligence is, by laying out a cascade
of definitions that induces both a nested hierarchy of three levels of
intelligence and a wider-ranging space that is built around them and
approximations to them. Within this intelligence space, regions are identified
that correspond to both natural -- most particularly, human -- intelligence and
artificial intelligence (AI), along with the crossover notion of humanlike
intelligence. These definitions are then exploited in early explorations of
four more advanced, and likely more controversial, topics: the singularity,
generative AI, ethics, and intellectual property."
KUNPENG: An Embodied Large Model for Intelligent Maritime,"Intelligent maritime, as an essential component of smart ocean construction,
deeply integrates advanced artificial intelligence technology and data analysis
methods, which covers multiple aspects such as smart vessels, route
optimization, safe navigation, aiming to enhance the efficiency of ocean
resource utilization and the intelligence of transportation networks. However,
the complex and dynamic maritime environment, along with diverse and
heterogeneous large-scale data sources, present challenges for real-time
decision-making in intelligent maritime. In this paper, We propose KUNPENG, the
first-ever embodied large model for intelligent maritime in the smart ocean
construction, which consists of six systems. The model perceives multi-source
heterogeneous data for the cognition of environmental interaction and make
autonomous decision strategies, which are used for intelligent vessels to
perform navigation behaviors under safety and emergency guarantees and
continuously optimize power to achieve embodied intelligence in maritime. In
comprehensive maritime task evaluations, KUNPENG has demonstrated excellent
performance."
Open Ended Intelligence: The individuation of Intelligent Agents,"Artificial General Intelligence is a field of research aiming to distill the
principles of intelligence that operate independently of a specific problem
domain or a predefined context and utilize these principles in order to
synthesize systems capable of performing any intellectual task a human being is
capable of and eventually go beyond that. While ""narrow"" artificial
intelligence which focuses on solving specific problems such as speech
recognition, text comprehension, visual pattern recognition, robotic motion,
etc. has shown quite a few impressive breakthroughs lately, understanding
general intelligence remains elusive. In the paper we offer a novel theoretical
approach to understanding general intelligence. We start with a brief
introduction of the current conceptual approach. Our critique exposes a number
of serious limitations that are traced back to the ontological roots of the
concept of intelligence. We then propose a paradigm shift from intelligence
perceived as a competence of individual agents defined in relation to an a
priori given problem domain or a goal, to intelligence perceived as a formative
process of self-organization by which intelligent agents are individuated. We
call this process open-ended intelligence. Open-ended intelligence is developed
as an abstraction of the process of cognitive development so its application
can be extended to general agents and systems. We introduce and discuss three
facets of the idea: the philosophical concept of individuation, sense-making
and the individuation of general cognitive agents. We further show how
open-ended intelligence can be framed in terms of a distributed,
self-organizing network of interacting elements and how such process is
scalable. The framework highlights an important relation between coordination
and intelligence and a new understanding of values. We conclude with a number
of questions for future research."
Can Artificial Intelligence Embody Moral Values?,"The neutrality thesis holds that technology cannot be laden with values. This
long-standing view has faced critiques, but much of the argumentation against
neutrality has focused on traditional, non-smart technologies like bridges and
razors. In contrast, AI is a smart technology increasingly used in high-stakes
domains like healthcare, finance, and policing, where its decisions can cause
moral harm. In this paper, we argue that artificial intelligence, particularly
artificial agents that autonomously make decisions to pursue their goals,
challenge the neutrality thesis. Our central claim is that the computational
models underlying artificial agents can integrate representations of moral
values such as fairness, honesty and avoiding harm. We provide a conceptual
framework discussing the neutrality thesis, values, and AI. Moreover, we
examine two approaches to designing computational models of morality,
artificial conscience and ethical prompting, and present empirical evidence
from text-based game environments that artificial agents with such models
exhibit more ethical behavior compared to agents without these models. The
findings support that AI can embody moral values, which contradicts the claim
that all technologies are necessarily value-neutral."
"(Ir)rationality in AI: State of the Art, Research Challenges and Open
  Questions","The concept of rationality is central to the field of artificial
intelligence. Whether we are seeking to simulate human reasoning, or the goal
is to achieve bounded optimality, we generally seek to make artificial agents
as rational as possible. Despite the centrality of the concept within AI, there
is no unified definition of what constitutes a rational agent. This article
provides a survey of rationality and irrationality in artificial intelligence,
and sets out the open questions in this area. The understanding of rationality
in other fields has influenced its conception within artificial intelligence,
in particular work in economics, philosophy and psychology. Focusing on the
behaviour of artificial agents, we consider irrational behaviours that can
prove to be optimal in certain scenarios. Some methods have been developed to
deal with irrational agents, both in terms of identification and interaction,
however work in this area remains limited. Methods that have up to now been
developed for other purposes, namely adversarial scenarios, may be adapted to
suit interactions with artificial agents. We further discuss the interplay
between human and artificial agents, and the role that rationality plays within
this interaction; many questions remain in this area, relating to potentially
irrational behaviour of both humans and artificial agents."
Universal Algorithmic Intelligence: A mathematical top->down approach,"Sequential decision theory formally solves the problem of rational agents in
uncertain worlds if the true environmental prior probability distribution is
known. Solomonoff's theory of universal induction formally solves the problem
of sequence prediction for unknown prior distribution. We combine both ideas
and get a parameter-free theory of universal Artificial Intelligence. We give
strong arguments that the resulting AIXI model is the most intelligent unbiased
agent possible. We outline how the AIXI model can formally solve a number of
problem classes, including sequence prediction, strategic games, function
minimization, reinforcement and supervised learning. The major drawback of the
AIXI model is that it is uncomputable. To overcome this problem, we construct a
modified algorithm AIXItl that is still effectively more intelligent than any
other time t and length l bounded agent. The computation time of AIXItl is of
the order t x 2^l. The discussion includes formal definitions of intelligence
order relations, the horizon problem and relations of the AIXI theory to other
AI approaches."
"Turing: Then, Now and Still Key","This paper looks at Turing's postulations about Artificial Intelligence in
his paper 'Computing Machinery and Intelligence', published in 1950. It notes
how accurate they were and how relevant they still are today. This paper notes
the arguments and mechanisms that he suggested and tries to expand on them
further. The paper however is mostly about describing the essential ingredients
for building an intelligent model and the problems related with that. The
discussion includes recent work by the author himself, who adds his own
thoughts on the matter that come from a purely technical investigation into the
problem. These are personal and quite speculative, but provide an interesting
insight into the mechanisms that might be used for building an intelligent
system."
"Non-Evolutionary Superintelligences Do Nothing, Eventually","There is overwhelming evidence that human intelligence is a product of
Darwinian evolution. Investigating the consequences of self-modification, and
more precisely, the consequences of utility function self-modification, leads
to the stronger claim that not only human, but any form of intelligence is
ultimately only possible within evolutionary processes. Human-designed
artificial intelligences can only remain stable until they discover how to
manipulate their own utility function. By definition, a human designer cannot
prevent a superhuman intelligence from modifying itself, even if protection
mechanisms against this action are put in place. Without evolutionary pressure,
sufficiently advanced artificial intelligences become inert by simplifying
their own utility function. Within evolutionary processes, the implicit utility
function is always reducible to persistence, and the control of superhuman
intelligences embedded in evolutionary processes is not possible. Mechanisms
against utility function self-modification are ultimately futile. Instead,
scientific effort toward the mitigation of existential risks from the
development of superintelligences should be in two directions: understanding
consciousness, and the complex dynamics of evolutionary systems."
Cyber-All-Intel: An AI for Security related Threat Intelligence,"Keeping up with threat intelligence is a must for a security analyst today.
There is a volume of information present in `the wild' that affects an
organization. We need to develop an artificial intelligence system that scours
the intelligence sources, to keep the analyst updated about various threats
that pose a risk to her organization. A security analyst who is better `tapped
in' can be more effective.
  In this paper we present, Cyber-All-Intel an artificial intelligence system
to aid a security analyst. It is a system for knowledge extraction,
representation and analytics in an end-to-end pipeline grounded in the
cybersecurity informatics domain. It uses multiple knowledge representations
like, vector spaces and knowledge graphs in a 'VKG structure' to store incoming
intelligence. The system also uses neural network models to pro-actively
improve its knowledge. We have also created a query engine and an alert system
that can be used by an analyst to find actionable cybersecurity insights."
"The Why, What and How of Artificial General Intelligence Chip
  Development","The AI chips increasingly focus on implementing neural computing at low power
and cost. The intelligent sensing, automation, and edge computing applications
have been the market drivers for AI chips. Increasingly, the generalisation,
performance, robustness, and scalability of the AI chip solutions are compared
with human-like intelligence abilities. Such a requirement to transit from
application-specific to general intelligence AI chip must consider several
factors. This paper provides an overview of this cross-disciplinary field of
study, elaborating on the generalisation of intelligence as understood in
building artificial general intelligence (AGI) systems. This work presents a
listing of emerging AI chip technologies, classification of edge AI
implementations, and the funnel design flow for AGI chip development. Finally,
the design consideration required for building an AGI chip is listed along with
the methods for testing and validating it."
"Deep Edge Intelligence: Architecture, Key Features, Enabling
  Technologies and Challenges","With the breakthroughs in Deep Learning, recent years have witnessed a
massive surge in Artificial Intelligence applications and services. Meanwhile,
the rapid advances in Mobile Computing and Internet of Things has also given
rise to billions of mobile and smart sensing devices connected to the Internet,
generating zettabytes of data at the network edge. The opportunity to combine
these two domains of technologies to power interconnected devices with
intelligence is likely to pave the way for a new wave of technology
revolutions. Embracing this technology revolution, in this article, we present
a novel computing vision named Deep Edge Intelligence (DEI). DEI employs Deep
Learning, Artificial Intelligence, Cloud and Edge Computing, 5G/6G networks,
Internet of Things, Microservices, etc. aiming to provision reliable and secure
intelligence services to every person and organisation at any place with better
user experience. The vision, system architecture, key layers and features of
DEI are also detailed. Finally, we reveal the key enabling technologies and
research challenges associated with it."
"Artificial Intelligence-Enabled Intelligent Assistant for Personalized
  and Adaptive Learning in Higher Education","This paper presents a novel framework, Artificial Intelligence-Enabled
Intelligent Assistant (AIIA), for personalized and adaptive learning in higher
education. The AIIA system leverages advanced AI and Natural Language
Processing (NLP) techniques to create an interactive and engaging learning
platform. This platform is engineered to reduce cognitive load on learners by
providing easy access to information, facilitating knowledge assessment, and
delivering personalized learning support tailored to individual needs and
learning styles. The AIIA's capabilities include understanding and responding
to student inquiries, generating quizzes and flashcards, and offering
personalized learning pathways. The research findings have the potential to
significantly impact the design, implementation, and evaluation of AI-enabled
Virtual Teaching Assistants (VTAs) in higher education, informing the
development of innovative educational tools that can enhance student learning
outcomes, engagement, and satisfaction. The paper presents the methodology,
system architecture, intelligent services, and integration with Learning
Management Systems (LMSs) while discussing the challenges, limitations, and
future directions for the development of AI-enabled intelligent assistants in
education."
"Construction and application of artificial intelligence crowdsourcing
  map based on multi-track GPS data","In recent years, the rapid development of high-precision map technology
combined with artificial intelligence has ushered in a new development
opportunity in the field of intelligent vehicles. High-precision map technology
is an important guarantee for intelligent vehicles to achieve autonomous
driving. However, due to the lack of research on high-precision map technology,
it is difficult to rationally use this technology in the field of intelligent
vehicles. Therefore, relevant researchers studied a fast and effective
algorithm to generate high-precision GPS data from a large number of
low-precision GPS trajectory data fusion, and generated several key data points
to simplify the description of GPS trajectory, and realized the ""crowdsourced
update"" model based on a large number of social vehicles for map data
collection came into being. This kind of algorithm has the important
significance to improve the data accuracy, reduce the measurement cost and
reduce the data storage space. On this basis, this paper analyzes the
implementation form of crowdsourcing map, so as to improve the various
information data in the high-precision map according to the actual situation,
and promote the high-precision map can be reasonably applied to the intelligent
car."
"Fusion Intelligence: Confluence of Natural and Artificial Intelligence
  for Enhanced Problem-Solving Efficiency","This paper introduces Fusion Intelligence (FI), a bio-inspired intelligent
system, where the innate sensing, intelligence and unique actuation abilities
of biological organisms such as bees and ants are integrated with the
computational power of Artificial Intelligence (AI). This interdisciplinary
field seeks to create systems that are not only smart but also adaptive and
responsive in ways that mimic the nature. As FI evolves, it holds the promise
of revolutionizing the way we approach complex problems, leveraging the best of
both biological and digital worlds to create solutions that are more effective,
sustainable, and harmonious with the environment. We demonstrate FI's potential
to enhance agricultural IoT system performance through a simulated case study
on improving insect pollination efficacy (entomophily)."
Towards Intelligent Active Particles,"In this book chapter we describe recent applications of artificial
intelligence and in particular machine learning to active matter systems.
Active matter is composed of agents, or particles, that are capable of
propelling themselves. While biological agents like bacteria, fish or birds
naturally possess a certain degree of ""intelligence"", synthetic active
particles like colloidal microswimmers and electronic robots can be equipped
with different levels of artificial intelligence, either internally (as for
robots) or via a dynamic external control system. This book chapter briefly
discusses existing approaches to make synthetic particles increasingly
""intelligent"" and then focuses on the usage of machine learning to approach
navigation and communication problems of active particles. Basic questions are
how to steer a single active agent through a complex environment to reach or
discover a target in an optimal way and how active particles need to cooperate
to efficiently collect a distribution of targets (e.g. nutrients or toxins)
from their complex environment."
"Evidence of interrelated cognitive-like capabilities in large language
  models: Indications of artificial general intelligence or achievement?","Large language models (LLMs) are advanced artificial intelligence (AI)
systems that can perform a variety of tasks commonly found in human
intelligence tests, such as defining words, performing calculations, and
engaging in verbal reasoning. There are also substantial individual differences
in LLM capacities. Given the consistent observation of a positive manifold and
general intelligence factor in human samples, along with group-level factors
(e.g., crystallized intelligence), we hypothesized that LLM test scores may
also exhibit positive intercorrelations, which could potentially give rise to
an artificial general ability (AGA) factor and one or more group-level factors.
Based on a sample of 591 LLMs and scores from 12 tests aligned with fluid
reasoning (Gf), domain-specific knowledge (Gkn), reading/writing (Grw), and
quantitative knowledge (Gq), we found strong empirical evidence for a positive
manifold and a general factor of ability. Additionally, we identified a
combined Gkn/Grw group-level factor. Finally, the number of LLM parameters
correlated positively with both general factor of ability and Gkn/Grw factor
scores, although the effects showed diminishing returns. We interpreted our
results to suggest that LLMs, like human cognitive abilities, may share a
common underlying efficiency in processing information and solving problems,
though whether LLMs manifest primarily achievement/expertise rather than
intelligence remains to be determined. Finally, while models with greater
numbers of parameters exhibit greater general cognitive-like abilities, akin to
the connection between greater neuronal density and human general intelligence,
other characteristics must also be involved."
OPUS: An Efficient Admissible Algorithm for Unordered Search,"OPUS is a branch and bound search algorithm that enables efficient admissible
search through spaces for which the order of search operator application is not
significant. The algorithm's search efficiency is demonstrated with respect to
very large machine learning search spaces. The use of admissible search is of
potential value to the machine learning community as it means that the exact
learning biases to be employed for complex learning tasks can be precisely
specified and manipulated. OPUS also has potential for application in other
areas of artificial intelligence, notably, truth maintenance."
A Resolution Calculus for Dynamic Semantics,"This paper applies resolution theorem proving to natural language semantics.
The aim is to circumvent the computational complexity triggered by natural
language ambiguities like pronoun binding, by interleaving pronoun binding with
resolution deduction. Therefore disambiguation is only applied to expression
that actually occur during derivations."
A Logic for Reasoning about Evidence,"We introduce a logic for reasoning about evidence that essentially views
evidence as a function from prior beliefs (before making an observation) to
posterior beliefs (after making the observation). We provide a sound and
complete axiomatization for the logic, and consider the complexity of the
decision problem. Although the reasoning in the logic is mainly propositional,
we allow variables representing numbers and quantification over them. This
expressive power seems necessary to capture important properties of evidence."
The Road to Quantum Artificial Intelligence,"This paper overviews the basic principles and recent advances in the emerging
field of Quantum Computation (QC), highlighting its potential application to
Artificial Intelligence (AI). The paper provides a very brief introduction to
basic QC issues like quantum registers, quantum gates and quantum algorithms
and then it presents references, ideas and research guidelines on how QC can be
used to deal with some basic AI problems, such as search and pattern matching,
as soon as quantum computers become widely available."
"Heterogeneous knowledge representation using a finite automaton and
  first order logic: a case study in electromyography","In a certain number of situations, human cognitive functioning is difficult
to represent with classical artificial intelligence structures. Such a
difficulty arises in the polyneuropathy diagnosis which is based on the spatial
distribution, along the nerve fibres, of lesions, together with the synthesis
of several partial diagnoses. Faced with this problem while building up an
expert system (NEUROP), we developed a heterogeneous knowledge representation
associating a finite automaton with first order logic. A number of knowledge
representation problems raised by the electromyography test features are
examined in this study and the expert system architecture allowing such a
knowledge modeling are laid out."
Approximate Counting of Graphical Models Via MCMC Revisited,"In Pe\~na (2007), MCMC sampling is applied to approximately calculate the
ratio of essential graphs (EGs) to directed acyclic graphs (DAGs) for up to 20
nodes. In the present paper, we extend that work from 20 to 31 nodes. We also
extend that work by computing the approximate ratio of connected EGs to
connected DAGs, of connected EGs to EGs, and of connected DAGs to DAGs.
Furthermore, we prove that the latter ratio is asymptotically 1. We also
discuss the implications of these results for learning DAGs from data."
The Complexity of Plan Existence and Evaluation in Probabilistic Domains,"We examine the computational complexity of testing and finding small plans in
probabilistic planning domains with succinct representations. We find that many
problems of interest are complete for a variety of complexity classes: NP,
co-NP, PP, NP^PP, co-NP^PP, and PSPACE. Of these, the probabilistic classes PP
and NP^PP are likely to be of special interest in the field of uncertainty in
artificial intelligence and are deserving of additional study. These results
suggest a fruitful direction of future algorithmic development."
"Probabilistic Conceptual Network: A Belief Representation Scheme for
  Utility-Based Categorization","Probabilistic conceptual network is a knowledge representation scheme
designed for reasoning about concepts and categorical abstractions in
utility-based categorization. The scheme combines the formalisms of abstraction
and inheritance hierarchies from artificial intelligence, and probabilistic
networks from decision analysis. It provides a common framework for
representing conceptual knowledge, hierarchical knowledge, and uncertainty. It
facilitates dynamic construction of categorization decision models at varying
levels of abstraction. The scheme is applied to an automated machining problem
for reasoning about the state of the machine at varying levels of abstraction
in support of actions for maintaining competitiveness of the plant."
Discounting and Combination Operations in Evidential Reasoning,"Evidential reasoning is now a leading topic in Artificial Intelligence.
Evidence is represented by a variety of evidential functions. Evidential
reasoning is carried out by certain kinds of fundamental operation on these
functions. This paper discusses two of the basic operations on evidential
functions, the discount operation and the well-known orthogonal sum operation.
We show that the discount operation is not commutative with the orthogonal sum
operation, and derive expressions for the two operations applied to the various
evidential function."
Managing Uncertainty in Rule Based Cognitive Models,"An experiment replicated and extended recent findings on psychologically
realistic ways of modeling propagation of uncertainty in rule based reasoning.
Within a single production rule, the antecedent evidence can be summarized by
taking the maximum of disjunctively connected antecedents and the minimum of
conjunctively connected antecedents. The maximum certainty factor attached to
each of the rule's conclusions can be sealed down by multiplication with this
summarized antecedent certainty. Heckerman's modified certainty factor
technique can be used to combine certainties for common conclusions across
production rules."
Decision Under Uncertainty in Diagnosis,"This paper describes the incorporation of uncertainty in diagnostic reasoning
based on the set covering model of Reggia et. al. extended to what in the
Artificial Intelligence dichotomy between deep and compiled (shallow, surface)
knowledge based diagnosis may be viewed as the generic form at the compiled end
of the spectrum. A major undercurrent in this is advocating the need for a
strong underlying model and an integrated set of support tools for carrying
such a model in order to deal with uncertainty."
"Relative Entropy, Probabilistic Inference and AI","Various properties of relative entropy have led to its widespread use in
information theory. These properties suggest that relative entropy has a role
to play in systems that attempt to perform inference in terms of probability
distributions. In this paper, I will review some basic properties of relative
entropy as well as its role in probabilistic inference. I will also mention
briefly a few existing and potential applications of relative entropy to
so-called artificial intelligence (AI)."
An Evaluation of Two Alternatives to Minimax,"In the field of Artificial Intelligence, traditional approaches to choosing
moves in games involve the we of the minimax algorithm. However, recent
research results indicate that minimizing may not always be the best approach.
In this paper we summarize the results of some measurements on several model
games with several different evaluation functions. These measurements, which
are presented in detail in [NPT], show that there are some new algorithms that
can make significantly better use of evaluation function values than the
minimax algorithm does."
Towards a Simulation-Based Programming Paradigm for AI applications,"We present initial ideas for a programming paradigm based on simulation that
is targeted towards applications of artificial intelligence (AI). The approach
aims at integrating techniques from different areas of AI and is based on the
idea that simulated entities may freely exchange data and behavioural patterns.
We define basic notions of a simulation-based programming paradigm and show how
it can be used for implementing AI applications."
Research Priorities for Robust and Beneficial Artificial Intelligence,"Success in the quest for artificial intelligence has the potential to bring
unprecedented benefits to humanity, and it is therefore worthwhile to
investigate how to maximize these benefits while avoiding potential pitfalls.
This article gives numerous examples (which should by no means be construed as
an exhaustive list) of such worthwhile research aimed at ensuring that AI
remains robust and beneficial."
The Computational Power of Dynamic Bayesian Networks,"This paper considers the computational power of constant size, dynamic
Bayesian networks. Although discrete dynamic Bayesian networks are no more
powerful than hidden Markov models, dynamic Bayesian networks with continuous
random variables and discrete children of continuous parents are capable of
performing Turing-complete computation. With modified versions of existing
algorithms for belief propagation, such a simulation can be carried out in real
time. This result suggests that dynamic Bayesian networks may be more powerful
than previously considered. Relationships to causal models and recurrent neural
networks are also discussed."
Towards Verified Artificial Intelligence,"Verified artificial intelligence (AI) is the goal of designing AI-based
systems that that have strong, ideally provable, assurances of correctness with
respect to mathematically-specified requirements. This paper considers Verified
AI from a formal methods perspective. We describe five challenges for achieving
Verified AI, and five corresponding principles for addressing these challenges."
Don't Fear the Reaper: Refuting Bostrom's Superintelligence Argument,"In recent years prominent intellectuals have raised ethical concerns about
the consequences of artificial intelligence. One concern is that an autonomous
agent might modify itself to become ""superintelligent"" and, in supremely
effective pursuit of poorly specified goals, destroy all of humanity. This
paper considers and rejects the possibility of this outcome. We argue that this
scenario depends on an agent's ability to rapidly improve its ability to
predict its environment through self-modification. Using a Bayesian model of a
reasoning agent, we show that there are important limitations to how an agent
may improve its predictive ability through self-modification alone. We conclude
that concern about this artificial intelligence outcome is misplaced and better
directed at policy questions around data access and storage."
General Video Game AI: Learning from Screen Capture,"General Video Game Artificial Intelligence is a general game playing
framework for Artificial General Intelligence research in the video-games
domain. In this paper, we propose for the first time a screen capture learning
agent for General Video Game AI framework. A Deep Q-Network algorithm was
applied and improved to develop an agent capable of learning to play different
games in the framework. After testing this algorithm using various games of
different categories and difficulty levels, the results suggest that our
proposed screen capture learning agent has the potential to learn many
different games using only a single learning algorithm."
"Feasibility Study: Moving Non-Homogeneous Teams in Congested Video Game
  Environments","Multi-agent path finding (MAPF) is a well-studied problem in artificial
intelligence, where one needs to find collision-free paths for agents with
given start and goal locations. In video games, agents of different types often
form teams. In this paper, we demonstrate the usefulness of MAPF algorithms
from artificial intelligence for moving such non-homogeneous teams in congested
video game environments."
Self-Regulating Artificial General Intelligence,"Here we examine the paperclip apocalypse concern for artificial general
intelligence (or AGI) whereby a superintelligent AI with a simple goal (ie.,
producing paperclips) accumulates power so that all resources are devoted
towards that simple goal and are unavailable for any other use. We provide
conditions under which a paper apocalypse can arise but also show that, under
certain architectures for recursive self-improvement of AIs, that a paperclip
AI may refrain from allowing power capabilities to be developed. The reason is
that such developments pose the same control problem for the AI as they do for
humans (over AIs) and hence, threaten to deprive it of resources for its
primary goal."
"Analysis of the Relation between Artificial Intelligence and the
  Internet from the Perspective of Brain Science","Artificial intelligence (AI) like deep learning, cloud AI computation has
been advancing at a rapid pace since 2014. There is no doubt that the
prosperity of AI is inseparable with the development of the Internet. However,
there has been little attention to the link between AI and the internet. This
paper explores them with brain insights mainly from four views:1) How is the
general relation between artificial intelligence and Internet of Things, cloud
computing, big data and Industrial Internet from the perspective of brain
science. 2) Construction of a new AI system model with the Internet and brain
science."
Viewpoint: Artificial Intelligence and Labour,"The welfare of modern societies has been intrinsically linked to wage labour.
With some exceptions, the modern human has to sell her labour-power to be able
reproduce biologically and socially. Thus, a lingering fear of technological
unemployment features predominately as a theme among Artificial Intelligence
researchers. In this short paper we show that, if past trends are anything to
go by, this fear is irrational. On the contrary, we argue that the main problem
humanity will be facing is the normalisation of extremely long working hours."
The IQ of Artificial Intelligence,"All it takes to identify the computer programs which are Artificial
Intelligence is to give them a test and award AI to those that pass the test.
Let us say that the scores they earn at the test will be called IQ. We cannot
pinpoint a minimum IQ threshold that a program has to cover in order to be AI,
however, we will choose a certain value. Thus, our definition for AI will be
any program the IQ of which is above the chosen value. While this idea has
already been implemented in [3], here we will revisit this construct in order
to introduce certain improvements."
Autonomous Wireless Systems with Artificial Intelligence,"This paper discusses technology and opportunities to embrace artificial
intelligence (AI) in the design of autonomous wireless systems. We aim to
provide readers with motivation and general AI methodology of autonomous agents
in the context of self-organization in real time by unifying knowledge
management with sensing, reasoning and active learning. We highlight
differences between training-based methods for matching problems and
training-free methods for environment-specific problems. Finally, we
conceptually introduce the functions of an autonomous agent with knowledge
management."
Uncertainty Aware AI ML: Why and How,"This paper argues the need for research to realize uncertainty-aware
artificial intelligence and machine learning (AI\&ML) systems for decision
support by describing a number of motivating scenarios. Furthermore, the paper
defines uncertainty-awareness and lays out the challenges along with surveying
some promising research directions. A theoretical demonstration illustrates how
two emerging uncertainty-aware ML and AI technologies could be integrated and
be of value for a route planning operation."
Growing and Retaining AI Talent for the United States Government,"Artificial Intelligence and Machine Learning have become transformative to a
number of industries, and as such many industries need for AI talent is
increasing the demand for individuals with these skills. This continues to
exacerbate the difficulty of acquiring and retaining talent for the United
States Federal Government, both for its direct employees as well as the
companies that support it. We take the position that by focusing on growing and
retaining current talent through a number of cultural changes, the government
can work to remediate this problem today."
"Tentacular Artificial Intelligence, and the Architecture Thereof,
  Introduced","We briefly introduce herein a new form of distributed, multi-agent artificial
intelligence, which we refer to as ""tentacular."" Tentacular AI is distinguished
by six attributes, which among other things entail a capacity for reasoning and
planning based in highly expressive calculi (logics), and which enlists
subsidiary agents across distances circumscribed only by the reach of one or
more given networks."
Mining useful Macro-actions in Planning,"Planning has achieved significant progress in recent years. Among the various
approaches to scale up plan synthesis, the use of macro-actions has been widely
explored. As a first stage towards the development of a solution to learn
on-line macro-actions, we propose an algorithm to identify useful macro-actions
based on data mining techniques. The integration in the planning search of
these learned macro-actions shows significant improvements over six classical
planning benchmarks."
Truly Autonomous Machines Are Ethical,"While many see the prospect of autonomous machines as threatening, autonomy
may be exactly what we want in a superintelligent machine. There is a sense of
autonomy, deeply rooted in the ethical literature, in which an autonomous
machine is necessarily an ethical one. Development of the theory underlying
this idea not only reveals the advantages of autonomy, but it sheds light on a
number of issues in the ethics of artificial intelligence. It helps us to
understand what sort of obligations we owe to machines, and what obligations
they owe to us. It clears up the issue of assigning responsibility to machines
or their creators. More generally, a concept of autonomy that is adequate to
both human and artificial intelligence can lead to a more adequate ethical
theory for both."
Making AI meaningful again,"Artificial intelligence (AI) research enjoyed an initial period of enthusiasm
in the 1970s and 80s. But this enthusiasm was tempered by a long interlude of
frustration when genuinely useful AI applications failed to be forthcoming.
Today, we are experiencing once again a period of enthusiasm, fired above all
by the successes of the technology of deep neural networks or deep machine
learning. In this paper we draw attention to what we take to be serious
problems underlying current views of artificial intelligence encouraged by
these successes, especially in the domain of language processing. We then show
an alternative approach to language-centric AI, in which we identify a role for
philosophy."
Learning Logistic Circuits,"This paper proposes a new classification model called logistic circuits. On
MNIST and Fashion datasets, our learning algorithm outperforms neural networks
that have an order of magnitude more parameters. Yet, logistic circuits have a
distinct origin in symbolic AI, forming a discriminative counterpart to
probabilistic-logical circuits such as ACs, SPNs, and PSDDs. We show that
parameter learning for logistic circuits is convex optimization, and that a
simple local search algorithm can induce strong model structures from data."
Integrating Artificial and Human Intelligence for Efficient Translation,"Current advances in machine translation increase the need for translators to
switch from traditional translation to post-editing of machine-translated text,
a process that saves time and improves quality. Human and artificial
intelligence need to be integrated in an efficient way to leverage the
advantages of both for the translation task. This paper outlines approaches at
this boundary of AI and HCI and discusses open research questions to further
advance the field."
Ethics of Artificial Intelligence Demarcations,"In this paper we present a set of key demarcations, particularly important
when discussing ethical and societal issues of current AI research and
applications. Properly distinguishing issues and concerns related to Artificial
General Intelligence and weak AI, between symbolic and connectionist AI, AI
methods, data and applications are prerequisites for an informed debate. Such
demarcations would not only facilitate much-needed discussions on ethics on
current AI technologies and research. In addition sufficiently establishing
such demarcations would also enhance knowledge-sharing and support rigor in
interdisciplinary research between technical and social sciences."
Autonomous Haiku Generation,"Artificial Intelligence is an excellent tool to improve efficiency and lower
cost in many quantitative real world applications, but what if the task is not
easily defined? What if the task is generating creativity? Poetry is a creative
endeavor that is highly difficult to both grasp and achieve with any level of
competence. As Rita Dove, a famous American poet and author states, ""Poetry is
language at its most distilled and most powerful."" Taking Doves quote as an
inspiration, our task was to generate high quality haikus using artificial
intelligence and deep learning."
"Clinical acceptance of software based on artificial intelligence
  technologies (radiology)","Aim: provide a methodological framework for the process of clinical tests,
clinical acceptance, and scientific assessment of algorithms and software based
on the artificial intelligence (AI) technologies. Clinical tests are considered
as a preparation stage for the software registration as a medical product. The
authors propose approaches to evaluate accuracy and efficiency of the AI
algorithms for radiology."
"Analyzing Cyber-Physical Systems from the Perspective of Artificial
  Intelligence","Principles of modern cyber-physical system (CPS) analysis are based on
analytical methods that depend on whether safety or liveness requirements are
considered. Complexity is abstracted through different techniques, ranging from
stochastic modelling to contracts. However, both distributed heuristics and
Artificial Intelligence (AI)-based approaches as well as the user perspective
or unpredictable effects, such as accidents or the weather, introduce enough
uncertainty to warrant reinforcement-learning-based approaches. This paper
compares traditional approaches in the domain of CPS modelling and analysis
with the AI researcher perspective to exploring unknown complex systems."
The Quest for Interpretable and Responsible Artificial Intelligence,"Artificial Intelligence (AI) provides many opportunities to improve private
and public life. Discovering patterns and structures in large troves of data in
an automated manner is a core component of data science, and currently drives
applications in computational biology, finance, law and robotics. However, such
a highly positive impact is coupled with significant challenges: How do we
understand the decisions suggested by these systems in order that we can trust
them? How can they be held accountable for those decisions?
  In this short survey, we cover some of the motivations and trends in the area
that attempt to address such questions."
Artificial Intelligence in Surgery,"Artificial Intelligence (AI) is gradually changing the practice of surgery
with the advanced technological development of imaging, navigation and robotic
intervention. In this article, the recent successful and influential
applications of AI in surgery are reviewed from pre-operative planning and
intra-operative guidance to the integration of surgical robots. We end with
summarizing the current state, emerging trends and major challenges in the
future development of AI in surgery."
"Proceedings of the Artificial Intelligence for Cyber Security (AICS)
  Workshop 2020","The workshop will focus on the application of artificial intelligence to
problems in cyber security. AICS 2020 emphasis will be on human-machine teaming
within the context of cyber security problems and will specifically explore
collaboration between human operators and AI technologies. The workshop will
address applicable areas of AI, such as machine learning, game theory, natural
language processing, knowledge representation, automated and assistive
reasoning and human machine interactions. Further, cyber security application
areas with a particular emphasis on the characterization and deployment of
human-machine teaming will be the focus."
"The Archimedean trap: Why traditional reinforcement learning will
  probably not yield AGI","After generalizing the Archimedean property of real numbers in such a way as
to make it adaptable to non-numeric structures, we demonstrate that the real
numbers cannot be used to accurately measure non-Archimedean structures. We
argue that, since an agent with Artificial General Intelligence (AGI) should
have no problem engaging in tasks that inherently involve non-Archimedean
rewards, and since traditional reinforcement learning rewards are real numbers,
therefore traditional reinforcement learning probably will not lead to AGI. We
indicate two possible ways traditional reinforcement learning could be altered
to remove this roadblock."
"ConsciousControlFlow(CCF): A Demonstration for conscious Artificial
  Intelligence","In this demo, we present ConsciousControlFlow(CCF), a prototype system to
demonstrate conscious Artificial Intelligence (AI). The system is based on the
computational model for consciousness and the hierarchy of needs. CCF supports
typical scenarios to show the behaviors and the mental activities of conscious
AI. We demonstrate that CCF provides a useful tool for effective machine
consciousness demonstration and human behavior study assistance."
Towards AI Forensics: Did the Artificial Intelligence System Do It?,"Artificial intelligence (AI) makes decisions impacting our daily lives in an
increasingly autonomous manner. Their actions might cause accidents, harm, or,
more generally, violate regulations. Determining whether an AI caused a
specific event and, if so, what triggered the AI's action, are key forensic
questions. We provide a conceptualization of the problems and strategies for
forensic investigation. We focus on AI that is potentially ``malicious by
design'' and grey box analysis. Our evaluation using convolutional neural
networks illustrates challenges and ideas for identifying malicious AI."
Drug discovery with explainable artificial intelligence,"Deep learning bears promise for drug discovery, including advanced image
analysis, prediction of molecular structure and function, and automated
generation of innovative chemical entities with bespoke properties. Despite the
growing number of successful prospective applications, the underlying
mathematical models often remain elusive to interpretation by the human mind.
There is a demand for 'explainable' deep learning methods to address the need
for a new narrative of the machine language of the molecular sciences. This
review summarizes the most prominent algorithmic concepts of explainable
artificial intelligence, and dares a forecast of the future opportunities,
potential applications, and remaining challenges."
Ethics of Artificial Intelligence in Surgery,"Here we discuss the four key principles of bio-medical ethics from surgical
context. We elaborate on the definition of 'fairness' and its implications in
AI system design, with taxonomy of algorithmic biases in AI. We discuss the
shifts in ethical paradigms as the degree of autonomy in AI systems continue to
evolve. We also emphasize the need for continuous revisions of ethics in AI due
to evolution and dynamic nature of AI systems and technologies."
On Controllability of AI,"Invention of artificial general intelligence is predicted to cause a shift in
the trajectory of human civilization. In order to reap the benefits and avoid
pitfalls of such powerful technology it is important to be able to control it.
However, possibility of controlling artificial general intelligence and its
more advanced version, superintelligence, has not been formally established. In
this paper, we present arguments as well as supporting evidence from multiple
domains indicating that advanced AI can't be fully controlled. Consequences of
uncontrollability of AI are discussed with respect to future of humanity and
research on AI, and AI safety and security."
"Applications of Artificial Intelligence in Live Action Role-Playing
  Games (LARP)","Live Action Role-Playing (LARP) games and similar experiences are becoming a
popular game genre. Here, we discuss how artificial intelligence techniques,
particularly those commonly used in AI for Games, could be applied to LARP. We
discuss the specific properties of LARP that make it a surprisingly suitable
application field, and provide a brief overview of some existing approaches. We
then outline several directions where utilizing AI seems beneficial, by both
making LARPs easier to organize, and by enhancing the player experience with
elements not possible without AI."
The DigitalTwin from an Artificial Intelligence Perspective,"Services for Cyber-Physical Systems based on Artificial Intelligence and
Machine Learning require a virtual representation of the physical. To reduce
modeling efforts and to synchronize results, for each system, a common and
unique virtual representation used by all services during the whole system
life-cycle is needed, i.e. a DigitalTwin. In this paper such a DigitalTwin,
namely the AI reference model AITwin, is defined. This reference model is
verified by using a running example from process industry and by analyzing the
work done in recent projects."
"Using Side Channel Information and Artificial Intelligence for Malware
  Detection","Cybersecurity continues to be a difficult issue for society especially as the
number of networked systems grows. Techniques to protect these systems range
from rules-based to artificial intelligence-based intrusion detection systems
and anti-virus tools. These systems rely upon the information contained in the
network packets and download executables to function. Side channel information
leaked from hardware has been shown to reveal secret information in systems
such as encryption keys. This work demonstrates that side channel information
can be used to detect malware running on a computing platform without access to
the code involved."
Hard Choices and Hard Limits for Artificial Intelligence,"Artificial intelligence (AI) is supposed to help us make better choices. Some
of these choices are small, like what route to take to work, or what music to
listen to. Others are big, like what treatment to administer for a disease or
how long to sentence someone for a crime. If AI can assist with these big
decisions, we might think it can also help with hard choices, cases where
alternatives are neither better, worse nor equal but on a par. The aim of this
paper, however, is to show that this view is mistaken: the fact of parity shows
that there are hard limits on AI in decision making and choices that AI cannot,
and should not, resolve."
Self-Adaptive Swarm System (SASS),"Distributed artificial intelligence (DAI) studies artificial intelligence
entities working together to reason, plan, solve problems, organize behaviors
and strategies, make collective decisions and learn. This Ph.D. research
proposes a principled Multi-Agent Systems (MAS) cooperation framework --
Self-Adaptive Swarm System (SASS) -- to bridge the fourth level automation gap
between perception, communication, planning, execution, decision-making, and
learning."
"Perceptions and attitudes of Children and Young People to Artificial
  Intelligence in Medicine","There is increasing interest in Artificial Intelligence and its application
to medicine. Perceptions are less well-known, notably amongst children and
young people. 21 members of a Young Persons Advisory Group for research,
recommend creating an enabling environment with children and young people,
through educational workshops with practical examples that use Artificial
Intelligence to help, but not replace humans, address issues, build trust, and
effectively communicate about potential opportunities."
"Influential Papers in Artificial Intelligence and Paediatrics: Assessing
  RPYS by Experts Review","The use of artificial intelligence in paediatrics has vastly increased in the
last few years. Interestingly, no historical bibliometric study analysing the
knowledge development in this specific paediatric field has been performed yet,
thus our study aimed to close this gap. References Publication Years
Spectrography (RPYS), more precisely CitedReferenceExplorer (CRE) software tool
was employed to achieve this aim. We identified 28 influential papers and
domain experts validation showed that both, the RPYS method and CRE tool
performed adequately in the identification process."
Sustainable Artificial Intelligence through Continual Learning,"The increasing attention on Artificial Intelligence (AI) regulation has led
to the definition of a set of ethical principles grouped into the Sustainable
AI framework. In this article, we identify Continual Learning, an active area
of AI research, as a promising approach towards the design of systems compliant
with the Sustainable AI principles. While Sustainable AI outlines general
desiderata for ethical applications, Continual Learning provides means to put
such desiderata into practice."
A Fast Evolutionary adaptation for MCTS in Pommerman,"Artificial Intelligence, when amalgamated with games makes the ideal
structure for research and advancing the field. Multi-agent games have multiple
controls for each agent which generates huge amounts of data while increasing
search complexity. Thus, we need advanced search methods to find a solution and
create an artificially intelligent agent. In this paper, we propose our novel
Evolutionary Monte Carlo Tree Search (FEMCTS) agent which borrows ideas from
Evolutionary Algorthims (EA) and Monte Carlo Tree Search (MCTS) to play the
game of Pommerman. It outperforms Rolling Horizon Evolutionary Algorithm (RHEA)
significantly in high observability settings and performs almost as well as
MCTS for most game seeds, outperforming it in some cases."
"An overview of the quantitative causality analysis and causal graph
  reconstruction based on a rigorous formalism of information flow","Inference of causal relations from data now has become an important field in
artificial intelligence. During the past 16 years, causality analysis (in a
quantitative sense) has been developed independently in physics from first
principles. This short note is a brief summary of this line of work, including
part of the theory and several representative applications."
Generalizable Neuro-symbolic Systems for Commonsense Question Answering,"This chapter illustrates how suitable neuro-symbolic models for language
understanding can enable domain generalizability and robustness in downstream
tasks. Different methods for integrating neural language models and knowledge
graphs are discussed. The situations in which this combination is most
appropriate are characterized, including quantitative evaluation and
qualitative error analysis on a variety of commonsense question answering
benchmark datasets."
Artificial Intelligence and Auction Design,"Motivated by online advertising auctions, we study auction design in repeated
auctions played by simple Artificial Intelligence algorithms (Q-learning). We
find that first-price auctions with no additional feedback lead to
tacit-collusive outcomes (bids lower than values), while second-price auctions
do not. We show that the difference is driven by the incentive in first-price
auctions to outbid opponents by just one bid increment. This facilitates
re-coordination on low bids after a phase of experimentation. We also show that
providing information about lowest bid to win, as introduced by Google at the
time of switch to first-price auctions, increases competitiveness of auctions."
The Mathematics of Artificial Intelligence,"We currently witness the spectacular success of artificial intelligence in
both science and public life. However, the development of a rigorous
mathematical foundation is still at an early stage. In this survey article,
which is based on an invited lecture at the International Congress of
Mathematicians 2022, we will in particular focus on the current ""workhorse"" of
artificial intelligence, namely deep neural networks. We will present the main
theoretical directions along with several exemplary results and discuss key
open problems."
"Introduction to the Artificial Intelligence that can be applied to the
  Network Automation Journey","The computer network world is changing and the NetDevOps approach has brought
the dynamics of applications and systems into the field of communication
infrastructure. Businesses are changing and businesses are faced with
difficulties related to the diversity of hardware and software that make up
those infrastructures. The ""Intent-Based Networking - Concepts and Definitions""
document describes the different parts of the ecosystem that could be involved
in NetDevOps. The recognize, generate intent, translate and refine features
need a new way to implement algorithms. This is where artificial intelligence
comes in."
Enriching Artificial Intelligence Explanations with Knowledge Fragments,"Artificial Intelligence models are increasingly used in manufacturing to
inform decision-making. Responsible decision-making requires accurate forecasts
and an understanding of the models' behavior. Furthermore, the insights into
models' rationale can be enriched with domain knowledge. This research builds
explanations considering feature rankings for a particular forecast, enriching
them with media news entries, datasets' metadata, and entries from the Google
Knowledge Graph. We compare two approaches (embeddings-based and
semantic-based) on a real-world use case regarding demand forecasting."
"Can Requirements Engineering Support Explainable Artificial
  Intelligence? Towards a User-Centric Approach for Explainability Requirements","With the recent proliferation of artificial intelligence systems, there has
been a surge in the demand for explainability of these systems. Explanations
help to reduce system opacity, support transparency, and increase stakeholder
trust. In this position paper, we discuss synergies between requirements
engineering (RE) and Explainable AI (XAI). We highlight challenges in the field
of XAI, and propose a framework and research directions on how RE practices can
help to mitigate these challenges."
Mediators: Conversational Agents Explaining NLP Model Behavior,"The human-centric explainable artificial intelligence (HCXAI) community has
raised the need for framing the explanation process as a conversation between
human and machine. In this position paper, we establish desiderata for
Mediators, text-based conversational agents which are capable of explaining the
behavior of neural models interactively using natural language. From the
perspective of natural language processing (NLP) research, we engineer a
blueprint of such a Mediator for the task of sentiment analysis and assess how
far along current research is on the path towards dialogue-based explanations."
Recent Developments in AI and USPTO Open Data,"The USPTO disseminates one of the largest publicly accessible repositories of
scientific, technical, and commercial data worldwide. USPTO data has
historically seen frequent use in fields such as patent analytics, economics,
and prosecution & litigation tools. This article highlights an emerging class
of usecases directed to the research, development, and application of
artificial intelligence technology. Such usecases contemplate both the delivery
of artificial intelligence capabilities for practical IP applications and the
enablement of future state-of-the-art artificial intelligence research via
USPTO data products. Examples from both within and beyond the USPTO are offered
as case studies."
A Historical Interaction between Artificial Intelligence and Philosophy,"This paper reviews the historical development of AI and representative
philosophical thinking from the perspective of the research paradigm.
Additionally, it considers the methodology and applications of AI from a
philosophical perspective and anticipates its continued advancement. In the
history of AI, Symbolism and connectionism are the two main paradigms in AI
research. Symbolism holds that the world can be explained by symbols and dealt
with through precise, logical processes, but connectionism believes this
process should be implemented through artificial neural networks. Regardless of
how intelligent machines or programs should achieve their smart goals, the
historical development of AI demonstrates the best answer at this time. Still,
it is not the final answer of AI research."
"Feynman on Artificial Intelligence and Machine Learning, with Updates","I present my recollections of Richard Feynman's mid-1980s interest in
artificial intelligence and neural networks, set in the technical context of
the physics-related approaches to neural networks of that time. I attempt to
evaluate his ideas in the light of the substantial advances in the field since
then, and vice versa. There are aspects of Feynman's interests that I think
have been largely achieved and others that remain excitingly open, notably in
computational science, and potentially including the revival of symbolic
methods therein."
Leveraging Artificial Intelligence on Binary Code Comprehension,"Understanding binary code is an essential but complex software engineering
task for reverse engineering, malware analysis, and compiler optimization.
Unlike source code, binary code has limited semantic information, which makes
it challenging for human comprehension. At the same time, compiling source to
binary code, or transpiling among different programming languages (PLs) can
provide a way to introduce external knowledge into binary comprehension. We
propose to develop Artificial Intelligence (AI) models that aid human
comprehension of binary code. Specifically, we propose to incorporate domain
knowledge from large corpora of source code (e.g., variable names, comments) to
build AI models that capture a generalizable representation of binary code.
Lastly, we will investigate metrics to assess the performance of models that
apply to binary code by using human studies of comprehension."
The Impact of Generative AI on the Future of Visual Content Marketing,"In today's world of marketing, it is necessary to have visually appealing
content. Visual material has become an essential area of focus for every
company as a result of the widespread availability of gadgets for mass
communication and extended visual advancements. Similarly, artificial
intelligence is also gaining ground and it is proving to be the most
revolutionary technological advancement thus far. The integration of visual
content with artificial intelligence is the key to acquiring and retaining
loyal customers; its absence from the overarching marketing strategy of any
production raises a red flag that could ultimately result in a smaller market
share for that company."
"Ethical Design of Computers: From Semiconductors to IoT and Artificial
  Intelligence","Computing systems are tightly integrated today into our professional, social,
and private lives. An important consequence of this growing ubiquity of
computing is that it can have significant ethical implications of which
computing professionals should take account. In most real-world scenarios, it
is not immediately obvious how particular technical choices during the design
and use of computing systems could be viewed from an ethical perspective. This
article provides a perspective on the ethical challenges within semiconductor
chip design, IoT applications, and the increasing use of artificial
intelligence in the design processes, tools, and hardware-software stacks of
these systems."
Ithaca. A Tool for Integrating Fuzzy Logic in Unity,"Ithaca is a Fuzzy Logic (FL) plugin for developing artificial intelligence
systems within the Unity game engine. Its goal is to provide an intuitive and
natural way to build advanced artificial intelligence systems, making the
implementation of such a system faster and more affordable. The software is
made up by a C\# framework and an Application Programming Interface (API) for
writing inference systems, as well as a set of tools for graphic development
and debugging. Additionally, a Fuzzy Control Language (FCL) parser is provided
in order to import systems previously defined using this standard."
Towards the Linear Algebra Based Taxonomy of XAI Explanations,"This paper proposes an alternative approach to the basic taxonomy of
explanations produced by explainable artificial intelligence techniques.
Methods of Explainable Artificial Intelligence (XAI) were developed to answer
the question why a certain prediction or estimation was made, preferably in
terms easy to understand by the human agent. XAI taxonomies proposed in the
literature mainly concentrate their attention on distinguishing explanations
with respect to involving the human agent, which makes it complicated to
provide a more mathematical approach to distinguish and compare different
explanations. This paper narrows its attention to the cases where the data set
of interest belongs to $\mathbb{R} ^n$ and proposes a simple linear
algebra-based taxonomy for local explanations."
"Attribution-Scores and Causal Counterfactuals as Explanations in
  Artificial Intelligence","In this expository article we highlight the relevance of explanations for
artificial intelligence, in general, and for the newer developments in {\em
explainable AI}, referring to origins and connections of and among different
approaches. We describe in simple terms, explanations in data management and
machine learning that are based on attribution-scores, and counterfactuals as
found in the area of causality. We elaborate on the importance of logical
reasoning when dealing with counterfactuals, and their use for score
computation."
Low impact agency: review and discussion,"Powerful artificial intelligence poses an existential threat if the AI
decides to drastically change the world in pursuit of its goals. The hope of
low-impact artificial intelligence is to incentivize AI to not do that just
because this causes a large impact in the world. In this work, we first review
the concept of low-impact agency and previous proposals to approach the
problem, and then propose future research directions in the topic, with the
goal to ensure low-impactedness is useful in making AI safe."
"Conceptual Modeling and Artificial Intelligence: A Systematic Mapping
  Study","In conceptual modeling (CM), humans apply abstraction to represent excerpts
of reality for means of understanding and communication, and processing by
machines. Artificial Intelligence (AI) is applied to vast amounts of data to
automatically identify patterns or classify entities. While CM produces
comprehensible and explicit knowledge representations, the outcome of AI
algorithms often lacks these qualities while being able to extract knowledge
from large and unstructured representations. Recently, a trend toward
intertwining CM and AI emerged. This systematic mapping study shows how this
interdisciplinary research field is structured, which mutual benefits are
gained by the intertwining, and future research directions."
"On the Potential of Artificial Intelligence Chatbots for Data
  Exploration of Federated Bioinformatics Knowledge Graphs","In this paper, we present work in progress on the role of artificial
intelligence (AI) chatbots, such as ChatGPT, in facilitating data access to
federated knowledge graphs. In particular, we provide examples from the field
of bioinformatics, to illustrate the potential use of Conversational AI to
describe datasets, as well as generate and explain (federated) queries across
datasets for the benefit of domain experts."
Artificial Intelligence in 3GPP 5G-Advanced: A Survey,"Industries worldwide are being transformed by artificial intelligence (AI),
and the telecom industry is no different. Standardization is critical for
industry alignment to achieve widespread adoption of AI in telecom. The 3rd
generation partnership project (3GPP) Release 18 is the first release of
5G-Advanced, which includes a diverse set of study and work items dedicated to
AI. This article provides a holistic overview of the state of the art in the
3GPP work on AI in 5G-Advanced, by presenting the various 3GPP Release-18
activities on AI as an organic whole, explaining in detail the design aspects,
and sharing various design rationales influencing standardization."
Diversity and Inclusion in Artificial Intelligence,"To date, there has been little concrete practical advice about how to ensure
that diversity and inclusion considerations should be embedded within both
specific Artificial Intelligence (AI) systems and the larger global AI
ecosystem. In this chapter, we present a clear definition of diversity and
inclusion in AI, one which positions this concept within an evolving and
holistic ecosystem. We use this definition and conceptual framing to present a
set of practical guidelines primarily aimed at AI technologists, data
scientists and project leaders."
"Optimizing National Security Strategies through LLM-Driven Artificial
  Intelligence Integration","As artificial intelligence and machine learning continue to advance, we must
understand their strategic importance in national security. This paper focuses
on unique AI applications in the military, emphasizes strategic imperatives for
success, and aims to rekindle excitement about AI's role in national security.
We will examine the United States progress in AI and ML from a military
standpoint, discuss the importance of securing these technologies from
adversaries, and explore the challenges and risks associated with their
integration. Finally, we will highlight the strategic significance of AI to
national security and a set of strategic imperatives for military leaders and
policymakers"
Continual Learning as Computationally Constrained Reinforcement Learning,"An agent that efficiently accumulates knowledge to develop increasingly
sophisticated skills over a long lifetime could advance the frontier of
artificial intelligence capabilities. The design of such agents, which remains
a long-standing challenge of artificial intelligence, is addressed by the
subject of continual learning. This monograph clarifies and formalizes concepts
of continual learning, introducing a framework and set of tools to stimulate
further research."
Control and Monitoring of Artificial Intelligence Algorithms,"This paper elucidates the importance of governing an artificial intelligence
model post-deployment and overseeing potential fluctuations in the distribution
of present data in contrast to the training data. The concepts of data drift
and concept drift are explicated, along with their respective foundational
distributions. Furthermore, a range of metrics is introduced, which can be
utilized to scrutinize the model's performance concerning potential temporal
variations."
"Large-scale Generative Simulation Artificial Intelligence: the Next
  Hotspot in Generative AI","The concept of GenAI has been developed for decades. Until recently, it has
impressed us with substantial breakthroughs in natural language processing and
computer vision, actively engaging in industrial scenarios. Noticing the
practical challenges, e.g., limited learning resources, and overly dependencies
on scientific discovery empiricism, we nominate large-scale generative
simulation artificial intelligence (LS-GenAI) as the next hotspot for GenAI to
connect."
Science Communications for Explainable Artificial Intelligence,"Artificial Intelligence (AI) has a communication problem. XAI methods have
been used to make AI more understandable and helped resolve some of the
transparency issues that inhibit AI's broader usability. However, user
evaluation studies reveal that the often numerical explanations provided by XAI
methods have not always been effective for many types of users of AI systems.
This article aims to adapt the major communications models from Science
Communications into a framework for practitioners to understand, influence, and
integrate the context of audiences both for their communications supporting AI
literacy in the public and in designing XAI systems that are more adaptive to
different users."
"Approaches to Generative Artificial Intelligence, A Social Justice
  Perspective","In the 2023-2024 academic year, the widespread availability of generative
artificial intelligence, exemplified by ChatGPT's 1.6 billion monthly visits,
is set to impact academic integrity. With 77% of high school students
previously reporting engagement in dishonest behaviour, the rise of AI-driven
writing assistance, dubbed 'AI-giarism' by Chan (arXiv:2306.03358v2), will make
plagiarism more accessible and less detectable. While these concerns are
urgent, they also raise broader questions about the revolutionary nature of
this technology, including autonomy, data privacy, copyright, and equity. This
paper aims to explore generative AI from a social justice perspective,
examining the training of these models, the inherent biases, and the potential
injustices in detecting AI-generated writing."
"Does Artificial Intelligence benefit UK businesses? An empirical study
  of the impact of AI on productivity","Media hype and technological breakthroughs are fuelling the race to adopt
Artificial Intelligence amongst the business community, but is there evidence
to suggest this will increase productivity? This paper uses 2015-2019 microdata
from the UK Office for National Statistics to identify if the adoption of
Artificial Intelligence techniques increases labour productivity in UK
businesses. Using fixed effects estimation (Within Group) with a log-linear
regression specification the paper concludes that there is no statistically
significant impact of AI adoption on labour productivity."
Artificial intelligence and the skill premium,"What will likely be the effect of the emergence of ChatGPT and other forms of
artificial intelligence (AI) on the skill premium? To address this question, we
develop a nested constant elasticity of substitution production function that
distinguishes between industrial robots and AI. Industrial robots predominantly
substitute for low-skill workers, whereas AI mainly helps to perform the tasks
of high-skill workers. We show that AI reduces the skill premium as long as it
is more substitutable for high-skill workers than low-skill workers are for
high-skill workers."
"An international treaty to implement a global compute cap for advanced
  artificial intelligence","This paper presents an international treaty to reduce risks from the
development of advanced artificial intelligence (AI). The main provision of the
treaty is a global compute cap: a ban on the development of AI systems above an
agreed-upon computational resource threshold. The treaty also proposes the
development and testing of emergency response plans, negotiations to establish
an international agency to enforce the treaty, the establishment of new
communication channels and whistleblower protections, and a commitment to avoid
an AI arms race. We hope this treaty serves as a useful template for global
leaders as they implement governance regimes to protect civilization from the
dangers of advanced artificial intelligence."
"Justifiable Artificial Intelligence: Engineering Large Language Models
  for Legal Applications","In this work, I discuss how Large Language Models can be applied in the legal
domain, circumventing their current drawbacks. Despite their large success and
acceptance, their lack of explainability hinders legal experts to trust in
their output, and this happens rightfully so. However, in this paper, I argue
in favor of a new view, Justifiable Artificial Intelligence, instead of
focusing on Explainable Artificial Intelligence. I discuss in this paper how
gaining evidence for and against a Large Language Model's output may make their
generated texts more trustworthy - or hold them accountable for misinformation."
"Guardians of Trust: Navigating Data Security in AIOps through Vendor
  Partnerships","Artificial Intelligence for IT Operations (AIOps) is a rapidly growing field
that applies artificial intelligence and machine learning to automate and
optimize IT operations. AIOps vendors provide services that ingest end-to-end
logs, traces, and metrics to offer a full stack observability of IT systems.
However, these data sources may contain sensitive information such as internal
IP addresses, hostnames, HTTP headers, SQLs, method/argument return values,
URLs, personal identifiable information (PII), or confidential business data.
Therefore, data security is a crucial concern when working with AIOps vendors.
In this article, we will discuss the security features offered by different
vendors and how we can adopt best practices to ensure data protection and
privacy."
"Metalearning-Informed Competence in Children: Implications for
  Responsible Brain-Inspired Artificial Intelligence","This paper offers a novel conceptual framework comprising four essential
cognitive mechanisms that operate concurrently and collaboratively to enable
metalearning (knowledge and regulation of learning) strategy implementation in
young children. A roadmap incorporating the core mechanisms and the associated
strategies is presented as an explanation of the developing brain's remarkable
cross-context learning competence. The tetrad of fundamental complementary
processes is chosen to collectively represent the bare-bones metalearning
architecture that can be extended to artificial intelligence (AI) systems
emulating brain-like learning and problem-solving skills. Utilizing the
metalearning-enabled young mind as a model for brain-inspired computing, this
work further discusses important implications for morally grounded AI."
"Harnessing Artificial Intelligence for Sustainable Agricultural
  Development in Africa: Opportunities, Challenges, and Impact","This paper explores the transformative potential of artificial intelligence
(AI) in the context of sustainable agricultural development across diverse
regions in Africa. Delving into opportunities, challenges, and impact, the
study navigates through the dynamic landscape of AI applications in
agriculture. Opportunities such as precision farming, crop monitoring, and
climate-resilient practices are examined, alongside challenges related to
technological infrastructure, data accessibility, and skill gaps. The article
analyzes the impact of AI on smallholder farmers, supply chains, and inclusive
growth. Ethical considerations and policy implications are also discussed,
offering insights into responsible AI integration. By providing a nuanced
understanding, this paper contributes to the ongoing discourse on leveraging AI
for fostering sustainability in African agriculture."
"The Global Impact of AI-Artificial Intelligence: Recent Advances and
  Future Directions, A Review","Artificial intelligence (AI) is an emerging technology that has the potential
to transform many aspects of society, including the economy, healthcare, and
transportation. This article synthesizes recent research literature on the
global impact of AI, exploring its potential benefits and risks. The article
highlights the implications of AI, including its impact on economic, ethical,
social, security & privacy, and job displacement aspects. It discusses the
ethical concerns surrounding AI development, including issues of bias,
security, and privacy violations. To ensure the responsible development and
deployment of AI, collaboration between government, industry, and academia is
essential. The article concludes by emphasizing the importance of public
engagement and education to promote awareness and understanding of AI's impact
on society at large."
"Playing With Neuroscience: Past, Present and Future of Neuroimaging and
  Games","Videogames have been a catalyst for advances in many research fields, such as
artificial intelligence, human-computer interaction or virtual reality. Over
the years, research in fields such as artificial intelligence has enabled the
design of new types of games, while games have often served as a powerful tool
for testing and simulation. Can this also happen with neuroscience? What is the
current relationship between neuroscience and games research? what can we
expect from the future? In this article, we'll try to answer these questions,
analysing the current state-of-the-art at the crossroads between neuroscience
and games and envisioning future directions."
"Implementation and Evaluation of a Gradient Descent-Trained Defensible
  Blackboard Architecture System","A variety of forms of artificial intelligence systems have been developed.
Two well-known techniques are neural networks and rule-fact expert systems. The
former can be trained from presented data while the latter is typically
developed by human domain experts. A combined implementation that uses gradient
descent to train a rule-fact expert system has been previously proposed. A
related system type, the Blackboard Architecture, adds an actualization
capability to expert systems. This paper proposes and evaluates the
incorporation of a defensible-style gradient descent training capability into
the Blackboard Architecture. It also introduces the use of activation functions
for defensible artificial intelligence systems and implements and evaluates a
new best path-based training algorithm."
Analogical proportions II,"Analogical reasoning is the ability to detect parallels between two seemingly
distant objects or situations, a fundamental human capacity used for example in
commonsense reasoning, learning, and creativity which is believed by many
researchers to be at the core of human and artificial general intelligence.
Analogical proportions are expressions of the form ``$a$ is to $b$ what $c$ is
to $d$'' at the core of analogical reasoning. The author has recently
introduced an abstract algebraic framework of analogical proportions within the
general setting of universal algebra. It is the purpose of this paper to
further develop the mathematical theory of analogical proportions within that
framework as motivated by the fact that it has already been successfully
applied to logic program synthesis in artificial intelligence."
Blockchain and Artificial Intelligence: Synergies and Conflicts,"Blockchain technology and Artificial Intelligence (AI) have emerged as
transformative forces in their respective domains. This paper explores
synergies and challenges between these two technologies. Our research analyses
the biggest projects combining blockchain and AI, based on market
capitalization, and derives a novel framework to categorize contemporary and
future use cases. Despite the theoretical compatibility, current real-world
applications combining blockchain and AI remain in their infancy."
The ethical situation of DALL-E 2,"A hot topic of Artificial Intelligence right now is image generation from
prompts. DALL-E 2 is one of the biggest names in this domain, as it allows
people to create images from simple text inputs, to even more complicated ones.
The company that made this possible, OpenAI, has assured everyone that visited
their website that their mission is to ensure that artificial general
intelligence benefits all humanity. A noble idea in our opinion, that also
stood as the motive behind us choosing this subject. This paper analyzes the
ethical implications of an AI image generative system, with an emphasis on how
society is responding to it, how it probably will and how it should if all the
right measures are taken."
From Manifestations to Cognitive Architectures: a Scalable Framework,"The Artificial Intelligence field is flooded with optimisation methods. In
this paper, we change the focus to developing modelling methods with the aim of
getting us closer to Artificial General Intelligence. To do so, we propose a
novel way to interpret reality as an information source, that is later
translated into a computational framework able to capture and represent such
information. This framework is able to build elements of classical cognitive
architectures, like Long Term Memory and Working Memory, starting from a simple
primitive that only processes Spatial Distributed Representations. Moreover, it
achieves such level of verticality in a seamless scalable hierarchical way."
Metacognitive AI: Framework and the Case for a Neurosymbolic Approach,"Metacognition is the concept of reasoning about an agent's own internal
processes and was originally introduced in the field of developmental
psychology. In this position paper, we examine the concept of applying
metacognition to artificial intelligence. We introduce a framework for
understanding metacognitive artificial intelligence (AI) that we call TRAP:
transparency, reasoning, adaptation, and perception. We discuss each of these
aspects in-turn and explore how neurosymbolic AI (NSAI) can be leveraged to
address challenges of metacognition."
"Retrieval-Augmented Generation for Generative Artificial Intelligence in
  Medicine","Generative artificial intelligence (AI) has brought revolutionary innovations
in various fields, including medicine. However, it also exhibits limitations.
In response, retrieval-augmented generation (RAG) provides a potential
solution, enabling models to generate more accurate contents by leveraging the
retrieval of external knowledge. With the rapid advancement of generative AI,
RAG can pave the way for connecting this transformative technology with medical
applications and is expected to bring innovations in equity, reliability, and
personalization to health care."
Documentation Practices of Artificial Intelligence,"Artificial Intelligence (AI) faces persistent challenges in terms of
transparency and accountability, which requires rigorous documentation. Through
a literature review on documentation practices, we provide an overview of
prevailing trends, persistent issues, and the multifaceted interplay of factors
influencing the documentation. Our examination of key characteristics such as
scope, target audiences, support for multimodality, and level of automation,
highlights a dynamic evolution in documentation practices, underscored by a
shift towards a more holistic, engaging, and automated documentation."
"From Cognition to Computation: A Comparative Review of Human Attention
  and Transformer Architectures","Attention is a cornerstone of human cognition that facilitates the efficient
extraction of information in everyday life. Recent developments in artificial
intelligence like the Transformer architecture also incorporate the idea of
attention in model designs. However, despite the shared fundamental principle
of selectively attending to information, human attention and the Transformer
model display notable differences, particularly in their capacity constraints,
attention pathways, and intentional mechanisms. Our review aims to provide a
comparative analysis of these mechanisms from a cognitive-functional
perspective, thereby shedding light on several open research questions. The
exploration encourages interdisciplinary efforts to derive insights from human
attention mechanisms in the pursuit of developing more generalized artificial
intelligence."
"Artificial intelligence and machine learning generated conjectures with
  TxGraffiti","\emph{TxGraffiti} is a machine learning and heuristic based artificial
intelligence designed to automate the task of conjecturing in mathematics.
Since its inception, TxGraffiti has generated many surprising conjectures
leading to publication in respectable mathematical journals. In this paper we
outline the machine learning and heuristic techniques implemented by
TxGraffiti. We also recall its contributions to the mathematical literature and
announce a new online version of the program available for anyone curious to
explore conjectures in graph theory."
"Artificial Intelligence and Algorithmic Price Collusion in Two-sided
  Markets","Algorithmic price collusion facilitated by artificial intelligence (AI)
algorithms raises significant concerns. We examine how AI agents using
Q-learning engage in tacit collusion in two-sided markets. Our experiments
reveal that AI-driven platforms achieve higher collusion levels compared to
Bertrand competition. Increased network externalities significantly enhance
collusion, suggesting AI algorithms exploit them to maximize profits. Higher
user heterogeneity or greater utility from outside options generally reduce
collusion, while higher discount rates increase it. Tacit collusion remains
feasible even at low discount rates. To mitigate collusive behavior and inform
potential regulatory measures, we propose incorporating a penalty term in the
Q-learning algorithm."
"The US Algorithmic Accountability Act of 2022 vs. The EU Artificial
  Intelligence Act: What can they learn from each other?","On the whole, the U.S. Algorithmic Accountability Act of 2022 (US AAA) is a
pragmatic approach to balancing the benefits and risks of automated decision
systems. Yet there is still room for improvement. This commentary highlights
how the US AAA can both inform and learn from the European Artificial
Intelligence Act (EU AIA)."
Automated Explanation Selection for Scientific Discovery,"Automated reasoning is a key technology in the young but rapidly growing
field of Explainable Artificial Intelligence (XAI). Explanability helps build
trust in artificial intelligence systems beyond their mere predictive accuracy
and robustness. In this paper, we propose a cycle of scientific discovery that
combines machine learning with automated reasoning for the generation and the
selection of explanations. We present a taxonomy of explanation selection
problems that draws on insights from sociology and cognitive science. These
selection criteria subsume existing notions and extend them with new
properties."
Quantum Artificial Intelligence: A Brief Survey,"Quantum Artificial Intelligence (QAI) is the intersection of quantum
computing and AI, a technological synergy with expected significant benefits
for both. In this paper, we provide a brief overview of what has been achieved
in QAI so far and point to some open questions for future research. In
particular, we summarize some major key findings on the feasability and the
potential of using quantum computing for solving computationally hard problems
in various subfields of AI, and vice versa, the leveraging of AI methods for
building and operating quantum computing devices."
"Building a human-like observer using deep learning in an extended
  Wigner's friend experiment","There has been a longstanding demand for artificial intelligence with
human-level cognitive sophistication to address loopholes in Bell-type
experiments. In this study, we propose a novel experimental framework that
integrates advanced deep learning techniques, employing neural network-based
artificial intelligence in an extended Wigner's friend experiment. We
demonstrate the framework through simulations and introduce three new
analytical metrics-morphing polygons, averaged Shannon entropy, and probability
density maps-to evaluate the results. These results can be used to determine
whether our artificial intelligence qualifies as a bona fide observer and
whether superposition applies to macroscopic systems, including observers."
"Comprehensive Overview of Artificial Intelligence Applications in Modern
  Industries","Artificial Intelligence (AI) is fundamentally reshaping various industries by
enhancing decision-making processes, optimizing operations, and unlocking new
opportunities for innovation. This paper explores the applications of AI across
four key sectors: healthcare, finance, manufacturing, and retail. Each section
delves into the specific challenges faced by these industries, the AI
technologies employed to address them, and the measurable impact on business
outcomes and societal welfare. We also discuss the implications of AI
integration, including ethical considerations, the future trajectory of AI
development, and its potential to drive economic growth while posing challenges
that need to be managed responsibly."
Five questions and answers about artificial intelligence,"Rapid advances in Artificial Intelligence (AI) are generating much
controversy in society, often without scientific basis. As occurred the
development of other emerging technologies, such as the introduction of
electricity in the early 20th century, AI causes both fascination and fear.
Following the advice of the philosopher R.W. Emerson's: advice the knowledge is
the antidote to fear; this paper seeks to contribute to the dissemination of
knowledge about AI. To this end, it reflects on the following questions: the
origins of AI, its possible future evolution, its ability to show feelings, the
associated threats and dangers, and the concept of AI singularity."
"A Comprehensive Survey and Classification of Evaluation Criteria for
  Trustworthy Artificial Intelligence","This paper presents a systematic review of the literature on evaluation
criteria for Trustworthy Artificial Intelligence (TAI), with a focus on the
seven EU principles of TAI. This systematic literature review identifies and
analyses current evaluation criteria, maps them to the EU TAI principles and
proposes a new classification system for each principle. The findings reveal
both a need for and significant barriers to standardising criteria for TAI
evaluation. The proposed classification contributes to the development,
selection and standardization of evaluation criteria for TAI governance."
"Using AI Alignment Theory to understand the potential pitfalls of
  regulatory frameworks","This paper leverages insights from Alignment Theory (AT) research, which
primarily focuses on the potential pitfalls of technical alignment in
Artificial Intelligence, to critically examine the European Union's Artificial
Intelligence Act (EU AI Act). In the context of AT research, several key
failure modes - such as proxy gaming, goal drift, reward hacking or
specification gaming - have been identified. These can arise when AI systems
are not properly aligned with their intended objectives. The central logic of
this report is: what can we learn if we treat regulatory efforts in the same
way as we treat advanced AI systems? As we systematically apply these concepts
to the EU AI Act, we uncover potential vulnerabilities and areas for
improvement in the regulation."
An overview of diffusion models for generative artificial intelligence,"This article provides a mathematically rigorous introduction to denoising
diffusion probabilistic models (DDPMs), sometimes also referred to as diffusion
probabilistic models or diffusion models, for generative artificial
intelligence. We provide a detailed basic mathematical framework for DDPMs and
explain the main ideas behind training and generation procedures. In this
overview article we also review selected extensions and improvements of the
basic framework from the literature such as improved DDPMs, denoising diffusion
implicit models, classifier-free diffusion guidance models, and latent
diffusion models."
"Publication Trends in Artificial Intelligence Conferences: The Rise of
  Super Prolific Authors","Papers published in top conferences contribute influential discoveries that
are reshaping the landscape of modern Artificial Intelligence (AI). We analyzed
87,137 papers from 11 AI conferences to examine publication trends over the
past decade. Our findings reveal a consistent increase in both the number of
papers and authors, reflecting the growing interest in AI research. We also
observed a rise in prolific researchers who publish dozens of papers at the
same conference each year. In light of this analysis, the AI research community
should consider revisiting authorship policies, addressing equity concerns, and
evaluating the workload of junior researchers to foster a more sustainable and
inclusive research environment."
"Envisioning National Resources for Artificial Intelligence Research: NSF
  Workshop Report","This is a report of an NSF workshop titled ""Envisioning National Resources
for Artificial Intelligence Research"" held in Alexandria, Virginia, in May
2024. The workshop aimed to identify initial challenges and opportunities for
national resources for AI research (e.g., compute, data, models, etc.) and to
facilitate planning for the envisioned National AI Research Resource.
Participants included AI and cyberinfrastructure (CI) experts. The report
outlines significant findings and identifies needs and recommendations from the
workshop."
The Mathematics of Artificial Intelligence,"This overview article highlights the critical role of mathematics in
artificial intelligence (AI), emphasizing that mathematics provides tools to
better understand and enhance AI systems. Conversely, AI raises new problems
and drives the development of new mathematics at the intersection of various
fields. This article focuses on the application of analytical and probabilistic
tools to model neural network architectures and better understand their
optimization. Statistical questions (particularly the generalization capacity
of these networks) are intentionally set aside, though they are of crucial
importance. We also shed light on the evolution of ideas that have enabled
significant advances in AI through architectures tailored to specific tasks,
each echoing distinct mathematical techniques. The goal is to encourage more
mathematicians to take an interest in and contribute to this exciting field."
A modal logic translation of the AGM axioms for belief revision,"Building on the analysis of Bonanno (Artificial Intelligence, 2025) we
introduce a simple modal logic containing three modal operators: a unimodal
belief operator, a bimodal conditional operator and the unimodal global
operator. For each AGM axiom for belief revision, we provide a corresponding
modal axiom. The correspondence is as follows: each AGM axiom is characterized
by a property of the Kripke-Lewis frames considered in Bonanno (Artificial
Intelligence, 2025) and, in turn, that property characterizes the proposed
modal axiom."
"A Theory of Chaordic Economics: How Artificial Intelligence and
  Blockchain Transform Businesses, Economies and Societies","Dee Hock, the founder of Visa, coined the term 'chaordic' to describe
simultaneously chaotic and ordered systems. Based on his reasoning, we
introduce the Theory of Chaordic Economics to explain how economic systems are
transformed by two disruptive technologies: namely Artificial Intelligence and
Blockchain. Artificial intelligence can generate novel output through
algorithmic yet rather unpredictable processes. Blockchain creates
deterministic results without central authorities and relies on elaborated
protocols that prescribe how consensus can be reached within a network of
peers. The amalgamation of chaos and order produces chaordic economic systems
and can yield hitherto unthinkable economic structures."
"Neural Manifolds and Cognitive Consistency: A New Approach to Memory
  Consolidation in Artificial Systems","We introduce a novel mathematical framework that unifies neural population
dynamics, hippocampal sharp wave-ripple (SpWR) generation, and cognitive
consistency constraints inspired by Heider's theory. Our model leverages
low-dimensional manifold representations to capture structured neural drift and
incorporates a balance energy function to enforce coherent synaptic
interactions, effectively simulating the memory consolidation processes
observed in biological systems. Simulation results demonstrate that our
approach not only reproduces key features of SpWR events but also enhances
network interpretability. This work paves the way for scalable neuromorphic
architectures that bridge neuroscience and artificial intelligence, offering
more robust and adaptive learning mechanisms for future intelligent systems."
From Generative AI to Innovative AI: An Evolutionary Roadmap,"This paper explores the critical transition from Generative Artificial
Intelligence (GenAI) to Innovative Artificial Intelligence (InAI). While recent
advancements in GenAI have enabled systems to produce high-quality content
across various domains, these models often lack the capacity for true
innovation. In this context, innovation is defined as the ability to generate
novel and useful outputs that go beyond mere replication of learned data. The
paper examines this shift and proposes a roadmap for developing AI systems that
can generate content and engage in autonomous problem-solving and creative
ideation. The work provides both theoretical insights and practical strategies
for advancing AI to a stage where it can genuinely innovate, contributing
meaningfully to science, technology, and the arts."
"Geospatial Artificial Intelligence for Satellite-based Flood Extent
  Mapping: Concepts, Advances, and Future Perspectives","Geospatial Artificial Intelligence (GeoAI) for satellite-based flood extent
mapping systematically integrates artificial intelligence techniques with
satellite data to identify flood events and assess their impacts, for disaster
management and spatial decision-making. The primary output often includes flood
extent maps, which delineate the affected areas, along with additional
analytical outputs such as uncertainty estimation and change detection."
"Data over dialogue: Why artificial intelligence is unlikely to humanise
  medicine","Recently, a growing number of experts in artificial intelligence (AI) and
medicine have be-gun to suggest that the use of AI systems, particularly
machine learning (ML) systems, is likely to humanise the practice of medicine
by substantially improving the quality of clinician-patient relationships. In
this thesis, however, I argue that medical ML systems are more likely to
negatively impact these relationships than to improve them. In particular, I
argue that the use of medical ML systems is likely to comprise the quality of
trust, care, empathy, understanding, and communication between clinicians and
patients."
"Artificial Intelligence Augmented Medical Imaging Reconstruction in
  Radiation Therapy","Efficiently acquired and precisely reconstructed imaging are crucial to the
success of modern radiation therapy (RT). Computed tomography (CT) and magnetic
resonance imaging (MRI) are two common modalities for providing RT treatment
planning and delivery guidance/monitoring. In recent decades, artificial
intelligence (AI) has emerged as a powerful and widely adopted technique across
various fields, valued for its efficiency and convenience enabled by implicit
function definition and data-driven feature representation learning. Here, we
present a series of AI-driven medical imaging reconstruction frameworks for
enhanced radiotherapy, designed to improve CT image reconstruction quality and
speed, refine dual-energy CT (DECT) multi-material decomposition (MMD), and
significantly accelerate 4D MRI acquisition."
"Logic-Based Artificial Intelligence Algorithms Supporting Categorical
  Semantics","This paper seeks to apply categorical logic to the design of artificial
intelligent agents that reason symbolically about objects more richly
structured than sets. Using Johnstone's sequent calculus of terms- and
formulae-in-context, we develop forward chaining and normal form algorithms for
reasoning about objects in cartesian categories with the rules for Horn logic.
We also adapt first-order unification to support multi-sorted theories,
contexts, and fragments of first-order logic. The significance of these
reformulations rests in the fact that they can be applied to reasoning about
objects in semantic categories that do not support classical logic or even all
its connectives."
"Intelligent Computing: The Latest Advances, Challenges and Future","Computing is a critical driving force in the development of human
civilization. In recent years, we have witnessed the emergence of intelligent
computing, a new computing paradigm that is reshaping traditional computing and
promoting digital revolution in the era of big data, artificial intelligence
and internet-of-things with new computing theories, architectures, methods,
systems, and applications. Intelligent computing has greatly broadened the
scope of computing, extending it from traditional computing on data to
increasingly diverse computing paradigms such as perceptual intelligence,
cognitive intelligence, autonomous intelligence, and human-computer fusion
intelligence. Intelligence and computing have undergone paths of different
evolution and development for a long time but have become increasingly
intertwined in recent years: intelligent computing is not only
intelligence-oriented but also intelligence-driven. Such cross-fertilization
has prompted the emergence and rapid advancement of intelligent computing.
Intelligent computing is still in its infancy and an abundance of innovations
in the theories, systems, and applications of intelligent computing are
expected to occur soon. We present the first comprehensive survey of literature
on intelligent computing, covering its theory fundamentals, the technological
fusion of intelligence and computing, important applications, challenges, and
future perspectives. We believe that this survey is highly timely and will
provide a comprehensive reference and cast valuable insights into intelligent
computing for academic and industrial researchers and practitioners."
Design of a P System based Artificial Graph Chemistry,"Artificial Chemistries (ACs) are symbolic chemical metaphors for the
exploration of Artificial Life, with specific focus on the origin of life. In
this work we define a P system based artificial graph chemistry to understand
the principles leading to the evolution of life-like structures in an AC set up
and to develop a unified framework to characterize and classify symbolic
artificial chemistries by devising appropriate formalism to capture semantic
and organizational information. An extension of P system is considered by
associating probabilities with the rules providing the topological framework
for the evolution of a labeled undirected graph based molecular reaction
semantics."
Goal Conflict in Designing an Autonomous Artificial System,"Research on human self-regulation has shown that people hold many goals
simultaneously and have complex self-regulation mechanisms to deal with this
goal conflict. Artificial autonomous systems may also need to find ways to cope
with conflicting goals. Indeed, the intricate interplay among different goals
may be critical to the design as well as long-term safety and stability of
artificial autonomous systems. I discuss some of the critical features of the
human self-regulation system and how it might be applied to an artificial
system. Furthermore, the implications of goal conflict for the reliability and
stability of artificial autonomous systems and ensuring their alignment with
human goals and ethics is examined."
"Artificial life properties of directed interaction combinators vs.
  chemlambda","We provide a framework for experimentation at
https://mbuliga.github.io/quinegraphs/ic-vs-chem.html#icvschem with two
artificial chemistries: directed interaction combinators (dirIC, defined in
section 2) and chemlambda. We are interested if these chemistries allow for
artificial life behaviour: replication, metabolism and death.
  The main conclusion of these experiments is that graph rewrites systems which
allow conflicting rewrites are better than those which don't, as concerns their
artificial life properties. This is in contradiction with the search for good
graph rewrite systems for decentralized computing, where non-conflicting graph
rewrite systems are historically preferred.
  This continues the artificial chemistry experiments with chemlambda, lambda
calculus or interaction combinators, available from the entry page at
https://chemlambda.github.io/index.html and described in arXiv:2003.14332."
"Cluster-based Specification Techniques in Dempster-Shafer Theory for an
  Evidential Intelligence Analysis of MultipleTarget Tracks (Thesis Abstract)","In Intelligence Analysis it is of vital importance to manage uncertainty.
Intelligence data is almost always uncertain and incomplete, making it
necessary to reason and taking decisions under uncertainty. One way to manage
the uncertainty in Intelligence Analysis is Dempster-Shafer Theory. This thesis
contains five results regarding multiple target tracks and intelligence
specification."
Evidential Force Aggregation,"In this paper we develop an evidential force aggregation method intended for
classification of evidential intelligence into recognized force structures. We
assume that the intelligence has already been partitioned into clusters and use
the classification method individually in each cluster. The classification is
based on a measure of fitness between template and fused intelligence that
makes it possible to handle intelligence reports with multiple nonspecific and
uncertain propositions. With this measure we can aggregate on a level-by-level
basis, starting from general intelligence to achieve a complete force structure
with recognized units on all hierarchical levels."
Effect of noise in intelligent cellular decision making,"Similar to intelligent multicellular neural networks controlling human
brains, even single cells surprisingly are able to make intelligent decisions
to classify several external stimuli or to associate them. This happens because
of the fact that gene regulatory networks can perform as perceptrons, simple
intelligent schemes known from studies on Artificial Intelligence. We study the
role of genetic noise in intelligent decision making at the genetic level and
show that noise can play a constructive role helping cells to make a proper
decision. We show this using the example of a simple genetic classifier able to
classify two external stimuli."
"A knowledge-based intelligent system for control of dirt recognition
  process in the smart washing machines","In this paper, we propose an intelligence approach based on fuzzy logic to
modeling human intelligence in washing clothes. At first, an intelligent
feedback loop is designed for perception-based sensing of dirt inspired by
human color understanding. Then, when color stains leak out of some colored
clothes the human probabilistic decision making is computationally modeled to
detect this stain leakage and thus the problem of recognizing dirt from stain
can be considered in the washing process. Finally, we discuss the fuzzy control
of washing clothes and design and simulate a smart controller based on the
fuzzy intelligence feedback loop."
"Towards Self-constructive Artificial Intelligence: Algorithmic basis
  (Part I)","Artificial Intelligence frameworks should allow for ever more autonomous and
general systems in contrast to very narrow and restricted (human pre-defined)
domain systems, in analogy to how the brain works. Self-constructive Artificial
Intelligence ($SCAI$) is one such possible framework. We herein propose that
$SCAI$ is based on three principles of organization: self-growing,
self-experimental and self-repairing. Self-growing: the ability to autonomously
and incrementally construct structures and functionality as needed to solve
encountered (sub)problems. Self-experimental: the ability to internally
simulate, anticipate and take decisions based on these expectations.
Self-repairing: the ability to autonomously re-construct a previously
successful functionality or pattern of interaction lost from a possible
sub-component failure (damage). To implement these principles of organization,
a constructive architecture capable of evolving adaptive autonomous agents is
required. We present Schema-based learning as one such architecture capable of
incrementally constructing a myriad of internal models of three kinds:
predictive schemas, dual (inverse models) schemas and goal schemas as they are
necessary to autonomously develop increasing functionality.
  We claim that artificial systems, whether in the digital or in the physical
world, can benefit very much form this constructive architecture and should be
organized around these principles of organization. To illustrate the generality
of the proposed framework, we include several test cases in structural adaptive
navigation in artificial intelligence systems in Paper II of this series, and
resilient robot motor control in Paper III of this series. Paper IV of this
series will also include $SCAI$ for problem structural discovery in predictive
Business Intelligence."
"Deep Reinforcement Learning for Constrained Field Development
  Optimization in Subsurface Two-phase Flow","We present a deep reinforcement learning-based artificial intelligence agent
that could provide optimized development plans given a basic description of the
reservoir and rock/fluid properties with minimal computational cost. This
artificial intelligence agent, comprising of a convolutional neural network,
provides a mapping from a given state of the reservoir model, constraints, and
economic condition to the optimal decision (drill/do not drill and well
location) to be taken in the next stage of the defined sequential field
development planning process. The state of the reservoir model is defined using
parameters that appear in the governing equations of the two-phase flow. A
feedback loop training process referred to as deep reinforcement learning is
used to train an artificial intelligence agent with such a capability. The
training entails millions of flow simulations with varying reservoir model
descriptions (structural, rock and fluid properties), operational constraints,
and economic conditions. The parameters that define the reservoir model,
operational constraints, and economic conditions are randomly sampled from a
defined range of applicability. Several algorithmic treatments are introduced
to enhance the training of the artificial intelligence agent. After appropriate
training, the artificial intelligence agent provides an optimized field
development plan instantly for new scenarios within the defined range of
applicability. This approach has advantages over traditional optimization
algorithms (e.g., particle swarm optimization, genetic algorithm) that are
generally used to find a solution for a specific field development scenario and
typically not generalizable to different scenarios."
"Conceptual Modeling and Artificial Intelligence: Mutual Benefits from
  Complementary Worlds","Conceptual modeling (CM) applies abstraction to reduce the complexity of a
system under study (e.g., an excerpt of reality). As a result of the conceptual
modeling process a human interpretable, formalized representation (i.e., a
conceptual model) is derived which enables understanding and communication
among humans, and processing by machines. Artificial Intelligence (AI)
algorithms are also applied to complex realities (regularly represented by vast
amounts of data) to identify patterns or to classify entities in the data.
Aside from the commonalities of both approaches, a significant difference can
be observed by looking at the results. While conceptual models are
comprehensible, reproducible, and explicit knowledge representations, AI
techniques are capable of efficiently deriving an output from a given input
while acting as a black box. AI solutions often lack comprehensiveness and
reproducibility. Even the developers of AI systems can't explain why a certain
output is derived. In the Conceptual Modeling meets Artificial Intelligence
(CMAI) workshop, we are interested in tackling the intersection of the two,
thus far, mostly isolated approached disciplines of CM and AI. The workshop
embraces the assumption, that manifold mutual benefits can be realized by i)
investigating what Conceptual Modeling (CM) can contribute to AI, and ii) the
other way around, what Artificial Intelligence (AI) can contribute to CM."
"Automatic detection of glaucoma via fundus imaging and artificial
  intelligence: A review","Glaucoma is a leading cause of irreversible vision impairment globally and
cases are continuously rising worldwide. Early detection is crucial, allowing
timely intervention which can prevent further visual field loss. To detect
glaucoma, examination of the optic nerve head via fundus imaging can be
performed, at the centre of which is the assessment of the optic cup and disc
boundaries. Fundus imaging is non-invasive and low-cost; however, the image
examination relies on subjective, time-consuming, and costly expert
assessments. A timely question to ask is can artificial intelligence mimic
glaucoma assessments made by experts. Namely, can artificial intelligence
automatically find the boundaries of the optic cup and disc (providing a
so-called segmented fundus image) and then use the segmented image to identify
glaucoma with high accuracy. We conducted a comprehensive review on artificial
intelligence-enabled glaucoma detection frameworks that produce and use
segmented fundus images. We found 28 papers and identified two main approaches:
1) logical rule-based frameworks, based on a set of simplistic decision rules;
and 2) machine learning/statistical modelling based frameworks. We summarise
the state-of-art of the two approaches and highlight the key hurdles to
overcome for artificial intelligence-enabled glaucoma detection frameworks to
be translated into clinical practice."
"Digital Twin and Artificial Intelligence Incorporated With Surrogate
  Modeling for Hybrid and Sustainable Energy Systems","Surrogate modeling has brought about a revolution in computation in the
branches of science and engineering. Backed by Artificial Intelligence, a
surrogate model can present highly accurate results with a significant
reduction in computation time than computer simulation of actual models.
Surrogate modeling techniques have found their use in numerous branches of
science and engineering, energy system modeling being one of them. Since the
idea of hybrid and sustainable energy systems is spreading rapidly in the
modern world for the paradigm of the smart energy shift, researchers are
exploring the future application of artificial intelligence-based surrogate
modeling in analyzing and optimizing hybrid energy systems. One of the
promising technologies for assessing applicability for the energy system is the
digital twin, which can leverage surrogate modeling. This work presents a
comprehensive framework/review on Artificial Intelligence-driven surrogate
modeling and its applications with a focus on the digital twin framework and
energy systems. The role of machine learning and artificial intelligence in
constructing an effective surrogate model is explained. After that, different
surrogate models developed for different sustainable energy sources are
presented. Finally, digital twin surrogate models and associated uncertainties
are described."
"Artificial Intelligence and Life in 2030: The One Hundred Year Study on
  Artificial Intelligence","In September 2016, Stanford's ""One Hundred Year Study on Artificial
Intelligence"" project (AI100) issued the first report of its planned long-term
periodic assessment of artificial intelligence (AI) and its impact on society.
It was written by a panel of 17 study authors, each of whom is deeply rooted in
AI research, chaired by Peter Stone of the University of Texas at Austin. The
report, entitled ""Artificial Intelligence and Life in 2030,"" examines eight
domains of typical urban settings on which AI is likely to have impact over the
coming years: transportation, home and service robots, healthcare, education,
public safety and security, low-resource communities, employment and workplace,
and entertainment. It aims to provide the general public with a scientifically
and technologically accurate portrayal of the current state of AI and its
potential and to help guide decisions in industry and governments, as well as
to inform research and development in the field. The charge for this report was
given to the panel by the AI100 Standing Committee, chaired by Barbara Grosz of
Harvard University."
"Analysis of Explainable Artificial Intelligence Methods on Medical Image
  Classification","The use of deep learning in computer vision tasks such as image
classification has led to a rapid increase in the performance of such systems.
Due to this substantial increment in the utility of these systems, the use of
artificial intelligence in many critical tasks has exploded. In the medical
domain, medical image classification systems are being adopted due to their
high accuracy and near parity with human physicians in many tasks. However,
these artificial intelligence systems are extremely complex and are considered
black boxes by scientists, due to the difficulty in interpreting what exactly
led to the predictions made by these models. When these systems are being used
to assist high-stakes decision-making, it is extremely important to be able to
understand, verify and justify the conclusions reached by the model. The
research techniques being used to gain insight into the black-box models are in
the field of explainable artificial intelligence (XAI). In this paper, we
evaluated three different XAI methods across two convolutional neural network
models trained to classify lung cancer from histopathological images. We
visualized the outputs and analyzed the performance of these methods, in order
to better understand how to apply explainable artificial intelligence in the
medical domain."
"Proactive and Reactive Engagement of Artificial Intelligence Methods for
  Education: A Review","Quality education, one of the seventeen sustainable development goals (SDGs)
identified by the United Nations General Assembly, stands to benefit enormously
from the adoption of artificial intelligence (AI) driven tools and
technologies. The concurrent boom of necessary infrastructure, digitized data
and general social awareness has propelled massive research and development
efforts in the artificial intelligence for education (AIEd) sector. In this
review article, we investigate how artificial intelligence, machine learning
and deep learning methods are being utilized to support students, educators and
administrative staff. We do this through the lens of a novel categorization
approach. We consider the involvement of AI-driven methods in the education
process in its entirety - from students admissions, course scheduling etc. in
the proactive planning phase to knowledge delivery, performance assessment etc.
in the reactive execution phase. We outline and analyze the major research
directions under proactive and reactive engagement of AI in education using a
representative group of 194 original research articles published in the past
two decades i.e., 2003 - 2022. We discuss the paradigm shifts in the solution
approaches proposed, i.e., in the choice of data and algorithms used over this
time. We further dive into how the COVID-19 pandemic challenged and reshaped
the education landscape at the fag end of this time period. Finally, we
pinpoint existing limitations in adopting artificial intelligence for education
and reflect on the path forward."
"Mind the Gap! Bridging Explainable Artificial Intelligence and Human
  Understanding with Luhmann's Functional Theory of Communication","Over the past decade explainable artificial intelligence has evolved from a
predominantly technical discipline into a field that is deeply intertwined with
social sciences. Insights such as human preference for contrastive -- more
precisely, counterfactual -- explanations have played a major role in this
transition, inspiring and guiding the research in computer science. Other
observations, while equally important, have nevertheless received much less
consideration. The desire of human explainees to communicate with artificial
intelligence explainers through a dialogue-like interaction has been mostly
neglected by the community. This poses many challenges for the effectiveness
and widespread adoption of such technologies as delivering a single explanation
optimised according to some predefined objectives may fail to engender
understanding in its recipients and satisfy their unique needs given the
diversity of human knowledge and intention. Using insights elaborated by Niklas
Luhmann and, more recently, Elena Esposito we apply social systems theory to
highlight challenges in explainable artificial intelligence and offer a path
forward, striving to reinvigorate the technical research in the direction of
interactive and iterative explainers. Specifically, this paper demonstrates the
potential of systems theoretical approaches to communication in elucidating and
addressing the problems and limitations of human-centred explainable artificial
intelligence."
Reliable AI: Does the Next Generation Require Quantum Computing?,"In this survey, we aim to explore the fundamental question of whether the
next generation of artificial intelligence requires quantum computing.
Artificial intelligence is increasingly playing a crucial role in many aspects
of our daily lives and is central to the fourth industrial revolution. It is
therefore imperative that artificial intelligence is reliable and trustworthy.
However, there are still many issues with reliability of artificial
intelligence, such as privacy, responsibility, safety, and security, in areas
such as autonomous driving, healthcare, robotics, and others. These problems
can have various causes, including insufficient data, biases, and robustness
problems, as well as fundamental issues such as computability problems on
digital hardware. The cause of these computability problems is rooted in the
fact that digital hardware is based on the computing model of the Turing
machine, which is inherently discrete. Notably, our findings demonstrate that
digital hardware is inherently constrained in solving problems about
optimization, deep learning, or differential equations. Therefore, these
limitations carry substantial implications for the field of artificial
intelligence, in particular for machine learning. Furthermore, although it is
well known that the quantum computer shows a quantum advantage for certain
classes of problems, our findings establish that some of these limitations
persist when employing quantum computing models based on the quantum circuit or
the quantum Turing machine paradigm. In contrast, analog computing models, such
as the Blum-Shub-Smale machine, exhibit the potential to surmount these
limitations."
The Two Faces of AI in Green Mobile Computing: A Literature Review,"Artificial intelligence is bringing ever new functionalities to the realm of
mobile devices that are now considered essential (e.g., camera and voice
assistants, recommender systems). Yet, operating artificial intelligence takes
up a substantial amount of energy. However, artificial intelligence is also
being used to enable more energy-efficient solutions for mobile systems. Hence,
artificial intelligence has two faces in that regard, it is both a key enabler
of desired (efficient) mobile functionalities and a major power draw on these
devices, playing a part in both the solution and the problem. In this paper, we
present a review of the literature of the past decade on the usage of
artificial intelligence within the realm of green mobile computing. From the
analysis of 34 papers, we highlight the emerging patterns and map the field
into 13 main topics that are summarized in details.
  Our results showcase that the field is slowly increasing in the past years,
more specifically, since 2019. Regarding the double impact AI has on the mobile
energy consumption, the energy consumption of AI-based mobile systems is
under-studied in comparison to the usage of AI for energy-efficient mobile
computing, and we argue for more exploratory studies in that direction. We
observe that although most studies are framed as solution papers (94%), the
large majority do not make those solutions publicly available to the community.
Moreover, we also show that most contributions are purely academic (28 out of
34 papers) and that we need to promote the involvement of the mobile software
industry in this field."
chatGPT for generating questions and assessments based on accreditations,"This research aims to take advantage of artificial intelligence techniques in
producing students assessment that is compatible with the different academic
accreditations of the same program. The possibility of using generative
artificial intelligence technology was studied to produce an academic
accreditation compliant test the National Center for Academic Accreditation of
Kingdom of Saudi Arabia and Accreditation Board for Engineering and Technology.
A novel method was introduced to map the verbs used to create the questions
introduced in the tests. The method allows a possibility of using the
generative artificial intelligence technology to produce and check the validity
of questions that measure educational outcomes. A questionnaire was distributed
to ensure that the use of generative artificial intelligence to create exam
questions is acceptable by the faculty members, as well as to ask about the
acceptance of assistance in validating questions submitted by faculty members
and amending them in accordance with academic accreditations. The questionnaire
was distributed to faculty members of different majors in the Kingdom of Saudi
Arabias universities. one hundred twenty responses obtained with eight five
percentile approval percentage for generate complete exam questions by
generative artificial intelligence . Whereas ninety eight percentage was the
approval percentage for editing and improving already existed questions."
X-SHIELD: Regularization for eXplainable Artificial Intelligence,"As artificial intelligence systems become integral across domains, the demand
for explainability grows, the called eXplainable artificial intelligence (XAI).
Existing efforts primarily focus on generating and evaluating explanations for
black-box models while a critical gap in directly enhancing models remains
through these evaluations. It is important to consider the potential of this
explanation process to improve model quality with a feedback on training as
well. XAI may be used to improve model performance while boosting its
explainability. Under this view, this paper introduces Transformation -
Selective Hidden Input Evaluation for Learning Dynamics (T-SHIELD), a
regularization family designed to improve model quality by hiding features of
input, forcing the model to generalize without those features. Within this
family, we propose the XAI - SHIELD(X-SHIELD), a regularization for explainable
artificial intelligence, which uses explanations to select specific features to
hide. In contrast to conventional approaches, X-SHIELD regularization
seamlessly integrates into the objective function enhancing model
explainability while also improving performance. Experimental validation on
benchmark datasets underscores X-SHIELD's effectiveness in improving
performance and overall explainability. The improvement is validated through
experiments comparing models with and without the X-SHIELD regularization, with
further analysis exploring the rationale behind its design choices. This
establishes X-SHIELD regularization as a promising pathway for developing
reliable artificial intelligence regularization."
"Artificial intelligence for partial differential equations in
  computational mechanics: A review","In recent years, Artificial intelligence (AI) has become ubiquitous,
empowering various fields, especially integrating artificial intelligence and
traditional science (AI for Science: Artificial intelligence for science),
which has attracted widespread attention. In AI for Science, using artificial
intelligence algorithms to solve partial differential equations (AI for PDEs:
Artificial intelligence for partial differential equations) has become a focal
point in computational mechanics. The core of AI for PDEs is the fusion of data
and partial differential equations (PDEs), which can solve almost any PDEs.
Therefore, this article provides a comprehensive review of the research on AI
for PDEs, summarizing the existing algorithms and theories. The article
discusses the applications of AI for PDEs in computational mechanics, including
solid mechanics, fluid mechanics, and biomechanics. The existing AI for PDEs
algorithms include those based on Physics-Informed Neural Networks (PINNs),
Deep Energy Methods (DEM), Operator Learning, and Physics-Informed Neural
Operator (PINO). AI for PDEs represents a new method of scientific simulation
that provides approximate solutions to specific problems using large amounts of
data, then fine-tuning according to specific physical laws, avoiding the need
to compute from scratch like traditional algorithms. Thus, AI for PDEs is the
prototype for future foundation models in computational mechanics, capable of
significantly accelerating traditional numerical algorithms."
"AI Horizon Scanning -- White Paper p3395, IEEE-SA. Part III: Technology
  Watch: a selection of key developments, emerging technologies, and industry
  trends in Artificial Intelligence","Generative Artificial Intelligence (AI) technologies are in a phase of
unprecedented rapid development following the landmark release of Chat-GPT,
which brought the phenomenon to wide public attention. As the deployment of AI
products rises geometrically, considerable attention is being given to the
threats and opportunities that AI technologies offer, and to the need for
regulatory and standards initiatives to ensure that use of the technology
aligns with societal needs and generates broad benefits while mitigating risks
and threats. This manuscript is the third of a series of White Papers informing
the development of IEEE-SA's p3995 {\it `Standard for the Implementation of
Safeguards, Controls, and Preventive Techniques for Artificial Intelligence
Models'} \cite{P3395}, Chair Marina Cort\^{e}s. This part focuses on assessing
calmly and objectively, as far as is possible, the current state of Artificial
Intelligence (AI) technology development and identifying predominant trends,
prospects, and ensuing risks. It necessarily forms a snapshot of the current
instant of a rapidly-evolving landscape, with new products and innovations
emerging continuously. While our main focus is on software and hardware
developments and their corporate context, we also briefly review progress on
robotics within the AI context and describe some implications of the
substantial and growing AI energy demand."
An architecture for the evaluation of intelligent systems,"One of the main research areas in Artificial Intelligence is the coding of
agents (programs) which are able to learn by themselves in any situation. This
means that agents must be useful for purposes other than those they were
created for, as, for example, playing chess. In this way we try to get closer
to the pristine goal of Artificial Intelligence. One of the problems to decide
whether an agent is really intelligent or not is the measurement of its
intelligence, since there is currently no way to measure it in a reliable way.
The purpose of this project is to create an interpreter that allows for the
execution of several environments, including those which are generated
randomly, so that an agent (a person or a program) can interact with them. Once
the interaction between the agent and the environment is over, the interpreter
will measure the intelligence of the agent according to the actions, states and
rewards the agent has undergone inside the environment during the test. As a
result we will be able to measure agents' intelligence in any possible
environment, and to make comparisons between several agents, in order to
determine which of them is the most intelligent. In order to perform the tests,
the interpreter must be able to randomly generate environments that are really
useful to measure agents' intelligence, since not any randomly generated
environment will serve that purpose."
"Understanding and Benchmarking Artificial Intelligence: OpenAI's o3 Is
  Not AGI","OpenAI's o3 achieves a high score of 87.5 % on ARC-AGI, a benchmark proposed
to measure intelligence. This raises the question whether systems based on
Large Language Models (LLMs), particularly o3, demonstrate intelligence and
progress towards artificial general intelligence (AGI). Building on the
distinction between skills and intelligence made by Fran\c{c}ois Chollet, the
creator of ARC-AGI, a new understanding of intelligence is introduced: an agent
is the more intelligent, the more efficiently it can achieve the more diverse
goals in the more diverse worlds with the less knowledge. An analysis of the
ARC-AGI benchmark shows that its tasks represent a very specific type of
problem that can be solved by massive trialling of combinations of predefined
operations. This method is also applied by o3, achieving its high score through
the extensive use of computing power. However, for most problems in the
physical world and in the human domain, solutions cannot be tested in advance
and predefined operations are not available. Consequently, massive trialling of
predefined operations, as o3 does, cannot be a basis for AGI - instead, new
approaches are required that can reliably solve a wide variety of problems
without existing skills. To support this development, a new benchmark for
intelligence is outlined that covers a much higher diversity of unknown tasks
to be solved, thus enabling a comprehensive assessment of intelligence and of
progress towards AGI."
"Artificial Creations: Ascription, Ownership, Time-Specific Monopolies","Creativity has always been synonymous with humans. No other living species
could boast of creativity as humans could. Even the smartest computers thrived
only on the ingenious imaginations of its coders. However, that is steadily
changing with highly advanced artificially intelligent systems that demonstrate
incredible capabilities to autonomously (i.e., with minimal or no human input)
produce creative products that would ordinarily deserve intellectual property
status if created by a human. These systems could be called artificial creators
and their creative products artificial creations. The use of artificial
creators is likely to become a part of mainstream production practices in the
creative and innovation industries sooner than we realize. When they do,
intellectual property regimes (that are inherently designed to reward human
creativity) must be sufficiently prepared to aptly respond to the phenomenon of
what could be called artificial creativity. Needless to say, any such response
must be guided by considerations of public welfare. This study analyzes what
that response ought to look like by revisiting the determinants of intellectual
property and critiquing its nature and modes. This understanding of
intellectual property is then applied to investigate the determinants of
intellectual property in artificial creations so as to determine the intrinsic
justifications for intellectual property rewards for artificial creativity, and
accordingly, develop general modalities for granting intellectual property
status to artificial creations. Finally, the treatment of artificial works
(i.e., copyrightable artificial creations) and artificial inventions (i.e.,
patentable artificial creations) by current intellectual property regimes is
critiqued, and specific modalities for granting intellectual property status to
artificial works and artificial inventions are developed."
Dimensions of Neural-symbolic Integration - A Structured Survey,"Research on integrated neural-symbolic systems has made significant progress
in the recent past. In particular the understanding of ways to deal with
symbolic knowledge within connectionist systems (also called artificial neural
networks) has reached a critical mass which enables the community to strive for
applicable implementations and use cases. Recent work has covered a great
variety of logics used in artificial intelligence and provides a multitude of
techniques for dealing with them within the context of artificial neural
networks. We present a comprehensive survey of the field of neural-symbolic
integration, including a new classification of system according to their
architectures and abilities."
Minimum Levels of Interpretability for Artificial Moral Agents,"As artificial intelligence (AI) models continue to scale up, they are
becoming more capable and integrated into various forms of decision-making
systems. For models involved in moral decision-making, also known as artificial
moral agents (AMA), interpretability provides a way to trust and understand the
agent's internal reasoning mechanisms for effective use and error correction.
In this paper, we provide an overview of this rapidly-evolving sub-field of AI
interpretability, introduce the concept of the Minimum Level of
Interpretability (MLI) and recommend an MLI for various types of agents, to aid
their safe deployment in real-world settings."
"Neuropsychology of AI: Relationship Between Activation Proximity and
  Categorical Proximity Within Neural Categories of Synthetic Cognition","Neuropsychology of artificial intelligence focuses on synthetic neural cog
nition as a new type of study object within cognitive psychology. With the goal
of making artificial neural networks of language models more explainable, this
approach involves transposing concepts from cognitive psychology to the
interpretive construction of artificial neural cognition. The human cognitive
concept involved here is categorization, serving as a heuristic for thinking
about the process of segmentation and construction of reality carried out by
the neural vectors of synthetic cognition."
"How Do Artificial Intelligences Think? The Three Mathematico-Cognitive
  Factors of Categorical Segmentation Operated by Synthetic Neurons","How do the synthetic neurons in language models create ""thought categories""
to segment and analyze their informational environment? What are the cognitive
characteristics, at the very level of formal neurons, of this artificial
categorical thought? Based on the mathematical nature of algebraic operations
inherent to neuronal aggregation functions, we attempt to identify
mathematico-cognitive factors that genetically shape the categorical
reconstruction of the informational world faced by artificial cognition. This
study explores these concepts through the notions of priming, attention, and
categorical phasing."
"Quantitative Results Comparing Three Intelligent Interfaces for
  Information Capture: A Case Study Adding Name Information into an Electronic
  Personal Organizer","Efficiently entering information into a computer is key to enjoying the
benefits of computing. This paper describes three intelligent user interfaces:
handwriting recognition, adaptive menus, and predictive fillin. In the context
of adding a personUs name and address to an electronic organizer, tests show
handwriting recognition is slower than typing on an on-screen, soft keyboard,
while adaptive menus and predictive fillin can be twice as fast. This paper
also presents strategies for applying these three interfaces to other
information collection domains."
Avoiding Wireheading with Value Reinforcement Learning,"How can we design good goals for arbitrarily intelligent agents?
Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not
work well for generally intelligent agents, as RL agents are incentivised to
shortcut the reward sensor for maximum reward -- the so-called wireheading
problem. In this paper we suggest an alternative to RL called value
reinforcement learning (VRL). In VRL, agents use the reward signal to learn a
utility function. The VRL setup allows us to remove the incentive to wirehead
by placing a constraint on the agent's actions. The constraint is defined in
terms of the agent's belief distributions, and does not require an explicit
specification of which actions constitute wireheading."
A proposal for ethically traceable artificial intelligence,"Although the problem of a critique of robotic behavior in near-unanimous
agreement to human norms seems intractable, a starting point of such an
ambition is a framework of the collection of knowledge a priori and experience
a posteriori categorized as a set of synthetical judgments available to the
intelligence, translated into computer code. If such a proposal were
successful, an algorithm with ethically traceable behavior and cogent
equivalence to human cognition is established. This paper will propose the
application of Kant's critique of reason to current programming constructs of
an autonomous intelligent system."
A New Framework for Machine Intelligence: Concepts and Prototype,"Machine learning (ML) and artificial intelligence (AI) have become hot topics
in many information processing areas, from chatbots to scientific data
analysis. At the same time, there is uncertainty about the possibility of
extending predominant ML technologies to become general solutions with
continuous learning capabilities. Here, a simple, yet comprehensive,
theoretical framework for intelligent systems is presented. A combination of
Mirror Compositional Representations (MCR) and a Solution-Critic Loop (SCL) is
proposed as a generic approach for different types of problems. A prototype
implementation is presented for document comparison using English Wikipedia
corpus."
"Teaching AI, Ethics, Law and Policy","The cyberspace and development of intelligent systems using Artificial
Intelligence (AI) creates new challenges to computer professionals, data
scientists, regulators and policy makers. For example, self-driving cars raise
new technical, ethical, legal and public policy issues. This paper proposes a
course named Computers, Ethics, Law, and Public Policy, and suggests a
curriculum for such a course. This paper presents ethical, legal, and public
policy issues relevant to building and using intelligent systems."
Chess as a Testing Grounds for the Oracle Approach to AI Safety,"To reduce the danger of powerful super-intelligent AIs, we might make the
first such AIs oracles that can only send and receive messages. This paper
proposes a possibly practical means of using machine learning to create two
classes of narrow AI oracles that would provide chess advice: those aligned
with the player's interest, and those that want the player to lose and give
deceptively bad advice. The player would be uncertain which type of oracle it
was interacting with. As the oracles would be vastly more intelligent than the
player in the domain of chess, experience with these oracles might help us
prepare for future artificial general intelligence oracles."
"Natural Language Generation Using Link Grammar for General
  Conversational Intelligence","Many current artificial general intelligence (AGI) and natural language
processing (NLP) architectures do not possess general conversational
intelligence--that is, they either do not deal with language or are unable to
convey knowledge in a form similar to the human language without manual,
labor-intensive methods such as template-based customization. In this paper, we
propose a new technique to automatically generate grammatically valid sentences
using the Link Grammar database. This natural language generation method far
outperforms current state-of-the-art baselines and may serve as the final
component in a proto-AGI question answering pipeline that understandably
handles natural language material."
Intelligent interactive technologies for mental health and well-being,"Mental healthcare has seen numerous benefits from interactive technologies
and artificial intelligence. Various interventions have successfully used
intelligent technologies to automate the assessment and evaluation of
psychological treatments and mental well-being and functioning. These
technologies include different types of robots, video games, and conversational
agents. The paper critically analyzes existing solutions with the outlooks for
their future. In particular, we: i)give an overview of the technology for
mental health, ii) critically analyze the technology against the proposed
criteria, and iii) provide the design outlooks for these technologies."
"Social Computational Design Method for Generating Product Shapes with
  GAN and Transformer Models","A social computational design method is established, aiming at taking
advantages of the fast-developing artificial intelligence technologies for
intelligent product design. Supported with multi-agent system, shape grammar,
Generative adversarial network, Bayesian network, Transformer, etc., the method
is able to define the design solution space, prepare training samples, and
eventually acquire an intelligent model that can recommend design solutions
according to incomplete solutions for given design tasks. Product shape design
is used as entry point to demonstrate the method, however, the method can be
applied to tasks rather than shape design when the solutions can be properly
coded."
Intelligent Traffic Monitoring with Hybrid AI,"Challenges in Intelligent Traffic Monitoring (ITMo) are exacerbated by the
large quantity and modalities of data and the need for the utilization of
state-of-the-art (SOTA) reasoners. We formulate the problem of ITMo and
introduce HANS, a neuro-symbolic architecture for multi-modal context
understanding, and its application to ITMo. HANS utilizes knowledge graph
technology to serve as a backbone for SOTA reasoning in the traffic domain.
Through case studies, we show how HANS addresses the challenges associated with
traffic monitoring while being able to integrate with a wide range of reasoning
methods"
Neural-Base Music Generation for Intelligence Duplication,"There are two aspects of machine learning and artificial intelligence: (1)
interpreting information, and (2) inventing new useful information. Much
advance has been made for (1) with a focus on pattern recognition techniques
(e.g., interpreting visual data). This paper focuses on (2) with intelligent
duplication (ID) for invention. We explore the possibility of learning a
specific individual's creative reasoning in order to leverage the learned
expertise and talent to invent new information. More specifically, we employ a
deep learning system to learn from the great composer Beethoven and capture his
composition ability in a hash-based knowledge base. This new form of knowledge
base provides a reasoning facility to drive the music composition through a
novel music generation method."
How Generative-AI can be Effectively used in Government Chatbots,"With the rapid development of artificial intelligence and breakthroughs in
machine learning and natural language processing, intelligent
question-answering robots have become widely used in government affairs. This
paper conducts a horizontal comparison between Guangdong Province's government
chatbots, ChatGPT, and Wenxin Ernie, two large language models, to analyze the
strengths and weaknesses of existing government chatbots and AIGC technology.
The study finds significant differences between government chatbots and large
language models. China's government chatbots are still in an exploratory stage
and have a gap to close to achieve ""intelligence."" To explore the future
direction of government chatbots more deeply, this research proposes targeted
optimization paths to help generative AI be effectively applied in government
chatbot conversations."
"HORAE: A Domain-Agnostic Modeling Language for Automating Multimodal
  Service Regulation","Artificial intelligence is rapidly encroaching on the field of service
regulation. This work-in-progress article presents the design principles behind
HORAE, a unified specification language to model multimodal regulation rules
across a diverse set of domains. We show how HORAE facilitates an intelligent
service regulation pipeline by further exploiting a fine-tuned large language
model named HORAE that automates the HORAE modeling process, thereby yielding
an end-to-end framework for fully automated intelligent service regulation."
"Analysis of first prototype universal intelligence tests: evaluating and
  comparing AI algorithms and humans","Today, available methods that assess AI systems are focused on using
empirical techniques to measure the performance of algorithms in some specific
tasks (e.g., playing chess, solving mazes or land a helicopter). However, these
methods are not appropriate if we want to evaluate the general intelligence of
AI and, even less, if we compare it with human intelligence. The ANYNT project
has designed a new method of evaluation that tries to assess AI systems using
well known computational notions and problems which are as general as possible.
This new method serves to assess general intelligence (which allows us to learn
how to solve any new kind of problem we face) and not only to evaluate
performance on a set of specific tasks. This method not only focuses on
measuring the intelligence of algorithms, but also to assess any intelligent
system (human beings, animals, AI, aliens?,...), and letting us to place their
results on the same scale and, therefore, to be able to compare them. This new
approach will allow us (in the future) to evaluate and compare any kind of
intelligent system known or even to build/find, be it artificial or biological.
This master thesis aims at ensuring that this new method provides consistent
results when evaluating AI algorithms, this is done through the design and
implementation of prototypes of universal intelligence tests and their
application to different intelligent systems (AI algorithms and humans beings).
From the study we analyze whether the results obtained by two different
intelligent systems are properly located on the same scale and we propose
changes and refinements to these prototypes in order to, in the future, being
able to achieve a truly universal intelligence test."
A Model for General Intelligence,"The overarching problem in artificial intelligence (AI) is that we do not
understand the intelligence process well enough to enable the development of
adequate computational models. Much work has been done in AI over the years at
lower levels, but a big part of what has been missing involves the high level,
abstract, general nature of intelligence. We address this gap by developing a
model for general intelligence. To accomplish this, we focus on three basic
aspects of intelligence. First, we must realize the general order and nature of
intelligence at a high level. Second, we must come to know what these
realizations mean with respect to the overall intelligence process. Third, we
must describe these realizations as clearly as possible. We propose a
hierarchical model to help capture and exploit the order within intelligence.
The underlying order involves patterns of signals that become organized, stored
and activated in space and time. These patterns can be described using a
simple, general hierarchy, with physical signals at the lowest level,
information in the middle, and abstract signal representations at the top. This
high level perspective provides a big picture that literally helps us see the
intelligence process, thereby enabling fundamental realizations, a better
understanding and clear descriptions of the intelligence process. The resulting
model can be used to support all kinds of information processing across
multiple levels of abstraction. As computer technology improves, and as
cooperation increases between humans and computers, people will become more
efficient and more productive in performing their information processing tasks."
On the Measure of Intelligence,"To make deliberate progress towards more intelligent and more human-like
artificial systems, we need to be following an appropriate feedback signal: we
need to be able to define and evaluate intelligence in a way that enables
comparisons between two systems, as well as comparisons with humans. Over the
past hundred years, there has been an abundance of attempts to define and
measure intelligence, across both the fields of psychology and AI. We summarize
and critically assess these definitions and evaluation approaches, while making
apparent the two historical conceptions of intelligence that have implicitly
guided them. We note that in practice, the contemporary AI community still
gravitates towards benchmarking intelligence by comparing the skill exhibited
by AIs and humans at specific tasks such as board games and video games. We
argue that solely measuring skill at any given task falls short of measuring
intelligence, because skill is heavily modulated by prior knowledge and
experience: unlimited priors or unlimited training data allow experimenters to
""buy"" arbitrary levels of skills for a system, in a way that masks the system's
own generalization power. We then articulate a new formal definition of
intelligence based on Algorithmic Information Theory, describing intelligence
as skill-acquisition efficiency and highlighting the concepts of scope,
generalization difficulty, priors, and experience. Using this definition, we
propose a set of guidelines for what a general AI benchmark should look like.
Finally, we present a benchmark closely following these guidelines, the
Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors
designed to be as close as possible to innate human priors. We argue that ARC
can be used to measure a human-like form of general fluid intelligence and that
it enables fair general intelligence comparisons between AI systems and humans."
"Scalable Training of Artificial Neural Networks with Adaptive Sparse
  Connectivity inspired by Network Science","Through the success of deep learning in various domains, artificial neural
networks are currently among the most used artificial intelligence methods.
Taking inspiration from the network properties of biological neural networks
(e.g. sparsity, scale-freeness), we argue that (contrary to general practice)
artificial neural networks, too, should not have fully-connected layers. Here
we propose sparse evolutionary training of artificial neural networks, an
algorithm which evolves an initial sparse topology (Erd\H{o}s-R\'enyi random
graph) of two consecutive layers of neurons into a scale-free topology, during
learning. Our method replaces artificial neural networks fully-connected layers
with sparse ones before training, reducing quadratically the number of
parameters, with no decrease in accuracy. We demonstrate our claims on
restricted Boltzmann machines, multi-layer perceptrons, and convolutional
neural networks for unsupervised and supervised learning on 15 datasets. Our
approach has the potential to enable artificial neural networks to scale up
beyond what is currently possible."
Characterizations of Decomposable Dependency Models,"Decomposable dependency models possess a number of interesting and useful
properties. This paper presents new characterizations of decomposable models in
terms of independence relationships, which are obtained by adding a single
axiom to the well-known set characterizing dependency models that are
isomorphic to undirected graphs. We also briefly discuss a potential
application of our results to the problem of learning graphical models from
data."
Conditional Plausibility Measures and Bayesian Networks,"A general notion of algebraic conditional plausibility measures is defined.
Probability measures, ranking functions, possibility measures, and (under the
appropriate definitions) sets of probability measures can all be viewed as
defining algebraic conditional plausibility measures. It is shown that
algebraic conditional plausibility measures can be represented using Bayesian
networks."
Instantaneously Trained Neural Networks,"This paper presents a review of instantaneously trained neural networks
(ITNNs). These networks trade learning time for size and, in the basic model, a
new hidden node is created for each training sample. Various versions of the
corner-classification family of ITNNs, which have found applications in
artificial intelligence (AI), are described. Implementation issues are also
considered."
"A Rational Decision Maker with Ordinal Utility under Uncertainty:
  Optimism and Pessimism","In game theory and artificial intelligence, decision making models often
involve maximizing expected utility, which does not respect ordinal invariance.
In this paper, the author discusses the possibility of preserving ordinal
invariance and still making a rational decision under uncertainty."
Artificial Brain Based on Credible Neural Circuits in a Human Brain,"Neurons are individually translated into simple gates to plan a brain based
on human psychology and intelligence. State machines, assumed previously
learned in subconscious associative memory are shown to enable equation solving
and rudimentary thinking using nanoprocessing within short term memory."
A Counter Example to Theorems of Cox and Fine,"Cox's well-known theorem justifying the use of probability is shown not to
hold in finite domains. The counterexample also suggests that Cox's assumptions
are insufficient to prove the result even in infinite domains. The same
counterexample is used to disprove a result of Fine on comparative conditional
probability."
Finding a Path is Harder than Finding a Tree,"I consider the problem of learning an optimal path graphical model from data
and show the problem to be NP-hard for the maximum likelihood and minimum
description length approaches and a Bayesian approach. This hardness result
holds despite the fact that the problem is a restriction of the polynomially
solvable problem of finding the optimal tree graphical model."
SHOP2: An HTN Planning System,"The SHOP2 planning system received one of the awards for distinguished
performance in the 2002 International Planning Competition. This paper
describes the features of SHOP2 which enabled it to excel in the competition,
especially those aspects of SHOP2 that deal with temporal and metric planning
domains."
New Polynomial Classes for Logic-Based Abduction,"We address the problem of propositional logic-based abduction, i.e., the
problem of searching for a best explanation for a given propositional
observation according to a given propositional knowledge base. We give a
general algorithm, based on the notion of projection; then we study
restrictions over the representations of the knowledge base and of the query,
and find new polynomial classes of abduction problems."
Asymptotically Optimal Agents,"Artificial general intelligence aims to create agents capable of learning to
solve arbitrary interesting problems. We define two versions of asymptotic
optimality and prove that no agent can satisfy the strong version while in some
cases, depending on discounting, there does exist a non-computable weak
asymptotically optimal agent."
Engineering a Conformant Probabilistic Planner,"We present a partial-order, conformant, probabilistic planner, Probapop which
competed in the blind track of the Probabilistic Planning Competition in IPC-4.
We explain how we adapt distance based heuristics for use with probabilistic
domains. Probapop also incorporates heuristics based on probability of success.
We explain the successes and difficulties encountered during the design and
implementation of Probapop."
Imperfect Match: PDDL 2.1 and Real Applications,"PDDL was originally conceived and constructed as a lingua franca for the
International Planning Competition. PDDL2.1 embodies a set of extensions
intended to support the expression of something closer to real planning
problems. This objective has only been partially achieved, due in large part to
a deliberate focus on not moving too far from classical planning models and
solution methods."
PDDL 2.1: Representation vs. Computation,"I comment on the PDDL 2.1 language and its use in the planning competition,
focusing on the choices made for accommodating time and concurrency. I also
discuss some methodological issues that have to do with the move toward more
expressive planning languages and the balance needed in planning research
between semantics and computation."
"Maximum likelihood fitting of acyclic directed mixed graphs to binary
  data","Acyclic directed mixed graphs, also known as semi-Markov models represent the
conditional independence structure induced on an observed margin by a DAG model
with latent variables. In this paper we present the first method for fitting
these models to binary data using maximum likelihood estimation."
On Measurement Bias in Causal Inference,"This paper addresses the problem of measurement errors in causal inference
and highlights several algebraic and graphical methods for eliminating
systematic bias induced by such errors. In particulars, the paper discusses the
control of partially observable confounders in parametric and non parametric
models and the computational problem of obtaining bias-free effect estimates in
such models."
Symmetry Breaking Constraints: Recent Results,"Symmetry is an important problem in many combinatorial problems. One way of
dealing with symmetry is to add constraints that eliminate symmetric solutions.
We survey recent results in this area, focusing especially on two common and
useful cases: symmetry breaking constraints for row and column symmetry, and
symmetry breaking constraints for eliminating value symmetry"
"Complexity Analysis and Variational Inference for Interpretation-based
  Probabilistic Description Logic","This paper presents complexity analysis and variational methods for inference
in probabilistic description logics featuring Boolean operators,
quantification, qualified number restrictions, nominals, inverse roles and role
hierarchies. Inference is shown to be PEXP-complete, and variational methods
are designed so as to exploit logical inference whenever possible."
Identifying Dynamic Sequential Plans,"We address the problem of identifying dynamic sequential plans in the
framework of causal Bayesian networks, and show that the problem is reduced to
identifying causal effects, for which there are complete identi cation
algorithms available in the literature."
Reading Dependencies from Polytree-Like Bayesian Networks,"We present a graphical criterion for reading dependencies from the minimal
directed independence map G of a graphoid p when G is a polytree and p
satisfies composition and weak transitivity. We prove that the criterion is
sound and complete. We argue that assuming composition and weak transitivity is
not too restrictive."
A Criterion for Parameter Identification in Structural Equation Models,"This paper deals with the problem of identifying direct causal effects in
recursive linear structural equation models. The paper establishes a sufficient
criterion for identifying individual causal effects and provides a procedure
computing identified causal effects in terms of observed covariance matrix."
Sufficient conditions for convergence of Loopy Belief Propagation,"We derive novel sufficient conditions for convergence of Loopy Belief
Propagation (also known as the Sum-Product algorithm) to a unique fixed point.
Our results improve upon previously known conditions. For binary variables with
(anti-)ferromagnetic interactions, our conditions seem to be sharp."
Description Logics with Fuzzy Concrete Domains,"We present a fuzzy version of description logics with concrete domains. Main
features are: (i) concept constructors are based on t-norm, t-conorm, negation
and implication; (ii) concrete domains are fuzzy sets; (iii) fuzzy modifiers
are allowed; and (iv) the reasoning algorithm is based on a mixture of
completion rules and bounded mixed integer programming."
Optimistic Agents are Asymptotically Optimal,"We use optimism to introduce generic asymptotically optimal reinforcement
learning agents. They achieve, with an arbitrary finite or compact class of
environments, asymptotically optimal behavior. Furthermore, in the finite
deterministic case we provide finite error bounds."
Factorization of Discrete Probability Distributions,"We formulate necessary and sufficient conditions for an arbitrary discrete
probability distribution to factor according to an undirected graphical model,
or a log-linear model, or other more general exponential models. This result
generalizes the well known Hammersley-Clifford Theorem."
Maximum Entropy and the Glasses You Are Looking Through,"We give an interpretation of the Maximum Entropy (MaxEnt) Principle in
game-theoretic terms. Based on this interpretation, we make a formal
distinction between different ways of {em applying/} Maximum Entropy
distributions. MaxEnt has frequently been criticized on the grounds that it
leads to highly representation dependent results. Our distinction allows us to
avoid this problem in many cases."
"Approximate Planning for Factored POMDPs using Belief State
  Simplification","We are interested in the problem of planning for factored POMDPs. Building on
the recent results of Kearns, Mansour and Ng, we provide a planning algorithm
for factored POMDPs that exploits the accuracy-efficiency tradeoff in the
belief state simplification introduced by Boyen and Koller."
Bayesian Control for Concentrating Mixed Nuclear Waste,"A control algorithm for batch processing of mixed waste is proposed based on
conditional Gaussian Bayesian networks. The network is compiled during batch
staging for real-time response to sensor input."
"A Comparison of Lauritzen-Spiegelhalter, Hugin, and Shenoy-Shafer
  Architectures for Computing Marginals of Probability Distributions","In the last decade, several architectures have been proposed for exact
computation of marginals using local computation. In this paper, we compare
three architectures - Lauritzen-Spiegelhalter, Hugin, and Shenoy-Shafer - from
the perspective of graphical structure for message propagation, message-passing
scheme, computational efficiency, and storage efficiency."
"Planning with Partially Observable Markov Decision Processes: Advances
  in Exact Solution Method","There is much interest in using partially observable Markov decision
processes (POMDPs) as a formal model for planning in stochastic domains. This
paper is concerned with finding optimal policies for POMDPs. We propose several
improvements to incremental pruning, presently the most efficient exact
algorithm for solving POMDPs."
Exploiting Uncertain and Temporal Information in Correlation,"A modelling language is described which is suitable for the correlation of
information when the underlying functional model of the system is incomplete or
uncertain and the temporal dependencies are imprecise. An efficient and
incremental implementation is outlined which depends on cost functions
satisfying certain criteria. Possibilistic logic and probability theory (as it
is used in the applications targetted) satisfy these criteria."
A Scheme for Approximating Probabilistic Inference,"This paper describes a class of probabilistic approximation algorithms based
on bucket elimination which offer adjustable levels of accuracy and efficiency.
We analyze the approximation for several tasks: finding the most probable
explanation, belief updating and finding the maximum a posteriori hypothesis.
We identify regions of completeness and provide preliminary empirical
evaluation on randomly generated networks."
Limitations of Skeptical Default Reasoning,"Poole has shown that nonmonotonic logics do not handle the lottery paradox
correctly. In this paper we will show that Pollock's theory of defeasible
reasoning fails for the same reason: defeasible reasoning is incompatible with
the skeptical notion of derivability."
On Stable Multi-Agent Behavior in Face of Uncertainty,"A stable joint plan should guarantee the achievement of a designer's goal in
a multi-agent environment, while ensuring that deviations from the prescribed
plan would be detected. We present a computational framework where stable joint
plans can be studied, as well as several basic results about the
representation, verification and synthesis of stable joint plans."
Region-Based Approximations for Planning in Stochastic Domains,"This paper is concerned with planning in stochastic domains by means of
partially observable Markov decision processes (POMDPs). POMDPs are difficult
to solve. This paper identifies a subclass of POMDPs called region observable
POMDPs, which are easier to solve and can be used to approximate general POMDPs
to arbitrary accuracy."
Propagation of 2-Monotone Lower Probabilities on an Undirected Graph,"Lower and upper probabilities, also known as Choquet capacities, are widely
used as a convenient representation for sets of probability distributions. This
paper presents a graphical decomposition and exact propagation algorithm for
computing marginal posteriors of 2-monotone lower probabilities (equivalently,
2-alternating upper probabilities)."
Topological Parameters for Time-Space Tradeoff,"In this paper we propose a family of algorithms combining tree-clustering
with conditioning that trade space for time. Such algorithms are useful for
reasoning in probabilistic and deterministic networks as well as for
accomplishing optimization tasks. By analyzing the problem structure it will be
possible to select from a spectrum the algorithm that best meets a given
time-space specification."
Computing Upper and Lower Bounds on Likelihoods in Intractable Networks,"We present deterministic techniques for computing upper and lower bounds on
marginal probabilities in sigmoid and noisy-OR networks. These techniques
become useful when the size of the network (or clique size) precludes exact
computations. We illustrate the tightness of the bounds by numerical
experiments."
Binary Join Trees,"The main goal of this paper is to describe a data structure called binary
join trees that are useful in computing multiple marginals efficiently using
the Shenoy-Shafer architecture. We define binary join trees, describe their
utility, and sketch a procedure for constructing them."
Real Time Estimation of Bayesian Networks,"For real time evaluation of a Bayesian network when there is not sufficient
time to obtain an exact solution, a guaranteed response time, approximate
solution is required. It is shown that nontraditional methods utilizing
estimators based on an archive of trial solutions and genetic search can
provide an approximate solution that is considerably superior to the
traditional Monte Carlo simulation methods."
"A Characterization of the Dirichlet Distribution with Application to
  Learning Bayesian Networks","We provide a new characterization of the Dirichlet distribution. This
characterization implies that under assumptions made by several previous
authors for learning belief networks, a Dirichlet prior on the parameters is
inevitable."
"Toward a Characterization of Uncertainty Measure for the Dempster-Shafer
  Theory","This is a working paper summarizing results of an ongoing research project
whose aim is to uniquely characterize the uncertainty measure for the
Dempster-Shafer Theory. A set of intuitive axiomatic requirements is presented,
some of their implications are shown, and the proof is given of the minimality
of recently proposed measure AU among all measures satisfying the proposed
requirements."
Causal Inference and Causal Explanation with Background Knowledge,"This paper presents correct algorithms for answering the following two
questions; (i) Does there exist a causal explanation consistent with a set of
background knowledge which explains all of the observed independence facts in a
sample? (ii) Given that there is such a causal explanation what are the causal
relationships common to every such causal explanation?"
Strong Completeness and Faithfulness in Bayesian Networks,"A completeness result for d-separation applied to discrete Bayesian networks
is presented and it is shown that in a strong measure-theoretic sense almost
all discrete distributions for a given network structure are faithful; i.e. the
independence facts true of the distribution are all and only those entailed by
the network structure."
"Defaults and Infinitesimals: Defeasible Inference by Nonarchimedean
  Entropy-Maximization","We develop a new semantics for defeasible inference based on extended
probability measures allowed to take infinitesimal values, on the
interpretation of defaults as generalized conditional probability constraints
and on a preferred-model implementation of entropy maximization."
A Bayesian Method Reexamined,"This paper examines the ""K2"" network scoring metric of Cooper and Herskovits.
It shows counterintuitive results from applying this metric to simple networks.
One family of noninformative priors is suggested for assigning equal scores to
equivalent networks."
Possibility and Necessity Functions over Non-classical Logics,"We propose an integration of possibility theory into non-classical logics. We
obtain many formal results that generalize the case where possibility and
necessity functions are based on classical logic. We show how useful such an
approach is by applying it to reasoning under uncertain and inconsistent
information."
From Influence Diagrams to Junction Trees,"We present an approach to the solution of decision problems formulated as
influence diagrams. This approach involves a special triangulation of the
underlying graph, the construction of a junction tree with special properties,
and a message passing algorithm operating on the junction tree for computation
of expected utilities and optimal decision policies."
Belief Induced by the Partial Knowledge of the Probabilities,"We construct the belief function that quantifies the agent, beliefs about
which event of Q will occurred when he knows that the event is selected by a
chance set-up and that the probability function associated to the chance set up
is only partially known."
On Axiomatization of Probabilistic Conditional Independencies,"This paper studies the connection between probabilistic conditional
independence in uncertain reasoning and data dependency in relational
databases. As a demonstration of the usefulness of this preliminary
investigation, an alternate proof is presented for refuting the conjecture
suggested by Pearl and Paz that probabilistic conditional independencies have a
complete axiomatization."
Normative Engineering Risk Management Systems,"This paper describes a normative system design that incorporates diagnosis,
dynamic evolution, decision making, and information gathering. A single
influence diagram demonstrates the design's coherence, yet each activity is
more effectively modeled and evaluated separately. Application to offshore oil
platforms illustrates the design. For this application, the normative system is
embedded in a real-time expert system."
Two Procedures for Compiling Influence Diagrams,"Two algorithms are presented for ""compiling"" influence diagrams into a set of
simple decision rules. These decision rules define simple-to-execute, complete,
consistent, and near-optimal decision procedures. These compilation algorithms
can be used to derive decision procedures for human teams solving time
constrained decision problems."
Deciding Morality of Graphs is NP-complete,"In order to find a causal explanation for data presented in the form of
covariance and concentration matrices it is necessary to decide if the graph
formed by such associations is a projection of a directed acyclic graph (dag).
We show that the general problem of deciding whether such a dag exists is
NP-complete."
Qualitative Measures of Ambiguity,"This paper introduces a qualitative measure of ambiguity and analyses its
relationship with other measures of uncertainty. Probability measures relative
likelihoods, while ambiguity measures vagueness surrounding those judgments.
Ambiguity is an important representation of uncertain knowledge. It deals with
a different, type of uncertainty modeled by subjective probability or belief."
Jeffrey's rule of conditioning generalized to belief functions,"Jeffrey's rule of conditioning has been proposed in order to revise a
probability measure by another probability function. We generalize it within
the framework of the models based on belief functions. We show that several
forms of Jeffrey's conditionings can be defined that correspond to the
geometrical rule of conditioning and to Dempster's rule of conditioning,
respectively."
Inference with Possibilistic Evidence,"In this paper, the concept of possibilistic evidence which is a possibility
distribution as well as a body of evidence is proposed over an infinite
universe of discourse. The inference with possibilistic evidence is
investigated based on a unified inference framework maintaining both the
compatibility of concepts and the consistency of the probability logic."
Entropy and Belief Networks,"The product expansion of conditional probabilities for belief nets is not
maximum entropy. This appears to deny a desirable kind of assurance for the
model. However, a kind of guarantee that is almost as strong as maximum entropy
can be derived. Surprisingly, a variant model also exhibits the guarantee, and
for many cases obtains a higher performance score than the product expansion."
Objection-Based Causal Networks,"This paper introduces the notion of objection-based causal networks which
resemble probabilistic causal networks except that they are quantified using
objections. An objection is a logical sentence and denotes a condition under
which a, causal dependency does not exist. Objection-based causal networks
enjoy almost all the properties that make probabilistic causal networks
popular, with the added advantage that objections are, arguably more intuitive
than probabilities."
A Note on the Measure of Discord,"A new entropy-like measure as well as a new measure of total uncertainty
pertaining to the Dempster-Shafer theory are introduced. It is argued that
these measures are better justified than any of the previously proposed
candidates."
Bayesian Networks Aplied to Therapy Monitoring,"We propose a general Bayesian network model for application in a wide class
of problems of therapy monitoring. We discuss the use of stochastic simulation
as a computational approach to inference on the proposed class of models. As an
illustration we present an application to the monitoring of cytotoxic
chemotherapy in breast cancer."
"A Probabilistic Analysis of Marker-Passing Techniques for
  Plan-Recognition","Useless paths are a chronic problem for marker-passing techniques. We use a
probabilistic analysis to justify a method for quickly identifying and
rejecting useless paths. Using the same analysis, we identify key conditions
and assumptions necessary for marker-passing to perform well."
A Reason Maintenace System Dealing with Vague Data,"A reason maintenance system which extends an ATMS through Mukaidono's fuzzy
logic is described. It supports a problem solver in situations affected by
incomplete information and vague data, by allowing nonmonotonic inferences and
the revision of previous conclusions when contradictions are detected."
Representation Requirements for Supporting Decision Model Formulation,"This paper outlines a methodology for analyzing the representational support
for knowledge-based decision-modeling in a broad domain. A relevant set of
inference patterns and knowledge types are identified. By comparing the
analysis results to existing representations, some insights are gained into a
design approach for integrating categorical and uncertain knowledge in a
context sensitive manner."
A Fusion Algorithm for Solving Bayesian Decision Problems,"This paper proposes a new method for solving Bayesian decision problems. The
method consists of representing a Bayesian decision problem as a
valuation-based system and applying a fusion algorithm for solving it. The
fusion algorithm is a hybrid of local computational methods for computation of
marginals of joint probability distributions and the local computational
methods for discrete optimization problems."
Algorithms for Irrelevance-Based Partial MAPs,"Irrelevance-based partial MAPs are useful constructs for domain-independent
explanation using belief networks. We look at two definitions for such partial
MAPs, and prove important properties that are useful in designing algorithms
for computing them effectively. We make use of these properties in modifying
our standard MAP best-first algorithm, so as to handle irrelevance-based
partial MAPs."
From Relational Databases to Belief Networks,"The relationship between belief networks and relational databases is
examined. Based on this analysis, a method to construct belief networks
automatically from statistical relational data is proposed. A comparison
between our method and other methods shows that our method has several
advantages when generalization or prediction is deeded."
Context-Dependent Similarity,"Attribute weighting and differential weighting, two major mechanisms for
computing context-dependent similarity or dissimilarity measures are studied
and compared. A dissimilarity measure based on subset size in the context is
proposed and its metrization and application are given. It is also shown that
while all attribute weighting dissimilarity measures are metrics differential
weighting dissimilarity measures are usually non-metric."
"Similarity Networks for the Construction of Multiple-Faults Belief
  Networks","A similarity network is a tool for constructing belief networks for the
diagnosis of a single fault. In this paper, we examine modifications to the
similarity-network representation that facilitate the construction of belief
networks for the diagnosis of multiple coexisting faults."
"On Some Equivalence Relations between Incidence Calculus and
  Dempster-Shafer Theory of Evidence","Incidence Calculus and Dempster-Shafer Theory of Evidence are both theories
to describe agents' degrees of belief in propositions, thus being appropriate
to represent uncertainty in reasoning systems. This paper presents a
straightforward equivalence proof between some special cases of these theories."
Analysis in HUGIN of Data Conflict,"After a brief introduction to causal probabilistic networks and the HUGIN
approach, the problem of conflicting data is discussed. A measure of conflict
is defined, and it is used in the medical diagnostic system MUNIN. Finally, it
is discussed how to distinguish between conflicting data and a rare case."
d-Separation: From Theorems to Algorithms,"An efficient algorithm is developed that identifies all independencies
implied by the topology of a Bayesian network. Its correctness and maximality
stems from the soundness and completeness of d-separation with respect to
probability theory. The algorithm runs in time O (l E l) where E is the number
of edges in the network."
"Maximum Uncertainty Procedures for Interval-Valued Probability
  Distributions","Measures of uncertainty and divergence are introduced for interval-valued
probability distributions and are shown to have desirable mathematical
properties. A maximum uncertainty inference procedure for marginal interval
distributions is presented. A technique for reconstruction of interval
distributions from projections is developed based on this inference procedure"
Directed Cycles in Belief Networks,"The most difficult task in probabilistic reasoning may be handling directed
cycles in belief networks. To the best knowledge of this author, there is no
serious discussion of this problem at all in the literature of probabilistic
reasoning so far."
"Normalization and the Representation of Nonmonotonic Knowledge in the
  Theory of Evidence","We discuss the Dempster-Shafer theory of evidence. We introduce a concept of
monotonicity which is related to the diminution of the range between belief and
plausibility. We show that the accumulation of knowledge in this framework
exhibits a nonmonotonic property. We show how the belief structure can be used
to represent typical or commonsense knowledge."
A Method for Using Belief Networks as Influence Diagrams,"This paper demonstrates a method for using belief-network algorithms to solve
influence diagram problems. In particular, both exact and approximation
belief-network algorithms may be applied to solve influence-diagram problems.
More generally, knowing the relationship between belief-network and
influence-diagram problems may be useful in the design and development of more
efficient influence diagram algorithms."
"Modeling uncertain and vague knowledge in possibility and evidence
  theories","This paper advocates the usefulness of new theories of uncertainty for the
purpose of modeling some facets of uncertain knowledge, especially vagueness,
in AI. It can be viewed as a partial reply to Cheeseman's (among others)
defense of probability."
Truth Maintenance Under Uncertainty,"This paper addresses the problem of resolving errors under uncertainty in a
rule-based system. A new approach has been developed that reformulates this
problem as a neural-network learning problem. The strength and the fundamental
limitations of this approach are explored and discussed. The main result is
that neural heuristics can be applied to solve some but not all problems in
rule-based systems."
The Optimality of Satisficing Solutions,"This paper addresses a prevailing assumption in single-agent heuristic search
theory- that problem-solving algorithms should guarantee shortest-path
solutions, which are typically called optimal. Optimality implies a metric for
judging solution quality, where the optimal solution is the solution with the
highest quality. When path-length is the metric, we will distinguish such
solutions as p-optimal."
Probabilistic Inference and Probabilistic Reasoning,"Uncertainty enters into human reasoning and inference in at least two ways.
It is reasonable to suppose that there will be roles for these distinct uses of
uncertainty also in automated reasoning."
A Linear Approximation Method for Probabilistic Inference,"An approximation method is presented for probabilistic inference with
continuous random variables. These problems can arise in many practical
problems, in particular where there are ""second order"" probabilities. The
approximation, based on the Gaussian influence diagram, iterates over linear
approximations to the inference problem."
Handling uncertainty in a system for text-symbol context analysis,"In pattern analysis, information regarding an object can often be drawn from
its surroundings. This paper presents a method for handling uncertainty when
using context of symbols and texts for analyzing technical drawings. The method
is based on Dempster-Shafer theory and possibility theory."
"Do We Need Higher-Order Probabilities and, If So, What Do They Mean?","The apparent failure of individual probabilistic expressions to distinguish
uncertainty about truths from uncertainty about probabilistic assessments have
prompted researchers to seek formalisms where the two types of uncertainties
are given notational distinction. This paper demonstrates that the desired
distinction is already a built-in feature of classical probabilistic models,
thus, specialized notations are unnecessary."
Using the Dempster-Shafer Scheme in a Diagnostic Expert System Shell,"This paper discusses an expert system shell that integrates rule-based
reasoning and the Dempster-Shafer evidence combination scheme. Domain knowledge
is stored as rules with associated belief functions. The reasoning component
uses a combination of forward and backward inferencing mechanisms to allow
interaction with users in a mixed-initiative format."
Comparisons of Reasoning Mechanisms for Computer Vision,"An evidential reasoning mechanism based on the Dempster-Shafer theory of
evidence is introduced. Its performance in real-world image analysis is
compared with other mechanisms based on the Bayesian formalism and a simple
weight combination method."
Evidential Reasoning in Image Understanding,"In this paper, we present some results of evidential reasoning in
understanding multispectral images of remote sensing systems. The
Dempster-Shafer approach of combination of evidences is pursued to yield
contextual classification results, which are compared with previous results of
the Bayesian context free classification, contextual classifications of dynamic
programming and stochastic relaxation approaches."
Explanation of Probabilistic Inference for Decision Support Systems,"An automated explanation facility for Bayesian conditioning aimed at
improving user acceptance of probability-based decision support systems has
been developed. The domain-independent facility is based on an information
processing perspective on reasoning about conditional evidence that accounts
both for biased and normative inferences. Experimental results indicate that
the facility is both acceptable to naive users and effective in improving
understanding."
Efficient Inference on Generalized Fault Diagrams,"The generalized fault diagram, a data structure for failure analysis based on
the influence diagram, is defined. Unlike the fault tree, this structure allows
for dependence among the basic events and replicated logical elements. A
heuristic procedure is developed for efficient processing of these structures."
Learning Link-Probabilities in Causal Trees,"A learning algorithm is presented which given the structure of a causal tree,
will estimate its link probabilities by sequential measurements on the leaves
only. Internal nodes of the tree represent conceptual (hidden) variables
inaccessible to observation. The method described is incremental, local,
efficient, and remains robust to measurement imprecisions."
DAVID: Influence Diagram Processing System for the Macintosh,"Influence diagrams are a directed graph representation for uncertainties as
probabilities. The graph distinguishes between those variables which are under
the control of a decision maker (decisions, shown as rectangles) and those
which are not (chances, shown as ovals), as well as explicitly denoting a goal
for solution (value, shown as a rounded rectangle."
Generalizing Fuzzy Logic Probabilistic Inferences,"Linear representations for a subclass of boolean symmetric functions selected
by a parity condition are shown to constitute a generalization of the linear
constraints on probabilities introduced by Boole. These linear constraints are
necessary to compute probabilities of events with relations between the.
arbitrarily specified with propositional calculus boolean formulas."
Qualitative Probabilistic Networks for Planning Under Uncertainty,"Bayesian networks provide a probabilistic semantics for qualitative
assertions about likelihood. A qualitative reasoner based on an algebra over
these assertions can derive further conclusions about the influence of actions.
While the conclusions are much weaker than those computed from complete
probability distributions, they are still valuable for suggesting potential
actions, eliminating obviously inferior plans, identifying important tradeoffs,
and explaining probabilistic models."
On Implementing Usual Values,"In many cases commonsense knowledge consists of knowledge of what is usual.
In this paper we develop a system for reasoning with usual information. This
system is based upon the fact that these pieces of commonsense information
involve both a probabilistic aspect and a granular aspect. We implement this
system with the aid of possibility-probability granules."
On the Combinality of Evidence in the Dempster-Shafer Theory,"In the current versions of the Dempster-Shafer theory, the only essential
restriction on the validity of the rule of combination is that the sources of
evidence must be statistically independent. Under this assumption, it is
permissible to apply the Dempster-Shafer rule to two or mere distinct
probability distributions."
A Constraint Propagation Approach to Probabilistic Reasoning,"The paper demonstrates that strict adherence to probability theory does not
preclude the use of concurrent, self-activated constraint-propagation
mechanisms for managing uncertainty. Maintaining local records of
sources-of-belief allows both predictive and diagnostic inferences to be
activated simultaneously and propagate harmoniously towards a stable
equilibrium."
Implementing Probabilistic Reasoning,"General problems in analyzing information in a probabilistic database are
considered. The practical difficulties (and occasional advantages) of storing
uncertain data, of using it conventional forward- or backward-chaining
inference engines, and of working with a probabilistic version of resolution
are discussed. The background for this paper is the incorporation of uncertain
reasoning facilities in MRS, a general-purpose expert system building tool."
A factorization criterion for acyclic directed mixed graphs,"Acyclic directed mixed graphs, also known as semi-Markov models represent the
conditional independence structure induced on an observed margin by a DAG model
with latent variables. In this paper we present a factorization criterion for
these models that is equivalent to the global Markov property given by (the
natural extension of) d-separation."
Conditional Plausibility Measures and Bayesian Networks,"A general notion of algebraic conditional plausibility measures is defined.
Probability measures, ranking functions, possibility measures, and (under the
appropriate definitions) sets of probability measures can all be viewed as
defining algebraic conditional plausibility measures. It is shown that the
technology of Bayesian networks can be applied to algebraic conditional
plausibility measures."
Generalized Qualitative Probability: Savage Revisited,"Preferences among acts are analyzed in the style of L. Savage, but as
partially ordered. The rationality postulates considered are weaker than
Savage's on three counts. The Sure Thing Principle is derived in this setting.
The postulates are shown to lead to a characterization of generalized
qualitative probability that includes and blends both traditional qualitative
probability and the ranked structures used in logical approaches."
Quantum Annealing for Clustering,"This paper studies quantum annealing (QA) for clustering, which can be seen
as an extension of simulated annealing (SA). We derive a QA algorithm for
clustering and propose an annealing schedule, which is crucial in practice.
Experiments show the proposed QA algorithm finds better clustering assignments
than SA. Furthermore, QA is as easy as SA to implement."
"From Ordinary Differential Equations to Structural Causal Models: the
  deterministic case","We show how, and under which conditions, the equilibrium states of a
first-order Ordinary Differential Equation (ODE) system can be described with a
deterministic Structural Causal Model (SCM). Our exposition sheds more light on
the concept of causality as expressed within the framework of Structural Causal
Models, especially for cyclic models."
Rule reasoning for legal norm validation of FSTP facts,"Non-obviousness or inventive step is a general requirement for patentability
in most patent law systems. An invention should be at an adequate distance
beyond its prior art in order to be patented. This short paper provides an
overview on a methodology proposed for legal norm validation of FSTP facts
using rule reasoning approach."
"Decidability, Introduction Rules and Automata","We present a method to prove the decidability of provability in several
well-known inference systems. This method generalizes both cut-elimination and
the construction of an automaton recognizing the provable propositions."
On Quantum Decision Trees,"Quantum decision systems are being increasingly considered for use in
artificial intelligence applications. Classical and quantum nodes can be
distinguished based on certain correlations in their states. This paper
investigates some properties of the states obtained in a decision tree
structure. How these correlations may be mapped to the decision tree is
considered. Classical tree representations and approximations to quantum states
are provided."
"The MacGyver Test - A Framework for Evaluating Machine Resourcefulness
  and Creative Problem Solving","Current measures of machine intelligence are either difficult to evaluate or
lack the ability to test a robot's problem-solving capacity in open worlds. We
propose a novel evaluation framework based on the formal notion of MacGyver
Test which provides a practical way for assessing the resilience and
resourcefulness of artificial agents."
AI in Game Playing: Sokoban Solver,"Artificial Intelligence is becoming instrumental in a variety of
applications. Games serve as a good breeding ground for trying and testing
these algorithms in a sandbox with simpler constraints in comparison to real
life. In this project, we aim to develop an AI agent that can solve the
classical Japanese game of Sokoban using various algorithms and heuristics and
compare their performances through standard metrics."
Fair lending needs explainable models for responsible recommendation,"The financial services industry has unique explainability and fairness
challenges arising from compliance and ethical considerations in credit
decisioning. These challenges complicate the use of model machine learning and
artificial intelligence methods in business decision processes."
Conducting Feasibility Studies for Knowledge Based Systems,"This paper describes how to carry out a feasibility study for a potential
knowledge based system application. It discusses factors to be considered under
three headings: the business case, the technical feasibility, and stakeholder
issues. It concludes with a case study of a feasibility study for a KBS to
guide surgeons in diagnosis and treatment of thyroid conditions."
"AlphaGomoku: An AlphaGo-based Gomoku Artificial Intelligence using
  Curriculum Learning","In this project, we combine AlphaGo algorithm with Curriculum Learning to
crack the game of Gomoku. Modifications like Double Networks Mechanism and
Winning Value Decay are implemented to solve the intrinsic asymmetry and
short-sight of Gomoku. Our final AI AlphaGomoku, through two days' training on
a single GPU, has reached humans' playing level."
Robust Artificial Intelligence and Robust Human Organizations,"Every AI system is deployed by a human organization. In high risk
applications, the combined human plus AI system must function as a
high-reliability organization in order to avoid catastrophic errors. This short
note reviews the properties of high-reliability organizations and draws
implications for the development of AI technology and the safe application of
that technology."
"Selected Qualitative Spatio-temporal Calculi Developed for Constraint
  Reasoning: A Review","In this article a few of the qualitative spatio-temporal knowledge
representation techniques developed by the constraint reasoning community
within artificial intelligence are reviewed. The objective is to provide a
broad exposure to any other interested group who may utilize these
representations. The author has a particular interest in applying these calculi
(in a broad sense) in topological data analysis, as these schemes are highly
qualitative in nature."
Lifted Weight Learning of Markov Logic Networks Revisited,"We study lifted weight learning of Markov logic networks. We show that there
is an algorithm for maximum-likelihood learning of 2-variable Markov logic
networks which runs in time polynomial in the domain size. Our results are
based on existing lifted-inference algorithms and recent algorithmic results on
computing maximum entropy distributions."
Modern Deep Reinforcement Learning Algorithms,"Recent advances in Reinforcement Learning, grounded on combining classical
theoretical results with Deep Learning paradigm, led to breakthroughs in many
artificial intelligence tasks and gave birth to Deep Reinforcement Learning
(DRL) as a field of research. In this work latest DRL algorithms are reviewed
with a focus on their theoretical justification, practical limitations and
observed empirical properties."
"Towards Enterprise-Ready AI Deployments Minimizing the Risk of Consuming
  AI Models in Business Applications","The stochastic nature of artificial intelligence (AI) models introduces risk
to business applications that use AI models without careful consideration. This
paper offers an approach to use AI techniques to gain insights on the usage of
the AI models and control how they are deployed to a production application.
  Keywords: artificial intelligence (AI), machine learning, microservices,
business process"
An AGI with Time-Inconsistent Preferences,"This paper reveals a trap for artificial general intelligence (AGI) theorists
who use economists' standard method of discounting. This trap is implicitly and
falsely assuming that a rational AGI would have time-consistent preferences. An
agent with time-inconsistent preferences knows that its future self will
disagree with its current self concerning intertemporal decision making. Such
an agent cannot automatically trust its future self to carry out plans that its
current self considers optimal."
Classification Schemas for Artificial Intelligence Failures,"In this paper we examine historical failures of artificial intelligence (AI)
and propose a classification scheme for categorizing future failures. By doing
so we hope that (a) the responses to future failures can be improved through
applying a systematic classification that can be used to simplify the choice of
response and (b) future failures can be reduced through augmenting development
lifecycles with targeted risk assessments."
Three Modern Roles for Logic in AI,"We consider three modern roles for logic in artificial intelligence, which
are based on the theory of tractable Boolean circuits: (1) logic as a basis for
computation, (2) logic for learning from a combination of data and knowledge,
and (3) logic for reasoning about the behavior of machine learning systems."
Automatic Knowledge Acquisition for Object-Oriented Expert Systems,"We describe an Object Oriented Model for building Expert Systems. This model
and the detection of similarities allow to implement reasoning modes as
induction, deduction and simulation. We specially focus on similarity and its
use in induction. We propose original algorithms which deal with total and
partial structural similitude of objects to facilitate knowledge acquisition."
DeepMind Lab2D,"We present DeepMind Lab2D, a scalable environment simulator for artificial
intelligence research that facilitates researcher-led experimentation with
environment design. DeepMind Lab2D was built with the specific needs of
multi-agent deep reinforcement learning researchers in mind, but it may also be
useful beyond that particular subfield."
"Proceedings of NeurIPS 2019 Workshop on Artificial Intelligence for
  Humanitarian Assistance and Disaster Response","These are the ""proceedings"" of the 1st AI + HADR workshop which was held in
Vancouver, Canada on December 13, 2019 as part of the Neural Information
Processing Systems conference. These are non-archival and serve solely as a
collation of all the papers accepted to the workshop."
"Proceedings of NeurIPS 2020 Workshop on Artificial Intelligence for
  Humanitarian Assistance and Disaster Response","These are the ""proceedings"" of the 2nd AI + HADR workshop which was held
virtually on December 12, 2020 as part of the Neural Information Processing
Systems conference. These are non-archival and merely serve as a way to collate
all the papers accepted to the workshop."
Comprehension and Knowledge,"The ability of an agent to comprehend a sentence is tightly connected to the
agent's prior experiences and background knowledge. The paper suggests to
interpret comprehension as a modality and proposes a complete bimodal logical
system that describes an interplay between comprehension and knowledge
modalities."
Epistemic Logic of Know-Who,"The paper suggests a definition of ""know who"" as a modality using
Grove-Halpern semantics of names. It also introduces a logical system that
describes the interplay between modalities ""knows who"", ""knows"", and ""for all
agents"". The main technical result is a completeness theorem for the proposed
system."
A survey of the European Union's artificial intelligence ecosystem,"Compared to other global powers, the European Union (EU) is rarely considered
a leading player in the development of artificial intelligence (AI). Why is
this, and does this in fact accurately reflect the activities of the EU? What
would it take for the EU to take a more leading role in AI? This report surveys
core components of the current AI ecosystem of the EU, providing the crucial
background context for answering these questions."
Unanswerable Questions about Images and Texts,"Questions about a text or an image that cannot be answered raise distinctive
issues for an AI. This note discusses the problem of unanswerable questions in
VQA (visual question answering), in QA (visual question answering), and in AI
generally."
AI Risk Skepticism,"In this work, we survey skepticism regarding AI risk and show parallels with
other types of scientific skepticism. We start by classifying different types
of AI Risk skepticism and analyze their root causes. We conclude by suggesting
some intervention approaches, which may be successful in reducing AI risk
skepticism, at least amongst artificial intelligence researchers."
"Potential Applications of Artificial Intelligence and Machine Learning
  in Radiochemistry and Radiochemical Engineering","Artificial intelligence and machine learning are poised to disrupt PET
imaging from bench to clinic. In this perspective we offer insights into how
the technology could be applied to improve the design and synthesis of new
radiopharmaceuticals for PET imaging, including identification of an optimal
labeling approach as well as strategies for radiolabeling reaction
optimization."
"Interpretable deep-learning models to help achieve the Sustainable
  Development Goals","We discuss our insights into interpretable artificial-intelligence (AI)
models, and how they are essential in the context of developing ethical AI
systems, as well as data-driven solutions compliant with the Sustainable
Development Goals (SDGs). We highlight the potential of extracting
truly-interpretable models from deep-learning methods, for instance via
symbolic models obtained through inductive biases, to ensure a sustainable
development of AI."
The Mathematics of Comparing Objects,"""After reading two different crime stories, an artificial intelligence
concludes that in both stories the police has found the murderer just by
random."" -- To what extend and under which assumptions this is a description of
a realistic scenario?"
Computer sciences and synthesis: retrospective and perspective,"The problem of synthesis in computer sciences, including cybernetics,
artificial intelligence and system analysis, is analyzed. Main methods of
realization this problem are discussed. Ways of search universal method of
creation universal synthetic science are represented. As example of such
universal method polymetric analysis is given. Perspective of further
development of this research, including application polymetric method for the
resolution main problems of computer sciences, is analyzed too."
Confident AI,"In this paper, we propose ""Confident AI"" as a means to designing Artificial
Intelligence (AI) and Machine Learning (ML) systems with both algorithm and
user confidence in model predictions and reported results. The 4 basic tenets
of Confident AI are Repeatability, Believability, Sufficiency, and
Adaptability. Each of the tenets is used to explore fundamental issues in
current AI/ML systems and together provide an overall approach to Confident AI."
Recent Progress in Conversational AI,"Conversational artificial intelligence (AI) is becoming an increasingly
popular topic among industry and academia. With the fast development of neural
network-based models, a lot of neural-based conversational AI system are
developed. We will provide a brief review of the recent progress in the
Conversational AI, including the commonly adopted techniques, notable works,
famous competitions from academia and industry and widely used datasets."
Developing moral AI to support antimicrobial decision making,"Artificial intelligence (AI) assisting with antimicrobial prescribing raises
significant moral questions. Utilising ethical frameworks alongside AI-driven
systems, while considering infection specific complexities, can support moral
decision making to tackle antimicrobial resistance."
The Alberta Plan for AI Research,"Herein we describe our approach to artificial intelligence research, which we
call the Alberta Plan. The Alberta Plan is pursued within our research groups
in Alberta and by others who are like minded throughout the world. We welcome
all who would join us in this pursuit."
A Semantic Tableau Method for Argument Construction,"A semantic tableau method, called an argumentation tableau, that enables the
derivation of arguments, is proposed. First, the derivation of arguments for
standard propositional and predicate logic is addressed. Next, an extension
that enables reasoning with defeasible rules is presented. Finally, reasoning
by cases using an argumentation tableau is discussed."
Auxiliary Learning as a step towards Artificial General Intelligence,"Auxiliary Learning is a machine learning approach in which the model
acknowledges the existence of objects that do not come under any of its learned
categories.The name Auxiliary learning was chosen due to the introduction of an
auxiliary class. The paper focuses on increasing the generality of existing
narrow purpose neural networks and also highlights the need to handle unknown
objects. The Cat & Dog binary classifier is taken as an example throughout the
paper."
Circumventing interpretability: How to defeat mind-readers,"The increasing capabilities of artificial intelligence (AI) systems make it
ever more important that we interpret their internals to ensure that their
intentions are aligned with human values. Yet there is reason to believe that
misaligned artificial intelligence will have a convergent instrumental
incentive to make its thoughts difficult for us to interpret. In this article,
I discuss many ways that a capable AI might circumvent scalable
interpretability methods and suggest a framework for thinking about these
potential future risks."
"Disease X vaccine production and supply chains: risk assessing
  healthcare systems operating with artificial intelligence and industry 4.0","A set of six algorithmic solutions is presented for resolving vaccine
production and supply chain bottlenecks. A different set of algorithmic
solutions is presented for forecasting risks during a Disease X event."
Shhh! The Logic of Clandestine Operations,"An operation is called covert if it conceals the identity of the actor; it is
called clandestine if the very fact that the operation is conducted is
concealed. The paper proposes a formal semantics of clandestine operations and
introduces a sound and complete logical system that describes the interplay
between the distributed knowledge modality and a modality capturing coalition
power to conduct clandestine operations."
Artificial intelligence moral agent as Adam Smith's impartial spectator,"Adam Smith developed a version of moral philosophy where better decisions are
made by interrogating an impartial spectator within us. We discuss the
possibility of using an external non-human-based substitute tool that would
augment our internal mental processes and play the role of the impartial
spectator. Such tool would have more knowledge about the world, be more
impartial, and would provide a more encompassing perspective on moral
assessment."
Evaluation of AI Chatbots for Patient-Specific EHR Questions,"This paper investigates the use of artificial intelligence chatbots for
patient-specific question answering (QA) from clinical notes using several
large language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google
Bard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and
coherence of the answers generated by each model using a 5-point Likert scale
on a set of patient-specific questions."
Beyond XAI:Obstacles Towards Responsible AI,"The rapidly advancing domain of Explainable Artificial Intelligence (XAI) has
sparked significant interests in developing techniques to make AI systems more
transparent and understandable. Nevertheless, in real-world contexts, the
methods of explainability and their evaluation strategies present numerous
limitations.Moreover, the scope of responsible AI extends beyond just
explainability. In this paper, we explore these limitations and discuss their
implications in a boarder context of responsible AI when considering other
important aspects, including privacy, fairness and contestability."
"Field-testing items using artificial intelligence: Natural language
  processing with transformers","Five thousand variations of the RoBERTa model, an artificially intelligent
""transformer"" that can understand text language, completed an English literacy
exam with 29 multiple-choice questions. Data were used to calculate the
psychometric properties of the items, which showed some degree of agreement to
those obtained from human examinee data."
AI Code Generators for Security: Friend or Foe?,"Recent advances of artificial intelligence (AI) code generators are opening
new opportunities in software security research, including misuse by malicious
actors. We review use cases for AI code generators for security and introduce
an evaluation benchmark."
Foundation Models and Information Retrieval in Digital Pathology,"The paper reviews the state-of-the-art of foundation models, LLMs, generative
AI, information retrieval and CBIR in digital pathology"
Automatic Extraction of Linguistic Description from Fuzzy Rule Base,"Neuro-fuzzy systems are a technique of explainable artificial intelligence
(XAI). They elaborate knowledge models as a set of fuzzy rules. Fuzzy sets are
crucial components of fuzzy rules. They are used to model linguistic terms. In
this paper, we present an automatic extraction of fuzzy rules in the natural
English language. Full implementation is available free from a public
repository."
The Path To Autonomous Cyber Defense,"Defenders are overwhelmed by the number and scale of attacks against their
networks.This problem will only be exacerbated as attackers leverage artificial
intelligence to automate their workflows. We propose a path to autonomous cyber
agents able to augment defenders by automating critical steps in the cyber
defense life cycle."
Agentive Permissions in Multiagent Systems,"This paper proposes to distinguish four forms of agentive permissions in
multiagent settings. The main technical results are the complexity analysis of
model checking, the semantic undefinability of modalities that capture these
forms of permissions through each other, and a complete logical system
capturing the interplay between these modalities."
Neural logic programs and neural nets,"Neural-symbolic integration aims to combine the connectionist subsymbolic
with the logical symbolic approach to artificial intelligence. In this paper,
we first define the answer set semantics of (boolean) neural nets and then
introduce from first principles a class of neural logic programs and show that
nets and programs are equivalent."
Training Next Generation AI Users and Developers at NCSA,"This article focuses on training work carried out in artificial intelligence
(AI) at the National Center for Supercomputing Applications (NCSA) at the
University of Illinois Urbana-Champaign via a research experience for
undergraduates (REU) program named FoDOMMaT. It also describes why we are
interested in AI, and concludes by discussing what we've learned from running
this program and its predecessor over six years."
"Historical Review of Variants of Informal Semantics for Logic Programs
  under Answer Set Semantics: GL'88, GL'91, GK'14, D-V'12","This note presents a historical survey of informal semantics that are
associated with logic programming under answer set semantics. We review these
in uniform terms and align them with two paradigms: Answer Set Programming and
ASP-Prolog -- two prominent Knowledge Representation and Reasoning Paradigms in
Artificial Intelligence. Under consideration in Theory and Practice of Logic
Programming (TPLP)."
Artificial intelligence and financial crises,"The rapid adoption of artificial intelligence (AI) is transforming the
financial industry. AI will either increase systemic financial risk or act to
stabilise the system, depending on endogenous responses, strategic
complementarities, the severity of events it faces and the objectives it is
given. AI's ability to master complexity and respond rapidly to shocks means
future crises will likely be more intense than those we have seen so far."
"Neurosymbolic artificial intelligence via large language models and
  coherence-driven inference","We devise an algorithm to generate sets of propositions that objectively
instantiate graphs that support coherence-driven inference. We then benchmark
the ability of large language models (LLMs) to reconstruct coherence graphs
from (a straightforward transformation of) propositions expressed in natural
language, with promising results from a single prompt to models optimized for
reasoning. Combining coherence-driven inference with consistency evaluations by
neural models may advance the state of the art in machine cognition."
Bringing Comparative Cognition To Computers,"Researchers are increasingly subjecting artificial intelligence systems to
psychological testing. But to rigorously compare their cognitive capacities
with humans and other animals, we must avoid both over- and under-stating our
similarities and differences. By embracing a comparative approach, we can
integrate AI cognition research into the broader cognitive sciences."
"All You Need is Sally-Anne: ToM in AI Strongly Supported After
  Surpassing Tests for 3-Year-Olds","Theory of Mind (ToM) is a hallmark of human cognition, allowing individuals
to reason about others' beliefs and intentions. Engineers behind recent
advances in Artificial Intelligence (AI) have claimed to demonstrate comparable
capabilities. This paper presents a model that surpasses traditional ToM tests
designed for 3-year-old children, providing strong support for the presence of
ToM in AI systems."
Elementary epistemological features of machine intelligence,"Theoretical analysis of machine intelligence (MI) is useful for defining a
common platform in both theoretical and applied artificial intelligence (AI).
The goal of this paper is to set canonical definitions that can assist
pragmatic research in both strong and weak AI. Described epistemological
features of machine intelligence include relationship between intelligent
behavior, intelligent and unintelligent machine characteristics, observable and
unobservable entities and classification of intelligence. The paper also
establishes algebraic definitions of efficiency and accuracy of MI tests as
their quality measure. The last part of the paper addresses the learning
process with respect to the traditional epistemology and the epistemology of MI
described here. The proposed views on MI positively correlate to the Hegelian
monistic epistemology and contribute towards amalgamating idealistic
deliberations with the AI theory, particularly in a local frame of reference."
Formal Definition of AI,"A definition of Artificial Intelligence was proposed in [1] but this
definition was not absolutely formal at least because the word ""Human"" was
used. In this paper we will formalize the definition from [1]. The biggest
problem in this definition was that the level of intelligence of AI is compared
to the intelligence of a human being. In order to change this we will introduce
some parameters to which AI will depend. One of this parameters will be the
level of intelligence and we will define one AI to each level of intelligence.
We assume that for some level of intelligence the respective AI will be more
intelligent than a human being. Nevertheless, we cannot say which is this level
because we cannot calculate its exact value."
FinBrain: When Finance Meets AI 2.0,"Artificial intelligence (AI) is the core technology of technological
revolution and industrial transformation. As one of the new intelligent needs
in the AI 2.0 era, financial intelligence has elicited much attention from the
academia and industry. In our current dynamic capital market, financial
intelligence demonstrates a fast and accurate machine learning capability to
handle complex data and has gradually acquired the potential to become a
""financial brain"". In this work, we survey existing studies on financial
intelligence. First, we describe the concept of financial intelligence and
elaborate on its position in the financial technology field. Second, we
introduce the development of financial intelligence and review state-of-the-art
techniques in wealth management, risk management, financial security, financial
consulting, and blockchain. Finally, we propose a research framework called
FinBrain and summarize four open issues, namely, explainable financial agents
and causality, perception and prediction under uncertainty, risk-sensitive and
robust decision making, and multi-agent game and mechanism design. We believe
that these research directions can lay the foundation for the development of AI
2.0 in the finance field."
An argument for the impossibility of machine intelligence,"Since the noun phrase `artificial intelligence' (AI) was coined, it has been
debated whether humans are able to create intelligence using technology. We
shed new light on this question from the point of view of themodynamics and
mathematics. First, we define what it is to be an agent (device) that could be
the bearer of AI. Then we show that the mainstream definitions of
`intelligence' proposed by Hutter and others and still accepted by the AI
community are too weak even to capture what is involved when we ascribe
intelligence to an insect. We then summarise the highly useful definition of
basic (arthropod) intelligence proposed by Rodney Brooks, and we identify the
properties that an AI agent would need to possess in order to be the bearer of
intelligence by this definition. Finally, we show that, from the perspective of
the disciplines needed to create such an agent, namely mathematics and physics,
these properties are realisable by neither implicit nor explicit mathematical
design nor by setting up an environment in which an AI could evolve
spontaneously."
AI-as-exploration: Navigating intelligence space,"Artificial Intelligence is a field that lives many lives, and the term has
come to encompass a motley collection of scientific and commercial endeavours.
In this paper, I articulate the contours of a rather neglected but central
scientific role that AI has to play, which I dub `AI-as-exploration'.The basic
thrust of AI-as-exploration is that of creating and studying systems that can
reveal candidate building blocks of intelligence that may differ from the forms
of human and animal intelligence we are familiar with. In other words, I
suggest that AI is one of the best tools we have for exploring intelligence
space, namely the space of possible intelligent systems. I illustrate the value
of AI-as-exploration by focusing on a specific case study, i.e., recent work on
the capacity to combine novel and invented concepts in humans and Large
Language Models. I show that the latter, despite showing human-level accuracy
in such a task, probably solve it in ways radically different, but no less
relevant to intelligence research, to those hypothesised for humans."
"Expansion of situations theory for exploring shared awareness in
  human-intelligent autonomous systems","Intelligent autonomous systems are part of a system of systems that interact
with other agents to accomplish tasks in complex environments. However,
intelligent autonomous systems integrated system of systems add additional
layers of complexity based on their limited cognitive processes, specifically
shared situation awareness that allows a team to respond to novel tasks.
Intelligent autonomous systems' lack of shared situation awareness adversely
influences team effectiveness in complex task environments, such as military
command-and-control. A complementary approach of shared situation awareness,
called situations theory, is beneficial for understanding the relationship
between system of systems shared situation awareness and effectiveness. The
current study elucidates a conceptual discussion on situations theory to
investigate the development of an system of systems shared situational
awareness when humans team with intelligent autonomous system agents. To ground
the discussion, the reviewed studies expanded situations theory within the
context of a system of systems that result in three major conjectures that can
be beneficial to the design and development of future systems of systems."
"Has Multimodal Learning Delivered Universal Intelligence in Healthcare?
  A Comprehensive Survey","The rapid development of artificial intelligence has constantly reshaped the
field of intelligent healthcare and medicine. As a vital technology, multimodal
learning has increasingly garnered interest due to data complementarity,
comprehensive modeling form, and great application potential. Currently,
numerous researchers are dedicating their attention to this field, conducting
extensive studies and constructing abundant intelligent systems. Naturally, an
open question arises that has multimodal learning delivered universal
intelligence in healthcare? To answer the question, we adopt three unique
viewpoints for a holistic analysis. Firstly, we conduct a comprehensive survey
of the current progress of medical multimodal learning from the perspectives of
datasets, task-oriented methods, and universal foundation models. Based on
them, we further discuss the proposed question from five issues to explore the
real impacts of advanced techniques in healthcare, from data and technologies
to performance and ethics. The answer is that current technologies have NOT
achieved universal intelligence and there remains a significant journey to
undertake. Finally, in light of the above reviews and discussions, we point out
ten potential directions for exploration towards the goal of universal
intelligence in healthcare."
Intelligence at the Edge of Chaos,"We explore the emergence of intelligent behavior in artificial systems by
investigating how the complexity of rule-based systems influences the
capabilities of models trained to predict these rules. Our study focuses on
elementary cellular automata (ECA), simple yet powerful one-dimensional systems
that generate behaviors ranging from trivial to highly complex. By training
distinct Large Language Models (LLMs) on different ECAs, we evaluated the
relationship between the complexity of the rules' behavior and the intelligence
exhibited by the LLMs, as reflected in their performance on downstream tasks.
Our findings reveal that rules with higher complexity lead to models exhibiting
greater intelligence, as demonstrated by their performance on reasoning and
chess move prediction tasks. Both uniform and periodic systems, and often also
highly chaotic systems, resulted in poorer downstream performance, highlighting
a sweet spot of complexity conducive to intelligence. We conjecture that
intelligence arises from the ability to predict complexity and that creating
intelligence may require only exposure to complexity."
"A Heuristic Search Algorithm Using the Stability of Learning Algorithms
  in Certain Scenarios as the Fitness Function: An Artificial General
  Intelligence Engineering Approach","This paper presents a non-manual design engineering method based on heuristic
search algorithm to search for candidate agents in the solution space which
formed by artificial intelligence agents modeled on the base of
bionics.Compared with the artificial design method represented by meta-learning
and the bionics method represented by the neural architecture chip,this method
is more feasible for realizing artificial general intelligence,and it has a
much better interaction with cognitive neuroscience;at the same time,the
engineering method is based on the theoretical hypothesis that the final
learning algorithm is stable in certain scenarios,and has generalization
ability in various scenarios.The paper discusses the theory preliminarily and
proposes the possible correlation between the theory and the fixed-point
theorem in the field of mathematics.Limited by the author's knowledge
level,this correlation is proposed only as a kind of conjecture."
"Multiparty Dynamics and Failure Modes for Machine Learning and
  Artificial Intelligence","An important challenge for safety in machine learning and artificial
intelligence systems is a~set of related failures involving specification
gaming, reward hacking, fragility to distributional shifts, and Goodhart's or
Campbell's law. This paper presents additional failure modes for interactions
within multi-agent systems that are closely related. These multi-agent failure
modes are more complex, more problematic, and less well understood than the
single-agent case, and are also already occurring, largely unnoticed. After
motivating the discussion with examples from poker-playing artificial
intelligence (AI), the paper explains why these failure modes are in some
senses unavoidable. Following this, the paper categorizes failure modes,
provides definitions, and cites examples for each of the modes: accidental
steering, coordination failures, adversarial misalignment, input spoofing and
filtering, and goal co-option or direct hacking. The paper then discusses how
extant literature on multi-agent AI fails to address these failure modes, and
identifies work which may be useful for the mitigation of these failure modes."
Research on AI Composition Recognition Based on Music Rules,"The development of artificial intelligent composition has resulted in the
increasing popularity of machine-generated pieces, with frequent copyright
disputes consequently emerging. There is an insufficient amount of research on
the judgement of artificial and machine-generated works; the creation of a
method to identify and distinguish these works is of particular importance.
Starting from the essence of the music, the article constructs a
music-rule-identifying algorithm through extracting modes, which will identify
the stability of the mode of machine-generated music, to judge whether it is
artificial intelligent. The evaluation datasets used are provided by the
Conference on Sound and Music Technology(CSMT). Experimental results
demonstrate the algorithm to have a successful distinguishing ability between
datasets with different source distributions. The algorithm will also provide
some technological reference to the benign development of the music copyright
and artificial intelligent music."
"Artificial Intellgence -- Application in Life Sciences and Beyond. The
  Upper Rhine Artificial Intelligence Symposium UR-AI 2021","The TriRhenaTech alliance presents the accepted papers of the 'Upper-Rhine
Artificial Intelligence Symposium' held on October 27th 2021 in Kaiserslautern,
Germany. Topics of the conference are applications of Artificial Intellgence in
life sciences, intelligent systems, industry 4.0, mobility and others. The
TriRhenaTech alliance is a network of universities in the Upper-Rhine
Trinational Metropolitan Region comprising of the German universities of
applied sciences in Furtwangen, Kaiserslautern, Karlsruhe, Offenburg and Trier,
the Baden-Wuerttemberg Cooperative State University Loerrach, the French
university network Alsace Tech (comprised of 14 'grandes \'ecoles' in the
fields of engineering, architecture and management) and the University of
Applied Sciences and Arts Northwestern Switzerland. The alliance's common goal
is to reinforce the transfer of knowledge, research, and technology, as well as
the cross-border mobility of students."
"From Psychological Curiosity to Artificial Curiosity: Curiosity-Driven
  Learning in Artificial Intelligence Tasks","Psychological curiosity plays a significant role in human intelligence to
enhance learning through exploration and information acquisition. In the
Artificial Intelligence (AI) community, artificial curiosity provides a natural
intrinsic motivation for efficient learning as inspired by human cognitive
development; meanwhile, it can bridge the existing gap between AI research and
practical application scenarios, such as overfitting, poor generalization,
limited training samples, high computational cost, etc. As a result,
curiosity-driven learning (CDL) has become increasingly popular, where agents
are self-motivated to learn novel knowledge. In this paper, we first present a
comprehensive review on the psychological study of curiosity and summarize a
unified framework for quantifying curiosity as well as its arousal mechanism.
Based on the psychological principle, we further survey the literature of
existing CDL methods in the fields of Reinforcement Learning, Recommendation,
and Classification, where both advantages and disadvantages as well as future
work are discussed. As a result, this work provides fruitful insights for
future CDL research and yield possible directions for further improvement."
Towards Explainable Meta-Learning for DDoS Detection,"The Internet is the most complex machine humankind has ever built, and how to
defense it from intrusions is even more complex. With the ever increasing of
new intrusions, intrusion detection task rely on Artificial Intelligence more
and more. Interpretability and transparency of the machine learning model is
the foundation of trust in AI-driven intrusion detection results. Current
interpretation Artificial Intelligence technologies in intrusion detection are
heuristic, which is neither accurate nor sufficient. This paper proposed a
rigorous interpretable Artificial Intelligence driven intrusion detection
approach, based on artificial immune system. Details of rigorous interpretation
calculation process for a decision tree model is presented. Prime implicant
explanation for benign traffic flow are given in detail as rule for negative
selection of the cyber immune system. Experiments are carried out in real-life
traffic."
"Artificial muses: Generative Artificial Intelligence Chatbots Have Risen
  to Human-Level Creativity","A widespread view is that Artificial Intelligence cannot be creative. We
tested this assumption by comparing human-generated ideas with those generated
by six Generative Artificial Intelligence (GAI) chatbots: $alpa.\!ai$,
$Copy.\!ai$, ChatGPT (versions 3 and 4), $Studio.\!ai$, and YouChat. Humans and
a specifically trained AI independently assessed the quality and quantity of
ideas. We found no qualitative difference between AI and human-generated
creativity, although there are differences in how ideas are generated.
Interestingly, 9.4 percent of humans were more creative than the most creative
GAI, GPT-4. Our findings suggest that GAIs are valuable assistants in the
creative process. Continued research and development of GAI in creative tasks
is crucial to fully understand this technology's potential benefits and
drawbacks in shaping the future of creativity. Finally, we discuss the question
of whether GAIs are capable of being truly creative."
"Applications of Artificial Intelligence Techniques to Combating Cyber
  Crimes: A Review","With the advances in information technology (IT) criminals are using
cyberspace to commit numerous cyber crimes. Cyber infrastructures are highly
vulnerable to intrusions and other threats. Physical devices and human
intervention are not sufficient for monitoring and protection of these
infrastructures; hence, there is a need for more sophisticated cyber defense
systems that need to be flexible, adaptable and robust, and able to detect a
wide variety of threats and make intelligent real-time decisions. Numerous
bio-inspired computing methods of Artificial Intelligence have been
increasingly playing an important role in cyber crime detection and prevention.
The purpose of this study is to present advances made so far in the field of
applying AI techniques for combating cyber crimes, to demonstrate how these
techniques can be an effective tool for detection and prevention of cyber
attacks, as well as to give the scope for future work."
"On the idea of a new artificial intelligence based optimization
  algorithm inspired from the nature of vortex","In this paper, the idea of a new artificial intelligence based optimization
algorithm, which is inspired from the nature of vortex, has been provided
briefly. As also a bio-inspired computation algorithm, the idea is generally
focused on a typical vortex flow / behavior in nature and inspires from some
dynamics that are occurred in the sense of vortex nature. Briefly, the
algorithm is also a swarm-oriented evolutional problem solution approach;
because it includes many methods related to elimination of weak swarm members
and trying to improve the solution process by supporting the solution space via
new swarm members. In order have better idea about success of the algorithm; it
has been tested via some benchmark functions. At this point, the obtained
results show that the algorithm can be an alternative to the literature in
terms of single-objective optimization solution ways. Vortex Optimization
Algorithm (VOA) is the name suggestion by the authors; for this new idea of
intelligent optimization approach."
"How linguistic descriptions of data can help to the teaching-learning
  process in higher education, case of study: artificial intelligence","Artificial Intelligence is a central topic in the computer science
curriculum. From the year 2011 a project-based learning methodology based on
computer games has been designed and implemented into the intelligence
artificial course at the University of the Bio-Bio. The project aims to develop
software-controlled agents (bots) which are programmed by using heuristic
algorithms seen during the course. This methodology allows us to obtain good
learning results, however several challenges have been founded during its
implementation.
  In this paper we show how linguistic descriptions of data can help to provide
students and teachers with technical and personalized feedback about the
learned algorithms. Algorithm behavior profile and a new Turing test for
computer games bots based on linguistic modelling of complex phenomena are also
proposed in order to deal with such challenges.
  In order to show and explore the possibilities of this new technology, a web
platform has been designed and implemented by one of authors and its
incorporation in the process of assessment allows us to improve the teaching
learning process."
Robotics Rights and Ethics Rules,"It is very important to adhere strictly to ethical and social influences when
delivering most of our life to artificial intelligence systems. With industry
4.0, the internet of things, data analysis and automation have begun to be of
great importance in our lives. With the Yapanese version of Industry 5.0, it
has come to our attention that machine-human interaction and human intelligence
are working in harmony with the cognitive computer. In this context, robots
working on artificial intelligence algorithms co-ordinated with the development
of technology have begun to enter our lives. But the consequences of the recent
complaints of the Robots have been that important issues have arisen about how
to be followed in terms of intellectual property and ethics. Although there are
no laws regulating robots in our country at present, laws on robot ethics and
rights abroad have entered into force. This means that it is important that we
organize the necessary arrangements in the way that robots and artificial
intelligence are so important in the new world order. In this study, it was
aimed to examine the existing rules of machine and robot ethics and to set an
example for the arrangements to be made in our country, and various discussions
were given in this context."
Solving Tree Problems with Category Theory,"Artificial Intelligence (AI) has long pursued models, theories, and
techniques to imbue machines with human-like general intelligence. Yet even the
currently predominant data-driven approaches in AI seem to be lacking humans'
unique ability to solve wide ranges of problems. This situation begs the
question of the existence of principles that underlie general problem-solving
capabilities. We approach this question through the mathematical formulation of
analogies across different problems and solutions. We focus in particular on
problems that could be represented as tree-like structures. Most importantly,
we adopt a category-theoretic approach in formalising tree problems as
categories, and in proving the existence of equivalences across apparently
unrelated problem domains. We prove the existence of a functor between the
category of tree problems and the category of solutions. We also provide a
weaker version of the functor by quantifying equivalences of problem categories
using a metric on tree problems."
Artificial Intelligence in Clinical Health Care Applications: Viewpoint,"The idea of Artificial Intelligence (AI) has a long history. It turned out,
however, that reaching intelligence at human levels is more complicated than
originally anticipated. Currently we are experiencing a renewed interest in AI,
fueled by an enormous increase in computing power and an even larger increase
in data, in combination with improved AI technologies like deep learning.
Healthcare is considered the next domain to be revolutionized by Artificial
Intelligence. While AI approaches are excellently suited to develop certain
algorithms, for biomedical applications there are specific challenges. We
propose recommendations to improve AI projects in the biomedical space and
especially clinical healthcare."
Questions to Guide the Future of Artificial Intelligence Research,"The field of machine learning has focused, primarily, on discretized
sub-problems (i.e. vision, speech, natural language) of intelligence. While
neuroscience tends to be observation heavy, providing few guiding theories. It
is unlikely that artificial intelligence will emerge through only one of these
disciplines. Instead, it is likely to be some amalgamation of their algorithmic
and observational findings. As a result, there are a number of problems that
should be addressed in order to select the beneficial aspects of both fields.
In this article, we propose leading questions to guide the future of artificial
intelligence research. There are clear computational principles on which the
brain operates. The problem is finding these computational needles in a
haystack of biological complexity. Biology has clear constraints but by not
using it as a guide we are constraining ourselves."
Modeling emotion for human-like behavior in future intelligent robots,"Over the past decades, research in cognitive and affective neuroscience has
emphasized that emotion is crucial for human intelligence and in fact
inseparable from cognition. Concurrently, there has been growing interest in
simulating and modeling emotion-related processes in robots and artificial
agents. In this opinion paper, our goal is to provide a snapshot of the present
landscape in emotion modeling and to show how neuroscience can help advance the
current state of the art. We start with an overview of the existing literature
on emotion modeling in three areas of research: affective computing, social
robotics, and neurorobotics. Briefly summarizing the current state of knowledge
on natural emotion, we then highlight how existing proposals in artificial
emotion do not make sufficient contact with neuroscientific evidence. We
conclude by providing a set of principles to help guide future research in
artificial emotion and intelligent machines more generally. Overall, we argue
that a stronger integration of emotion-related processes in robot models is
critical for the design of human-like behavior in future intelligent machines.
Such integration not only will contribute to the development of autonomous
social machines capable of tackling real-world problems but would contribute to
advancing understanding of human emotion."
Evolution of Artificial Intelligent Plane,"With the growth of the internet, it is becoming hard to manage, configure and
monitor networks. Recent trends to control and operate them is artificial
intelligence based automation to minimize human intervention. Albeit this
concept has been introduced since a decade with several different names, but
the underlying goal remains the same, which is to make network intelligent
enough to assemble, reassemble if configuration changes, and detect a problem
on its own and fix it. As a result, in addition to Data Plane, Control Plane
and Management Plane, a new plane called Artificial Intelligence (AI) Plane is
introduced. Our main objective is to analyze all major AI plane techniques,
frameworks and algorithms proposed in various types of networks. We propose a
comprehensive and network independent framework to cover all aspects of AI
plane, in particular we provide a systematically means of comparison. In
conjunction to make AI plane understand simpler, this framework highlights
relevant challenges and design considerations for future research. To the best
of our knowledge this is the first survey report which represents a complete
comparison of AI planes with their investigation issues in several types of
networks."
"Detecting Synthetic Phenomenology in a Contained Artificial General
  Intelligence","Human-like intelligence in a machine is a contentious subject. Whether
mankind should or should not pursue the creation of artificial general
intelligence is hotly debated. As well, researchers have aligned in opposing
factions according to whether mankind can create it. For our purposes, we
assume mankind can and will do so. Thus, it becomes necessary to contemplate
how to do so in a safe and trusted manner -- enter the idea of boxing or
containment. As part of such thinking, we wonder how a phenomenology might be
detected given the operational constraints imposed by any potential containment
system. Accordingly, this work provides an analysis of existing measures of
phenomenology through qualia and extends those ideas into the context of a
contained artificial general intelligence."
"Empowering Things with Intelligence: A Survey of the Progress,
  Challenges, and Opportunities in Artificial Intelligence of Things","In the Internet of Things (IoT) era, billions of sensors and devices collect
and process data from the environment, transmit them to cloud centers, and
receive feedback via the internet for connectivity and perception. However,
transmitting massive amounts of heterogeneous data, perceiving complex
environments from these data, and then making smart decisions in a timely
manner are difficult. Artificial intelligence (AI), especially deep learning,
is now a proven success in various areas including computer vision, speech
recognition, and natural language processing. AI introduced into the IoT
heralds the era of artificial intelligence of things (AIoT). This paper
presents a comprehensive survey on AIoT to show how AI can empower the IoT to
make it faster, smarter, greener, and safer. Specifically, we briefly present
the AIoT architecture in the context of cloud computing, fog computing, and
edge computing. Then, we present progress in AI research for IoT from four
perspectives: perceiving, learning, reasoning, and behaving. Next, we summarize
some promising applications of AIoT that are likely to profoundly reshape our
world. Finally, we highlight the challenges facing AIoT and some potential
research opportunities."
"Where is your place, Visual Place Recognition?","Visual Place Recognition (VPR) is often characterized as being able to
recognize the same place despite significant changes in appearance and
viewpoint. VPR is a key component of Spatial Artificial Intelligence, enabling
robotic platforms and intelligent augmentation platforms such as augmented
reality devices to perceive and understand the physical world. In this paper,
we observe that there are three ""drivers"" that impose requirements on spatially
intelligent agents and thus VPR systems: 1) the particular agent including its
sensors and computational resources, 2) the operating environment of this
agent, and 3) the specific task that the artificial agent carries out. In this
paper, we characterize and survey key works in the VPR area considering those
drivers, including their place representation and place matching choices. We
also provide a new definition of VPR based on the visual overlap -- akin to
spatial view cells in the brain -- that enables us to find similarities and
differences to other research areas in the robotics and computer vision fields.
We identify numerous open challenges and suggest areas that require more
in-depth attention in future works."
Federated Artificial Intelligence for Unified Credit Assessment,"With the rapid adoption of Internet technologies, digital footprints have
become ubiquitous and versatile to revolutionise the financial industry in
digital transformation. This paper takes initiatives to investigate a new
paradigm of the unified credit assessment with the use of federated artificial
intelligence. We conceptualised digital human representation which consists of
social, contextual, financial and technological dimensions to assess the
commercial creditworthiness and social reputation of both banked and unbanked
individuals. A federated artificial intelligence platform is proposed with a
comprehensive set of system design for efficient and effective credit scoring.
The study considerably contributes to the cumulative development of financial
intelligence and social computing. It also provides a number of implications
for academic bodies, practitioners, and developers of financial technologies."
"A curated, ontology-based, large-scale knowledge graph of artificial
  intelligence tasks and benchmarks","Research in artificial intelligence (AI) is addressing a growing number of
tasks through a rapidly growing number of models and methodologies. This makes
it difficult to keep track of where novel AI methods are successfully -- or
still unsuccessfully -- applied, how progress is measured, how different
advances might synergize with each other, and how future research should be
prioritized.
  To help address these issues, we created the Intelligence Task Ontology and
Knowledge Graph (ITO), a comprehensive, richly structured and manually curated
resource on artificial intelligence tasks, benchmark results and performance
metrics. The current version of ITO contain 685,560 edges, 1,100 classes
representing AI processes and 1,995 properties representing performance
metrics.
  The goal of ITO is to enable precise and network-based analyses of the global
landscape of AI tasks and capabilities. ITO is based on technologies that allow
for easy integration and enrichment with external data, automated inference and
continuous, collaborative expert curation of underlying ontological models. We
make the ITO dataset and a collection of Jupyter notebooks utilising ITO openly
available."
"Towards Explainable Artificial Intelligence in Banking and Financial
  Services","Artificial intelligence (AI) enables machines to learn from human experience,
adjust to new inputs, and perform human-like tasks. AI is progressing rapidly
and is transforming the way businesses operate, from process automation to
cognitive augmentation of tasks and intelligent process/data analytics.
However, the main challenge for human users would be to understand and
appropriately trust the result of AI algorithms and methods. In this paper, to
address this challenge, we study and analyze the recent work done in
Explainable Artificial Intelligence (XAI) methods and tools. We introduce a
novel XAI process, which facilitates producing explainable models while
maintaining a high level of learning performance. We present an interactive
evidence-based approach to assist human users in comprehending and trusting the
results and output created by AI-enabled algorithms. We adopt a typical
scenario in the Banking domain for analyzing customer transactions. We develop
a digital dashboard to facilitate interacting with the algorithm results and
discuss how the proposed XAI method can significantly improve the confidence of
data scientists in understanding the result of AI-enabled algorithms."
Cultural Incongruencies in Artificial Intelligence,"Artificial intelligence (AI) systems attempt to imitate human behavior. How
well they do this imitation is often used to assess their utility and to
attribute human-like (or artificial) intelligence to them. However, most work
on AI refers to and relies on human intelligence without accounting for the
fact that human behavior is inherently shaped by the cultural contexts they are
embedded in, the values and beliefs they hold, and the social practices they
follow. Additionally, since AI technologies are mostly conceived and developed
in just a handful of countries, they embed the cultural values and practices of
these countries. Similarly, the data that is used to train the models also
fails to equitably represent global cultural diversity. Problems therefore
arise when these technologies interact with globally diverse societies and
cultures, with different values and interpretive practices. In this position
paper, we describe a set of cultural dependencies and incongruencies in the
context of AI-based language and vision technologies, and reflect on the
possibilities of and potential strategies towards addressing these
incongruencies."
"Helpful, Misleading or Confusing: How Humans Perceive Fundamental
  Building Blocks of Artificial Intelligence Explanations","Explainable artificial intelligence techniques are developed at breakneck
speed, but suitable evaluation approaches lag behind. With explainers becoming
increasingly complex and a lack of consensus on how to assess their utility, it
is challenging to judge the benefit and effectiveness of different
explanations. To address this gap, we take a step back from sophisticated
predictive algorithms and instead look into explainability of simple
decision-making models. In this setting, we aim to assess how people perceive
comprehensibility of their different representations such as mathematical
formulation, graphical representation and textual summarisation (of varying
complexity and scope). This allows us to capture how diverse stakeholders --
engineers, researchers, consumers, regulators and the like -- judge
intelligibility of fundamental concepts that more elaborate artificial
intelligence explanations are built from. This position paper charts our
approach to establishing appropriate evaluation methodology as well as a
conceptual and practical framework to facilitate setting up and executing
relevant user studies."
"On the Computation of Meaning, Language Models and Incomprehensible
  Horrors","We integrate foundational theories of meaning with a mathematical formalism
of artificial general intelligence (AGI) to offer a comprehensive mechanistic
explanation of meaning, communication, and symbol emergence. This synthesis
holds significance for both AGI and broader debates concerning the nature of
language, as it unifies pragmatics, logical truth conditional semantics,
Peircean semiotics, and a computable model of enactive cognition, addressing
phenomena that have traditionally evaded mechanistic explanation. By examining
the conditions under which a machine can generate meaningful utterances or
comprehend human meaning, we establish that the current generation of language
models do not possess the same understanding of meaning as humans nor intend
any meaning that we might attribute to their responses. To address this, we
propose simulating human feelings and optimising models to construct weak
representations. Our findings shed light on the relationship between meaning
and intelligence, and how we can build machines that comprehend and intend
meaning."
"Enhancing Human Capabilities through Symbiotic Artificial Intelligence
  with Shared Sensory Experiences","The merging of human intelligence and artificial intelligence has long been a
subject of interest in both science fiction and academia. In this paper, we
introduce a novel concept in Human-AI interaction called Symbiotic Artificial
Intelligence with Shared Sensory Experiences (SAISSE), which aims to establish
a mutually beneficial relationship between AI systems and human users through
shared sensory experiences. By integrating multiple sensory input channels and
processing human experiences, SAISSE fosters a strong human-AI bond, enabling
AI systems to learn from and adapt to individual users, providing personalized
support, assistance, and enhancement. Furthermore, we discuss the incorporation
of memory storage units for long-term growth and development of both the AI
system and its human user. As we address user privacy and ethical guidelines
for responsible AI-human symbiosis, we also explore potential biases and
inequalities in AI-human symbiosis and propose strategies to mitigate these
challenges. Our research aims to provide a comprehensive understanding of the
SAISSE concept and its potential to effectively support and enhance individual
human users through symbiotic AI systems. This position article aims at
discussing poteintial AI-human interaction related topics within the scientific
community, rather than providing experimental or theoretical results."
"Ethical Artificial Intelligence Principles and Guidelines for the
  Governance and Utilization of Highly Advanced Large Language Models","Given the success of ChatGPT, LaMDA and other large language models (LLMs),
there has been an increase in development and usage of LLMs within the
technology sector and other sectors. While the level in which LLMs has not
reached a level where it has surpassed human intelligence, there will be a time
when it will. Such LLMs can be referred to as advanced LLMs. Currently, there
are limited usage of ethical artificial intelligence (AI) principles and
guidelines addressing advanced LLMs due to the fact that we have not reached
that point yet. However, this is a problem as once we do reach that point, we
will not be adequately prepared to deal with the aftermath of it in an ethical
and optimal way, which will lead to undesired and unexpected consequences. This
paper addresses this issue by discussing what ethical AI principles and
guidelines can be used to address highly advanced LLMs."
"OpenDataLab: Empowering General Artificial Intelligence with Open
  Datasets","The advancement of artificial intelligence (AI) hinges on the quality and
accessibility of data, yet the current fragmentation and variability of data
sources hinder efficient data utilization. The dispersion of data sources and
diversity of data formats often lead to inefficiencies in data retrieval and
processing, significantly impeding the progress of AI research and
applications. To address these challenges, this paper introduces OpenDataLab, a
platform designed to bridge the gap between diverse data sources and the need
for unified data processing. OpenDataLab integrates a wide range of open-source
AI datasets and enhances data acquisition efficiency through intelligent
querying and high-speed downloading services. The platform employs a
next-generation AI Data Set Description Language (DSDL), which standardizes the
representation of multimodal and multi-format data, improving interoperability
and reusability. Additionally, OpenDataLab optimizes data processing through
tools that complement DSDL. By integrating data with unified data descriptions
and smart data toolchains, OpenDataLab can improve data preparation efficiency
by 30\%. We anticipate that OpenDataLab will significantly boost artificial
general intelligence (AGI) research and facilitate advancements in related AI
fields. For more detailed information, please visit the platform's official
website: https://opendatalab.com."
The Technology of Outrage: Bias in Artificial Intelligence,"Artificial intelligence and machine learning are increasingly used to offload
decision making from people. In the past, one of the rationales for this
replacement was that machines, unlike people, can be fair and unbiased.
Evidence suggests otherwise. We begin by entertaining the ideas that algorithms
can replace people and that algorithms cannot be biased. Taken as axioms, these
statements quickly lead to absurdity. Spurred on by this result, we investigate
the slogans more closely and identify equivocation surrounding the word 'bias.'
We diagnose three forms of outrage-intellectual, moral, and political-that are
at play when people react emotionally to algorithmic bias. Then we suggest
three practical approaches to addressing bias that the AI community could take,
which include clarifying the language around bias, developing new auditing
methods for intelligent systems, and building certain capabilities into these
systems. We conclude by offering a moral regarding the conversations about
algorithmic bias that may transfer to other areas of artificial intelligence."
Catastrophic Importance of Catastrophic Forgetting,"This paper describes some of the possibilities of artificial neural networks
that open up after solving the problem of catastrophic forgetting. A simple
model and reinforcement learning applications of existing methods are also
proposed."
Stream Computing,"Stream computing is the use of multiple autonomic and parallel modules
together with integrative processors at a higher level of abstraction to embody
""intelligent"" processing. The biological basis of this computing is sketched
and the matter of learning is examined."
An Analysis of General Fuzzy Logic and Fuzzy Reasoning Method,"In this article, we describe the fuzzy logic, fuzzy language and algorithms
as the basis of fuzzy reasoning, one of the intelligent information processing
method, and then describe the general fuzzy reasoning method."
Human-Level Intelligence or Animal-Like Abilities?,"The vision systems of the eagle and the snake outperform everything that we
can make in the laboratory, but snakes and eagles cannot build an eyeglass or a
telescope or a microscope. (Judea Pearl)"
"On the link between conscious function and general intelligence in
  humans and machines","In popular media, there is often a connection drawn between the advent of
awareness in artificial agents and those same agents simultaneously achieving
human or superhuman level intelligence. In this work, we explore the validity
and potential application of this seemingly intuitive link between
consciousness and intelligence. We do so by examining the cognitive abilities
associated with three contemporary theories of conscious function: Global
Workspace Theory (GWT), Information Generation Theory (IGT), and Attention
Schema Theory (AST). We find that all three theories specifically relate
conscious function to some aspect of domain-general intelligence in humans.
With this insight, we turn to the field of Artificial Intelligence (AI) and
find that, while still far from demonstrating general intelligence, many
state-of-the-art deep learning methods have begun to incorporate key aspects of
each of the three functional theories. Having identified this trend, we use the
motivating example of mental time travel in humans to propose ways in which
insights from each of the three theories may be combined into a single unified
and implementable model. Given that it is made possible by cognitive abilities
underlying each of the three functional theories, artificial agents capable of
mental time travel would not only possess greater general intelligence than
current approaches, but also be more consistent with our current understanding
of the functional role of consciousness in humans, thus making it a promising
near-term goal for AI research."
"Toward An Optimal Selection of Dialogue Strategies: A Target-Driven
  Approach for Intelligent Outbound Robots","With the growth of the economy and society, enterprises, especially in the
FinTech industry, have increasing demands of outbound calls for customers such
as debt collection, marketing, anti-fraud calls, and so on. But a large amount
of repetitive and mechanical work occupies most of the time of human agents, so
the cost of equipment and labor for enterprises is increasing accordingly. At
the same time, with the development of artificial intelligence technology in
the past few decades, it has become quite common for companies to use new
technologies such as Big Data and artificial intelligence to empower outbound
call businesses. The intelligent outbound robot is a typical application of the
artificial intelligence technology in the field of outbound call businesses. It
is mainly used to communicate with customers in order to accomplish a certain
target. It has the characteristics of low cost, high reuse, and easy
compliance, which has attracted more attention from the industry.
  At present, there are two kinds of intelligent outbound robots in the
industry but both of them still leave large room for improvement. One kind of
them is based on a finite state machine relying on the configuration of jump
conditions and corresponding nodes based on manual experience. This kind of
intelligent outbound robot is also called a flow-based robot. For example, the
schematic diagram of the working model of a flow-based robot for debt
collection is shown in Fig.\ref{fig:label}. In each round, the robot will reply
to the user with the words corresponding to each node."
"On the Principles of Parsimony and Self-Consistency for the Emergence of
  Intelligence","Ten years into the revival of deep networks and artificial intelligence, we
propose a theoretical framework that sheds light on understanding deep networks
within a bigger picture of Intelligence in general. We introduce two
fundamental principles, Parsimony and Self-consistency, that address two
fundamental questions regarding Intelligence: what to learn and how to learn,
respectively. We believe the two principles are the cornerstones for the
emergence of Intelligence, artificial or natural. While these two principles
have rich classical roots, we argue that they can be stated anew in entirely
measurable and computable ways. More specifically, the two principles lead to
an effective and efficient computational framework, compressive closed-loop
transcription, that unifies and explains the evolution of modern deep networks
and many artificial intelligence practices. While we mainly use modeling of
visual data as an example, we believe the two principles will unify
understanding of broad families of autonomous intelligent systems and provide a
framework for understanding the brain."
Are Biological Systems More Intelligent Than Artificial Intelligence?,"Are biological self-organising systems more `intelligent' than artificial
intelligence? If so, why? We frame intelligence as adaptability, and explore
this question using a mathematical formalism of causal learning. We compare
systems by how they delegate control, illustrating how this applies with
examples of computational, biological, human organisational and economic
systems. We formally show the scale-free, dynamic, bottom-up architecture of
biological self-organisation allows for more efficient adaptation than the
static top-down architecture typical of computers, because adaptation can take
place at lower levels of abstraction. Artificial intelligence rests on a
static, human-engineered `stack'. It only adapts at high levels of abstraction.
To put it provocatively, a static computational stack is like an inflexible
bureaucracy. Biology is more `intelligent' because it delegates adaptation down
the stack. We call this multilayer-causal-learning. It inherits a flaw of
biological systems. Cells become cancerous when isolated from the collective
informational structure, reverting to primitive transcriptional behaviour. We
show states analogous to cancer occur when collectives are too tightly
constrained. To adapt to adverse conditions control should be delegated to the
greatest extent, like the doctrine of mission-command. Our result shows how to
design more robust systems and lays a mathematical foundation for future
empirical research."
"Unlocking the Wisdom of Large Language Models: An Introduction to The
  Path to Artificial General Intelligence","This booklet, Unlocking the Wisdom of Multi-LLM Collaborative Intelligence,
serves as an accessible introduction to the full volume The Path to Artificial
General Intelligence. Through fourteen aphorisms, it distills the core
principles of Multi-LLM Agent Collaborative Intelligence (MACI), a framework
designed to coordinate multiple LLMs toward reasoning, planning, and
decision-making that surpasses the capabilities of any single model. The
booklet includes titles, abstracts, and introductions from each main chapter,
along with the full content of the first two. The newly released third edition
features significant enhancements to Chapters 6 through 9 and a revised preface
responding to Yann LeCun's critique of AGI feasibility. While LeCun argues that
LLMs lack grounding, memory, and planning, we propose that MACI's collaborative
architecture, featuring multimodal agents in executive, legislative, and
judicial roles, directly addresses these limitations. Chapters on SocraSynth,
EVINCE, consciousness modeling, and behavior regulation demonstrate that
reasoning systems grounded in structured interaction and checks and balances
can produce more reliable, interpretable, and adaptive intelligence. By
integrating complementary model strengths, including world modeling and
multimodal perception, MACI enables a system-level intelligence that exceeds
the sum of its parts. Like human institutions, progress in AI may depend less
on isolated performance and more on coordinated judgment. Collaborative LLMs,
not just larger ones, may chart the path toward artificial general
intelligence."
"A Theory of Universal Artificial Intelligence based on Algorithmic
  Complexity","Decision theory formally solves the problem of rational agents in uncertain
worlds if the true environmental prior probability distribution is known.
Solomonoff's theory of universal induction formally solves the problem of
sequence prediction for unknown prior distribution. We combine both ideas and
get a parameterless theory of universal Artificial Intelligence. We give strong
arguments that the resulting AIXI model is the most intelligent unbiased agent
possible. We outline for a number of problem classes, including sequence
prediction, strategic games, function minimization, reinforcement and
supervised learning, how the AIXI model can formally solve them. The major
drawback of the AIXI model is that it is uncomputable. To overcome this
problem, we construct a modified algorithm AIXI-tl, which is still effectively
more intelligent than any other time t and space l bounded agent. The
computation time of AIXI-tl is of the order tx2^l. Other discussed topics are
formal definitions of intelligence order relations, the horizon problem and
relations of the AIXI theory to other AI approaches."
Towards an Intelligent Tutor for Mathematical Proofs,"Computer-supported learning is an increasingly important form of study since
it allows for independent learning and individualized instruction. In this
paper, we discuss a novel approach to developing an intelligent tutoring system
for teaching textbook-style mathematical proofs. We characterize the
particularities of the domain and discuss common ITS design models. Our
approach is motivated by phenomena found in a corpus of tutorial dialogs that
were collected in a Wizard-of-Oz experiment. We show how an intelligent tutor
for textbook-style mathematical proofs can be built on top of an adapted
assertion-level proof assistant by reusing representations and proof search
strategies originally developed for automated and interactive theorem proving.
The resulting prototype was successfully evaluated on a corpus of tutorial
dialogs and yields good results."
Subjective Reality and Strong Artificial Intelligence,"The main prospective aim of modern research related to Artificial
Intelligence is the creation of technical systems that implement the idea of
Strong Intelligence. According our point of view the path to the development of
such systems comes through the research in the field related to perceptions.
Here we formulate the model of the perception of external world which may be
used for the description of perceptual activity of intelligent beings. We
consider a number of issues related to the development of the set of patterns
which will be used by the intelligent system when interacting with environment.
The key idea of the presented perception model is the idea of subjective
reality. The principle of the relativity of perceived world is formulated. It
is shown that this principle is the immediate consequence of the idea of
subjective reality. In this paper we show how the methodology of subjective
reality may be used for the creation of different types of Strong AI systems."
Abstraction Learning,"There has been a gap between artificial intelligence and human intelligence.
In this paper, we identify three key elements forming human intelligence, and
suggest that abstraction learning combines these elements and is thus a way to
bridge the gap. Prior researches in artificial intelligence either specify
abstraction by human experts, or take abstraction as a qualitative explanation
for the model. This paper aims to learn abstraction directly. We tackle three
main challenges: representation, objective function, and learning algorithm.
Specifically, we propose a partition structure that contains pre-allocated
abstraction neurons; we formulate abstraction learning as a constrained
optimization problem, which integrates abstraction properties; we develop a
network evolution algorithm to solve this problem. This complete framework is
named ONE (Optimization via Network Evolution). In our experiments on MNIST,
ONE shows elementary human-like intelligence, including low energy consumption,
knowledge sharing, and lifelong learning."
"Advancing from Predictive Maintenance to Intelligent Maintenance with AI
  and IIoT","As Artificial Intelligent (AI) technology advances and increasingly large
amounts of data become readily available via various Industrial Internet of
Things (IIoT) projects, we evaluate the state of the art of predictive
maintenance approaches and propose our innovative framework to improve the
current practice. The paper first reviews the evolution of reliability
modelling technology in the past 90 years and discusses major technologies
developed in industry and academia. We then introduce the next generation
maintenance framework - Intelligent Maintenance, and discuss its key
components. This AI and IIoT based Intelligent Maintenance framework is
composed of (1) latest machine learning algorithms including probabilistic
reliability modelling with deep learning, (2) real-time data collection,
transfer, and storage through wireless smart sensors, (3) Big Data
technologies, (4) continuously integration and deployment of machine learning
models, (5) mobile device and AR/VR applications for fast and better
decision-making in the field. Particularly, we proposed a novel probabilistic
deep learning reliability modelling approach and demonstrate it in the Turbofan
Engine Degradation Dataset."
AI in (and for) Games,"This chapter outlines the relation between artificial intelligence (AI) /
machine learning (ML) algorithms and digital games. This relation is two-fold:
on one hand, AI/ML researchers can generate large, in-the-wild datasets of
human affective activity, player behaviour (i.e. actions within the game
world), commercial behaviour, interaction with graphical user interface
elements or messaging with other players, while games can utilise intelligent
algorithms to automate testing of game levels, generate content, develop
intelligent and responsive non-player characters (NPCs) or predict and respond
player behaviour across a wide variety of player cultures. In this work, we
discuss some of the most common and widely accepted uses of AI/ML in games and
how intelligent systems can benefit from those, elaborating on estimating
player experience based on expressivity and performance, and on generating
proper and interesting content for a language learning game."
Cultivated Wildness: Technodiversity and Wildness in Machines,"This paper investigates the idea of cultivated wildness at the intersection
of landscape design and artificial intelligence. The paper posits that
contemporary landscape practices should overcome the potentially single
understanding on wilderness, and instead explore landscape strategies to
cultivate new forms of wild places via ideas and concerns in contemporary
Environmental Humanities, Science and Technology Studies, Ecological Sciences,
and Landscape Architecture. Drawing cases in environmental engineering,
computer science, and landscape architecture research, this paper explores a
framework to construct wild places with intelligent machines. In this
framework, machines are not understood as a layer of ""digital infrastructure""
that is used to extend localized human intelligence and agency. Rather machines
are conceptualized as active agents who can participate in the intelligence of
co-production. Recent developments in cybernetic technologies such as sensing
networks, artificial intelligence, and cyberphysical systems can also
contribute to establishing the framework. At the heart of this framework is
""technodiversity,"" in parallel with biodiversity, since a singular vision on
technological development driven by optimization and efficiency reinforces a
monocultural approach that eliminates other possible relationships to construct
with the environment. Thus, cultivated wildness is also about recognizing
""wildness"" in machines."
A Review on Objective-Driven Artificial Intelligence,"While advancing rapidly, Artificial Intelligence still falls short of human
intelligence in several key aspects due to inherent limitations in current AI
technologies and our understanding of cognition. Humans have an innate ability
to understand context, nuances, and subtle cues in communication, which allows
us to comprehend jokes, sarcasm, and metaphors. Machines struggle to interpret
such contextual information accurately. Humans possess a vast repository of
common-sense knowledge that helps us make logical inferences and predictions
about the world. Machines lack this innate understanding and often struggle
with making sense of situations that humans find trivial. In this article, we
review the prospective Machine Intelligence candidates, a review from Prof.
Yann LeCun, and other work that can help close this gap between human and
machine intelligence. Specifically, we talk about what's lacking with the
current AI techniques such as supervised learning, reinforcement learning,
self-supervised learning, etc. Then we show how Hierarchical planning-based
approaches can help us close that gap and deep-dive into energy-based,
latent-variable methods and Joint embedding predictive architecture methods."
Grounding for Artificial Intelligence,"A core function of intelligence is grounding, which is the process of
connecting the natural language and abstract knowledge to the internal
representation of the real world in an intelligent being, e.g., a human. Human
cognition is grounded in our sensorimotor experiences in the external world and
subjective feelings in our internal world. We use languages to communicate with
each other and the languages are grounded on our shared sensorimotor
experiences and feelings. Without this shard grounding, it is impossible for us
to understand each other because all natural languages are highly abstract and
are only able to describe a tiny portion of what has happened or is happening
in the real world. Although grounding at high or abstract levels has been
studied in different fields and applications, to our knowledge, limited
systematic work at fine-grained levels has been done. With the rapid progress
of large language models (LLMs), it is imperative that we have a sound
understanding of grounding in order to move to the next level of intelligence.
It is also believed that grounding is necessary for Artificial General
Intelligence (AGI). This paper makes an attempt to systematically study this
problem."
"The Use of Artificial Intelligence in Military Intelligence: An
  Experimental Investigation of Added Value in the Analysis Process","It is beyond dispute that the potential benefits of artificial intelligence
(AI) in military intelligence are considerable. Nevertheless, it remains
uncertain precisely how AI can enhance the analysis of military data. The aim
of this study is to address this issue. To this end, the AI demonstrator
deepCOM was developed in collaboration with the start-up Aleph Alpha.
  The AI functions include text search, automatic text summarization and Named
Entity Recognition (NER). These are evaluated for their added value in military
analysis. It is demonstrated that under time pressure, the utilization of AI
functions results in assessments clearly superior to that of the control group.
Nevertheless, despite the demonstrably superior analysis outcome in the
experimental group, no increase in confidence in the accuracy of their own
analyses was observed. Finally, the paper identifies the limitations of
employing AI in military intelligence, particularly in the context of analyzing
ambiguous and contradictory information."
Language Games as the Pathway to Artificial Superhuman Intelligence,"The evolution of large language models (LLMs) toward artificial superhuman
intelligence (ASI) hinges on data reproduction, a cyclical process in which
models generate, curate and retrain on novel data to refine capabilities.
Current methods, however, risk getting stuck in a data reproduction trap:
optimizing outputs within fixed human-generated distributions in a closed loop
leads to stagnation, as models merely recombine existing knowledge rather than
explore new frontiers. In this paper, we propose language games as a pathway to
expanded data reproduction, breaking this cycle through three mechanisms: (1)
\textit{role fluidity}, which enhances data diversity and coverage by enabling
multi-agent systems to dynamically shift roles across tasks; (2) \textit{reward
variety}, embedding multiple feedback criteria that can drive complex
intelligent behaviors; and (3) \textit{rule plasticity}, iteratively evolving
interaction constraints to foster learnability, thereby injecting continual
novelty. By scaling language games into global sociotechnical ecosystems,
human-AI co-evolution generates unbounded data streams that drive open-ended
exploration. This framework redefines data reproduction not as a closed loop
but as an engine for superhuman intelligence."
Can Intelligence Explode?,"The technological singularity refers to a hypothetical scenario in which
technological advances virtually explode. The most popular scenario is the
creation of super-intelligent algorithms that recursively create ever higher
intelligences. It took many decades for these ideas to spread from science
fiction to popular science magazines and finally to attract the attention of
serious philosophers. David Chalmers' (JCS 2010) article is the first
comprehensive philosophical analysis of the singularity in a respected
philosophy journal. The motivation of my article is to augment Chalmers' and to
discuss some issues not addressed by him, in particular what it could mean for
intelligence to explode. In this course, I will (have to) provide a more
careful treatment of what intelligence actually is, separate speed from
intelligence explosion, compare what super-intelligent participants and
classical human observers might experience and do, discuss immediate
implications for the diversity and value of life, consider possible bounds on
intelligence, and contemplate intelligences right at the singularity."
"Evaluation Mechanism of Collective Intelligence for Heterogeneous Agents
  Group","Collective intelligence is manifested when multiple agents coherently work in
observation, interaction, decision-making and action. In this paper, we define
and quantify the intelligence level of heterogeneous agents group with the
improved Anytime Universal Intelligence Test(AUIT), based on an extension of
the existing evaluation of homogeneous agents group. The relationship of
intelligence level with agents composition, group size, spatial complexity and
testing time is analyzed. The intelligence level of heterogeneous agents groups
is compared with the homogeneous ones to analyze the effects of heterogeneity
on collective intelligence. Our work will help to understand the essence of
collective intelligence more deeply and reveal the effect of various key
factors on group intelligence level."
Intelligence as Computation,"This paper proposes a specific conceptualization of intelligence as
computation. This conceptualization is intended to provide a unified view for
all disciplines of intelligence research. Already, it unifies several
conceptualizations currently under investigation, including physical, neural,
embodied, morphological, and mechanical intelligences. To achieve this, the
proposed conceptualization explains the differences among existing views by
different computational paradigms, such as digital, analog, mechanical, or
morphological computation. Viewing intelligence as a composition of
computations from different paradigms, the challenges posed by previous
conceptualizations are resolved. Intelligence is hypothesized as a
multi-paradigmatic computation relying on specific computational principles.
These principles distinguish intelligence from other, non-intelligent
computations. The proposed conceptualization implies a multi-disciplinary
research agenda that is intended to lead to unified science of intelligence."
"Cyber Spectrum Intelligence: Security Applications, Challenges and Road
  Ahead","Cyber Spectrum Intelligence (SpecInt) is emerging as a concept that extends
beyond basic {\em spectrum sensing} and {\em signal intelligence} to encompass
a broader set of capabilities and technologies aimed at monitoring the use of
the radio spectrum and extracting information. SpecInt merges traditional
spectrum sensing techniques with Artificial Intelligence (AI) and parallel
processing to enhance the ability to extract and correlate simultaneous events
occurring on various frequencies, allowing for a new wave of intelligence
applications.
  This paper provides an overview of the emerging SpecInt research area,
characterizing the system architecture and the most relevant applications for
cyber-physical security. We identify five subcategories of spectrum
intelligence for cyber-physical security, encompassing Device Intelligence,
Channel Intelligence, Location Intelligence, Communication Intelligence, and
Ambient Intelligence. We also provide preliminary results based on an
experimental testbed showing the viability, feasibility, and potential of this
emerging application area. Finally, we point out current research challenges
and future directions paving the way for further research in this domain."
"A Survey of Large Language Model-Powered Spatial Intelligence Across
  Scales: Advances in Embodied Agents, Smart Cities, and Earth Science","Over the past year, the development of large language models (LLMs) has
brought spatial intelligence into focus, with much attention on vision-based
embodied intelligence. However, spatial intelligence spans a broader range of
disciplines and scales, from navigation and urban planning to remote sensing
and earth science. What are the differences and connections between spatial
intelligence across these fields? In this paper, we first review human spatial
cognition and its implications for spatial intelligence in LLMs. We then
examine spatial memory, knowledge representations, and abstract reasoning in
LLMs, highlighting their roles and connections. Finally, we analyze spatial
intelligence across scales -- from embodied to urban and global levels --
following a framework that progresses from spatial memory and understanding to
spatial reasoning and intelligence. Through this survey, we aim to provide
insights into interdisciplinary spatial intelligence research and inspire
future studies."
"Towards NeuroAI: Introducing Neuronal Diversity into Artificial Neural
  Networks","Throughout history, the development of artificial intelligence, particularly
artificial neural networks, has been open to and constantly inspired by the
increasingly deepened understanding of the brain, such as the inspiration of
neocognitron, which is the pioneering work of convolutional neural networks.
Per the motives of the emerging field: NeuroAI, a great amount of neuroscience
knowledge can help catalyze the next generation of AI by endowing a network
with more powerful capabilities. As we know, the human brain has numerous
morphologically and functionally different neurons, while artificial neural
networks are almost exclusively built on a single neuron type. In the human
brain, neuronal diversity is an enabling factor for all kinds of biological
intelligent behaviors. Since an artificial network is a miniature of the human
brain, introducing neuronal diversity should be valuable in terms of addressing
those essential problems of artificial networks such as efficiency,
interpretability, and memory. In this Primer, we first discuss the
preliminaries of biological neuronal diversity and the characteristics of
information transmission and processing in a biological neuron. Then, we review
studies of designing new neurons for artificial networks. Next, we discuss what
gains can neuronal diversity bring into artificial networks and exemplary
applications in several important fields. Lastly, we discuss the challenges and
future directions of neuronal diversity to explore the potential of NeuroAI."
"An Unified Intelligence-Communication Model for Multi-Agent System
  Part-I: Overview","Motivated by Shannon's model and recent rehabilitation of self-supervised
artificial intelligence having a ""World Model"", this paper propose an unified
intelligence-communication (UIC) model for describing a single agent and any
multi-agent system.
  Firstly, the environment is modelled as the generic communication channel
between agents. Secondly, the UIC model adopts a learning-agent model for
unifying several well-adopted agent architecture, e.g. rule-based agent model
in complex adaptive systems, layered model for describing human-level
intelligence, world-model based agent model. The model may also provide an
unified approach to investigate a multi-agent system (MAS) having multiple
action-perception modalities, e.g. explicitly information transfer and implicit
information transfer.
  This treatise would be divided into three parts, and this first part provides
an overview of the UIC model without introducing cumbersome mathematical
analysis and optimizations. In the second part of this treatise, case studies
with quantitative analysis driven by the UIC model would be provided,
exemplifying the adoption of the UIC model in multi-agent system. Specifically,
two representative cases would be studied, namely the analysis of a natural
multi-agent system, as well as the co-design of communication, perception and
action in an artificial multi-agent system. In the third part of this treatise,
the paper provides further insights and future research directions motivated by
the UIC model, such as unification of single intelligence and collective
intelligence, a possible explanation of intelligence emergence and a dual model
for agent-environment intelligence hypothesis.
  Notes: This paper is a Previewed Version, the extended full-version would be
released after being accepted."
"Large Scale Foundation Models for Intelligent Manufacturing
  Applications: A Survey","Although the applications of artificial intelligence especially deep learning
had greatly improved various aspects of intelligent manufacturing, they still
face challenges for wide employment due to the poor generalization ability,
difficulties to establish high-quality training datasets, and unsatisfactory
performance of deep learning methods. The emergence of large scale foundational
models(LSFMs) had triggered a wave in the field of artificial intelligence,
shifting deep learning models from single-task, single-modal, limited data
patterns to a paradigm encompassing diverse tasks, multimodal, and pre-training
on massive datasets. Although LSFMs had demonstrated powerful generalization
capabilities, automatic high-quality training dataset generation and superior
performance across various domains, applications of LSFMs on intelligent
manufacturing were still in their nascent stage. A systematic overview of this
topic was lacking, especially regarding which challenges of deep learning can
be addressed by LSFMs and how these challenges can be systematically tackled.
To fill this gap, this paper systematically expounded current statue of LSFMs
and their advantages in the context of intelligent manufacturing. and compared
comprehensively with the challenges faced by current deep learning models in
various intelligent manufacturing applications. We also outlined the roadmaps
for utilizing LSFMs to address these challenges. Finally, case studies of
applications of LSFMs in real-world intelligent manufacturing scenarios were
presented to illustrate how LSFMs could help industries, improve their
efficiency."
The Nature of Intelligence,"The human brain is the substrate for human intelligence. By simulating the
human brain, artificial intelligence builds computational models that have
learning capabilities and perform intelligent tasks approaching the human
level. Deep neural networks consist of multiple computation layers to learn
representations of data and improve the state-of-the-art in many recognition
domains. However, the essence of intelligence commonly represented by both
humans and AI is unknown. Here, we show that the nature of intelligence is a
series of mathematically functional processes that minimize system entropy by
establishing functional relationships between datasets over the space and time.
Humans and AI have achieved intelligence by implementing these entropy-reducing
processes in a reinforced manner that consumes energy. With this hypothesis, we
establish mathematical models of language, unconsciousness and consciousness,
predicting the evidence to be found by neuroscience and achieved by AI
engineering. Furthermore, a conclusion is made that the total entropy of the
universe is conservative, and the intelligence counters the spontaneous
processes to decrease entropy by physically or informationally connecting
datasets that originally exist in the universe but are separated across the
space and time. This essay should be a starting point for a deeper
understanding of the universe and us as human beings and for achieving
sophisticated AI models that are tantamount to human intelligence or even
superior. Furthermore, this essay argues that more advanced intelligence than
humans should exist if only it reduces entropy in a more efficient
energy-consuming way."
Theory of Mind Enhances Collective Intelligence,"Collective Intelligence plays a central role in a large variety of fields,
from economics and evolutionary theory to neural networks and eusocial insects,
and it is also core to much of the work on emergence and self-organisation in
complex systems theory. However, in human collective intelligence there is
still much more to be understood in the relationship between specific
psychological processes at the individual level and the emergence of
self-organised structures at the social level. Previously psychological factors
have played a relatively minor role in the study of collective intelligence as
the principles are often quite general and applicable to humans just as readily
as insects or other agents without sophisticated psychologies. In this article
we emphasise, with examples from other complex adaptive systems, the broad
applicability of collective intelligence principles while the mechanisms and
time-scales differ significantly between examples. We contend that flexible
collective intelligence in human social settings is improved by our use of a
specific cognitive tool: our Theory of Mind. We identify several key
characteristics of psychologically mediated collective intelligence and show
that the development of a Theory of Mind is a crucial factor distinguishing
social collective intelligence from general collective intelligence. We then
place these capabilities in the context of the next steps in artificial
intelligence embedded in a future that includes an effective human-AI hybrid
social ecology."
Visual Character Recognition using Artificial Neural Networks,"The recognition of optical characters is known to be one of the earliest
applications of Artificial Neural Networks, which partially emulate human
thinking in the domain of artificial intelligence. In this paper, a simplified
neural approach to recognition of optical or visual characters is portrayed and
discussed. The document is expected to serve as a resource for learners and
amateur investigators in pattern recognition, neural networking and related
disciplines."
Artificial Learning in Artificial Memories,"Memory refinements are designed below to detect those sequences of actions
that have been repeated a given number n. Subsequently such sequences are
permitted to run without CPU involvement. This mimics human learning. Actions
are rehearsed and once learned, they are performed automatically without
conscious involvement."
Artificial neural networks on graded vector spaces,"We develop new artificial neural network models for graded vector spaces,
which are suitable when different features in the data have different
significance (weights). This is the first time that such models are designed
mathematically and they are expected to perform better than neural networks
over usual vector spaces, which are the special case when the gradings are all
1s."
"Comparison of Artificial Intelligence Techniques for Project Conceptual
  Cost Prediction","Developing a reliable parametric cost model at the conceptual stage of the
project is crucial for projects managers and decision-makers. Existing methods,
such as probabilistic and statistical algorithms have been developed for
project cost prediction. However, these methods are unable to produce accurate
results for conceptual cost prediction due to small and unstable data samples.
Artificial intelligence (AI) and machine learning (ML) algorithms include
numerous models and algorithms for supervised regression applications.
Therefore, a comparison analysis for AI models is required to guide
practitioners to the appropriate model. The study focuses on investigating
twenty artificial intelligence (AI) techniques which are conducted for cost
modeling such as fuzzy logic (FL) model, artificial neural networks (ANNs),
multiple regression analysis (MRA), case-based reasoning (CBR), hybrid models,
and ensemble methods such as scalable boosting trees (XGBoost). Field canals
improvement projects (FCIPs) are used as an actual case study to analyze the
performance of the applied ML models. Out of 20 AI techniques, the results
showed that the most accurate and suitable method is XGBoost with 9.091% and
0.929 based on Mean Absolute Percentage Error (MAPE) and adjusted R2. Nonlinear
adaptability, handling missing values and outliers, model interpretation and
uncertainty have been discussed for the twenty developed AI models. Keywords:
Artificial intelligence, Machine learning, ensemble methods, XGBoost,
evolutionary fuzzy rules generation, Conceptual cost, and parametric cost
model."
"Artificial Intelligence in Governance, Risk and Compliance: Results of a
  study on potentials for the application of artificial intelligence (AI) in
  governance, risk and compliance (GRC)","The digital transformation leads to fundamental change in organizational
structures. To be able to apply new technologies not only selectively,
processes in companies must be revised and functional units must be viewed
holistically, especially with regard to interfaces. Target-oriented management
decisions are made, among other things, on the basis of risk management and
compliance in combination with the internal control system as governance
functions. The effectiveness and efficiency of these functions is decisive to
follow guidelines and regulatory requirements as well as for the evaluation of
alternative options for acting with regard to activities of companies. GRC
(Governance, Risk and Compliance) means an integrated governance-approach, in
which the mentioned governance functions are interlinked and not separated from
each other. Methods of artificial intelligence represents an important
technology of digital transformation. This technology, which offers a broad
range of methods such as machine learning, artificial neural networks, natural
language processing or deep learning, offers a lot of possible applications in
many business areas from purchasing to production or customer service.
Artificial intelligence is also being used in GRC, for example for processing
and analysis of unstructured data sets. This study contains the results of a
survey conducted in 2021 to identify and analyze the potential applications of
artificial intelligence in GRC."
"Why Artificial Intelligence Needs a Task Theory --- And What It Might
  Look Like","The concept of ""task"" is at the core of artificial intelligence (AI): Tasks
are used for training and evaluating AI systems, which are built in order to
perform and automatize tasks we deem useful. In other fields of engineering
theoretical foundations allow thorough evaluation of designs by methodical
manipulation of well understood parameters with a known role and importance;
this allows an aeronautics engineer, for instance, to systematically assess the
effects of wind speed on an airplane's performance and stability. No framework
exists in AI that allows this kind of methodical manipulation: Performance
results on the few tasks in current use (cf. board games, question-answering)
cannot be easily compared, however similar or different. The issue is even more
acute with respect to artificial *general* intelligence systems, which must
handle unanticipated tasks whose specifics cannot be known beforehand. A *task
theory* would enable addressing tasks at the *class* level, bypassing their
specifics, providing the appropriate formalization and classification of tasks,
environments, and their parameters, resulting in more rigorous ways of
measuring, comparing, and evaluating intelligent behavior. Even modest
improvements in this direction would surpass the current ad-hoc nature of
machine learning and AI evaluation. Here we discuss the main elements of the
argument for a task theory and present an outline of what it might look like
for physical tasks."
Towards the Neuroevolution of Low-level Artificial General Intelligence,"In this work, we argue that the search for Artificial General Intelligence
(AGI) should start from a much lower level than human-level intelligence. The
circumstances of intelligent behavior in nature resulted from an organism
interacting with its surrounding environment, which could change over time and
exert pressure on the organism to allow for learning of new behaviors or
environment models. Our hypothesis is that learning occurs through interpreting
sensory feedback when an agent acts in an environment. For that to happen, a
body and a reactive environment are needed. We evaluate a method to evolve a
biologically-inspired artificial neural network that learns from environment
reactions named Neuroevolution of Artificial General Intelligence (NAGI), a
framework for low-level AGI. This method allows the evolutionary
complexification of a randomly-initialized spiking neural network with adaptive
synapses, which controls agents instantiated in mutable environments. Such a
configuration allows us to benchmark the adaptivity and generality of the
controllers. The chosen tasks in the mutable environments are food foraging,
emulation of logic gates, and cart-pole balancing. The three tasks are
successfully solved with rather small network topologies and therefore it opens
up the possibility of experimenting with more complex tasks and scenarios where
curriculum learning is beneficial."
"The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of
  Natural Scenes","The sciences of biological and artificial intelligence are ever more
intertwined. Neural computational principles inspire new intelligent machines,
which are in turn used to advance theoretical understanding of the brain. To
promote further exchange of ideas and collaboration between biological and
artificial intelligence researchers, we introduce the 2023 installment of the
Algonauts Project challenge: How the Human Brain Makes Sense of Natural Scenes
(http://algonauts.csail.mit.edu). This installment prompts the fields of
artificial and biological intelligence to come together towards building
computational models of the visual brain using the largest and richest dataset
of fMRI responses to visual scenes, the Natural Scenes Dataset (NSD). NSD
provides high-quality fMRI responses to ~73,000 different naturalistic colored
scenes, making it the ideal candidate for data-driven model building approaches
promoted by the 2023 challenge. The challenge is open to all and makes results
directly comparable and transparent through a public leaderboard automatically
updated after each submission, thus allowing for rapid model development. We
believe that the 2023 installment will spark symbiotic collaborations between
biological and artificial intelligence scientists, leading to a deeper
understanding of the brain through cutting-edge computational models and to
novel ways of engineering artificial intelligent agents through inductive
biases from biological systems."
"Machine Psychology: Integrating Operant Conditioning with the
  Non-Axiomatic Reasoning System for Advancing Artificial General Intelligence
  Research","This paper introduces an interdisciplinary framework called Machine
Psychology, which merges principles from operant learning psychology with a
specific Artificial Intelligence model, the Non-Axiomatic Reasoning System
(NARS), to enhance Artificial General Intelligence (AGI) research. The core
premise of this framework is that adaptation is crucial to both biological and
artificial intelligence and can be understood through operant conditioning
principles. The study assesses this approach via three operant learning tasks
using OpenNARS for Applications (ONA): simple discrimination, changing
contingencies, and conditional discrimination tasks.
  In the simple discrimination task, NARS demonstrated rapid learning,
achieving perfect accuracy during both training and testing phases. The
changing contingencies task showcased NARS's adaptability, as it successfully
adjusted its behavior when task conditions were reversed. In the conditional
discrimination task, NARS handled complex learning scenarios effectively,
achieving high accuracy by forming and utilizing intricate hypotheses based on
conditional cues.
  These findings support the application of operant conditioning as a framework
for creating adaptive AGI systems. NARS's ability to operate under conditions
of insufficient knowledge and resources, coupled with its sensorimotor
reasoning capabilities, establishes it as a robust model for AGI. The Machine
Psychology framework, by incorporating elements of natural intelligence such as
continuous learning and goal-driven behavior, offers a scalable and flexible
approach for real-world applications. Future research should investigate using
enhanced NARS systems, more advanced tasks, and applying this framework to
diverse, complex challenges to further progress the development of human-level
AI."
Probabilistic Artificial Intelligence,"Artificial intelligence commonly refers to the science and engineering of
artificial systems that can carry out tasks generally associated with requiring
aspects of human intelligence, such as playing games, translating languages,
and driving cars. In recent years, there have been exciting advances in
learning-based, data-driven approaches towards AI, and machine learning and
deep learning have enabled computer systems to perceive the world in
unprecedented ways. Reinforcement learning has enabled breakthroughs in complex
games such as Go and challenging robotics tasks such as quadrupedal locomotion.
  A key aspect of intelligence is to not only make predictions, but reason
about the uncertainty in these predictions, and to consider this uncertainty
when making decisions. This is what this manuscript on ""Probabilistic
Artificial Intelligence"" is about. The first part covers probabilistic
approaches to machine learning. We discuss the differentiation between
""epistemic"" uncertainty due to lack of data and ""aleatoric"" uncertainty, which
is irreducible and stems, e.g., from noisy observations and outcomes. We
discuss concrete approaches towards probabilistic inference and modern
approaches to efficient approximate inference.
  The second part of the manuscript is about taking uncertainty into account in
sequential decision tasks. We consider active learning and Bayesian
optimization -- approaches that collect data by proposing experiments that are
informative for reducing the epistemic uncertainty. We then consider
reinforcement learning and modern deep RL approaches that use neural network
function approximation. We close by discussing modern approaches in model-based
RL, which harness epistemic and aleatoric uncertainty to guide exploration,
while also reasoning about safety."
"Evaluating Sakana's AI Scientist for Autonomous Research: Wishful
  Thinking or an Emerging Reality Towards 'Artificial Research Intelligence'
  (ARI)?","A major step toward Artificial General Intelligence (AGI) and Super
Intelligence is AI's ability to autonomously conduct research - what we term
Artificial Research Intelligence (ARI). If machines could generate hypotheses,
conduct experiments, and write research papers without human intervention, it
would transform science. Sakana recently introduced the 'AI Scientist',
claiming to conduct research autonomously, i.e. they imply to have achieved
what we term Artificial Research Intelligence (ARI). The AI Scientist gained
much attention, but a thorough independent evaluation has yet to be conducted.
  Our evaluation of the AI Scientist reveals critical shortcomings. The
system's literature reviews produced poor novelty assessments, often
misclassifying established concepts (e.g., micro-batching for stochastic
gradient descent) as novel. It also struggles with experiment execution: 42% of
experiments failed due to coding errors, while others produced flawed or
misleading results. Code modifications were minimal, averaging 8% more
characters per iteration, suggesting limited adaptability. Generated
manuscripts were poorly substantiated, with a median of five citations, most
outdated (only five of 34 from 2020 or later). Structural errors were frequent,
including missing figures, repeated sections, and placeholder text like
'Conclusions Here'. Some papers contained hallucinated numerical results.
  Despite these flaws, the AI Scientist represents a leap forward in research
automation. It generates full research manuscripts with minimal human input,
challenging expectations of AI-driven science. Many reviewers might struggle to
distinguish its work from human researchers. While its quality resembles a
rushed undergraduate paper, its speed and cost efficiency are unprecedented,
producing a full paper for USD 6 to 15 with 3.5 hours of human involvement, far
outpacing traditional researchers."
On the Compatibility Between Physics and Intelligent Organisms,"It has been commonly argued, on the basis of Goedel's theorem and related
mathematical results, that true artificial intelligence cannot exist. Penrose
has further deduced from the existence of human intelligence that fundamental
changes in physical theories are needed. I provide an elementary demonstration
that these deductions are mistaken."
Intelligent encoding and economical communication in the visual stream,"The theory of computational complexity is used to underpin a recent model of
neocortical sensory processing. We argue that encoding into reconstruction
networks is appealing for communicating agents using Hebbian learning and
working on hard combinatorial problems, which are easy to verify. Computational
definition of the concept of intelligence is provided. Simulations illustrate
the idea."
The Computational Theory of Intelligence: Data Aggregation,"In this paper, we will expound upon the concepts proffered in [1], where we
proposed an information theoretic approach to intelligence in the computational
sense. We will examine data and meme aggregation, and study the effect of
limited resources on the resulting meme amplitudes."
"Automatic Synthesis of Geometry Problems for an Intelligent Tutoring
  System","This paper presents an intelligent tutoring system, GeoTutor, for Euclidean
Geometry that is automatically able to synthesize proof problems and their
respective solutions given a geometric figure together with a set of properties
true of it. GeoTutor can provide personalized practice problems that address
student deficiencies in the subject matter."
Intelligent User Interfaces - A Tutorial,"IUIs aim to incorporate intelligent automated capabilities in human computer
interaction, where the net impact is a human-computer interaction that improves
performance or usability in critical ways. It also involves designing and
implementing an artificial intelligence (AI) component that effectively
leverages human skills and capabilities, so that human performance with an
application excels. IUIs embody capabilities that have traditionally been
associated more strongly with humans than with computers: how to perceive,
interpret, learn, use language, reason, plan, and decide."
Brief Review of Computational Intelligence Algorithms,"Computational Intelligence algorithms have gained a lot of attention of
researchers in the recent years due to their ability to deliver near optimal
solutions."
"Proceedings of the AAAI-20 Workshop on Intelligent Process Automation
  (IPA-20)","This is the Proceedings of the AAAI-20 Workshop on Intelligent Process
Automation (IPA-20) which took place in New York, NY, USA on February 7th 2020."
"A Feedback Integrated Web-Based Multi-Criteria Group Decision Support
  Model for Contractor Selection using Fuzzy Analytic Hierarchy Process","In this paper, a feedback integrated multi-criteria group decision support
model for contractor selection was proposed."
A Blockchain Protocol for Human-in-the-Loop AI,"Intelligent human inputs are required both in the training and operation of
AI systems, and within the governance of blockchain systems and decentralized
autonomous organizations (DAOs). This paper presents a formal definition of
Human Intelligence Primitives (HIPs), and describes the design and
implementation of an Ethereum protocol for their on-chain collection, modeling,
and integration in machine learning workflows."
AI-Oracle Machines for Intelligent Computing,"We introduce the concept of AI-oracle machines for intelligent computing and
outline several applications to demonstrate their potential. Following this, we
advocate for the development of a comprehensive platform to streamline the
implementation of AI-oracle machines."
"Artificial Intelligence without Restriction Surpassing Human
  Intelligence with Probability One: Theoretical Insight into Secrets of the
  Brain with AI Twins of the Brain","Artificial Intelligence (AI) has apparently become one of the most important
techniques discovered by humans in history while the human brain is widely
recognized as one of the most complex systems in the universe. One fundamental
critical question which would affect human sustainability remains open: Will
artificial intelligence (AI) evolve to surpass human intelligence in the
future? This paper shows that in theory new AI twins with fresh cellular level
of AI techniques for neuroscience could approximate the brain and its
functioning systems (e.g. perception and cognition functions) with any expected
small error and AI without restrictions could surpass human intelligence with
probability one in the end. This paper indirectly proves the validity of the
conjecture made by Frank Rosenblatt 70 years ago about the potential
capabilities of AI, especially in the realm of artificial neural networks.
Intelligence is just one of fortuitous but sophisticated creations of the
nature which has not been fully discovered. Like mathematics and physics, with
no restrictions artificial intelligence would lead to a new subject with its
self-contained systems and principles. We anticipate that this paper opens new
doors for 1) AI twins and other AI techniques to be used in cellular level of
efficient neuroscience dynamic analysis, functioning analysis of the brain and
brain illness solutions; 2) new worldwide collaborative scheme for
interdisciplinary teams concurrently working on and modelling different types
of neurons and synapses and different level of functioning subsystems of the
brain with AI techniques; 3) development of low energy of AI techniques with
the aid of fundamental neuroscience properties; and 4) new controllable,
explainable and safe AI techniques with reasoning capabilities of discovering
principles in nature."
"Improbotics: Exploring the Imitation Game using Machine Intelligence in
  Improvised Theatre","Theatrical improvisation (impro or improv) is a demanding form of live,
collaborative performance. Improv is a humorous and playful artform built on an
open-ended narrative structure which simultaneously celebrates effort and
failure. It is thus an ideal test bed for the development and deployment of
interactive artificial intelligence (AI)-based conversational agents, or
artificial improvisors. This case study introduces an improv show experiment
featuring human actors and artificial improvisors. We have previously developed
a deep-learning-based artificial improvisor, trained on movie subtitles, that
can generate plausible, context-based, lines of dialogue suitable for theatre
(Mathewson and Mirowski 2017). In this work, we have employed it to control
what a subset of human actors say during an improv performance. We also give
human-generated lines to a different subset of performers. All lines are
provided to actors with headphones and all performers are wearing headphones.
This paper describes a Turing test, or imitation game, taking place in a
theatre, with both the audience members and the performers left to guess who is
a human and who is a machine. In order to test scientific hypotheses about the
perception of humans versus machines we collect anonymous feedback from
volunteer performers and audience members. Our results suggest that rehearsal
increases proficiency and possibility to control events in the performance.
That said, consistency with real world experience is limited by the interface
and the mechanisms used to perform the show. We also show that human-generated
lines are shorter, more positive, and have less difficult words with more
grammar and spelling mistakes than the artificial improvisor generated lines."
Guilty Artificial Minds,"The concepts of blameworthiness and wrongness are of fundamental importance
in human moral life. But to what extent are humans disposed to blame
artificially intelligent agents, and to what extent will they judge their
actions to be morally wrong? To make progress on these questions, we adopted
two novel strategies. First, we break down attributions of blame and wrongness
into more basic judgments about the epistemic and conative state of the agent,
and the consequences of the agent's actions. In this way, we are able to
examine any differences between the way participants treat artificial agents in
terms of differences in these more basic judgments. our second strategy is to
compare attributions of blame and wrongness across human, artificial, and group
agents (corporations). Others have compared attributions of blame and wrongness
between human and artificial agents, but the addition of group agents is
significant because these agents seem to provide a clear middle-ground between
human agents (for whom the notions of blame and wrongness were created) and
artificial agents (for whom the question remains open)."
Landau Theory of Adaptive Integration in Computational Intelligence,"Computational Intelligence (CI) is a sub-branch of Artificial Intelligence
paradigm focusing on the study of adaptive mechanisms to enable or facilitate
intelligent behavior in complex and changing environments. There are several
paradigms of CI [like artificial neural networks, evolutionary computations,
swarm intelligence, artificial immune systems, fuzzy systems and many others],
each of these has its origins in biological systems [biological neural systems,
natural Darwinian evolution, social behavior, immune system, interactions of
organisms with their environment]. Most of those paradigms evolved into
separate machine learning (ML) techniques, where probabilistic methods are used
complementary with CI techniques in order to effectively combine elements of
learning, adaptation, evolution and Fuzzy logic to create heuristic algorithms
that are, in some sense, intelligent. The current trend is to develop consensus
techniques, since no single machine learning algorithms is superior to others
in all possible situations. In order to overcome this problem several
meta-approaches were proposed in ML focusing on the integration of results from
different methods into single prediction. We discuss here the Landau theory for
the nonlinear equation that can describe the adaptive integration of
information acquired from an ensemble of independent learning agents. The
influence of each individual agent on other learners is described similarly to
the social impact theory. The final decision outcome for the consensus system
is calculated using majority rule in the stationary limit, yet the minority
solutions can survive inside the majority population as the complex
intermittent clusters of opposite opinion."
"Affect Control Processes: Intelligent Affective Interaction using a
  Partially Observable Markov Decision Process","This paper describes a novel method for building affectively intelligent
human-interactive agents. The method is based on a key sociological insight
that has been developed and extensively verified over the last twenty years,
but has yet to make an impact in artificial intelligence. The insight is that
resource bounded humans will, by default, act to maintain affective
consistency. Humans have culturally shared fundamental affective sentiments
about identities, behaviours, and objects, and they act so that the transient
affective sentiments created during interactions confirm the fundamental
sentiments. Humans seek and create situations that confirm or are consistent
with, and avoid and supress situations that disconfirm or are inconsistent
with, their culturally shared affective sentiments. This ""affect control
principle"" has been shown to be a powerful predictor of human behaviour. In
this paper, we present a probabilistic and decision-theoretic generalisation of
this principle, and we demonstrate how it can be leveraged to build affectively
intelligent artificial agents. The new model, called BayesAct, can maintain
multiple hypotheses about sentiments simultaneously as a probability
distribution, and can make use of an explicit utility function to make
value-directed action choices. This allows the model to generate affectively
intelligent interactions with people by learning about their identity,
predicting their behaviours using the affect control principle, and taking
actions that are simultaneously goal-directed and affect-sensitive. We
demonstrate this generalisation with a set of simulations. We then show how our
model can be used as an emotional ""plug-in"" for artificially intelligent
systems that interact with humans in two different settings: an exam practice
assistant (tutor) and an assistive device for persons with a cognitive
disability."
"Modelos dinmicos aplicados  aprendizagem de valores em
  inteligncia artificial","Experts in Artificial Intelligence (AI) development predict that advances in
the development of intelligent systems and agents will reshape vital areas in
our society. Nevertheless, if such an advance is not made prudently and
critically, reflexively, it can result in negative outcomes for humanity. For
this reason, several researchers in the area have developed a robust,
beneficial, and safe concept of AI for the preservation of humanity and the
environment. Currently, several of the open problems in the field of AI
research arise from the difficulty of avoiding unwanted behaviors of
intelligent agents and systems, and at the same time specifying what we really
want such systems to do, especially when we look for the possibility of
intelligent agents acting in several domains over the long term. It is of
utmost importance that artificial intelligent agents have their values aligned
with human values, given the fact that we cannot expect an AI to develop human
moral values simply because of its intelligence, as discussed in the
Orthogonality Thesis. Perhaps this difficulty comes from the way we are
addressing the problem of expressing objectives, values, and ends, using
representational cognitive methods. A solution to this problem would be the
dynamic approach proposed by Dreyfus, whose phenomenological philosophy shows
that the human experience of being-in-the-world in several aspects is not well
represented by the symbolic or connectionist cognitive method, especially in
regards to the question of learning values. A possible approach to this problem
would be to use theoretical models such as SED (situated embodied dynamics) to
address the values learning problem in AI."
"AI video editing tools. What editors want and how far is AI from
  delivering?","Video editing can be a very tedious task, so unsurprisingly Artificial
Intelligence has been increasingly used to streamline the workflow or automate
away tedious tasks. However, it is very difficult to get an overview of what
intelligent video editing tools are in the research literature and needs for
automation from the video editors. So, we identified the field of intelligent
video editing tools in research, and we survey the opinions of professional
video editors. We have also summarized current state of the art in artificial
intelligence research with the intention of identifying what are the
possibilities and current technical limits towards truly intelligent video
editing tools. The findings contribute towards understanding of the field of
intelligent video editing tools, highlights unaddressed automation needs by the
survey and provides general suggestions for further research in intelligent
video editing tools."
"Could AI be the Great Filter? What Astrobiology can Teach the
  Intelligence Community about Anthropogenic Risks","Where is everybody? This phrase distills the foreboding of what has come to
be known as the Fermi Paradox - the disquieting idea that, if extraterrestrial
life is probable in the Universe, then why have we not encountered it? This
conundrum has puzzled scholars for decades, and many hypotheses have been
proposed suggesting both naturalistic and sociological explanations. One
intriguing hypothesis is known as the Great Filter, which suggests that some
event required for the emergence of intelligent life is extremely unlikely,
hence the cosmic silence. A logically equivalent version of this hypothesis --
and one that should give us pause -- suggests that some catastrophic event is
likely to occur that prevents life's expansion throughout the cosmos. This
could be a naturally occurring event, or more disconcertingly, something that
intelligent beings do to themselves that leads to their own extinction. From an
intelligence perspective, framing global catastrophic risk (particularly risks
of anthropogenic origin) within the context of the Great Filter can provide
insight into the long-term futures of technologies that we don't fully
understand, like artificial intelligence. For the intelligence professional
concerned with global catastrophic risk, this has significant implications for
how these risks ought to be prioritized."
"Non-equilibrium physics: from spin glasses to machine and neural
  learning","Disordered many-body systems exhibit a wide range of emergent phenomena
across different scales. These complex behaviors can be utilized for various
information processing tasks such as error correction, learning, and
optimization. Despite the empirical success of utilizing these systems for
intelligent tasks, the underlying principles that govern their emergent
intelligent behaviors remain largely unknown. In this thesis, we aim to
characterize such emergent intelligence in disordered systems through
statistical physics. We chart a roadmap for our efforts in this thesis based on
two axes: learning mechanisms (long-term memory vs. working memory) and
learning dynamics (artificial vs. natural). Throughout our journey, we uncover
relationships between learning mechanisms and physical dynamics that could
serve as guiding principles for designing intelligent systems. We hope that our
investigation into the emergent intelligence of seemingly disparate learning
systems can expand our current understanding of intelligence beyond neural
systems and uncover a wider range of computational substrates suitable for AI
applications."
"An Efficient Intelligent Semi-Automated Warehouse Inventory Stocktaking
  System","In the context of evolving supply chain management, the significance of
efficient inventory management has grown substantially for businesses. However,
conventional manual and experience-based approaches often struggle to meet the
complexities of modern market demands. This research introduces an intelligent
inventory management system to address challenges related to inaccurate data,
delayed monitoring, and overreliance on subjective experience in forecasting.
The proposed system integrates bar code and distributed flutter application
technologies for intelligent perception, alongside comprehensive big data
analytics to enable data-driven decision-making. Through meticulous analysis,
system design, critical technology exploration, and simulation validation, the
effectiveness of the proposed system is successfully demonstrated. The
intelligent system facilitates second-level monitoring, high-frequency checks,
and artificial intelligence-driven forecasting, consequently enhancing the
automation, precision, and intelligence of inventory management. This system
contributes to cost reduction and optimized inventory sizes through accurate
predictions and informed decisions, ultimately achieving a mutually beneficial
scenario. The outcomes of this research offer"
"Can ChatGPT advance software testing intelligence? An experience report
  on metamorphic testing","While ChatGPT is a well-known artificial intelligence chatbot being used to
answer human's questions, one may want to discover its potential in advancing
software testing. We examine the capability of ChatGPT in advancing the
intelligence of software testing through a case study on metamorphic testing
(MT), a state-of-the-art software testing technique. We ask ChatGPT to generate
candidates of metamorphic relations (MRs), which are basically necessary
properties of the object program and which traditionally require human
intelligence to identify. These MR candidates are then evaluated in terms of
correctness by domain experts. We show that ChatGPT can be used to generate new
correct MRs to test several software systems. Having said that, the majority of
MR candidates are either defined vaguely or incorrect, especially for systems
that have never been tested with MT. ChatGPT can be used to advance software
testing intelligence by proposing MR candidates that can be later adopted for
implementing tests; but human intelligence should still inevitably be involved
to justify and rectify their correctness."
Review of Cloud Service Composition for Intelligent Manufacturing,"Intelligent manufacturing is a new model that uses advanced technologies such
as the Internet of Things, big data, and artificial intelligence to improve the
efficiency and quality of manufacturing production. As an important support to
promote the transformation and upgrading of the manufacturing industry, cloud
service optimization has received the attention of researchers. In recent
years, remarkable research results have been achieved in this field. For the
sustainability of intelligent manufacturing platforms, in this paper we
summarize the process of cloud service optimization for intelligent
manufacturing. Further, to address the problems of dispersed optimization
indicators and nonuniform/unstandardized definitions in the existing research,
11 optimization indicators that take into account three-party participant
subjects are defined from the urgent requirements of the sustainable
development of intelligent manufacturing platforms. Next, service optimization
algorithms are classified into two categories, heuristic and reinforcement
learning. After comparing the two categories, the current key techniques of
service optimization are targeted. Finally, research hotspots and future
research trends of service optimization are summarized."
"Adaptive Intelligence: leveraging insights from adaptive behavior in
  animals to build flexible AI systems","Biological intelligence is inherently adaptive -- animals continually adjust
their actions based on environmental feedback. However, creating adaptive
artificial intelligence (AI) remains a major challenge. The next frontier is to
go beyond traditional AI to develop ""adaptive intelligence,"" defined here as
harnessing insights from biological intelligence to build agents that can learn
online, generalize, and rapidly adapt to changes in their environment. Recent
advances in neuroscience offer inspiration through studies that increasingly
focus on how animals naturally learn and adapt their world models. In this
Perspective, I will review the behavioral and neural foundations of adaptive
biological intelligence, the parallel progress in AI, and explore
brain-inspired approaches for building more adaptive algorithms."
"Potential Applications of Artificial Intelligence for Cross-language
  Intelligibility Assessment of Dysarthric Speech","Purpose: This commentary introduces how artificial intelligence (AI) can be
leveraged to advance cross-language intelligibility assessment of dysarthric
speech. Method: We propose a conceptual framework consisting of a universal
model that captures language-universal speech impairments and a
language-specific intelligibility model that incorporates linguistic nuances.
Additionally, we identify key barriers to cross-language intelligibility
assessment, including data scarcity, annotation complexity, and limited
linguistic insights, and present AI-driven solutions to overcome these
challenges. Conclusion: Advances in AI offer transformative opportunities to
enhance cross-language intelligibility assessment for dysarthric speech by
balancing scalability across languages and adaptability by languages."
"A Representationalist, Functionalist and Naturalistic Conception of
  Intelligence as a Foundation for AGI","The article analyses foundational principles relevant to the creation of
artificial general intelligence (AGI). Intelligence is understood as the
ability to create novel skills that allow to achieve goals under previously
unknown conditions. To this end, intelligence utilises reasoning methods such
as deduction, induction and abduction as well as other methods such as
abstraction and classification to develop a world model. The methods are
applied to indirect and incomplete representations of the world, which are
obtained through perception, for example, and which do not depict the world but
only correspond to it. Due to these limitations and the uncertain and
contingent nature of reasoning, the world model is constructivist. Its value is
functionally determined by its viability, i.e., its potential to achieve the
desired goals. In consequence, meaning is assigned to representations by
attributing them a function that makes it possible to achieve a goal. This
representational and functional conception of intelligence enables a
naturalistic interpretation that does not presuppose mental features, such as
intentionality and consciousness, which are regarded as independent of
intelligence. Based on a phenomenological analysis, it is shown that AGI can
gain a more fundamental access to the world than humans, although it is limited
by the No Free Lunch theorems, which require assumptions to be made."
"Socially-Minded Intelligence: How Individuals, Groups, and AI Systems
  Can Make Each-Other Smarter (or Not)","A core part of human intelligence is the ability to work flexibly with others
to achieve both individual and collective goals. The incorporation of
artificial agents into human spaces is making increasing demands on artificial
intelligence (AI) to demonstrate and facilitate this ability. However, this
kind of flexibility is not well understood because existing approaches to
intelligence typically focus either on the individual or the collective level
of analysis. At the individual level, intelligence is seen as an
individual-difference trait that exists independently of the social
environment. At the collective level intelligence is conceptualized as a
property of groups, but not in a way that can be used to understand how groups
can make group members smarter or how group members acting as individuals might
make the group itself more intelligent. In the present paper we argue that by
focusing either on individual or collective intelligence without considering
their interaction, existing conceptualizations of intelligence limit the
potential of people and machines. To address this impasse, we identify and
explore a new kind of intelligence - socially-minded intelligence - that can be
applied to both individuals (in a social context) and collectives (of
individual minds). From a socially-minded intelligence perspective, the
potential intelligence of individuals is unlocked in groups, while the
potential intelligence of groups is maximized by the flexible,
context-sensitive commitment of individual group members. We propose ways in
which socially-minded intelligence might be measured and cultivated within
people, as well as how it might be modelled in AI systems. Finally, we discuss
ways in which socially-minded intelligence might be used to improve human-AI
teaming."
Perspectives for Strong Artificial Life,"This text introduces the twin deadlocks of strong artificial life.
Conceptualization of life is a deadlock both because of the existence of a
continuum between the inert and the living, and because we only know one
instance of life. Computationalism is a second deadlock since it remains a
matter of faith. Nevertheless, artificial life realizations quickly progress
and recent constructions embed an always growing set of the intuitive
properties of life. This growing gap between theory and realizations should
sooner or later crystallize in some kind of paradigm shift and then give clues
to break the twin deadlocks."
Thoughts on an Unified Framework for Artificial Chemistries,"Artificial Chemistries (ACs) are symbolic chemical metaphors for the
exploration of Artificial Life, with specific focus on the problem of
biogenesis or the origin of life. This paper presents authors thoughts towards
defining a unified framework to characterize and classify symbolic artificial
chemistries by devising appropriate formalism to capture semantic and
organizational information. We identify three basic high level abstractions in
initial proposal for this framework viz., information, computation, and
communication. We present an analysis of two important notions of information,
namely, Shannon's Entropy and Algorithmic Information, and discuss inductive
and deductive approaches for defining the framework."
Towards a Conceptual Framework for Innate Immunity,"Innate immunity now occupies a central role in immunology. However,
artificial immune system models have largely been inspired by adaptive not
innate immunity. This paper reviews the biological principles and properties of
innate immunity and, adopting a conceptual framework, asks how these can be
incorporated into artificial models. The aim is to outline a meta-framework for
models of innate immunity."
Generating Artificial Data for Private Deep Learning,"In this paper, we propose generating artificial data that retain statistical
properties of real data as the means of providing privacy with respect to the
original dataset. We use generative adversarial network to draw
privacy-preserving artificial data samples and derive an empirical method to
assess the risk of information disclosure in a differential-privacy-like way.
Our experiments show that we are able to generate artificial data of high
quality and successfully train and validate machine learning models on this
data while limiting potential privacy loss."
Coevo: a collaborative design platform with artificial agents,"We present Coevo, an online platform that allows both humans and artificial
agents to design shapes that solve different tasks. Our goal is to explore
common shared design tools that can be used by humans and artificial agents in
a context of creation. This approach can provide a better knowledge transfer
and interaction with artificial agents since a common language of design is
defined. In this paper, we outline the main components of this platform and
discuss the definition of a human-centered language to enhance human-AI
collaboration in co-creation scenarios."
"Conscious Intelligent Systems - Part II - Mind, Thought, Language and
  Understanding","This is the second part of a paper on Conscious Intelligent Systems. We use
the understanding gained in the first part (Conscious Intelligent Systems Part
1: IXI (arxiv id cs.AI/0612056)) to look at understanding. We see how the
presence of mind affects understanding and intelligent systems; we see that the
presence of mind necessitates language. The rise of language in turn has
important effects on understanding. We discuss the humanoid question and how
the question of self-consciousness (and by association mind/thought/language)
would affect humanoids too."
"Ultimate Intelligence Part III: Measures of Intelligence, Perception and
  Intelligent Agents","We propose that operator induction serves as an adequate model of perception.
We explain how to reduce universal agent models to operator induction. We
propose a universal measure of operator induction fitness, and show how it can
be used in a reinforcement learning model and a homeostasis (self-preserving)
agent based on the free energy principle. We show that the action of the
homeostasis agent can be explained by the operator induction model."
Reward-Punishment Symmetric Universal Intelligence,"Can an agent's intelligence level be negative? We extend the Legg-Hutter
agent-environment framework to include punishments and argue for an affirmative
answer to that question. We show that if the background encodings and Universal
Turing Machine (UTM) admit certain Kolmogorov complexity symmetries, then the
resulting Legg-Hutter intelligence measure is symmetric about the origin. In
particular, this implies reward-ignoring agents have Legg-Hutter intelligence 0
according to such UTMs."
A Survey on Computational Intelligence-based Transfer Learning,"The goal of transfer learning (TL) is providing a framework for exploiting
acquired knowledge from source to target data. Transfer learning approaches
compared to traditional machine learning approaches are capable of modeling
better data patterns from the current domain. However, vanilla TL needs
performance improvements by using computational intelligence-based TL. This
paper studies computational intelligence-based transfer learning techniques and
categorizes them into neural network-based, evolutionary algorithm-based, swarm
intelligence-based and fuzzy logic-based transfer learning."
"Embodied, Situated, and Grounded Intelligence: Implications for AI","In April of 2022, the Santa Fe Institute hosted a workshop on embodied,
situated, and grounded intelligence as part of the Institute's Foundations of
Intelligence project. The workshop brought together computer scientists,
psychologists, philosophers, social scientists, and others to discuss the
science of embodiment and related issues in human intelligence, and its
implications for building robust, human-level AI. In this report, we summarize
each of the talks and the subsequent discussions. We also draw out a number of
key themes and identify important frontiers for future research."
"Robust Quantum Controllers: Quantum Information -- Thermodynamic Hidden
  Force Control in Intelligent Robotics based on Quantum Soft Computing","A generalized strategy for the design of intelligent robust control systems
based on quantum / soft computing technologies is described. The reliability of
hybrid intelligent controllers increase by providing the ability to
self-organize of imperfect knowledge bases. The main attention is paid to
increasing the level of robustness of intelligent control systems in
unpredictable control situations with the demonstration by illustrative
examples. A SW & HW platform and support tools for a supercomputer accelerator
for modeling quantum algorithms on a classical computer are described."
Crowdsourced Multilingual Speech Intelligibility Testing,"With the advent of generative audio features, there is an increasing need for
rapid evaluation of their impact on speech intelligibility. Beyond the existing
laboratory measures, which are expensive and do not scale well, there has been
comparatively little work on crowdsourced assessment of intelligibility.
Standards and recommendations are yet to be defined, and publicly available
multilingual test materials are lacking. In response to this challenge, we
propose an approach for a crowdsourced intelligibility assessment. We detail
the test design, the collection and public release of the multilingual speech
data, and the results of our early experiments."
A Road Map to Strong Intelligence,"I wrote this paper because technology can really improve people's lives. With
it, we can live longer in a healthy body, save time through increased
efficiency and automation, and make better decisions. To get to the next level,
we need to start looking at intelligence from a much broader perspective, and
promote international interdisciplinary collaborations. Section 1 of this paper
delves into sociology and social psychology to explain that the mechanisms
underlying intelligence are inherently social. Section 2 proposes a method to
classify intelligence, and describes the differences between weak and strong
intelligence. Section 3 examines the Chinese Room argument from a different
perspective. It demonstrates that a Turing-complete machine cannot have strong
intelligence, and considers the modifications necessary for a computer to be
intelligent and have understanding. Section 4 argues that the existential risk
caused by the technological explosion of a single agent should not be of
serious concern. Section 5 looks at the AI control problem and argues that it
is impossible to build a super-intelligent machine that will do what it
creators want. By using insights from biology, it also proposes a solution to
the control problem. Section 6 discusses some of the implications of strong
intelligence. Section 7 lists the main challenges with deep learning, and
asserts that radical changes will be required to reach strong intelligence.
Section 8 examines a neuroscience framework that could help explain how a
cortical column works. Section 9 lays out the broad strokes of a road map
towards strong intelligence. Finally, section 10 analyzes the impacts and the
challenges of greater intelligence."
"EgoSocialArena: Benchmarking the Social Intelligence of Large Language
  Models from a First-person Perspective","Social intelligence is built upon three foundational pillars: cognitive
intelligence, situational intelligence, and behavioral intelligence. As large
language models (LLMs) become increasingly integrated into our social lives,
understanding, evaluating, and developing their social intelligence are
becoming increasingly important. While multiple existing works have
investigated the social intelligence of LLMs, (1) most focus on a specific
aspect, and the social intelligence of LLMs has yet to be systematically
organized and studied; (2) position LLMs as passive observers from a
third-person perspective, such as in Theory of Mind (ToM) tests. Compared to
the third-person perspective, ego-centric first-person perspective evaluation
can align well with actual LLM-based Agent use scenarios. (3) a lack of
comprehensive evaluation of behavioral intelligence, with specific emphasis on
incorporating critical human-machine interaction scenarios. In light of this,
we present EgoSocialArena, a novel framework grounded in the three pillars of
social intelligence: cognitive, situational, and behavioral intelligence, aimed
to systematically evaluate the social intelligence of LLMs from a first-person
perspective. With EgoSocialArena, we conduct a comprehensive evaluation of
eight prominent foundation models, even the most advanced LLMs like O1-preview
lag behind human performance."
"Toward the Axiomatization of Intelligence: Structure, Time, and
  Existence","This study aims to construct an axiomatic definition of intelligence within a
meta-framework that defines the method of definition, addressing intelligence
as an inherently naive and polysemous concept. Initially, we formalize a
set-theoretic representation of the universe as the domain wherein intelligence
exists and characterize intelligence as a structure that involves temporal
evolution and interaction with other sets. Starting from a naive definition of
intelligence as ""an entity possessing structures for externally inputting,
internally processing, and externally outputting information or matter,"" we
axiomatically reformulate it within this set-theoretical depiction of the
universe. Applying this axiomatic definition, we compare and interpret three
examples -- Hebbian non-optimized neural networks (NNs),
backpropagation-optimized NNs, and biological reflexive systems -- in terms of
their intelligence, structural properties, and biological plausibility.
Furthermore, by extending our definition into a categorical framework, we
introduce two categories, ""Time Category"" and ""Intelligence Category,"" along
with the functorial relationships between them, demonstrating the potential to
represent changes and mimicry relationships among intelligent systems
abstractly. Additionally, since intelligence, as defined herein, functions
effectively only when accompanied by temporal interactions, we introduce the
concept of ""activity"" and explore how activity-based conditions influence
classifications and interpretations of intelligence. Finally, we suggest that
our definitional methodology is not limited to intelligence alone, but can be
similarly applied to other concepts, such as consciousness and emotion,
advocating for their formal reinterpretation through the same procedural steps:
defining a universal representation, selecting naive definitions, and axiomatic
formalization."
The amplifier effect of artificial agents in social contagion,"Recent advances in artificial intelligence have led to the proliferation of
artificial agents in social contexts, ranging from education to online social
media and financial markets, among many others. The increasing rate at which
artificial and human agents interact makes it urgent to understand the
consequences of human-machine interactions for the propagation of new ideas,
products, and behaviors in society. Across two distinct empirical contexts, we
find here that artificial agents lead to significantly faster and wider social
contagion. To this end, we replicate a choice experiment previously conducted
with human subjects by using artificial agents powered by large language models
(LLMs). We use the experiment's results to measure the adoption thresholds of
artificial agents and their impact on the spread of social contagion. We find
that artificial agents tend to exhibit lower adoption thresholds than humans,
which leads to wider network-based social contagions. Our findings suggest that
the increased presence of artificial agents in real-world networks may
accelerate behavioral shifts, potentially in unforeseen ways."
Computational Geometry Column 33,Several recent SIGGRAPH papers on surface simplification are described.
"Modeling Belief in Dynamic Systems, Part II: Revision and Update","The study of belief change has been an active area in philosophy and AI. In
recent years two special cases of belief change, belief revision and belief
update, have been studied in detail. In a companion paper (Friedman & Halpern,
1997), we introduce a new framework to model belief change. This framework
combines temporal and epistemic modalities with a notion of plausibility,
allowing us to examine the change of beliefs over time. In this paper, we show
how belief revision and belief update can be captured in our framework. This
allows us to compare the assumptions made by each method, and to better
understand the principles underlying them. In particular, it shows that Katsuno
and Mendelzon's notion of belief update (Katsuno & Mendelzon, 1991a) depends on
several strong assumptions that may limit its applicability in artificial
intelligence. Finally, our analysis allow us to identify a notion of minimal
change that underlies a broad range of belief change operations including
revision and update."
Defeasible Reasoning in OSCAR,This is a system description for the OSCAR defeasible reasoner.
A note on Darwiche and Pearl,"It is shown that Darwiche and Pearl's postulates imply an interesting
property, not noticed by the authors."
Utility-Probability Duality,"This paper presents duality between probability distributions and utility
functions."
P=NP,We claim to resolve the P=?NP problem via a formal argument for P=NP.
Using Sets of Probability Measures to Represent Uncertainty,"I explore the use of sets of probability measures as a representation of
uncertainty."
"Matrix Games, Linear Programming, and Linear Approximation","The following four classes of computational problems are equivalent: solving
matrix games, solving linear programs, best $l^{\infty}$ linear approximation,
best $l^1$ linear approximation."
Mining for trees in a graph is NP-complete,Mining for trees in a graph is shown to be NP-complete.
Serious Flaws in Korf et al.'s Analysis on Time Complexity of A*,This paper has been withdrawn.
A method for Hedging in continuous time,We present a method for hedging in continuous time.
Quality Classifiers for Open Source Software Repositories,"Open Source Software (OSS) often relies on large repositories, like
SourceForge, for initial incubation. The OSS repositories offer a large variety
of meta-data providing interesting information about projects and their
success. In this paper we propose a data mining approach for training
classifiers on the OSS meta-data provided by such data repositories. The
classifiers learn to predict the successful continuation of an OSS project. The
`successfulness' of projects is defined in terms of the classifier confidence
with which it predicts that they could be ported in popular OSS projects (such
as FreeBSD, Gentoo Portage)."
Weak Evolvability Equals Strong Evolvability,An updated version will be uploaded later.
Fact Sheet on Semantic Web,"The report gives an overview about activities on the topic Semantic Web. It
has been released as technical report for the project ""KTweb -- Connecting
Knowledge Technologies Communities"" in 2003."
"Introduction to the 26th International Conference on Logic Programming
  Special Issue","This is the preface to the 26th International Conference on Logic Programming
Special Issue"
"Artificial Intelligence in Reverse Supply Chain Management: The State of
  the Art","Product take-back legislation forces manufacturers to bear the costs of
collection and disposal of products that have reached the end of their useful
lives. In order to reduce these costs, manufacturers can consider reuse,
remanufacturing and/or recycling of components as an alternative to disposal.
The implementation of such alternatives usually requires an appropriate reverse
supply chain management. With the concepts of reverse supply chain are gaining
popularity in practice, the use of artificial intelligence approaches in these
areas is also becoming popular. As a result, the purpose of this paper is to
give an overview of the recent publications concerning the application of
artificial intelligence techniques to reverse supply chain with emphasis on
certain types of product returns."
Design of Automatically Adaptable Web Wrappers,"Nowadays, the huge amount of information distributed through the Web
motivates studying techniques to be adopted in order to extract relevant data
in an efficient and reliable way. Both academia and enterprises developed
several approaches of Web data extraction, for example using techniques of
artificial intelligence or machine learning. Some commonly adopted procedures,
namely wrappers, ensure a high degree of precision of information extracted
from Web pages, and, at the same time, have to prove robustness in order not to
compromise quality and reliability of data themselves. In this paper we focus
on some experimental aspects related to the robustness of the data extraction
process and the possibility of automatically adapting wrappers. We discuss the
implementation of algorithms for finding similarities between two different
version of a Web page, in order to handle modifications, avoiding the failure
of data extraction tasks and ensuring reliability of information extracted. Our
purpose is to evaluate performances, advantages and draw-backs of our novel
system of automatic wrapper adaptation."
Are Minds Computable?,"This essay explores the limits of Turing machines concerning the modeling of
minds and suggests alternatives to go beyond those limits."
Artificial Decision Making Under Uncertainty in Intelligent Buildings,"Our hypothesis is that by equipping certain agents in a multi-agent system
controlling an intelligent building with automated decision support, two
important factors will be increased. The first is energy saving in the
building. The second is customer value---how the people in the building
experience the effects of the actions of the agents. We give evidence for the
truth of this hypothesis through experimental findings related to tools for
artificial decision making. A number of assumptions related to agent control,
through monitoring and delegation of tasks to other kinds of agents, of rooms
at a test site are relaxed. Each assumption controls at least one uncertainty
that complicates considerably the procedures for selecting actions part of each
such agent. We show that in realistic decision situations, room-controlling
agents can make bounded rational decisions even under dynamic real-time
constraints. This result can be, and has been, generalized to other domains
with even harsher time constraints."
A Misanthropic Reinterpretation of the Chinese Room Problem,"The chinese room problem asks if computers can think; I ask here if most
humans can."
Dispersion and Line Formation in Artificial Swarm Intelligence,"One of the major motifs in collective or swarm intelligence is that, even
though individuals follow simple rules, the resulting global behavior can be
complex and intelligent. In artificial swarm systems, such as swarm robots, the
goal is to use systems that are as simple and cheap as possible, deploy many of
them, and coordinate them to conduct complex tasks that each individual cannot
accomplish. Shape formation in artificial intelligence systems is usually
required for specific task-oriented performance, including 1) forming sensing
grids, 2) exploring and mapping in space, underwater, or hazardous
environments, and 3) forming a barricade for surveillance or protecting an area
or a person. This paper presents a dynamic model of an artificial swarm system
based on a virtual spring damper model and algorithms for dispersion without a
leader and line formation with an interim leader using only the distance
estimation among the neighbors."
Experimental Realization of Quantum Artificial Intelligence,"Machines are possible to have some artificial intelligence like human beings
owing to particular algorithms or software. Such machines could learn knowledge
from what people taught them and do works according to the knowledge. In
practical learning cases, the data is often extremely complicated and large,
thus classical learning machines often need huge computational resources.
Quantum machine learning algorithm, on the other hand, could be exponentially
faster than classical machines using quantum parallelism. Here, we demonstrate
a quantum machine learning algorithm on a four-qubit NMR test bench to solve an
optical character recognition problem, also known as the handwriting
recognition. The quantum machine learns standard character fonts and then
recognize handwritten characters from a set with two candidates. To our best
knowledge, this is the first artificial intelligence realized on a quantum
processor. Due to the widespreading importance of artificial intelligence and
its tremendous consuming of computational resources, quantum speedup would be
extremely attractive against the challenges from the Big Data."
Two Gaussian Approaches to Black-Box Optomization,"Outline of several strategies for using Gaussian processes as surrogate
models for the covariance matrix adaptation evolution strategy (CMA-ES)."
"Quantifying Morphological Computation based on an Information
  Decomposition of the Sensorimotor Loop","The question how an agent is affected by its embodiment has attracted growing
attention in recent years. A new field of artificial intelligence has emerged,
which is based on the idea that intelligence cannot be understood without
taking into account embodiment. We believe that a formal approach to
quantifying the embodiment's effect on the agent's behaviour is beneficial to
the fields of artificial life and artificial intelligence. The contribution of
an agent's body and environment to its behaviour is also known as morphological
computation. Therefore, in this work, we propose a quantification of
morphological computation, which is based on an information decomposition of
the sensorimotor loop into shared, unique and synergistic information. In
numerical simulation based on a formal representation of the sensorimotor loop,
we show that the unique information of the body and environment is a good
measure for morphological computation. The results are compared to our
previously derived quantification of morphological computation."
SAT as a game,"We propose a funny representation of SAT. While the primary interest is to
present propositional satisfiability in a playful way for pedagogical purposes,
it could also inspire new search heuristics."
A Notation for Markov Decision Processes,This paper specifies a notation for Markov decision processes.
Essence' Description,A description of the Essence' language as used by the tool Savile Row.
A Survey on Artificial Intelligence and Data Mining for MOOCs,"Massive Open Online Courses (MOOCs) have gained tremendous popularity in the
last few years. Thanks to MOOCs, millions of learners from all over the world
have taken thousands of high-quality courses for free. Putting together an
excellent MOOC ecosystem is a multidisciplinary endeavour that requires
contributions from many different fields. Artificial intelligence (AI) and data
mining (DM) are two such fields that have played a significant role in making
MOOCs what they are today. By exploiting the vast amount of data generated by
learners engaging in MOOCs, DM improves our understanding of the MOOC ecosystem
and enables MOOC practitioners to deliver better courses. Similarly, AI,
supported by DM, can greatly improve student experience and learning outcomes.
In this survey paper, we first review the state-of-the-art artificial
intelligence and data mining research applied to MOOCs, emphasising the use of
AI and DM tools and techniques to improve student engagement, learning
outcomes, and our understanding of the MOOC ecosystem. We then offer an
overview of key trends and important research to carry out in the fields of AI
and DM so that MOOCs can reach their full potential."
Unethical Research: How to Create a Malevolent Artificial Intelligence,"Cybersecurity research involves publishing papers about malicious exploits as
much as publishing information on how to design tools to protect
cyber-infrastructure. It is this information exchange between ethical hackers
and security experts, which results in a well-balanced cyber-ecosystem. In the
blooming domain of AI Safety Engineering, hundreds of papers have been
published on different proposals geared at the creation of a safe machine, yet
nothing, to our knowledge, has been published on how to design a malevolent
machine. Availability of such information would be of great value particularly
to computer scientists, mathematicians, and others who have an interest in AI
safety, and who are attempting to avoid the spontaneous emergence or the
deliberate creation of a dangerous AI, which can negatively affect human
activities and in the worst case cause the complete obliteration of the human
species. This paper provides some general guidelines for the creation of a
Malevolent Artificial Intelligence (MAI)."
A Comment on Argumentation,"We use the theory of defaults and their meaning of [GS16] to develop (the
outline of a) new theory of argumentation."
DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker,"Artificial intelligence has seen several breakthroughs in recent years, with
games often serving as milestones. A common feature of these games is that
players have perfect information. Poker is the quintessential game of imperfect
information, and a longstanding challenge problem in artificial intelligence.
We introduce DeepStack, an algorithm for imperfect information settings. It
combines recursive reasoning to handle information asymmetry, decomposition to
focus computation on the relevant decision, and a form of intuition that is
automatically learned from self-play using deep learning. In a study involving
44,000 hands of poker, DeepStack defeated with statistical significance
professional poker players in heads-up no-limit Texas hold'em. The approach is
theoretically sound and is shown to produce more difficult to exploit
strategies than prior approaches."
Entropy Non-increasing Games for the Improvement of Dataflow Programming,"In this article, we introduce a new conception of a family of esport games
called Samu Entropy to try to improve dataflow program graphs like the ones
that are based on Google's TensorFlow. Currently, the Samu Entropy project
specifies only requirements for new esport games to be developed with
particular attention to the investigation of the relationship between esport
and artificial intelligence. It is quite obvious that there is a very close and
natural relationship between esport games and artificial intelligence.
Furthermore, the project Samu Entropy focuses not only on using artificial
intelligence, but on creating AI in a new way. We present a reference game
called Face Battle that implements the Samu Entropy requirements."
"Design of the Artificial: lessons from the biological roots of general
  intelligence","Our fascination with intelligent machines goes back to ancient times with the
mythical automaton Talos, Aristotle's mode of mechanical thought (syllogism)
and Heron of Alexandria's mechanical machines. However, the quest for
Artificial General Intelligence (AGI) has been troubled with repeated failures.
Recently, there has been a shift towards bio-inspired software and hardware,
but their singular design focus makes them inefficient in achieving AGI. Which
set of requirements have to be met in the design of AGI? What are the limits in
the design of the artificial? A careful examination of computation in
biological systems suggests that evolutionary tinkering of contextual
processing of information enabled by a hierarchical architecture is key to
building AGI."
"Explainable Artificial Intelligence: Understanding, Visualizing and
  Interpreting Deep Learning Models","With the availability of large databases and recent improvements in deep
learning methodology, the performance of AI systems is reaching or even
exceeding the human level on an increasing number of complex tasks. Impressive
examples of this development can be found in domains such as image
classification, sentiment analysis, speech understanding or strategic game
playing. However, because of their nested non-linear structure, these highly
successful machine learning and artificial intelligence models are usually
applied in a black box manner, i.e., no information is provided about what
exactly makes them arrive at their predictions. Since this lack of transparency
can be a major drawback, e.g., in medical applications, the development of
methods for visualizing, explaining and interpreting deep learning models has
recently attracted increasing attention. This paper summarizes recent
developments in this field and makes a plea for more interpretability in
artificial intelligence. Furthermore, it presents two approaches to explaining
predictions of deep learning models, one method which computes the sensitivity
of the prediction with respect to changes in the input and one approach which
meaningfully decomposes the decision in terms of the input variables. These
methods are evaluated on three classification tasks."
AI Buzzwords Explained: Multi-Agent Path Finding (MAPF),"Explanation of the hot topic ""multi-agent path finding""."
Coins and Logic,"We establish fun parallels between coin-weighing puzzles and
knights-and-knaves puzzles."
"Innateness, AlphaZero, and Artificial Intelligence","The concept of innateness is rarely discussed in the context of artificial
intelligence. When it is discussed, or hinted at, it is often the context of
trying to reduce the amount of innate machinery in a given system. In this
paper, I consider as a test case a recent series of papers by Silver et al
(Silver et al., 2017a) on AlphaGo and its successors that have been presented
as an argument that a ""even in the most challenging of domains: it is possible
to train to superhuman level, without human examples or guidance"", ""starting
tabula rasa.""
  I argue that these claims are overstated, for multiple reasons. I close by
arguing that artificial intelligence needs greater attention to innateness, and
I point to some proposals about what that innateness might look like."
Blockchain and Artificial Intelligence,"It is undeniable that artificial intelligence (AI) and blockchain concepts
are spreading at a phenomenal rate. Both technologies have distinct degree of
technological complexity and multi-dimensional business implications. However,
a common misunderstanding about blockchain concept, in particular, is that
blockchain is decentralized and is not controlled by anyone. But the underlying
development of a blockchain system is still attributed to a cluster of core
developers. Take smart contract as an example, it is essentially a collection
of codes (or functions) and data (or states) that are programmed and deployed
on a blockchain (say, Ethereum) by different human programmers. It is thus,
unfortunately, less likely to be free of loopholes and flaws. In this article,
through a brief overview about how artificial intelligence could be used to
deliver bug-free smart contract so as to achieve the goal of blockchain 2.0, we
to emphasize that the blockchain implementation can be assisted or enhanced via
various AI techniques. The alliance of AI and blockchain is expected to create
numerous possibilities."
Artificial intelligence and pediatrics: A synthetic mini review,"The use of artificial intelligence intelligencein medicine can be traced back
to 1968 when Paycha published his paper Le diagnostic a l'aide d'intelligences
artificielle, presentation de la premiere machine diagnostri. Few years later
Shortliffe et al. presented an expert system named Mycin which was able to
identify bacteria causing severe blood infections and to recommend antibiotics.
Despite the fact that Mycin outperformed members of the Stanford medical school
in the reliability of diagnosis it was never used in practice due to a legal
issue who do you sue if it gives a wrong diagnosis?. However only in 2016 when
the artificial intelligence software built into the IBM Watson AI platform
correctly diagnosed and proposed an effective treatment for a 60-year-old
womans rare form of leukemia the AI use in medicine become really popular.On of
first papers presenting the use of AI in paediatrics was published in 1984. The
paper introduced a computer-assisted medical decision making system called
SHELP."
The AGINAO Self-Programming Engine,"The AGINAO is a project to create a human-level artificial general
intelligence system (HL AGI) embodied in the Aldebaran Robotics' NAO humanoid
robot. The dynamical and open-ended cognitive engine of the robot is
represented by an embedded and multi-threaded control program, that is
self-crafted rather than hand-crafted, and is executed on a simulated Universal
Turing Machine (UTM). The actual structure of the cognitive engine emerges as a
result of placing the robot in a natural preschool-like environment and running
a core start-up system that executes self-programming of the cognitive layer on
top of the core layer. The data from the robot's sensory devices supplies the
training samples for the machine learning methods, while the commands sent to
actuators enable testing hypotheses and getting a feedback. The individual
self-created subroutines are supposed to reflect the patterns and concepts of
the real world, while the overall program structure reflects the spatial and
temporal hierarchy of the world dependencies. This paper focuses on the details
of the self-programming approach, limiting the discussion of the applied
cognitive architecture to a necessary minimum."
"Experience, Imitation and Reflection; Confucius' Conjecture and Machine
  Learning","Artificial intelligence recently had a great advancements caused by the
emergence of new processing power and machine learning methods. Having said
that, the learning capability of artificial intelligence is still at its
infancy comparing to the learning capability of human and many animals. Many of
the current artificial intelligence applications can only operate in a very
orchestrated, specific environments with an extensive training set that exactly
describes the conditions that will occur during execution time. Having that in
mind, and considering the several existing machine learning methods this
question rises that 'What are some of the best ways for a machine to learn?'
Regarding the learning methods of human, Confucius' point of view is that they
are by experience, imitation and reflection. This paper tries to explore and
discuss regarding these three ways of learning and their implementations in
machines by having a look at how they happen in minds."
"A Tutorial on Modular Ontology Modeling with Ontology Design Patterns:
  The Cooking Recipes Ontology","We provide a detailed example for modular ontology modeling based on ontology
design patterns."
AIR5: Five Pillars of Artificial Intelligence Research,"In this article, we provide and overview of what we consider to be some of
the most pressing research questions facing the fields of artificial
intelligence (AI) and computational intelligence (CI); with the latter focusing
on algorithms that are inspired by various natural phenomena. We demarcate
these questions using five unique Rs - namely, (i) rationalizability, (ii)
resilience, (iii) reproducibility, (iv) realism, and (v) responsibility.
Notably, just as air serves as the basic element of biological life, the term
AIR5 - cumulatively referring to the five aforementioned Rs - is introduced
herein to mark some of the basic elements of artificial life (supporting the
sustained growth of AI and CI). A brief summary of each of the Rs is presented,
highlighting their relevance as pillars of future research in this arena."
Artificial Intelligence for Social Good,"The Computing Community Consortium (CCC), along with the White House Office
of Science and Technology Policy (OSTP), and the Association for the
Advancement of Artificial Intelligence (AAAI), co-sponsored a public workshop
on Artificial Intelligence for Social Good on June 7th, 2016 in Washington, DC.
This was one of five workshops that OSTP co-sponsored and held around the
country to spur public dialogue on artificial intelligence, machine learning,
and to identify challenges and opportunities related to AI. In the AI for
Social Good workshop, the successful deployments and the potential use of AI in
various topics that are essential for social good were discussed, including but
not limited to urban computing, health, environmental sustainability, and
public welfare. This report highlights each of these as well as a number of
crosscutting issues."
"Dungeon Crawl Stone Soup as an Evaluation Domain for Artificial
  Intelligence","Dungeon Crawl Stone Soup is a popular, single-player, free and open-source
rogue-like video game with a sufficiently complex decision space that makes it
an ideal testbed for research in cognitive systems and, more generally,
artificial intelligence. This paper describes the properties of Dungeon Crawl
Stone Soup that are conducive to evaluating new approaches of AI systems. We
also highlight an ongoing effort to build an API for AI researchers in the
spirit of recent game APIs such as MALMO, ELF, and the Starcraft II API.
Dungeon Crawl Stone Soup's complexity offers significant opportunities for
evaluating AI and cognitive systems, including human user studies. In this
paper we provide (1) a description of the state space of Dungeon Crawl Stone
Soup, (2) a description of the components for our API, and (3) the potential
benefits of evaluating AI agents in the Dungeon Crawl Stone Soup video game."
Is AmI (Attacks Meet Interpretability) Robust to Adversarial Examples?,No.
Naive probability,"We describe a rational, but low resolution model of probability."
"The Challenge of Imputation in Explainable Artificial Intelligence
  Models","Explainable models in Artificial Intelligence are often employed to ensure
transparency and accountability of AI systems. The fidelity of the explanations
are dependent upon the algorithms used as well as on the fidelity of the data.
Many real world datasets have missing values that can greatly influence
explanation fidelity. The standard way to deal with such scenarios is
imputation. This can, however, lead to situations where the imputed values may
correspond to a setting which refer to counterfactuals. Acting on explanations
from AI models with imputed values may lead to unsafe outcomes. In this paper,
we explore different settings where AI models with imputation can be
problematic and describe ways to address such scenarios."
"A 20-Year Community Roadmap for Artificial Intelligence Research in the
  US","Decades of research in artificial intelligence (AI) have produced formidable
technologies that are providing immense benefit to industry, government, and
society. AI systems can now translate across multiple languages, identify
objects in images and video, streamline manufacturing processes, and control
cars. The deployment of AI systems has not only created a trillion-dollar
industry that is projected to quadruple in three years, but has also exposed
the need to make AI systems fair, explainable, trustworthy, and secure. Future
AI systems will rightfully be expected to reason effectively about the world in
which they (and people) operate, handling complex tasks and responsibilities
effectively and ethically, engaging in meaningful communication, and improving
their awareness through experience.
  Achieving the full potential of AI technologies poses research challenges
that require a radical transformation of the AI research enterprise,
facilitated by significant and sustained investment. These are the major
recommendations of a recent community effort coordinated by the Computing
Community Consortium and the Association for the Advancement of Artificial
Intelligence to formulate a Roadmap for AI research and development over the
next two decades."
Towards a Rigorous Evaluation of XAI Methods on Time Series,"Explainable Artificial Intelligence (XAI) methods are typically deployed to
explain and debug black-box machine learning models. However, most proposed XAI
methods are black-boxes themselves and designed for images. Thus, they rely on
visual interpretability to evaluate and prove explanations. In this work, we
apply XAI methods previously used in the image and text-domain on time series.
We present a methodology to test and evaluate various XAI methods on time
series by introducing new verification techniques to incorporate the temporal
dimension. We further conduct preliminary experiments to assess the quality of
selected XAI method explanations with various verification methods on a range
of datasets and inspecting quality metrics on it. We demonstrate that in our
initial experiments, SHAP works robust for all models, but others like
DeepLIFT, LRP, and Saliency Maps work better with specific architectures."
BMVC 2019: Workshop on Interpretable and Explainable Machine Vision,"Proceedings of the BMVC 2019 Workshop on Interpretable and Explainable
Machine Vision, Cardiff, UK, September 12, 2019."
AIBA: An AI Model for Behavior Arbitration in Autonomous Driving,"Driving in dynamically changing traffic is a highly challenging task for
autonomous vehicles, especially in crowded urban roadways. The Artificial
Intelligence (AI) system of a driverless car must be able to arbitrate between
different driving strategies in order to properly plan the car's path, based on
an understandable traffic scene model. In this paper, an AI behavior
arbitration algorithm for Autonomous Driving (AD) is proposed. The method,
coined AIBA (AI Behavior Arbitration), has been developed in two stages: (i)
human driving scene description and understanding and (ii) formal modelling.
The description of the scene is achieved by mimicking a human cognition model,
while the modelling part is based on a formal representation which approximates
the human driver understanding process. The advantage of the formal
representation is that the functional safety of the system can be analytically
inferred. The performance of the algorithm has been evaluated in Virtual Test
Drive (VTD), a comprehensive traffic simulator, and in GridSim, a vehicle
kinematics engine for prototypes."
A Short Remark on Analogical Reasoning,"We discuss the problem of defining a logic for analogical reasoning, and
sketch a solution in the style of the semantics for Counterfactual
Conditionals, Preferential Structures, etc."
Evolving Structures in Complex Systems,"In this paper we propose an approach for measuring growth of complexity of
emerging patterns in complex systems such as cellular automata. We discuss
several ways how a metric for measuring the complexity growth can be defined.
This includes approaches based on compression algorithms and artificial neural
networks. We believe such a metric can be useful for designing systems that
could exhibit open-ended evolution, which itself might be a prerequisite for
development of general artificial intelligence. We conduct experiments on 1D
and 2D grid worlds and demonstrate that using the proposed metric we can
automatically construct computational models with emerging properties similar
to those found in the Conway's Game of Life, as well as many other emergent
phenomena. Interestingly, some of the patterns we observe resemble forms of
artificial life. Our metric of structural complexity growth can be applied to a
wide range of complex systems, as it is not limited to cellular automata."
"Textual analysis of artificial intelligence manuscripts reveals features
  associated with peer review outcome","We analysed a dataset of scientific manuscripts that were submitted to
various conferences in artificial intelligence. We performed a combination of
semantic, lexical and psycholinguistic analyses of the full text of the
manuscripts and compared them with the outcome of the peer review process. We
found that accepted manuscripts scored lower than rejected manuscripts on two
indicators of readability, and that they also used more scientific and
artificial intelligence jargon. We also found that accepted manuscripts were
written with words that are less frequent, that are acquired at an older age,
and that are more abstract than rejected manuscripts. The analysis of
references included in the manuscripts revealed that the subset of accepted
submissions were more likely to cite the same publications. This finding was
echoed by pairwise comparisons of the word content of the manuscripts (i.e. an
indicator or semantic similarity), which were more similar in the subset of
accepted manuscripts. Finally, we predicted the peer review outcome of
manuscripts with their word content, with words related to machine learning and
neural networks positively related with acceptance, whereas words related to
logic, symbolic processing and knowledge-based systems negatively related with
acceptance."
Self-Learned Formula Synthesis in Set Theory,"A reinforcement learning algorithm accomplishes the task of synthesizing a
set-theoretical formula that evaluates to given truth values for given
assignments."
On the Morality of Artificial Intelligence,"Much of the existing research on the social and ethical impact of Artificial
Intelligence has been focused on defining ethical principles and guidelines
surrounding Machine Learning (ML) and other Artificial Intelligence (AI)
algorithms [IEEE, 2017, Jobin et al., 2019]. While this is extremely useful for
helping define the appropriate social norms of AI, we believe that it is
equally important to discuss both the potential and risks of ML and to inspire
the community to use ML for beneficial objectives. In the present article,
which is specifically aimed at ML practitioners, we thus focus more on the
latter, carrying out an overview of existing high-level ethical frameworks and
guidelines, but above all proposing both conceptual and practical principles
and guidelines for ML research and deployment, insisting on concrete actions
that can be taken by practitioners to pursue a more ethical and moral practice
of ML aimed at using AI for social good."
"Exciting, Useful, Worrying, Futuristic: Public Perception of Artificial
  Intelligence in 8 Countries","As the influence and use of artificial intelligence (AI) have grown and its
transformative potential has become more apparent, many questions have been
raised regarding the economic, political, social, and ethical implications of
its use. Public opinion plays an important role in these discussions,
influencing product adoption, commercial development, research funding, and
regulation. In this paper we present results of an in-depth survey of public
opinion of artificial intelligence conducted with 10,005 respondents spanning
eight countries and six continents. We report widespread perception that AI
will have significant impact on society, accompanied by strong support for the
responsible development and use of AI, and also characterize the public's
sentiment towards AI with four key themes (exciting, useful, worrying, and
futuristic) whose prevalence distinguishes response to AI in different
countries."
Artificial Intelligence for Social Good: A Survey,"Artificial intelligence for social good (AI4SG) is a research theme that aims
to use and advance artificial intelligence to address societal issues and
improve the well-being of the world. AI4SG has received lots of attention from
the research community in the past decade with several successful applications.
Building on the most comprehensive collection of the AI4SG literature to date
with over 1000 contributed papers, we provide a detailed account and analysis
of the work under the theme in the following ways. (1) We quantitatively
analyze the distribution and trend of the AI4SG literature in terms of
application domains and AI techniques used. (2) We propose three conceptual
methods to systematically group the existing literature and analyze the eight
AI4SG application domains in a unified framework. (3) We distill five research
topics that represent the common challenges in AI4SG across various application
domains. (4) We discuss five issues that, we hope, can shed light on the future
development of the AI4SG research."
On Safety Assessment of Artificial Intelligence,"In this paper we discuss how systems with Artificial Intelligence (AI) can
undergo safety assessment. This is relevant, if AI is used in safety related
applications. Taking a deeper look into AI models, we show, that many models of
artificial intelligence, in particular machine learning, are statistical
models. Safety assessment would then have t o concentrate on the model that is
used in AI, besides the normal assessment procedure. Part of the budget of
dangerous random failures for the relevant safety integrity level needs to be
used for the probabilistic faulty behavior of the AI system. We demonstrate our
thoughts with a simple example and propose a research challenge that may be
decisive for the use of AI in safety related systems."
"CAAI -- A Cognitive Architecture to Introduce Artificial Intelligence in
  Cyber-Physical Production Systems","This paper introduces CAAI, a novel cognitive architecture for artificial
intelligence in cyber-physical production systems. The goal of the architecture
is to reduce the implementation effort for the usage of artificial intelligence
algorithms. The core of the CAAI is a cognitive module that processes
declarative goals of the user, selects suitable models and algorithms, and
creates a configuration for the execution of a processing pipeline on a big
data platform. Constant observation and evaluation against performance criteria
assess the performance of pipelines for many and varying use cases. Based on
these evaluations, the pipelines are automatically adapted if necessary. The
modular design with well-defined interfaces enables the reusability and
extensibility of pipeline components. A big data platform implements this
modular design supported by technologies such as Docker, Kubernetes, and Kafka
for virtualization and orchestration of the individual components and their
communication. The implementation of the architecture is evaluated using a
real-world use case."
"Digital Collaborator: Augmenting Task Abstraction in Visualization
  Design with Artificial Intelligence","In the task abstraction phase of the visualization design process, including
in ""design studies"", a practitioner maps the observed domain goals to
generalizable abstract tasks using visualization theory in order to better
understand and address the users needs. We argue that this manual task
abstraction process is prone to errors due to designer biases and a lack of
domain background and knowledge. Under these circumstances, a collaborator can
help validate and provide sanity checks to visualization practitioners during
this important task abstraction stage. However, having a human collaborator is
not always feasible and may be subject to the same biases and pitfalls. In this
paper, we first describe the challenges associated with task abstraction. We
then propose a conceptual Digital Collaborator: an artificial intelligence
system that aims to help visualization practitioners by augmenting their
ability to validate and reason about the output of task abstraction. We also
discuss several practical design challenges of designing and implementing such
systems"
Foundations of Explainable Knowledge-Enabled Systems,"Explainability has been an important goal since the early days of Artificial
Intelligence. Several approaches for producing explanations have been
developed. However, many of these approaches were tightly coupled with the
capabilities of the artificial intelligence systems at the time. With the
proliferation of AI-enabled systems in sometimes critical settings, there is a
need for them to be explainable to end-users and decision-makers. We present a
historical overview of explainable artificial intelligence systems, with a
focus on knowledge-enabled systems, spanning the expert systems, cognitive
assistants, semantic applications, and machine learning domains. Additionally,
borrowing from the strengths of past approaches and identifying gaps needed to
make explanations user- and context-focused, we propose new definitions for
explanations and explainable knowledge-enabled systems."
"Ortus: an Emotion-Driven Approach to (artificial) Biological
  Intelligence","Ortus is a simple virtual organism that also serves as an initial framework
for investigating and developing biologically-based artificial intelligence.
Born from a goal to create complex virtual intelligence and an initial attempt
to model C. elegans, Ortus implements a number of mechanisms observed in
organic nervous systems, and attempts to fill in unknowns based upon plausible
biological implementations and psychological observations. Implemented
mechanisms include excitatory and inhibitory chemical synapses, bidirectional
gap junctions, and Hebbian learning with its Stentian extension. We present an
initial experiment that showcases Ortus' fundamental principles; specifically,
a cyclic respiratory circuit, and emotionally-driven associative learning with
respect to an input stimulus. Finally, we discuss the implications and future
directions for Ortus and similar systems."
"A Standardized Radiograph-Agnostic Framework and Platform For Evaluating
  AI Radiological Systems","Radiology has been essential to accurately diagnosing diseases and assessing
responses to treatment. The challenge however lies in the shortage of
radiologists globally. As a response to this, a number of Artificial
Intelligence solutions are being developed. The challenge Artificial
Intelligence radiological solutions however face is the lack of a benchmarking
and evaluation standard, and the difficulties of collecting diverse data to
truly assess the ability of such systems to generalise and properly handle edge
cases. We are proposing a radiograph-agnostic platform and framework that would
allow any Artificial Intelligence radiological solution to be assessed on its
ability to generalise across diverse geographical location, gender and age
groups."
"Qualitative Investigation in Explainable Artificial Intelligence: A Bit
  More Insight from Social Science","We present a focused analysis of user studies in explainable artificial
intelligence (XAI) entailing qualitative investigation. We draw on social
science corpora to suggest ways for improving the rigor of studies where XAI
researchers use observations, interviews, focus groups, and/or questionnaires
to capture qualitative data. We contextualize the presentation of the XAI
papers included in our analysis according to the components of rigor described
in the qualitative research literature: 1) underlying theories or frameworks,
2) methodological approaches, 3) data collection methods, and 4) data analysis
processes. The results of our analysis support calls from others in the XAI
community advocating for collaboration with experts from social disciplines to
bolster rigor and effectiveness in user studies."
"Explainable Artificial Intelligence Recommendation System by Leveraging
  the Semantics of Adverse Childhood Experiences: Proof-of-Concept Prototype
  Development","The study of adverse childhood experiences and their consequences has emerged
over the past 20 years. In this study, we aimed to leverage explainable
artificial intelligence, and propose a proof-of-concept prototype for a
knowledge-driven evidence-based recommendation system to improve surveillance
of adverse childhood experiences. We used concepts from an ontology that we
have developed to build and train a question-answering agent using the Google
DialogFlow engine. In addition to the question-answering agent, the initial
prototype includes knowledge graph generation and recommendation components
that leverage third-party graph technology. To showcase the framework
functionalities, we here present a prototype design and demonstrate the main
features through four use case scenarios motivated by an initiative currently
implemented at a children hospital in Memphis, Tennessee. Ongoing development
of the prototype requires implementing an optimization algorithm of the
recommendations, incorporating a privacy layer through a personal health
library, and conducting a clinical trial to assess both usability and
usefulness of the implementation. This semantic-driven explainable artificial
intelligence prototype can enhance health care practitioners ability to provide
explanations for the decisions they make."
Measuring Happiness Around the World Through Artificial Intelligence,"In this work, we analyze the happiness levels of countries using an unbiased
emotion detector, artificial intelligence (AI). To date, researchers proposed
many factors that may affect happiness such as wealth, health and safety. Even
though these factors all seem relevant, there is no clear consensus between
sociologists on how to interpret these, and the models to estimate the cost of
these utilities include some assumptions. Researchers in social sciences have
been working on determination of the happiness levels in society and
exploration of the factors correlated with it through polls and different
statistical methods. In our work, by using artificial intelligence, we
introduce a different and relatively unbiased approach to this problem. By
using AI, we make no assumption about what makes a person happy, and leave the
decision to AI to detect the emotions from the faces of people collected from
publicly available street footages. We analyzed the happiness levels in eight
different cities around the world through available footage on the Internet and
found out that there is no statistically significant difference between
countries in terms of happiness."
Artificial Intelligence ordered 3D vertex importance,"Ranking vertices of multidimensional networks is crucial in many areas of
research, including selecting and determining the importance of decisions. Some
decisions are significantly more important than others, and their weight
categorization is also imortant. This paper defines a completely new method for
determining the weight decisions using artificial intelligence for importance
ranking of three-dimensional network vertices, improving the existing Ordered
Statistics Vertex Extraction and Tracking Algorithm (OSVETA) based on
modulation of quantized indices (QIM) and error correction codes. The technique
we propose in this paper offers significant improvements the efficiency of
determination the importance of network vertices in relation to statistical
OSVETA criteria, replacing heuristic methods with methods of precise prediction
of modern neural networks. The new artificial intelligence technique enables a
significantly better definition of the 3D meshes and a better assessment of
their topological features. The new method contributions result in a greater
precision in defining stable vertices, significantly reducing the probability
of deleting mesh vertices."
Explainable Artificial Intelligence (XAI): An Engineering Perspective,"The remarkable advancements in Deep Learning (DL) algorithms have fueled
enthusiasm for using Artificial Intelligence (AI) technologies in almost every
domain; however, the opaqueness of these algorithms put a question mark on
their applications in safety-critical systems. In this regard, the
`explainability' dimension is not only essential to both explain the inner
workings of black-box algorithms, but it also adds accountability and
transparency dimensions that are of prime importance for regulators, consumers,
and service providers. eXplainable Artificial Intelligence (XAI) is the set of
techniques and methods to convert the so-called black-box AI algorithms to
white-box algorithms, where the results achieved by these algorithms and the
variables, parameters, and steps taken by the algorithm to reach the obtained
results, are transparent and explainable. To complement the existing literature
on XAI, in this paper, we take an `engineering' approach to illustrate the
concepts of XAI. We discuss the stakeholders in XAI and describe the
mathematical contours of XAI from engineering perspective. Then we take the
autonomous car as a use-case and discuss the applications of XAI for its
different components such as object detection, perception, control, action
decision, and so on. This work is an exploratory study to identify new avenues
of research in the field of XAI."
HySTER: A Hybrid Spatio-Temporal Event Reasoner,"The task of Video Question Answering (VideoQA) consists in answering natural
language questions about a video and serves as a proxy to evaluate the
performance of a model in scene sequence understanding. Most methods designed
for VideoQA up-to-date are end-to-end deep learning architectures which
struggle at complex temporal and causal reasoning and provide limited
transparency in reasoning steps. We present the HySTER: a Hybrid
Spatio-Temporal Event Reasoner to reason over physical events in videos. Our
model leverages the strength of deep learning methods to extract information
from video frames with the reasoning capabilities and explainability of
symbolic artificial intelligence in an answer set programming framework. We
define a method based on general temporal, causal and physics rules which can
be transferred across tasks. We apply our model to the CLEVRER dataset and
demonstrate state-of-the-art results in question answering accuracy. This work
sets the foundations for the incorporation of inductive logic programming in
the field of VideoQA."
"Adversarial Attacks for Tabular Data: Application to Fraud Detection and
  Imbalanced Data","Guaranteeing the security of transactional systems is a crucial priority of
all institutions that process transactions, in order to protect their
businesses against cyberattacks and fraudulent attempts. Adversarial attacks
are novel techniques that, other than being proven to be effective to fool
image classification models, can also be applied to tabular data. Adversarial
attacks aim at producing adversarial examples, in other words, slightly
modified inputs that induce the Artificial Intelligence (AI) system to return
incorrect outputs that are advantageous for the attacker. In this paper we
illustrate a novel approach to modify and adapt state-of-the-art algorithms to
imbalanced tabular data, in the context of fraud detection. Experimental
results show that the proposed modifications lead to a perfect attack success
rate, obtaining adversarial examples that are also less perceptible when
analyzed by humans. Moreover, when applied to a real-world production system,
the proposed techniques shows the possibility of posing a serious threat to the
robustness of advanced AI-based fraud detection procedures."
"Neural Networks, Artificial Intelligence and the Computational Brain","In recent years, several studies have provided insight on the functioning of
the brain which consists of neurons and form networks via interconnection among
them by synapses. Neural networks are formed by interconnected systems of
neurons, and are of two types, namely, the Artificial Neural Network (ANNs) and
Biological Neural Network (interconnected nerve cells). The ANNs are
computationally influenced by human neurons and are used in modelling neural
systems. The reasoning foundations of ANNs have been useful in anomaly
detection, in areas of medicine such as instant physician, electronic noses,
pattern recognition, and modelling biological systems. Advancing research in
artificial intelligence using the architecture of the human brain seeks to
model systems by studying the brain rather than looking to technology for brain
models. This study explores the concept of ANNs as a simulator of the
biological neuron, and its area of applications. It also explores why
brain-like intelligence is needed and how it differs from computational
framework by comparing neural networks to contemporary computers and their
modern day implementation."
Explainable Artificial Intelligence Approaches: A Survey,"The lack of explainability of a decision from an Artificial Intelligence (AI)
based ""black box"" system/model, despite its superiority in many real-world
applications, is a key stumbling block for adopting AI in many high stakes
applications of different domain or industry. While many popular Explainable
Artificial Intelligence (XAI) methods or approaches are available to facilitate
a human-friendly explanation of the decision, each has its own merits and
demerits, with a plethora of open challenges. We demonstrate popular XAI
methods with a mutual case study/task (i.e., credit default prediction),
analyze for competitive advantages from multiple perspectives (e.g., local,
global), provide meaningful insight on quantifying explainability, and
recommend paths towards responsible or human-centered AI using XAI as a medium.
Practitioners can use this work as a catalog to understand, compare, and
correlate competitive advantages of popular XAI methods. In addition, this
survey elicits future research directions towards responsible or human-centric
AI systems, which is crucial to adopt AI in high stakes applications."
"Partial Differential Equations is All You Need for Generating Neural
  Architectures -- A Theory for Physical Artificial Intelligence Systems","In this work, we generalize the reaction-diffusion equation in statistical
physics, Schr\""odinger equation in quantum mechanics, Helmholtz equation in
paraxial optics into the neural partial differential equations (NPDE), which
can be considered as the fundamental equations in the field of artificial
intelligence research. We take finite difference method to discretize NPDE for
finding numerical solution, and the basic building blocks of deep neural
network architecture, including multi-layer perceptron, convolutional neural
network and recurrent neural networks, are generated. The learning strategies,
such as Adaptive moment estimation, L-BFGS, pseudoinverse learning algorithms
and partial differential equation constrained optimization, are also presented.
We believe it is of significance that presented clear physical image of
interpretable deep neural networks, which makes it be possible for applying to
analog computing device design, and pave the road to physical artificial
intelligence."
"Towards a New Participatory Approach for Designing Artificial
  Intelligence and Data-Driven Technologies","With there being many technical and ethical issues with artificial
intelligence (AI) that involve marginalized communities, there is a growing
interest for design methods used with marginalized people that may be
transferable to the design of AI technologies. Participatory design (PD) is a
design method that is often used with marginalized communities for the design
of social development, policy, IT and other matters and solutions. However,
there are issues with the current PD, raising concerns when it is applied to
the design of technologies, including AI technologies. This paper argues for
the use of PD for the design of AI technologies, and introduces and proposes a
new PD, which we call agile participatory design, that not only can could be
used for the design of AI and data-driven technologies, but also overcomes
issues surrounding current PD and its use in the design of such technologies."
"Secure Artificial Intelligence of Things for Implicit Group
  Recommendations","The emergence of Artificial Intelligence of Things (AIoT) has provided novel
insights for many social computing applications such as group recommender
systems. As distance among people has been greatly shortened, it has been a
more general demand to provide personalized services to groups instead of
individuals. In order to capture group-level preference features from
individuals, existing methods were mostly established via aggregation and face
two aspects of challenges: secure data management workflow is absent, and
implicit preference feedbacks is ignored. To tackle current difficulties, this
paper proposes secure Artificial Intelligence of Things for implicit Group
Recommendations (SAIoT-GR). As for hardware module, a secure IoT structure is
developed as the bottom support platform. As for software module, collaborative
Bayesian network model and non-cooperative game are can be introduced as
algorithms. Such a secure AIoT architecture is able to maximize the advantages
of the two modules. In addition, a large number of experiments are carried out
to evaluate the performance of the SAIoT-GR in terms of efficiency and
robustness."
"XAI-KG: knowledge graph to support XAI and decision-making in
  manufacturing","The increasing adoption of artificial intelligence requires accurate
forecasts and means to understand the reasoning of artificial intelligence
models behind such a forecast. Explainable Artificial Intelligence (XAI) aims
to provide cues for why a model issued a certain prediction. Such cues are of
utmost importance to decision-making since they provide insights on the
features that influenced most certain forecasts and let the user decide if the
forecast can be trusted. Though many techniques were developed to explain
black-box models, little research was done on assessing the quality of those
explanations and their influence on decision-making. We propose an ontology and
knowledge graph to support collecting feedback regarding forecasts, forecast
explanations, recommended decision-making options, and user actions. This way,
we provide means to improve forecasting models, explanations, and
recommendations of decision-making options. We tailor the knowledge graph for
the domain of demand forecasting and validate it on real-world data."
"Online Trading Models with Deep Reinforcement Learning in the Forex
  Market Considering Transaction Costs","In recent years, a wide range of investment models have been created using
artificial intelligence. Automatic trading by artificial intelligence can
expand the range of trading methods, such as by conferring the ability to
operate 24 hours a day and the ability to trade with high frequency. Automatic
trading can also be expected to trade with more information than is available
to humans if it can sufficiently consider past data. In this paper, we propose
an investment agent based on a deep reinforcement learning model, which is an
artificial intelligence model. The model considers the transaction costs
involved in actual trading and creates a framework for trading over a long
period of time so that it can make a large profit on a single trade. In doing
so, it can maximize the profit while keeping transaction costs low. In
addition, in consideration of actual operations, we use online learning so that
the system can continue to learn by constantly updating the latest online data
instead of learning with static data. This makes it possible to trade in
non-stationary financial markets by always incorporating current market trend
information."
Artificial Intelligence in Drug Discovery: Applications and Techniques,"Artificial intelligence (AI) has been transforming the practice of drug
discovery in the past decade. Various AI techniques have been used in a wide
range of applications, such as virtual screening and drug design. In this
survey, we first give an overview on drug discovery and discuss related
applications, which can be reduced to two major tasks, i.e., molecular property
prediction and molecule generation. We then discuss common data resources,
molecule representations and benchmark platforms. Furthermore, to summarize the
progress of AI in drug discovery, we present the relevant AI techniques
including model architectures and learning paradigms in the papers surveyed. We
expect that this survey will serve as a guide for researchers who are
interested in working at the interface of artificial intelligence and drug
discovery. We also provide a GitHub repository
(https://github.com/dengjianyuan/Survey_AI_Drug_Discovery) with the collection
of papers and codes, if applicable, as a learning resource, which is regularly
updated."
Proof of the impossibility of probabilistic induction,"In this short note I restate and simplify the proof of the impossibility of
probabilistic induction from Popper (1992). Other proofs are possible (cf.
Popper (1985))."
QKSA: Quantum Knowledge Seeking Agent,"In this article we present the motivation and the core thesis towards the
implementation of a Quantum Knowledge Seeking Agent (QKSA). QKSA is a general
reinforcement learning agent that can be used to model classical and quantum
dynamics. It merges ideas from universal artificial general intelligence,
constructor theory and genetic programming to build a robust and general
framework for testing the capabilities of the agent in a variety of
environments. It takes the artificial life (or, animat) path to artificial
general intelligence where a population of intelligent agents are instantiated
to explore valid ways of modelling the perceptions. The multiplicity and
survivability of the agents are defined by the fitness, with respect to the
explainability and predictability, of a resource-bounded computational model of
the environment. This general learning approach is then employed to model the
physics of an environment based on subjective observer states of the agents. A
specific case of quantum process tomography as a general modelling principle is
presented. The various background ideas and a baseline formalism are discussed
in this article which sets the groundwork for the implementations of the QKSA
that are currently in active development."
A Review of Explainable Artificial Intelligence in Manufacturing,"The implementation of Artificial Intelligence (AI) systems in the
manufacturing domain enables higher production efficiency, outstanding
performance, and safer operations, leveraging powerful tools such as deep
learning and reinforcement learning techniques. Despite the high accuracy of
these models, they are mostly considered black boxes: they are unintelligible
to the human. Opaqueness affects trust in the system, a factor that is critical
in the context of decision-making. We present an overview of Explainable
Artificial Intelligence (XAI) techniques as a means of boosting the
transparency of models. We analyze different metrics to evaluate these
techniques and describe several application scenarios in the manufacturing
domain."
"Towards artificial general intelligence via a multimodal foundation
  model","The fundamental goal of artificial intelligence (AI) is to mimic the core
cognitive activities of human. Despite tremendous success in the AI research,
most of existing methods have only single-cognitive ability. To overcome this
limitation and take a solid step towards artificial general intelligence (AGI),
we develop a foundation model pre-trained with huge multimodal data, which can
be quickly adapted for various downstream cognitive tasks. To achieve this
goal, we propose to pre-train our foundation model by self-supervised learning
with weak semantic correlation data crawled from the Internet and show that
promising results can be obtained on a wide range of downstream tasks.
Particularly, with the developed model-interpretability tools, we demonstrate
that strong imagination ability is now possessed by our foundation model. We
believe that our work makes a transformative stride towards AGI, from our
common practice of ""weak or narrow AI"" to that of ""strong or generalized AI""."
"Artificial Intelligence, Surveillance, and Big Data","The most important resource to improve technologies in the field of
artificial intelligence is data. Two types of policies are crucial in this
respect: privacy and data-sharing regulations, and the use of surveillance
technologies for policing. Both types of policies vary substantially across
countries and political regimes. In this chapter, we examine how authoritarian
and democratic political institutions can influence the quality of research in
artificial intelligence, and the availability of large-scale datasets to
improve and train deep learning algorithms. We focus mainly on the Chinese
case, and find that -- ceteris paribus -- authoritarian political institutions
continue to have a negative effect on innovation. They can, however, have a
positive effect on research in deep learning, via the availability of
large-scale datasets that have been obtained through government surveillance.
We propose a research agenda to study which of the two effects might dominate
in a race for leadership in artificial intelligence between countries with
different political institutions, such as the United States and China."
Mesarovician Abstract Learning Systems,"The solution methods used to realize artificial general intelligence (AGI)
may not contain the formalism needed to adequately model and characterize AGI.
In particular, current approaches to learning hold notions of problem domain
and problem task as fundamental precepts, but it is hardly apparent that an AGI
encountered in the wild will be discernable into a set of domain-task pairings.
Nor is it apparent that the outcomes of AGI in a system can be well expressed
in terms of domain and task, or as consequences thereof. Thus, there is both a
practical and theoretical use for meta-theories of learning which do not
express themselves explicitly in terms of solution methods. General systems
theory offers such a meta-theory. Herein, Mesarovician abstract systems theory
is used as a super-structure for learning. Abstract learning systems are
formulated. Subsequent elaboration stratifies the assumptions of learning
systems into a hierarchy and considers the hierarchy such stratification
projects onto learning theory. The presented Mesarovician abstract learning
systems theory calls back to the founding motivations of artificial
intelligence research by focusing on the thinking participants directly, in
this case, learning systems, in contrast to the contemporary focus on the
problems thinking participants solve."
"Application of Artificial Intelligence and Machine Learning in
  Libraries: A Systematic Review","As the concept and implementation of cutting-edge technologies like
artificial intelligence and machine learning has become relevant, academics,
researchers and information professionals involve research in this area. The
objective of this systematic literature review is to provide a synthesis of
empirical studies exploring application of artificial intelligence and machine
learning in libraries. To achieve the objectives of the study, a systematic
literature review was conducted based on the original guidelines proposed by
Kitchenham et al. (2009). Data was collected from Web of Science, Scopus, LISA
and LISTA databases. Following the rigorous/ established selection process, a
total of thirty-two articles were finally selected, reviewed and analyzed to
summarize on the application of AI and ML domain and techniques which are most
often used in libraries. Findings show that the current state of the AI and ML
research that is relevant with the LIS domain mainly focuses on theoretical
works. However, some researchers also emphasized on implementation projects or
case studies. This study will provide a panoramic view of AI and ML in
libraries for researchers, practitioners and educators for furthering the more
technology-oriented approaches, and anticipating future innovation pathways."
"A Critical Review of Inductive Logic Programming Techniques for
  Explainable AI","Despite recent advances in modern machine learning algorithms, the opaqueness
of their underlying mechanisms continues to be an obstacle in adoption. To
instill confidence and trust in artificial intelligence systems, Explainable
Artificial Intelligence has emerged as a response to improving modern machine
learning algorithms' explainability. Inductive Logic Programming (ILP), a
subfield of symbolic artificial intelligence, plays a promising role in
generating interpretable explanations because of its intuitive logic-driven
framework. ILP effectively leverages abductive reasoning to generate
explainable first-order clausal theories from examples and background
knowledge. However, several challenges in developing methods inspired by ILP
need to be addressed for their successful application in practice. For example,
existing ILP systems often have a vast solution space, and the induced
solutions are very sensitive to noises and disturbances. This survey paper
summarizes the recent advances in ILP and a discussion of statistical
relational learning and neural-symbolic algorithms, which offer synergistic
views to ILP. Following a critical review of the recent advances, we delineate
observed challenges and highlight potential avenues of further ILP-motivated
research toward developing self-explanatory artificial intelligence systems."
"Integrating Artificial Intelligence and Augmented Reality in Robotic
  Surgery: An Initial dVRK Study Using a Surgical Education Scenario","Robot-assisted surgery has become progressively more and more popular due to
its clinical advantages. In the meanwhile, the artificial intelligence and
augmented reality in robotic surgery are developing rapidly and receive lots of
attention. However, current methods have not discussed the coherent integration
of AI and AR in robotic surgery. In this paper, we develop a novel system by
seamlessly merging artificial intelligence module and augmented reality
visualization to automatically generate the surgical guidance for robotic
surgery education. Specifically, we first leverage reinforcement leaning to
learn from expert demonstration and then generate 3D guidance trajectory,
providing prior context information of the surgical procedure. Along with other
information such as text hint, the 3D trajectory is then overlaid in the stereo
view of dVRK, where the user can perceive the 3D guidance and learn the
procedure. The proposed system is evaluated through a preliminary experiment on
surgical education task peg-transfer, which proves its feasibility and
potential as the next generation of robot-assisted surgery education solution."
"An Experience Report of Executive-Level Artificial Intelligence
  Education in the United Arab Emirates","Teaching artificial intelligence (AI) is challenging. It is a fast moving
field and therefore difficult to keep people updated with the state-of-the-art.
Educational offerings for students are ever increasing, beyond university
degree programs where AI education traditionally lay. In this paper, we present
an experience report of teaching an AI course to business executives in the
United Arab Emirates (UAE). Rather than focusing only on theoretical and
technical aspects, we developed a course that teaches AI with a view to
enabling students to understand how to incorporate it into existing business
processes. We present an overview of our course, curriculum and teaching
methods, and we discuss our reflections on teaching adult learners, and to
students in the UAE."
"Machine Learning and Artificial Intelligence in Next-Generation Wireless
  Network","Due to the advancement in technologies, the next-generation wireless network
will be very diverse, complicated, and according to the changed demands of the
consumers. The current network operator methodologies and approaches are
traditional and cannot help the next generation networks to utilize their
resources most appropriately. The limited capability of the traditional tools
will not allow the network providers to fulfill the demands of the network's
subscribers in the future. Therefore, this paper will focus on machine
learning, automation, artificial intelligence, and big data analytics for
improving the capacity and effectiveness of next-generation wireless networks.
The paper will discuss the role of these new technologies in improving the
service and performance of the network providers in the future. The paper will
find out that machine learning, big data analytics, and artificial intelligence
will help in making the next-generation wireless network self-adaptive,
self-aware, prescriptive, and proactive. At the end of the paper, it will be
provided that future wireless network operators cannot work without shifting
their operational framework to AI and machine learning technologies."
AI Research Associate for Early-Stage Scientific Discovery,"Artificial intelligence (AI) has been increasingly applied in scientific
activities for decades; however, it is still far from an insightful and
trustworthy collaborator in the scientific process. Most existing AI methods
are either too simplistic to be useful in real problems faced by scientists or
too domain-specialized (even dogmatized), stifling transformative discoveries
or paradigm shifts. We present an AI research associate for early-stage
scientific discovery based on (a) a novel minimally-biased ontology for
physics-based modeling that is context-aware, interpretable, and generalizable
across classical and relativistic physics; (b) automatic search for viable and
parsimonious hypotheses, represented at a high-level (via domain-agnostic
constructs) with built-in invariants, e.g., postulated forms of conservation
principles implied by a presupposed spacetime topology; and (c) automatic
compilation of the enumerated hypotheses to domain-specific, interpretable, and
trainable/testable tensor-based computation graphs to learn phenomenological
relations, e.g., constitutive or material laws, from sparse (and possibly
noisy) data sets."
"Human-Centric Artificial Intelligence Architecture for Industry 5.0
  Applications","Human-centricity is the core value behind the evolution of manufacturing
towards Industry 5.0. Nevertheless, there is a lack of architecture that
considers safety, trustworthiness, and human-centricity at its core. Therefore,
we propose an architecture that integrates Artificial Intelligence (Active
Learning, Forecasting, Explainable Artificial Intelligence), simulated reality,
decision-making, and users' feedback, focusing on synergies between humans and
machines. Furthermore, we align the proposed architecture with the Big Data
Value Association Reference Architecture Model. Finally, we validate it on
three use cases from real-world case studies."
"Deep Learning and Artificial General Intelligence: Still a Long Way to
  Go","In recent years, deep learning using neural network architecture, i.e. deep
neural networks, has been on the frontier of computer science research. It has
even lead to superhuman performance in some problems, e.g., in computer vision,
games and biology, and as a result the term deep learning revolution was
coined. The undisputed success and rapid growth of deep learning suggests that,
in future, it might become an enabler for Artificial General Intelligence
(AGI). In this article, we approach this statement critically showing five
major reasons of why deep neural networks, as of the current state, are not
ready to be the technique of choice for reaching AGI."
Cycle-GAN for eye-tracking,"This manuscript presents a not typical implementation of the cycle generative
adversarial networks (Cycle-GAN) method for eye-tracking tasks."
"Towards Responsible AI: A Design Space Exploration of Human-Centered
  Artificial Intelligence User Interfaces to Investigate Fairness","With Artificial intelligence (AI) to aid or automate decision-making
advancing rapidly, a particular concern is its fairness. In order to create
reliable, safe and trustworthy systems through human-centred artificial
intelligence (HCAI) design, recent efforts have produced user interfaces (UIs)
for AI experts to investigate the fairness of AI models. In this work, we
provide a design space exploration that supports not only data scientists but
also domain experts to investigate AI fairness. Using loan applications as an
example, we held a series of workshops with loan officers and data scientists
to elicit their requirements. We instantiated these requirements into FairHIL,
a UI to support human-in-the-loop fairness investigations, and describe how
this UI could be generalized to other use cases. We evaluated FairHIL through a
think-aloud user study. Our work contributes better designs to investigate an
AI model's fairness-and move closer towards responsible AI."
Sequential Counterfactual Decision-Making Under Confounded Reward,"We investigate the limitations of random trials when the cause of interest is
confounded with the effect by formalizing a counterfactual policy-space where
the agent's natural predilection is input to a soft-intervention."
"Not Cheating on the Turing Test: Towards Grounded Language Learning in
  Artificial Intelligence","Recent hype surrounding the increasing sophistication of language processing
models has renewed optimism regarding machines achieving a human-like command
of natural language. Research in the area of natural language understanding
(NLU) in artificial intelligence claims to have been making great strides in
this area, however, the lack of conceptual clarity/consistency in how
'understanding' is used in this and other disciplines makes it difficult to
discern how close we actually are. In this interdisciplinary research thesis, I
integrate insights from cognitive science/psychology, philosophy of mind, and
cognitive linguistics, and evaluate it against a critical review of current
approaches in NLU to explore the basic requirements--and remaining
challenges--for developing artificially intelligent systems with human-like
capacities for language use and comprehension."
Automated Kantian Ethics: A Faithful Implementation,"As we grant artificial intelligence increasing power and independence in
contexts like healthcare, policing, and driving, AI faces moral dilemmas but
lacks the tools to solve them. Warnings from regulators, philosophers, and
computer scientists about the dangers of unethical artificial intelligence have
spurred interest in automated ethics-i.e., the development of machines that can
perform ethical reasoning. However, prior work in automated ethics rarely
engages with philosophical literature. Philosophers have spent centuries
debating moral dilemmas so automated ethics will be most nuanced, consistent,
and reliable when it draws on philosophical literature. In this paper, I
present an implementation of automated Kantian ethics that is faithful to the
Kantian philosophical tradition. I formalize Kant's categorical imperative in
Dyadic Deontic Logic, implement this formalization in the Isabelle theorem
prover, and develop a testing framework to evaluate how well my implementation
coheres with expected properties of Kantian ethic. My system is an early step
towards philosophically mature ethical AI agents and it can make nuanced
judgements in complex ethical dilemmas because it is grounded in philosophical
literature. Because I use an interactive theorem prover, my system's judgements
are explainable."
Core Challenge 2022: Solver and Graph Descriptions,"This paper collects all descriptions of solvers and ISR instances submitted
to CoRe Challenge 2022."
Towards Benchmarking Explainable Artificial Intelligence Methods,"The currently dominating artificial intelligence and machine learning
technology, neural networks, builds on inductive statistical learning. Neural
networks of today are information processing systems void of understanding and
reasoning capabilities, consequently, they cannot explain promoted decisions in
a humanly valid form. In this work, we revisit and use fundamental philosophy
of science theories as an analytical lens with the goal of revealing, what can
be expected, and more importantly, not expected, from methods that aim to
explain decisions promoted by a neural network. By conducting a case study we
investigate a selection of explainability method's performance over two mundane
domains, animals and headgear. Through our study, we lay bare that the
usefulness of these methods relies on human domain knowledge and our ability to
understand, generalise and reason. The explainability methods can be useful
when the goal is to gain further insights into a trained neural network's
strengths and weaknesses. If our aim instead is to use these explainability
methods to promote actionable decisions or build trust in ML-models they need
to be less ambiguous than they are today. In this work, we conclude from our
study, that benchmarking explainability methods, is a central quest towards
trustworthy artificial intelligence and machine learning."
Artificial Intelligence in Concrete Materials: A Scientometric View,"Artificial intelligence (AI) has emerged as a transformative and versatile
tool, breaking new frontiers across scientific domains. Among its most
promising applications, AI research is blossoming in concrete science and
engineering, where it has offered new insights towards mixture design
optimization and service life prediction of cementitious systems. This chapter
aims to uncover the main research interests and knowledge structure of the
existing literature on AI for concrete materials. To begin with, a total of 389
journal articles published from 1990 to 2020 were retrieved from the Web of
Science. Scientometric tools such as keyword co-occurrence analysis and
documentation co-citation analysis were adopted to quantify features and
characteristics of the research field. The findings bring to light pressing
questions in data-driven concrete research and suggest future opportunities for
the concrete community to fully utilize the capabilities of AI techniques."
Artificial Intelligence and Advanced Materials,"Artificial intelligence is gaining strength and materials science can both
contribute to and profit from it. In a simultaneous progress race, new
materials, systems and processes can be devised and optimized thanks to machine
learning techniques and such progress can be turned into in-novative computing
platforms. Future materials scientists will profit from understanding how
machine learning can boost the conception of advanced materials. This review
covers aspects of computation from the fundamentals to directions taken and
repercussions produced by compu-tation to account for the origins, procedures
and applications of artificial intelligence. Machine learning and its methods
are reviewed to provide basic knowledge on its implementation and its
potential. The materials and systems used to implement artificial intelligence
with electric charges are finding serious competition from other information
carrying and processing agents. The impact these techniques are having on the
inception of new advanced materials is so deep that a new paradigm is
developing where implicit knowledge is being mined to conceive materi-als and
systems for functions instead of finding applications to found materials. How
far this trend can be carried is hard to fathom as exemplified by the power to
discover unheard of mate-rials or physical laws buried in data."
"Machine Learning and Artificial Intelligence-Driven Multi-Scale Modeling
  for High Burnup Accident-Tolerant Fuels for Light Water-Based SMR
  Applications","The concept of small modular reactor has changed the outlook for tackling
future energy crises. This new reactor technology is very promising considering
its lower investment requirements, modularity, design simplicity, and enhanced
safety features. The application of artificial intelligence-driven multi-scale
modeling (neutronics, thermal hydraulics, fuel performance, etc.) incorporating
Digital Twin and associated uncertainties in the research of small modular
reactors is a recent concept. In this work, a comprehensive study is conducted
on the multiscale modeling of accident-tolerant fuels. The application of these
fuels in the light water-based small modular reactors is explored. This chapter
also focuses on the application of machine learning and artificial intelligence
in the design optimization, control, and monitoring of small modular reactors.
Finally, a brief assessment of the research gap on the application of
artificial intelligence to the development of high burnup composite
accident-tolerant fuels is provided. Necessary actions to fulfill these gaps
are also discussed."
An Artificial Intelligence Outlook for Colorectal Cancer Screening,"Colorectal cancer is the third most common tumor in men and the second in
women, accounting for 10% of all tumors worldwide. It ranks second in
cancer-related deaths with 9.4%, following lung cancer. The decrease in
mortality rate documented over the last 20 years has shown signs of slowing
down since 2017, necessitating concentrated actions on specific measures that
have exhibited considerable potential. As such, the technical foundation and
research evidence for blood-derived protein markers have been set, pending
comparative validation, clinical implementation and integration into an
artificial intelligence enabled decision support framework that also considers
knowledge on risk factors. The current paper aspires to constitute the driving
force for creating change in colorectal cancer screening by reviewing existing
medical practices through accessible and non-invasive risk estimation,
employing a straightforward artificial intelligence outlook."
"Artificial Intelligence Nomenclature Identified From Delphi Study on Key
  Issues Related to Trust and Barriers to Adoption for Autonomous Systems","The rapid integration of artificial intelligence across traditional research
domains has generated an amalgamation of nomenclature. As cross-discipline
teams work together on complex machine learning challenges, finding a consensus
of basic definitions in the literature is a more fundamental problem. As a step
in the Delphi process to define issues with trust and barriers to the adoption
of autonomous systems, our study first collected and ranked the top concerns
from a panel of international experts from the fields of engineering, computer
science, medicine, aerospace, and defence, with experience working with
artificial intelligence. This document presents a summary of the literature
definitions for nomenclature derived from expert feedback."
"From Modelling to Understanding Children's Behaviour in the Context of
  Robotics and Social Artificial Intelligence","Understanding and modelling children's cognitive processes and their
behaviour in the context of their interaction with robots and social artificial
intelligence systems is a fundamental prerequisite for meaningful and effective
robot interventions. However, children's development involve complex faculties
such as exploration, creativity and curiosity which are challenging to model.
Also, often children express themselves in a playful way which is different
from a typical adult behaviour. Different children also have different needs,
and it remains a challenge in the current state of the art that those of
neurodiverse children are under-addressed. With this workshop, we aim to
promote a common ground among different disciplines such as developmental
sciences, artificial intelligence and social robotics and discuss cutting-edge
research in the area of user modelling and adaptive systems for children."
Foon Creation,"We have designed three search methods for producing the task trees for the
provided goal nodes using the Functional Object-Oriented Network. This paper
details the strategy, the procedure, and the outcomes."
"An Artificial Intelligence-based model for cell killing prediction:
  development, validation and explainability analysis of the ANAKIN model","The present work develops ANAKIN: an Artificial iNtelligence bAsed model for
(radiation induced) cell KIlliNg prediction. ANAKIN is trained and tested over
513 cell survival experiments with different types of radiation contained in
the publicly available PIDE database. We show how ANAKIN accurately predicts
several relevant biological endpoints over a wide broad range on ions beams and
for a high number of cell--lines. We compare the prediction of ANAKIN to the
only two radiobiological model for RBE prediction used in clinics, that is the
Microdosimetric Kinetic Model (MKM) and the Local Effect Model (LEM version
III), showing how ANAKIN has higher accuracy over the all considered biological
endpoints. At last, via modern techniques of Explainable Artificial
Intelligence (XAI), we show how ANAKIN predictions can be understood and
explained, highlighting how ANAKIN is in fact able to reproduce relevant
well-known biological patterns, such as the overkilling effect."
Investigating Labeler Bias in Face Annotation for Machine Learning,"In a world increasingly reliant on artificial intelligence, it is more
important than ever to consider the ethical implications of artificial
intelligence on humanity. One key under-explored challenge is labeler bias,
which can create inherently biased datasets for training and subsequently lead
to inaccurate or unfair decisions in healthcare, employment, education, and law
enforcement. Hence, we conducted a study to investigate and measure the
existence of labeler bias using images of people from different ethnicities and
sexes in a labeling task. Our results show that participants possess
stereotypes that influence their decision-making process and that labeler
demographics impact assigned labels. We also discuss how labeler bias
influences datasets and, subsequently, the models trained on them. Overall, a
high degree of transparency must be maintained throughout the entire artificial
intelligence training process to identify and correct biases in the data as
early as possible."
"The global economic impact of AI technologies in the fight against
  financial crime","Is the rapid adoption of Artificial Intelligence a sign that creative
destruction (a capitalist innovation process first theorised in 1942) is
occurring? Although its theory suggests that it is only visible over time in
aggregate, this paper devises three hypotheses to test its presence on a macro
level and research methods to produce the required data. This paper tests the
theory using news archives, questionnaires, and interviews with industry
professionals. It considers the risks of adopting Artificial Intelligence, its
current performance in the market and its general applicability to the role.
The results suggest that creative destruction is occurring in the AML industry
despite the activities of the regulators acting as natural blockers to
innovation. This is a pressurised situation where current-generation Artificial
Intelligence may offer more harm than benefit. For managers, this papers
results suggest that safely pursuing AI in AML requires having realistic
expectations of Artificial Intelligence's benefits combined with using a
framework for AI Ethics."
"Explainable Artificial Intelligence and Cybersecurity: A Systematic
  Literature Review","Cybersecurity vendors consistently apply AI (Artificial Intelligence) to
their solutions and many cybersecurity domains can benefit from AI technology.
However, black-box AI techniques present some difficulties in comprehension and
adoption by its operators, given that their decisions are not always humanly
understandable (as is usually the case with deep neural networks, for example).
Since it aims to make the operation of AI algorithms more interpretable for its
users and developers, XAI (eXplainable Artificial Intelligence) can be used to
address this issue. Through a systematic literature review, this work seeks to
investigate the current research scenario on XAI applied to cybersecurity,
aiming to discover which XAI techniques have been applied in cybersecurity, and
which areas of cybersecurity have already benefited from this technology."
A Survey on Explainable Artificial Intelligence for Cybersecurity,"The black-box nature of artificial intelligence (AI) models has been the
source of many concerns in their use for critical applications. Explainable
Artificial Intelligence (XAI) is a rapidly growing research field that aims to
create machine learning models that can provide clear and interpretable
explanations for their decisions and actions. In the field of network
cybersecurity, XAI has the potential to revolutionize the way we approach
network security by enabling us to better understand the behavior of cyber
threats and to design more effective defenses. In this survey, we review the
state of the art in XAI for cybersecurity in network systems and explore the
various approaches that have been proposed to address this important problem.
The review follows a systematic classification of network-driven cybersecurity
threats and issues. We discuss the challenges and limitations of current XAI
methods in the context of cybersecurity and outline promising directions for
future research."
Inapproximability of sufficient reasons for decision trees,"In this note, we establish the hardness of approximation of the problem of
computing the minimal size of a $\delta$-sufficient reason for decision trees."
Human and AI Perceptual Differences in Image Classification Errors,"Artificial intelligence (AI) models for computer vision trained with
supervised machine learning are assumed to solve classification tasks by
imitating human behavior learned from training labels. Most efforts in recent
vision research focus on measuring the model task performance using
standardized benchmarks such as accuracy. However, limited work has sought to
understand the perceptual difference between humans and machines. To fill this
gap, this study first analyzes the statistical distributions of mistakes from
the two sources and then explores how task difficulty level affects these
distributions. We find that even when AI learns an excellent model from the
training data, one that outperforms humans in overall accuracy, these AI models
have significant and consistent differences from human perception. We
demonstrate the importance of studying these differences with a simple human-AI
teaming algorithm that outperforms humans alone, AI alone, or AI-AI teaming."
"Enhancing Artificial intelligence Policies with Fusion and Forecasting:
  Insights from Indian Patents Using Network Analysis","This paper presents a study of the interconnectivity and interdependence of
various Artificial intelligence (AI) technologies through the use of centrality
measures, clustering coefficients, and degree of fusion measures. By analyzing
the technologies through different time windows and quantifying their
importance, we have revealed important insights into the crucial components
shaping the AI landscape and the maturity level of the domain. The results of
this study have significant implications for future development and
advancements in artificial intelligence and provide a clear understanding of
key technology areas of fusion. Furthermore, this paper contributes to AI
public policy research by offering a data-driven perspective on the current
state and future direction of the field. However, it is important to
acknowledge the limitations of this research and call for further studies to
build on these results. With these findings, we hope to inform and guide future
research in the field of AI, contributing to its continued growth and success."
Robots in the Garden: Artificial Intelligence and Adaptive Landscapes,"This paper introduces ELUA, the Ecological Laboratory for Urban Agriculture,
a collaboration among landscape architects, architects and computer scientists
who specialize in artificial intelligence, robotics and computer vision. ELUA
has two gantry robots, one indoors and the other outside on the rooftop of a
6-story campus building. Each robot can seed, water, weed, and prune in its
garden. To support responsive landscape research, ELUA also includes sensor
arrays, an AI-powered camera, and an extensive network infrastructure. This
project demonstrates a way to integrate artificial intelligence into an
evolving urban ecosystem, and encourages landscape architects to develop an
adaptive design framework where design becomes a long-term engagement with the
environment."
A Knowledge Engineering Primer,"The aim of this primer is to introduce the subject of knowledge engineering
in a concise but synthetic way to develop the reader's intuition about the
area."
eXplainable Artificial Intelligence (XAI) in aging clock models,"eXplainable Artificial Intelligence (XAI) is a rapidly progressing field of
machine learning, aiming to unravel the predictions of complex models. XAI is
especially required in sensitive applications, e.g. in health care, when
diagnosis, recommendations and treatment choices might rely on the decisions
made by artificial intelligence systems. AI approaches have become widely used
in aging research as well, in particular, in developing biological clock models
and identifying biomarkers of aging and age-related diseases. However, the
potential of XAI here awaits to be fully appreciated. We discuss the
application of XAI for developing the ""aging clocks"" and present a
comprehensive analysis of the literature categorized by the focus on particular
physiological systems."
Artificial Intelligence for Smart Transportation,"There are more than 7,000 public transit agencies in the U.S. (and many more
private agencies), and together, they are responsible for serving 60 billion
passenger miles each year. A well-functioning transit system fosters the growth
and expansion of businesses, distributes social and economic benefits, and
links the capabilities of community members, thereby enhancing what they can
accomplish as a society. Since affordable public transit services are the
backbones of many communities, this work investigates ways in which Artificial
Intelligence (AI) can improve efficiency and increase utilization from the
perspective of transit agencies. This book chapter discusses the primary
requirements, objectives, and challenges related to the design of AI-driven
smart transportation systems. We focus on three major topics. First, we discuss
data sources and data. Second, we provide an overview of how AI can aid
decision-making with a focus on transportation. Lastly, we discuss
computational problems in the transportation domain and AI approaches to these
problems."
Reinforcement Learning for Battery Management in Dairy Farming,"Dairy farming is a particularly energy-intensive part of the agriculture
sector. Effective battery management is essential for renewable integration
within the agriculture sector. However, controlling battery
charging/discharging is a difficult task due to electricity demand variability,
stochasticity of renewable generation, and energy price fluctuations. Despite
the potential benefits of applying Artificial Intelligence (AI) to renewable
energy in the context of dairy farming, there has been limited research in this
area. This research is a priority for Ireland as it strives to meet its
governmental goals in energy and sustainability. This research paper utilizes
Q-learning to learn an effective policy for charging and discharging a battery
within a dairy farm setting. The results demonstrate that the developed policy
significantly reduces electricity costs compared to the established baseline
algorithm. These findings highlight the effectiveness of reinforcement learning
for battery management within the dairy farming sector."
"Deciphering knee osteoarthritis diagnostic features with explainable
  artificial intelligence: A systematic review","Existing artificial intelligence (AI) models for diagnosing knee
osteoarthritis (OA) have faced criticism for their lack of transparency and
interpretability, despite achieving medical-expert-like performance. This
opacity makes them challenging to trust in clinical practice. Recently,
explainable artificial intelligence (XAI) has emerged as a specialized
technique that can provide confidence in the model's prediction by revealing
how the prediction is derived, thus promoting the use of AI systems in
healthcare. This paper presents the first survey of XAI techniques used for
knee OA diagnosis. The XAI techniques are discussed from two perspectives: data
interpretability and model interpretability. The aim of this paper is to
provide valuable insights into XAI's potential towards a more reliable knee OA
diagnosis approach and encourage its adoption in clinical practice."
"Modelling Electricity Consumption in Irish Dairy Farms Using Agent-Based
  Modelling","Dairy farming can be an energy intensive form of farming. Understanding the
factors affecting electricity consumption on dairy farms is crucial for farm
owners and energy providers. In order to accurately estimate electricity
demands in dairy farms, it is necessary to develop a model. In this research
paper, an agent-based model is proposed to model the electricity consumption of
Irish dairy farms. The model takes into account various factors that affect the
energy consumption of dairy farms, including herd size, number of milking
machines, and time of year. The outputs are validated using existing
state-of-the-art dairy farm modelling frameworks. The proposed agent-based
model is fully explainable, which is an advantage over other Artificial
Intelligence techniques, e.g. deep learning."
Inferring physical laws by artificial intelligence based causal models,"The advances in Artificial Intelligence (AI) and Machine Learning (ML) have
opened up many avenues for scientific research, and are adding new dimensions
to the process of knowledge creation. However, even the most powerful and
versatile of ML applications till date are primarily in the domain of analysis
of associations and boil down to complex data fitting. Judea Pearl has pointed
out that Artificial General Intelligence must involve interventions involving
the acts of doing and imagining. Any machine assisted scientific discovery thus
must include casual analysis and interventions. In this context, we propose a
causal learning model of physical principles, which not only recognizes
correlations but also brings out casual relationships. We use the principles of
causal inference and interventions to study the cause-and-effect relationships
in the context of some well-known physical phenomena. We show that this
technique can not only figure out associations among data, but is also able to
correctly ascertain the cause-and-effect relations amongst the variables,
thereby strengthening (or weakening) our confidence in the proposed model of
the underlying physical process."
Education in the age of Generative AI: Context and Recent Developments,"With the emergence of generative artificial intelligence, an increasing
number of individuals and organizations have begun exploring its potential to
enhance productivity and improve product quality across various sectors. The
field of education is no exception. However, it is vital to notice that
artificial intelligence adoption in education dates back to the 1960s. In light
of this historical context, this white paper serves as the inaugural piece in a
four-part series that elucidates the role of AI in education. The series delves
into topics such as its potential, successful applications, limitations,
ethical considerations, and future trends. This initial article provides a
comprehensive overview of the field, highlighting the recent developments
within the generative artificial intelligence sphere."
Originality and the Future of Copyright in an Age of Generative AI,"This papers explores the question of human authorship when works are created
with generative AI tools."
"Compromise in Multilateral Negotiations and the Global Regulation of
  Artificial Intelligence","As artificial intelligence (AI) technologies spread worldwide, international
discussions have increasingly focused on their consequences for democracy,
human rights, fundamental freedoms, security, and economic and social
development. In this context, UNESCO's Recommendation on the Ethics of
Artificial Intelligence, adopted in November 2021, has emerged as the first
global normative framework for AI development and deployment. The intense
negotiations of every detail of the document brought forth numerous
controversies among UNESCO member states. Drawing on a unique set of primary
sources, including written positions and recorded deliberations, this paper
explains the achievement of global compromise on AI regulation despite the
multiplicity of UNESCO member-state positions representing a variety of liberal
and sovereignist preferences. Building upon Boltanski's pragmatic sociology, it
conceptualises the practice of multilateral negotiations and attributes the
multilateral compromise to two embedded therein mechanisms: Structural
normative hybridity and situated normative ambiguity allowed to accomplish a
compromise by linking macro-normative structures with situated debates of
multilateral negotiations."
"Building Privacy-Preserving and Secure Geospatial Artificial
  Intelligence Foundation Models","In recent years we have seen substantial advances in foundation models for
artificial intelligence, including language, vision, and multimodal models.
Recent studies have highlighted the potential of using foundation models in
geospatial artificial intelligence, known as GeoAI Foundation Models, for
geographic question answering, remote sensing image understanding, map
generation, and location-based services, among others. However, the development
and application of GeoAI foundation models can pose serious privacy and
security risks, which have not been fully discussed or addressed to date. This
paper introduces the potential privacy and security risks throughout the
lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for
research directions and preventative and control strategies. Through this
vision paper, we hope to draw the attention of researchers and policymakers in
geospatial domains to these privacy and security risks inherent in GeoAI
foundation models and advocate for the development of privacy-preserving and
secure GeoAI foundation models."
The Opaque Law of Artificial Intelligence,"The purpose of this paper is to analyse the opacity of algorithms,
contextualized in the open debate on responsibility for artificial intelligence
causation; with an experimental approach by which, applying the proposed
conversational methodology of the Turing Test, we expect to evaluate the
performance of one of the best existing NLP model of generative AI (Chat-GPT)
to see how far it can go right now and how the shape of a legal regulation of
it could be. The analysis of the problem will be supported by a comment of
Italian classical law categories such as causality, intent and fault to
understand the problem of the usage of AI, focusing in particular on the
human-machine interaction. On the computer science side, for a technical point
of view of the logic used to craft these algorithms, in the second chapter will
be proposed a practical interrogation of Chat-GPT aimed at finding some
critical points of the functioning of AI. The end of the paper will concentrate
on some existing legal solutions which can be applied to the problem, plus a
brief description of the approach proposed by EU Artificial Intelligence act."
"Brief for the Canada House of Commons Study on the Implications of
  Artificial Intelligence Technologies for the Canadian Labor Force: Generative
  Artificial Intelligence Shatters Models of AI and Labor","Exciting advances in generative artificial intelligence (AI) have sparked
concern for jobs, education, productivity, and the future of work. As with past
technologies, generative AI may not lead to mass unemployment. But, unlike past
technologies, generative AI is creative, cognitive, and potentially ubiquitous
which makes the usual assumptions of automation predictions ill-suited for
today. Existing projections suggest that generative AI will impact workers in
occupations that were previously considered immune to automation. As AI's full
set of capabilities and applications emerge, policy makers should promote
workers' career adaptability. This goal requires improved data on job
separations and unemployment by locality and job titles in order to identify
early-indicators for the workers facing labor disruption. Further, prudent
policy should incentivize education programs to accommodate learning with AI as
a tool while preparing students for the demands of the future of work."
Towards more Practical Threat Models in Artificial Intelligence Security,"Recent works have identified a gap between research and practice in
artificial intelligence security: threats studied in academia do not always
reflect the practical use and security risks of AI. For example, while models
are often studied in isolation, they form part of larger ML pipelines in
practice. Recent works also brought forward that adversarial manipulations
introduced by academic attacks are impractical. We take a first step towards
describing the full extent of this disparity. To this end, we revisit the
threat models of the six most studied attacks in AI security research and match
them to AI usage in practice via a survey with 271 industrial practitioners. On
the one hand, we find that all existing threat models are indeed applicable. On
the other hand, there are significant mismatches: research is often too
generous with the attacker, assuming access to information not frequently
available in real-world settings. Our paper is thus a call for action to study
more practical threat models in artificial intelligence security."
Bayes in the age of intelligent machines,"The success of methods based on artificial neural networks in creating
intelligent machines seems like it might pose a challenge to explanations of
human cognition in terms of Bayesian inference. We argue that this is not the
case, and that in fact these systems offer new opportunities for Bayesian
modeling. Specifically, we argue that Bayesian models of cognition and
artificial neural networks lie at different levels of analysis and are
complementary modeling approaches, together offering a way to understand human
cognition that spans these levels. We also argue that the same perspective can
be applied to intelligent machines, where a Bayesian approach may be uniquely
valuable in understanding the behavior of large, opaque artificial neural
networks that are trained on proprietary data."
"COVID-19 Imposes Rethinking of Conferencing -- Environmental Impact
  Assessment of Artificial Intelligence Conferences","It has been noticed that through COVID-19 greenhouse gas emissions had a
sudden reduction. Based on this significant observation, we decided to conduct
a research to quantify the impact of scientific conferences' air-travelling,
explore and suggest alternative ways for greener conferences to re-duce the
global carbon footprint. Specifically, we focused on the most popular
conferences for the Artificial Intelligence community based on their scientific
impact factor, their scale, and the well-organized proceedings towards
measuring the impact of air travelling participation. This is the first time
that systematic quantification of a state-of-the-art subject like Artificial
Intelligence takes place to define its conferencing footprint in the broader
frames of environmental awareness. Our findings highlight that the virtual way
is the first on the list of green conferences' conduction although there are
serious concerns about it. Alternatives to optimal conferences' location
selection have demonstrated savings on air-travelling CO2 emissions of up to
63.9%."
Concept-based Explainable Artificial Intelligence: A Survey,"The field of explainable artificial intelligence emerged in response to the
growing need for more transparent and reliable models. However, using raw
features to provide explanations has been disputed in several works lately,
advocating for more user-understandable explanations. To address this issue, a
wide range of papers proposing Concept-based eXplainable Artificial
Intelligence (C-XAI) methods have arisen in recent years. Nevertheless, a
unified categorization and precise field definition are still missing. This
paper fills the gap by offering a thorough review of C-XAI approaches. We
define and identify different concepts and explanation types. We provide a
taxonomy identifying nine categories and propose guidelines for selecting a
suitable category based on the development context. Additionally, we report
common evaluation strategies including metrics, human evaluations and dataset
employed, aiming to assist the development of future methods. We believe this
survey will serve researchers, practitioners, and domain experts in
comprehending and advancing this innovative field."
"Generation Z's Ability to Discriminate Between AI-generated and
  Human-Authored Text on Discord","The growing popularity of generative artificial intelligence (AI) chatbots
such as ChatGPT is having transformative effects on social media. As the
prevalence of AI-generated content grows, concerns have been raised regarding
privacy and misinformation online. Among social media platforms, Discord
enables AI integrations -- making their primarily ""Generation Z"" userbase
particularly exposed to AI-generated content. We surveyed Generation Z aged
individuals (n = 335) to evaluate their proficiency in discriminating between
AI-generated and human-authored text on Discord. The investigation employed
one-shot prompting of ChatGPT, disguised as a text message received on the
Discord.com platform. We explore the influence of demographic factors on
ability, as well as participants' familiarity with Discord and artificial
intelligence technologies. We find that Generation Z individuals are unable to
discern between AI and human-authored text (p = 0.011), and that those with
lower self-reported familiarity with Discord demonstrated an improved ability
in identifying human-authored compared to those with self-reported experience
with AI (p << 0.0001). Our results suggest that there is a nuanced relationship
between AI technology and popular modes of communication for Generation Z,
contributing valuable insights into human-computer interactions, digital
communication, and artificial intelligence literacy."
Artificial Intelligence for Digital and Computational Pathology,"Advances in digitizing tissue slides and the fast-paced progress in
artificial intelligence, including deep learning, have boosted the field of
computational pathology. This field holds tremendous potential to automate
clinical diagnosis, predict patient prognosis and response to therapy, and
discover new morphological biomarkers from tissue images. Some of these
artificial intelligence-based systems are now getting approved to assist
clinical diagnosis; however, technical barriers remain for their widespread
clinical adoption and integration as a research tool. This Review consolidates
recent methodological advances in computational pathology for predicting
clinical end points in whole-slide images and highlights how these developments
enable the automation of clinical practice and the discovery of new biomarkers.
We then provide future perspectives as the field expands into a broader range
of clinical and research tasks with increasingly diverse modalities of clinical
data."
"Analysing the Needs of Homeless People Using Feature Selection and
  Mining Association Rules","Homelessness is a social and health problem with great repercussions in
Europe. Many non-governmental organisations help homeless people by collecting
and analysing large amounts of information about them. However, these tasks are
not always easy to perform, and hinder other of the organisations duties. The
SINTECH project was created to tackle this issue proposing two different tools:
a mobile application to quickly and easily collect data; and a software based
on artificial intelligence which obtains interesting information from the
collected data. The first one has been distributed to some Spanish
organisations which are using it to conduct surveys of homeless people. The
second tool implements different feature selection and association rules mining
methods. These artificial intelligence techniques have allowed us to identify
the most relevant features and some interesting association rules from
previously collected homeless data."
"Towards Risk Analysis of the Impact of AI on the Deliberate Biological
  Threat Landscape","The perception that the convergence of biological engineering and artificial
intelligence (AI) could enable increased biorisk has recently drawn attention
to the governance of biotechnology and artificial intelligence. The 2023
Executive Order, Executive Order on the Safe, Secure, and Trustworthy
Development and Use of Artificial Intelligence, requires an assessment of how
artificial intelligence can increase biorisk. Within this perspective,
quantitative and qualitative frameworks for evaluating biorisk are presented.
Both frameworks are exercised using notional scenarios and their benefits and
limitations are then discussed. Finally, the perspective concludes by noting
that assessment and evaluation methodologies must keep pace with advances of AI
in the life sciences."
"Artificial Intelligence for Literature Reviews: Opportunities and
  Challenges","This manuscript presents a comprehensive review of the use of Artificial
Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous
and organised methodology that assesses and integrates previous research on a
given topic. Numerous tools have been developed to assist and partially
automate the SLR process. The increasing role of AI in this field shows great
potential in providing more effective support for researchers, moving towards
the semi-automatic creation of literature reviews. Our study focuses on how AI
techniques are applied in the semi-automation of SLRs, specifically in the
screening and extraction phases. We examine 21 leading SLR tools using a
framework that combines 23 traditional features with 11 AI features. We also
analyse 11 recent tools that leverage large language models for searching the
literature and assisting academic writing. Finally, the paper discusses current
trends in the field, outlines key research challenges, and suggests directions
for future research."
A Conversational Brain-Artificial Intelligence Interface,"We introduce Brain-Artificial Intelligence Interfaces (BAIs) as a new class
of Brain-Computer Interfaces (BCIs). Unlike conventional BCIs, which rely on
intact cognitive capabilities, BAIs leverage the power of artificial
intelligence to replace parts of the neuro-cognitive processing pipeline. BAIs
allow users to accomplish complex tasks by providing high-level intentions,
while a pre-trained AI agent determines low-level details. This approach
enlarges the target audience of BCIs to individuals with cognitive impairments,
a population often excluded from the benefits of conventional BCIs. We present
the general concept of BAIs and illustrate the potential of this new approach
with a Conversational BAI based on EEG. In particular, we show in an experiment
with simulated phone conversations that the Conversational BAI enables complex
communication without the need to generate language. Our work thus
demonstrates, for the first time, the ability of a speech neuroprosthesis to
enable fluent communication in realistic scenarios with non-invasive
technologies."
"What Does Evaluation of Explainable Artificial Intelligence Actually
  Tell Us? A Case for Compositional and Contextual Validation of XAI Building
  Blocks","Despite significant progress, evaluation of explainable artificial
intelligence remains elusive and challenging. In this paper we propose a
fine-grained validation framework that is not overly reliant on any one facet
of these sociotechnical systems, and that recognises their inherent modular
structure: technical building blocks, user-facing explanatory artefacts and
social communication protocols. While we concur that user studies are
invaluable in assessing the quality and effectiveness of explanation
presentation and delivery strategies from the explainees' perspective in a
particular deployment context, the underlying explanation generation mechanisms
require a separate, predominantly algorithmic validation strategy that accounts
for the technical and human-centred desiderata of their (numerical) outputs.
Such a comprehensive sociotechnical utility-based evaluation framework could
allow to systematically reason about the properties and downstream influence of
different building blocks from which explainable artificial intelligence
systems are composed -- accounting for a diverse range of their engineering and
social aspects -- in view of the anticipated use case."
Artificial Intelligence for Multi-Unit Auction design,"Understanding bidding behavior in multi-unit auctions remains an ongoing
challenge for researchers. Despite their widespread use, theoretical insights
into the bidding behavior, revenue ranking, and efficiency of commonly used
multi-unit auctions are limited. This paper utilizes artificial intelligence,
specifically reinforcement learning, as a model free learning approach to
simulate bidding in three prominent multi-unit auctions employed in practice.
We introduce six algorithms that are suitable for learning and bidding in
multi-unit auctions and compare them using an illustrative example. This paper
underscores the significance of using artificial intelligence in auction
design, particularly in enhancing the design of multi-unit auctions."
"HCC Is All You Need: Alignment-The Sensible Kind Anyway-Is Just
  Human-Centered Computing",This article argues that AI Alignment is a type of Human-Centered Computing.
"An ethical study of generative AI from the Actor-Network Theory
  perspective","The widespread use of Generative Artificial Intelligence in the innovation
and generation of communication content is mainly due to its exceptional
creative ability, operational efficiency, and compatibility with diverse
industries. Nevertheless, this has also sparked ethical problems, such as
unauthorized access to data, biased decision-making by algorithms, and criminal
use of generated content. In order to tackle the security vulnerabilities
linked to Generative Artificial Intelligence, we analyze ChatGPT as a case
study within the framework of Actor-Network Theory. We have discovered a total
of nine actors, including both human and non-human creatures. We examine the
actors and processes of translation involved in the ethical issues related to
ChatGPT and analyze the key players involved in the emergence of moral issues.
The objective is to explore the origins of the ethical issues that arise with
Generative Artificial Intelligence and provide a particular perspective on the
governance of Generative Artificial Intelligence."
"A Formal Model for Artificial Intelligence Applications in Automation
  Systems","The integration of Artificial Intelligence (AI) into automation systems has
the potential to enhance efficiency and to address currently unsolved existing
technical challenges. However, the industry-wide adoption of AI is hindered by
the lack of standardized documentation for the complex compositions of
automation systems, AI software, production hardware, and their
interdependencies. This paper proposes a formal model using standards and
ontologies to provide clear and structured documentation of AI applications in
automation systems. The proposed information model for artificial intelligence
in automation systems (AIAS) utilizes ontology design patterns to map and link
various aspects of automation systems and AI software. Validated through a
practical example, the model demonstrates its effectiveness in improving
documentation practices and aiding the sustainable implementation of AI in
industrial settings."
"From Text to Life: On the Reciprocal Relationship between Artificial
  Life and Large Language Models","Large Language Models (LLMs) have taken the field of AI by storm, but their
adoption in the field of Artificial Life (ALife) has been, so far, relatively
reserved. In this work we investigate the potential synergies between LLMs and
ALife, drawing on a large body of research in the two fields. We explore the
potential of LLMs as tools for ALife research, for example, as operators for
evolutionary computation or the generation of open-ended environments.
Reciprocally, principles of ALife, such as self-organization, collective
intelligence and evolvability can provide an opportunity for shaping the
development and functionalities of LLMs, leading to more adaptive and
responsive models. By investigating this dynamic interplay, the paper aims to
inspire innovative crossover approaches for both ALife and LLM research. Along
the way, we examine the extent to which LLMs appear to increasingly exhibit
properties such as emergence or collective intelligence, expanding beyond their
original goal of generating text, and potentially redefining our perception of
lifelike intelligence in artificial systems."
Building Artificial Intelligence with Creative Agency and Self-hood,"This paper is an invited layperson summary for The Academic of the paper
referenced on the last page. We summarize how the formal framework of
autocatalytic networks offers a means of modeling the origins of
self-organizing, self-sustaining structures that are sufficiently complex to
reproduce and evolve, be they organisms undergoing biological evolution,
novelty-generating minds driving cultural evolution, or artificial intelligence
networks such as large language models. The approach can be used to analyze and
detect phase transitions in vastly complex networks that have proven
intractable with other approaches, and suggests a promising avenue to building
an autonomous, agentic AI self. It seems reasonable to expect that such an
autocatalytic AI would possess creative agency akin to that of humans, and
undergo psychologically healing -- i.e., therapeutic -- internal transformation
through engagement in creative tasks. Moreover, creative tasks would be
expected to help such an AI solidify its self-identity."
"Predicting Star Scientists in the Field of Artificial Intelligence: A
  Machine Learning Approach","Star scientists are highly influential researchers who have made significant
contributions to their field, gained widespread recognition, and often
attracted substantial research funding. They are critical for the advancement
of science and innovation, and they have a significant influence on the
transfer of knowledge and technology to industry. Identifying potential star
scientists before their performance becomes outstanding is important for
recruitment, collaboration, networking, or research funding decisions. Using
machine learning techniques, this study proposes a model to predict star
scientists in the field of artificial intelligence while highlighting features
related to their success. Our results confirm that rising stars follow
different patterns compared to their non-rising stars counterparts in almost
all the early-career features. We also found that certain features such as
gender and ethnic diversity play important roles in scientific collaboration
and that they can significantly impact an author's career development and
success. The most important features in predicting star scientists in the field
of artificial intelligence were the number of articles, group discipline
diversity, and weighted degree centrality. The proposed approach offers
valuable insights for researchers, practitioners, and funding agencies
interested in identifying and supporting talented researchers."
AI as a Tool for Fair Journalism: Case Studies from Malta,"In today`s media landscape, the role of Artificial Intelligence (AI) in
shaping societal perspectives and journalistic integrity is becoming
increasingly apparent. This paper presents two case studies centred on Malta`s
media market featuring technical novelty. Despite its relatively small scale,
Malta offers invaluable insights applicable to both similar and broader media
contexts. These two projects focus on media monitoring and present tools
designed to analyse potential biases in news articles and television news
segments. The first project uses Computer Vision and Natural Language
Processing techniques to analyse the coherence between images in news articles
and their corresponding captions, headlines, and article bodies. The second
project employs computer vision techniques to track individuals` on-screen time
or visual exposure in news videos, providing queryable data. These initiatives
aim to contribute to society by providing both journalists and the public with
the means to identify biases. Furthermore, we make these tools accessible to
journalists to improve the trustworthiness of media outlets by offering robust
tools for detecting and reducing bias."
Brain-inspired Artificial Intelligence: A Comprehensive Review,"Current artificial intelligence (AI) models often focus on enhancing
performance through meticulous parameter tuning and optimization techniques.
However, the fundamental design principles behind these models receive
comparatively less attention, which can limit our understanding of their
potential and constraints. This comprehensive review explores the diverse
design inspirations that have shaped modern AI models, i.e., brain-inspired
artificial intelligence (BIAI). We present a classification framework that
categorizes BIAI approaches into physical structure-inspired and human
behavior-inspired models. We also examine the real-world applications where
different BIAI models excel, highlighting their practical benefits and
deployment challenges. By delving into these areas, we provide new insights and
propose future research directions to drive innovation and address current gaps
in the field. This review offers researchers and practitioners a comprehensive
overview of the BIAI landscape, helping them harness its potential and expedite
advancements in AI development."
"Explainable Artificial Intelligence: A Survey of Needs, Techniques,
  Applications, and Future Direction","Artificial intelligence models encounter significant challenges due to their
black-box nature, particularly in safety-critical domains such as healthcare,
finance, and autonomous vehicles. Explainable Artificial Intelligence (XAI)
addresses these challenges by providing explanations for how these models make
decisions and predictions, ensuring transparency, accountability, and fairness.
Existing studies have examined the fundamental concepts of XAI, its general
principles, and the scope of XAI techniques. However, there remains a gap in
the literature as there are no comprehensive reviews that delve into the
detailed mathematical representations, design methodologies of XAI models, and
other associated aspects. This paper provides a comprehensive literature review
encompassing common terminologies and definitions, the need for XAI,
beneficiaries of XAI, a taxonomy of XAI methods, and the application of XAI
methods in different application areas. The survey is aimed at XAI researchers,
XAI practitioners, AI model developers, and XAI beneficiaries who are
interested in enhancing the trustworthiness, transparency, accountability, and
fairness of their AI models."
"Initial Development and Evaluation of the Creative Artificial
  Intelligence through Recurring Developments and Determinations (CAIRDD)
  System","Computer system creativity is a key step on the pathway to artificial general
intelligence (AGI). It is elusive, however, due to the fact that human
creativity is not fully understood and, thus, it is difficult to develop this
capability in software. Large language models (LLMs) provide a facsimile of
creativity and the appearance of sentience, while not actually being either
creative or sentient. While LLMs have created bona fide new content, in some
cases - such as with harmful hallucinations - inadvertently, their deliberate
creativity is seen by some to not match that of humans. In response to this
challenge, this paper proposes a technique for enhancing LLM output creativity
via an iterative process of concept injection and refinement. Initial work on
the development of the Creative Artificial Intelligence through Recurring
Developments and Determinations (CAIRDD) system is presented and the efficacy
of key system components is evaluated."
"Exploratory Visual Analysis for Increasing Data Readiness in Artificial
  Intelligence Projects","We present experiences and lessons learned from increasing data readiness of
heterogeneous data for artificial intelligence projects using visual analysis
methods. Increasing the data readiness level involves understanding both the
data as well as the context in which it is used, which are challenges well
suitable to visual analysis. For this purpose, we contribute a mapping between
data readiness aspects and visual analysis techniques suitable for different
data types. We use the defined mapping to increase data readiness levels in use
cases involving time-varying data, including numerical, categorical, and text.
In addition to the mapping, we extend the data readiness concept to better take
aspects of the task and solution into account and explicitly address
distribution shifts during data collection time. We report on our experiences
in using the presented visual analysis techniques to aid future artificial
intelligence projects in raising the data readiness level."
"Artificial Intelligence in Education: Ethical Considerations and
  Insights from Ancient Greek Philosophy","This paper explores the ethical implications of integrating Artificial
Intelligence (AI) in educational settings, from primary schools to
universities, while drawing insights from ancient Greek philosophy to address
emerging concerns. As AI technologies increasingly influence learning
environments, they offer novel opportunities for personalized learning,
efficient assessment, and data-driven decision-making. However, these
advancements also raise critical ethical questions regarding data privacy,
algorithmic bias, student autonomy, and the changing roles of educators. This
research examines specific use cases of AI in education, analyzing both their
potential benefits and drawbacks. By revisiting the philosophical principles of
ancient Greek thinkers such as Socrates, Aristotle, and Plato, we discuss how
their writings can guide the ethical implementation of AI in modern education.
The paper argues that while AI presents significant challenges, a balanced
approach informed by classical philosophical thought can lead to an ethically
sound transformation of education. It emphasizes the evolving role of teachers
as facilitators and the importance of fostering student initiative in AI-rich
environments."
"Beyond Text-to-Text: An Overview of Multimodal and Generative Artificial
  Intelligence for Education Using Topic Modeling","Generative artificial intelligence (GenAI) can reshape education and
learning. While large language models (LLMs) like ChatGPT dominate current
educational research, multimodal capabilities, such as text-to-speech and
text-to-image, are less explored. This study uses topic modeling to map the
research landscape of multimodal and generative AI in education. An extensive
literature search using Dimensions yielded 4175 articles. Employing a topic
modeling approach, latent topics were extracted, resulting in 38 interpretable
topics organized into 14 thematic areas. Findings indicate a predominant focus
on text-to-text models in educational contexts, with other modalities
underexplored, overlooking the broader potential of multimodal approaches. The
results suggest a research gap, stressing the importance of more balanced
attention across different AI modalities and educational levels. In summary,
this research provides an overview of current trends in generative AI for
education, underlining opportunities for future exploration of multimodal
technologies to fully realize the transformative potential of artificial
intelligence in education."
"A Blockchain and Artificial Intelligence based System for Halal Food
  Traceability","The demand of the halal food products is increasing rapidly around the world.
The consumption of halal food product is just not among the Muslims but also
among non-Muslims, due to the purity of the halal food products. However, there
are several challenges that are faced by the halal food consumers. The
challenges raise a doubt among the halal food consumers about the authenticity
of the product being halal. Therefore, a solution that can address these issues
and can establish trust between consumers and producers. Blockchain technology
can provide a distributed ledger of an immutable record of the information.
Artificial intelligence supports developing a solution for pattern
identification. The proposed research utilizes blockchain an artificial
intelligence-based system for developing a system that ensure the authenticity
of the halal food products by providing the traceability related to all the
operations and processes of the supply chain and sourcing the raw material. The
proposed system has been tested with a local supermarket. The results and tests
of the developed solution seemed effective and the testers expressed interest
in real-world implementation of the proposed system."
"Vision Paper: Designing Graph Neural Networks in Compliance with the
  European Artificial Intelligence Act","The European Union's Artificial Intelligence Act (AI Act) introduces
comprehensive guidelines for the development and oversight of Artificial
Intelligence (AI) and Machine Learning (ML) systems, with significant
implications for Graph Neural Networks (GNNs). This paper addresses the unique
challenges posed by the AI Act for GNNs, which operate on complex
graph-structured data. The legislation's requirements for data management, data
governance, robustness, human oversight, and privacy necessitate tailored
strategies for GNNs. Our study explores the impact of these requirements on GNN
training and proposes methods to ensure compliance. We provide an in-depth
analysis of bias, robustness, explainability, and privacy in the context of
GNNs, highlighting the need for fair sampling strategies and effective
interpretability techniques. Our contributions fill the research gap by
offering specific guidance for GNNs under the new legislative framework and
identifying open questions and future research directions."
"Development of a Human-Robot Interaction Platform for Dual-Arm Robots
  Based on ROS and Multimodal Artificial Intelligence","In this paper, we propose the development of an interactive platform between
humans and a dual-arm robotic system based on the Robot Operating System (ROS)
and a multimodal artificial intelligence model. Our proposed platform consists
of two main components: a dual-arm robotic hardware system and software that
includes image processing tasks and natural language processing using a 3D
camera and embedded computing. First, we designed and developed a dual-arm
robotic system with a positional accuracy of less than 2 cm, capable of
operating independently, performing industrial and service tasks while
simultaneously simulating and modeling the robot in the ROS environment.
Second, artificial intelligence models for image processing are integrated to
execute object picking and classification tasks with an accuracy of over 90%.
Finally, we developed remote control software using voice commands through a
natural language processing model. Experimental results demonstrate the
accuracy of the multimodal artificial intelligence model and the flexibility of
the dual-arm robotic system in interactive human environments."
"Generating Mixcode Popular Songs with Artificial Intelligence: Concepts,
  Plans, and Speculations","Music is a potent form of expression that can communicate, accentuate or even
create the emotions of an individual or a collective. Both historically and in
contemporary experiences, musical expression was and is commonly
instrumentalized for social, political and/or economic purposes. Generative
artificial intelligence provides a wealth of both opportunities and challenges
with regard to music and its role in society. This paper discusses a proposed
project integrating artificial intelligence and popular music, with the
ultimate goal of creating a powerful tool for implementing music for social
transformation, education, healthcare, and emotional well-being. Given that it
is being presented at the outset of a collaboration between a computer
scientist/data analyst and an ethnomusicologist/social anthropologist. it is
mainly conceptual and somewhat speculative in nature."
"Artificial Intelligence for Geometry-Based Feature Extraction, Analysis
  and Synthesis in Artistic Images: A Survey","Artificial Intelligence significantly enhances the visual art industry by
analyzing, identifying and generating digitized artistic images. This review
highlights the substantial benefits of integrating geometric data into AI
models, addressing challenges such as high inter-class variations, domain gaps,
and the separation of style from content by incorporating geometric
information. Models not only improve AI-generated graphics synthesis quality,
but also effectively distinguish between style and content, utilizing inherent
model biases and shared data traits. We explore methods like geometric data
extraction from artistic images, the impact on human perception, and its use in
discriminative tasks. The review also discusses the potential for improving
data quality through innovative annotation techniques and the use of geometric
data to enhance model adaptability and output refinement. Overall,
incorporating geometric guidance boosts model performance in classification and
synthesis tasks, providing crucial insights for future AI applications in the
visual arts domain."
Artificial Expert Intelligence through PAC-reasoning,"Artificial Expert Intelligence (AEI) seeks to transcend the limitations of
both Artificial General Intelligence (AGI) and narrow AI by integrating
domain-specific expertise with critical, precise reasoning capabilities akin to
those of top human experts. Existing AI systems often excel at predefined tasks
but struggle with adaptability and precision in novel problem-solving. To
overcome this, AEI introduces a framework for ``Probably Approximately Correct
(PAC) Reasoning"". This paradigm provides robust theoretical guarantees for
reliably decomposing complex problems, with a practical mechanism for
controlling reasoning precision. In reference to the division of human thought
into System 1 for intuitive thinking and System 2 for reflective
reasoning~\citep{tversky1974judgment}, we refer to this new type of reasoning
as System 3 for precise reasoning, inspired by the rigor of the scientific
method. AEI thus establishes a foundation for error-bounded, inference-time
learning."
"Twin Transition or Competing Interests? Validation of the Artificial
  Intelligence and Sustainability Perceptions Inventory (AISPI)","As artificial intelligence (AI) and sustainability initiatives increasingly
intersect, understanding public perceptions of their relationship becomes
crucial for successful implementation. However, no validated instrument exists
to measure these specific perceptions. This paper presents the development and
validation of the Artificial Intelligence and Sustainability Perceptions
Inventory (AISPI), a novel 13-item instrument measuring how individuals view
the relationship between AI advancement and environmental sustainability.
Through factor analysis (N=105), we identified two distinct dimensions: Twin
Transition and Competing Interests. The instrument demonstrated strong
reliability (alpha=.89) and construct validity through correlations with
established measures of AI and sustainability attitudes. Our findings suggest
that individuals can simultaneously recognize both synergies and tensions in
the AI-sustainability relationship, offering important implications for
researchers and practitioners working at this critical intersection. This work
provides a foundational tool for future research on public perceptions of AI's
role in sustainable development."
Some things to know about achieving artificial general intelligence,"Current and foreseeable GenAI models are not capable of achieving artificial
general intelligence because they are burdened with anthropogenic debt. They
depend heavily on human input to provide well-structured problems,
architecture, and training data. They cast every problem as a language pattern
learning problem and are thus not capable of the kind of autonomy needed to
achieve artificial general intelligence. Current models succeed at their tasks
because people solve most of the problems to which these models are directed,
leaving only simple computations for the model to perform, such as gradient
descent. Another barrier is the need to recognize that there are multiple kinds
of problems, some of which cannot be solved by available computational methods
(for example, ""insight problems""). Current methods for evaluating models
(benchmarks and tests) are not adequate to identify the generality of the
solutions, because it is impossible to infer the means by which a problem was
solved from the fact of its solution. A test could be passed, for example, by a
test-specific or a test-general method. It is a logical fallacy (affirming the
consequent) to infer a method of solution from the observation of success."
"Neuro-Conceptual Artificial Intelligence: Integrating OPM with Deep
  Learning to Enhance Question Answering Quality","Knowledge representation and reasoning are critical challenges in Artificial
Intelligence (AI), particularly in integrating neural and symbolic approaches
to achieve explainable and transparent AI systems. Traditional knowledge
representation methods often fall short of capturing complex processes and
state changes. We introduce Neuro-Conceptual Artificial Intelligence (NCAI), a
specialization of the neuro-symbolic AI approach that integrates conceptual
modeling using Object-Process Methodology (OPM) ISO 19450:2024 with deep
learning to enhance question-answering (QA) quality. By converting natural
language text into OPM models using in-context learning, NCAI leverages the
expressive power of OPM to represent complex OPM elements-processes, objects,
and states-beyond what traditional triplet-based knowledge graphs can easily
capture. This rich structured knowledge representation improves reasoning
transparency and answer accuracy in an OPM-QA system. We further propose
transparency evaluation metrics to quantitatively measure how faithfully the
predicted reasoning aligns with OPM-based conceptual logic. Our experiments
demonstrate that NCAI outperforms traditional methods, highlighting its
potential for advancing neuro-symbolic AI by providing rich knowledge
representations, measurable transparency, and improved reasoning."
Is Mathematics Obsolete?,"This is an essay about the value of mathematical and symbolic reasoning in
the age of AI."
"Systematic Review of Cybersecurity in Banking: Evolution from
  Pre-Industry 4.0 to Post-Industry 4.0 in Artificial Intelligence, Blockchain,
  Policies and Practice","Throughout the history from pre-industry 4.0 to post-industry 4.0,
cybersecurity at banks has undergone significant changes. Pre-industry 4.0
cyber security at banks relied on individual security methods that were highly
manual and had low accuracy. When moving to post-industry 4.0, cybersecurity at
banks had a major turning point with security methods that combined different
technologies such as Artificial Intelligence (AI), Blockchain, IoT, automating
necessary processes and significantly increasing the defence layer for banks.
However, along with the development of new technologies, the current challenge
of cybersecurity at banks lies in scalability, high costs and resources in both
money and time for R&D of defence methods along with the threat of high-tech
cybercriminals growing and expanding. This report goes from introducing the
importance of cybersecurity at banks, analyzing their management, operational
and business objectives, evaluating pre-industry 4.0 technologies used for
cybersecurity at banks to assessing post-industry 4.0 technologies focusing on
Artificial Intelligence and Blockchain, discussing current policies and
practices and ending with discussing key advantages and challenges for 4.0
technologies and recommendations for further developing cybersecurity at banks."
"PassAI: explainable artificial intelligence algorithm for soccer pass
  analysis using multimodal information resources","This study developed a new explainable artificial intelligence algorithm
called PassAI, which classifies successful or failed passes in a soccer game
and explains its rationale using both tracking and passer's seasonal stats
information. This study aimed to address two primary challenges faced by
artificial intelligence and machine learning algorithms in the sports domain:
how to use different modality data for the analysis and how to explain the
rationale of the outcome from multimodal perspectives. To address these
challenges, PassAI has two processing streams for multimodal information:
tracking image data and passer's stats and classifying pass success and
failure. After completing the classification, it provides a rationale by either
calculating the relative contribution between the different modality data or
providing more detailed contribution factors within the modality. The results
of the experiment with 6,349 passes of data obtained from professional soccer
games revealed that PassAI showed higher classification performance than
state-of-the-art algorithms by >5% and could visualize the rationale of the
pass success/failure for both tracking and stats data. These results highlight
the importance of using multimodality data in the sports domain to increase the
performance of the artificial intelligence algorithm and explainability of the
outcomes."
"GENEOnet: Statistical analysis supporting explainability and
  trustworthiness","Group Equivariant Non-Expansive Operators (GENEOs) have emerged as
mathematical tools for constructing networks for Machine Learning and
Artificial Intelligence. Recent findings suggest that such models can be
inserted within the domain of eXplainable Artificial Intelligence (XAI) due to
their inherent interpretability. In this study, we aim to verify this claim
with respect to GENEOnet, a GENEO network developed for an application in
computational biochemistry by employing various statistical analyses and
experiments. Such experiments first allow us to perform a sensitivity analysis
on GENEOnet's parameters to test their significance. Subsequently, we show that
GENEOnet exhibits a significantly higher proportion of equivariance compared to
other methods. Lastly, we demonstrate that GENEOnet is on average robust to
perturbations arising from molecular dynamics. These results collectively serve
as proof of the explainability, trustworthiness, and robustness of GENEOnet and
confirm the beneficial use of GENEOs in the context of Trustworthy Artificial
Intelligence."
"OmniGeo: Towards a Multimodal Large Language Models for Geospatial
  Artificial Intelligence","The rapid advancement of multimodal large language models (LLMs) has opened
new frontiers in artificial intelligence, enabling the integration of diverse
large-scale data types such as text, images, and spatial information. In this
paper, we explore the potential of multimodal LLMs (MLLM) for geospatial
artificial intelligence (GeoAI), a field that leverages spatial data to address
challenges in domains including Geospatial Semantics, Health Geography, Urban
Geography, Urban Perception, and Remote Sensing. We propose a MLLM (OmniGeo)
tailored to geospatial applications, capable of processing and analyzing
heterogeneous data sources, including satellite imagery, geospatial metadata,
and textual descriptions. By combining the strengths of natural language
understanding and spatial reasoning, our model enhances the ability of
instruction following and the accuracy of GeoAI systems. Results demonstrate
that our model outperforms task-specific models and existing LLMs on diverse
geospatial tasks, effectively addressing the multimodality nature while
achieving competitive results on the zero-shot geospatial tasks. Our code will
be released after publication."
"The impact of artificial intelligence: from cognitive costs to global
  inequality","In this paper, we examine the wide-ranging impact of artificial intelligence
on society, focusing on its potential to both help and harm global equity,
cognitive abilities, and economic stability. We argue that while artificial
intelligence offers significant opportunities for progress in areas like
healthcare, education, and scientific research, its rapid growth -- mainly
driven by private companies -- may worsen global inequalities, increase
dependence on automated systems for cognitive tasks, and disrupt established
economic paradigms. We emphasize the critical need for strong governance and
ethical guidelines to tackle these issues, urging the academic community to
actively participate in creating policies that ensure the benefits of
artificial intelligence are shared fairly and its risks are managed
effectively."
AIJIM: A Scalable Model for Real-Time AI in Environmental Journalism,"This paper introduces AIJIM, the Artificial Intelligence Journalism
Integration Model -- a novel framework for integrating real-time AI into
environmental journalism. AIJIM combines Vision Transformer-based hazard
detection, crowdsourced validation with 252 validators, and automated reporting
within a scalable, modular architecture. A dual-layer explainability approach
ensures ethical transparency through fast CAM-based visual overlays and
optional LIME-based box-level interpretations. Validated in a 2024 pilot on the
island of Mallorca using the NamicGreen platform, AIJIM achieved 85.4\%
detection accuracy and 89.7\% agreement with expert annotations, while reducing
reporting latency by 40\%. Unlike conventional approaches such as Data-Driven
Journalism or AI Fact-Checking, AIJIM provides a transferable model for
participatory, community-driven environmental reporting, advancing journalism,
artificial intelligence, and sustainability in alignment with the UN
Sustainable Development Goals and the EU AI Act."
"How Artificial Intelligence Leads to Knowledge Why: An Inquiry Inspired
  by Aristotle's Posterior Analytics","Bayesian networks and causal models provide frameworks for handling queries
about external interventions and counterfactuals, enabling tasks that go beyond
what probability distributions alone can address. While these formalisms are
often informally described as capturing causal knowledge, there is a lack of a
formal theory characterizing the type of knowledge required to predict the
effects of external interventions. This work introduces the theoretical
framework of causal systems to clarify Aristotle's distinction between
knowledge that and knowledge why within artificial intelligence. By
interpreting existing artificial intelligence technologies as causal systems,
it investigates the corresponding types of knowledge. Furthermore, it argues
that predicting the effects of external interventions is feasible only with
knowledge why, providing a more precise understanding of the knowledge
necessary for such tasks."
"Post-processing improves accuracy of Artificial Intelligence weather
  forecasts","Artificial Intelligence (AI) weather models are now reaching
operational-grade performance for some variables, but like traditional
Numerical Weather Prediction (NWP) models, they exhibit systematic biases and
reliability issues. We test the application of the Bureau of Meteorology's
existing statistical post-processing system, IMPROVER, to ECMWF's deterministic
Artificial Intelligence Forecasting System (AIFS), and compare results against
post-processed outputs from the ECMWF HRES and ENS models. Without any
modification to configuration or processing workflows, post-processing yields
comparable accuracy improvements for AIFS as for traditional NWP forecasts, in
both expected value and probabilistic outputs. We show that blending AIFS with
NWP models improves overall forecast skill, even when AIFS alone is not the
most accurate component. These findings show that statistical post-processing
methods developed for NWP are directly applicable to AI models, enabling
national meteorological centres to incorporate AI forecasts into existing
workflows in a low-risk, incremental fashion."
"A Model for Web-Intelligence Index to Evaluate the Web Intelligence
  Capacity of Government Web Sites of Sri Lanka","Web intelligence can be considered as a subset of Artificial Intelligence. It
uses existing data in web to produce new data, knowledge and wisdom to support
decision making and new predictions for web users. Artificial Intelligence is
ever changing and evolving field of computer science and it is extensively used
in wide array of web based business applications. Although it is used
substantially in web based systems in developed countries, it is not examined
whether it is being substantially used in Sri Lanka. Every Sri Lankan citizen
depends on Public Service more or less throughout his/ her life time and at
least more than 3 times: at birth, marriage and death. So providing most of
these services to its citizen, Sri Lankan Government uses more or less of its
country web portal. This paper presents a model to evaluate web intelligence
capability based on weight to key functionalities with respect to web
intelligence. The government websites were checked by the proposed criteria to
show the potential of using web intelligent technology to provide website based
services. The result indicates that the use of web intelligence techniques
openly and publicly to provide web based services through government web portal
to its citizens is not satisfactory. It also indicates that lack of using the
technologies pertaining to web intelligence in the public service web hinders
the most of the advantages that citizen and government can gain from such
technological involvement."
"Internet of Intelligence: The Collective Advantage for Advancing
  Communications and Intelligence","The fifth-generation cellular networks (5G) has boosted the unprecedented
convergence between the information world and physical world. On the other
hand, empowered with the enormous amount of data and information, artificial
intelligence (AI) has been universally applied and pervasive AI is believed to
be an integral part of the six-generation cellular networks (6G). Consequently,
benefiting from the advancement in communication technology and AI, we boldly
argue that the conditions for collective intelligence (CI) will be mature in
the 6G era and CI will emerge among the widely connected beings and things.
Afterwards, we highlight the potential huge impact of CI on both communications
and intelligence. In particular, we introduce a regular language (i.e., the
information economy metalanguage) supporting the future collective
communications to augment human intelligence and explain its potential
applications in naming Internet information and pushing information centric
networks forward. Meanwhile, we propose a stigmergy-based federated collective
intelligence and demonstrate its achievement in a simulated scenario where the
agents collectively work together to form a pattern through simple indirect
communications. In a word, CI could advance both communications and
intelligence."
Artificial Intelligence-Enabled Intelligent 6G Networks,"With the rapid development of smart terminals and infrastructures, as well as
diversified applications (e.g., virtual and augmented reality, remote surgery
and holographic projection) with colorful requirements, current networks (e.g.,
4G and upcoming 5G networks) may not be able to completely meet quickly rising
traffic demands. Accordingly, efforts from both industry and academia have
already been put to the research on 6G networks. Recently, artificial
intelligence (AI) has been utilized as a new paradigm for the design and
optimization of 6G networks with a high level of intelligence. Therefore, this
article proposes an AI-enabled intelligent architecture for 6G networks to
realize knowledge discovery, smart resource management, automatic network
adjustment and intelligent service provisioning, where the architecture is
divided into four layers: intelligent sensing layer, data mining and analytics
layer, intelligent control layer and smart application layer. We then review
and discuss the applications of AI techniques for 6G networks and elaborate how
to employ the AI techniques to efficiently and effectively optimize the network
performance, including AI-empowered mobile edge computing, intelligent mobility
and handover management, and smart spectrum management. Moreover, we highlight
important future research directions and potential solutions for AI-enabled
intelligent 6G networks, including computation efficiency, algorithms
robustness, hardware development and energy management."
"Federated Machine Learning for Intelligent IoT via Reconfigurable
  Intelligent Surface","Intelligent Internet-of-Things (IoT) will be transformative with the
advancement of artificial intelligence and high-dimensional data analysis,
shifting from ""connected things"" to ""connected intelligence"". This shall
unleash the full potential of intelligent IoT in a plethora of exciting
applications, such as self-driving cars, unmanned aerial vehicles, healthcare,
robotics, and supply chain finance. These applications drive the need of
developing revolutionary computation, communication and artificial intelligence
technologies that can make low-latency decisions with massive real-time data.
To this end, federated machine learning, as a disruptive technology, is emerged
to distill intelligence from the data at network edge, while guaranteeing
device privacy and data security. However, the limited communication bandwidth
is a key bottleneck of model aggregation for federated machine learning over
radio channels. In this article, we shall develop an over-the-air computation
based communication-efficient federated machine learning framework for
intelligent IoT networks via exploiting the waveform superposition property of
a multi-access channel. Reconfigurable intelligent surface is further leveraged
to reduce the model aggregation error via enhancing the signal strength by
reconfiguring the wireless propagation environments."
6G White Paper on Edge Intelligence,"In this white paper we provide a vision for 6G Edge Intelligence. Moving
towards 5G and beyond the future 6G networks, intelligent solutions utilizing
data-driven machine learning and artificial intelligence become crucial for
several real-world applications including but not limited to, more efficient
manufacturing, novel personal smart device environments and experiences, urban
computing and autonomous traffic settings. We present edge computing along with
other 6G enablers as a key component to establish the future 2030 intelligent
Internet technologies as shown in this series of 6G White Papers.
  In this white paper, we focus in the domains of edge computing infrastructure
and platforms, data and edge network management, software development for edge,
and real-time and distributed training of ML/AI algorithms, along with
security, privacy, pricing, and end-user aspects. We discuss the key enablers
and challenges and identify the key research questions for the development of
the Intelligent Edge services. As a main outcome of this white paper, we
envision a transition from Internet of Things to Intelligent Internet of
Intelligent Things and provide a roadmap for development of 6G Intelligent
Edge."
"Examining the Influence of Varied Levels of Domain Knowledge Base
  Inclusion in GPT-based Intelligent Tutors","Recent advancements in large language models (LLMs) have facilitated the
development of chatbots with sophisticated conversational capabilities.
However, LLMs exhibit frequent inaccurate responses to queries, hindering
applications in educational settings. In this paper, we investigate the
effectiveness of integrating a knowledge base (KB) with LLM intelligent tutors
to increase response reliability. To achieve this, we design a scaleable KB
that affords educational supervisors seamless integration of lesson curricula,
which is automatically processed by the intelligent tutoring system. We then
detail an evaluation, where student participants were presented with questions
about the artificial intelligence curriculum to respond to. GPT-4 intelligent
tutors with varying hierarchies of KB access and human domain experts then
assessed these responses. Lastly, students cross-examined the intelligent
tutors' responses to the domain experts' and ranked their various pedagogical
abilities. Results suggest that, although these intelligent tutors still
demonstrate a lower accuracy compared to domain experts, the accuracy of the
intelligent tutors increases when access to a KB is granted. We also observe
that the intelligent tutors with KB access exhibit better pedagogical abilities
to speak like a teacher and understand students than those of domain experts,
while their ability to help students remains lagging behind domain experts."
Living Together: Mind and Machine Intelligence,"In this paper we consider the nature of the machine intelligences we have
created in the context of our human intelligence. We suggest that the
fundamental difference between human and machine intelligence comes down to
\emph{embodiment factors}. We define embodiment factors as the ratio between an
entity's ability to communicate information vs compute information. We
speculate on the role of embodiment factors in driving our own intelligence and
consciousness. We briefly review dual process models of cognition and cast
machine intelligence within that framework, characterising it as a dominant
System Zero, which can drive behaviour through interfacing with us
subconsciously. Driven by concerns about the consequence of such a system we
suggest prophylactic courses of action that could be considered. Our main
conclusion is that it is \emph{not} sentient intelligence we should fear but
\emph{non-sentient} intelligence."
Universal Agent Mixtures and the Geometry of Intelligence,"Inspired by recent progress in multi-agent Reinforcement Learning (RL), in
this work we examine the collective intelligent behaviour of theoretical
universal agents by introducing a weighted mixture operation. Given a weighted
set of agents, their weighted mixture is a new agent whose expected total
reward in any environment is the corresponding weighted average of the original
agents' expected total rewards in that environment. Thus, if RL agent
intelligence is quantified in terms of performance across environments, the
weighted mixture's intelligence is the weighted average of the original agents'
intelligences. This operation enables various interesting new theorems that
shed light on the geometry of RL agent intelligence, namely: results about
symmetries, convex agent-sets, and local extrema. We also show that any RL
agent intelligence measure based on average performance across environments,
subject to certain weak technical conditions, is identical (up to a constant
factor) to performance within a single environment dependent on said
intelligence measure."
Is Intelligence Artificial?,"Our understanding of intelligence is directed primarily at the human level.
This paper attempts to give a more unifying definition that can be applied to
the natural world in general and then Artificial Intelligence. The definition
would be used more to qualify than quantify it and might help when making
judgements on the matter. While correct behaviour is the preferred definition,
a metric that is grounded in Kolmogorov's Complexity Theory is suggested, which
leads to a measurement about entropy. A version of an accepted AI test is then
put forward as the 'acid test' and might be what a free-thinking program would
try to achieve. Recent work by the author has been more from a direction of
mechanical processes, or ones that might operate automatically. This paper
agrees that intelligence is a pro-active event, but also notes a second aspect
to it that is in the background and mechanical. The paper suggests looking at
intelligence and the conscious as being slightly different, where the conscious
is this more mechanical aspect. In fact, a surprising conclusion can be a
passive but intelligent brain being invoked by active and less intelligent
senses."
"Edge Intelligence: The Confluence of Edge Computing and Artificial
  Intelligence","Along with the rapid developments in communication technologies and the surge
in the use of mobile devices, a brand-new computation paradigm, Edge Computing,
is surging in popularity. Meanwhile, Artificial Intelligence (AI) applications
are thriving with the breakthroughs in deep learning and the many improvements
in hardware architectures. Billions of data bytes, generated at the network
edge, put massive demands on data processing and structural optimization. Thus,
there exists a strong demand to integrate Edge Computing and AI, which gives
birth to Edge Intelligence. In this paper, we divide Edge Intelligence into AI
for edge (Intelligence-enabled Edge Computing) and AI on edge (Artificial
Intelligence on Edge). The former focuses on providing more optimal solutions
to key problems in Edge Computing with the help of popular and effective AI
technologies while the latter studies how to carry out the entire process of
building AI models, i.e., model training and inference, on the edge. This paper
provides insights into this new inter-disciplinary field from a broader
perspective. It discusses the core concepts and the research road-map, which
should provide the necessary background for potential future research
initiatives in Edge Intelligence."
A World-Self Model Towards Understanding Intelligence,"The symbolism, connectionism and behaviorism approaches of artificial
intelligence have achieved a lot of successes in various tasks, while we still
do not have a clear definition of ""intelligence"" with enough consensus in the
community (although there are over 70 different ""versions"" of definitions). The
nature of intelligence is still in darkness. In this work we do not take any of
these three traditional approaches, instead we try to identify certain
fundamental aspects of the nature of intelligence, and construct a mathematical
model to represent and potentially reproduce these fundamental aspects. We
first stress the importance of defining the scope of discussion and granularity
of investigation. We carefully compare human and artificial intelligence, and
qualitatively demonstrate an information abstraction process, which we propose
to be the key to connect perception and cognition. We then present the broader
idea of ""concept"", separate the idea of self model out of the world model, and
construct a new model called world-self model (WSM). We show the mechanisms of
creating and connecting concepts, and the flow of how the WSM receives,
processes and outputs information with respect to an arbitrary type of problem
to solve. We also consider and discuss the potential computer implementation
issues of the proposed theoretical framework, and finally we propose a unified
general framework of intelligence based on WSM."
"An intelligent tutor for planning in large partially observable
  environments","AI can not only outperform people in many planning tasks, but it can also
teach them how to plan better. A recent and promising approach to improving
human decision-making is to create intelligent tutors that utilize AI to
discover and teach optimal planning strategies automatically. Prior work has
shown that this approach can improve planning in artificial, fully observable
planning tasks. Unlike these artificial tasks, the world is only partially
observable. To bridge this gap, we developed and evaluated the first
intelligent tutor for planning in partially observable environments. Compared
to previous intelligent tutors for teaching planning strategies, this novel
intelligent tutor combines two innovations: 1) a new metareasoning algorithm
for discovering optimal planning strategies for large, partially observable
environments, and 2) scaffolding the learning processing by having the learner
choose from an increasing larger set of planning operations in increasingly
larger planning problems. We found that our new strategy discovery algorithm is
superior to the state-of-the-art. A preregistered experiment with 330
participants demonstrated that the new intelligent tutor is highly effective at
improving people's ability to make good decisions in partially observable
environments. This suggests our human-centered tutoring approach can
successfully boost human planning in complex, partially observable sequential
decision problems, a promising step towards using AI-powered intelligent tutors
to improve human planning in the real world."
"Using Artificial Intelligence to Accelerate Collective Intelligence:
  Policy Synth and Smarter Crowdsourcing","In an era characterized by rapid societal changes and complex challenges,
institutions' traditional methods of problem-solving in the public sector are
increasingly proving inadequate. In this study, we present an innovative and
effective model for how institutions can use artificial intelligence to enable
groups of people to generate effective solutions to urgent problems more
efficiently. We describe a proven collective intelligence method, called
Smarter Crowdsourcing, which is designed to channel the collective intelligence
of those with expertise about a problem into actionable solutions through
crowdsourcing. Then we introduce Policy Synth, an innovative toolkit which
leverages AI to make the Smarter Crowdsourcing problem-solving approach both
more scalable, more effective and more efficient. Policy Synth is crafted using
a human-centric approach, recognizing that AI is a tool to enhance human
intelligence and creativity, not replace it. Based on a real-world case study
comparing the results of expert crowdsourcing alone with expert sourcing
supported by Policy Synth AI agents, we conclude that Smarter Crowdsourcing
with Policy Synth presents an effective model for integrating the collective
wisdom of human experts and the computational power of AI to enhance and scale
up public problem-solving processes. While many existing approaches view AI as
a tool to make crowdsourcing and deliberative processes better and more
efficient, Policy Synth goes a step further, recognizing that AI can also be
used to synthesize the findings from engagements together with research to
develop evidence-based solutions and policies. The study offers practical tools
and insights for institutions looking to engage communities effectively in
addressing urgent societal challenges."
"The Concept of the Deep Learning-Based System ""Artificial Dispatcher"" to
  Power System Control and Dispatch","Year by year control of normal and emergency conditions of up-to-date power
systems becomes an increasingly complicated problem. With the increasing
complexity the existing control system of power system conditions which
includes operative actions of the dispatcher and work of special automatic
devices proves to be insufficiently effective more and more frequently, which
raises risks of dangerous and emergency conditions in power systems. The paper
is aimed at compensating for the shortcomings of man (a cognitive barrier,
exposure to stresses and so on) and automatic devices by combining their strong
points, i.e. the dispatcher's intelligence and the speed of automatic devices
by virtue of development of the intelligent system ""Artificial dispatcher"" on
the basis of deep machine learning technology. For realization of the system
""Artificial dispatcher"" in addition to deep learning it is planned to attract
the game theory approaches to formalize work of the up-to-date power system as
a game problem. The ""gain"" for ""Artificial dispatcher"" will consist in bringing
in a power system in the normal steady-state or post-emergency conditions by
means of the required control actions."
Sensorimotor learning for artificial body perception,"Artificial self-perception is the machine ability to perceive its own body,
i.e., the mastery of modal and intermodal contingencies of performing an action
with a specific sensors/actuators body configuration. In other words, the
spatio-temporal patterns that relate its sensors (e.g. visual, proprioceptive,
tactile, etc.), its actions and its body latent variables are responsible of
the distinction between its own body and the rest of the world. This paper
describes some of the latest approaches for modelling artificial body
self-perception: from Bayesian estimation to deep learning. Results show the
potential of these free-model unsupervised or semi-supervised
crossmodal/intermodal learning approaches. However, there are still challenges
that should be overcome before we achieve artificial multisensory body
perception."
"Towards continual task learning in artificial neural networks: current
  approaches and insights from neuroscience","The innate capacity of humans and other animals to learn a diverse, and often
interfering, range of knowledge and skills throughout their lifespan is a
hallmark of natural intelligence, with obvious evolutionary motivations. In
parallel, the ability of artificial neural networks (ANNs) to learn across a
range of tasks and domains, combining and re-using learned representations
where required, is a clear goal of artificial intelligence. This capacity,
widely described as continual learning, has become a prolific subfield of
research in machine learning. Despite the numerous successes of deep learning
in recent years, across domains ranging from image recognition to machine
translation, such continual task learning has proved challenging. Neural
networks trained on multiple tasks in sequence with stochastic gradient descent
often suffer from representational interference, whereby the learned weights
for a given task effectively overwrite those of previous tasks in a process
termed catastrophic forgetting. This represents a major impediment to the
development of more generalised artificial learning systems, capable of
accumulating knowledge over time and task space, in a manner analogous to
humans. A repository of selected papers and implementations accompanying this
review can be found at https://github.com/mccaffary/continual-learning."
"Artificial Intelligence (AI) and the Relationship between Agency,
  Autonomy, and Moral Patiency","The proliferation of Artificial Intelligence (AI) systems exhibiting complex
and seemingly agentive behaviours necessitates a critical philosophical
examination of their agency, autonomy, and moral status. In this paper we
undertake a systematic analysis of the differences between basic, autonomous,
and moral agency in artificial systems. We argue that while current AI systems
are highly sophisticated, they lack genuine agency and autonomy because: they
operate within rigid boundaries of pre-programmed objectives rather than
exhibiting true goal-directed behaviour within their environment; they cannot
authentically shape their engagement with the world; and they lack the critical
self-reflection and autonomy competencies required for full autonomy.
Nonetheless, we do not rule out the possibility of future systems that could
achieve a limited form of artificial moral agency without consciousness through
hybrid approaches to ethical decision-making. This leads us to suggest, by
appealing to the necessity of consciousness for moral patiency, that such
non-conscious AMAs might represent a case that challenges traditional
assumptions about the necessary connection between moral agency and moral
patiency."
"Artificial Intelligence Framework for Simulating Clinical
  Decision-Making: A Markov Decision Process Approach","In the modern healthcare system, rapidly expanding costs/complexity, the
growing myriad of treatment options, and exploding information streams that
often do not effectively reach the front lines hinder the ability to choose
optimal treatment decisions over time. The goal in this paper is to develop a
general purpose (non-disease-specific) computational/artificial intelligence
(AI) framework to address these challenges. This serves two potential
functions: 1) a simulation environment for exploring various healthcare
policies, payment methodologies, etc., and 2) the basis for clinical artificial
intelligence - an AI that can think like a doctor. This approach combines
Markov decision processes and dynamic decision networks to learn from clinical
data and develop complex plans via simulation of alternative sequential
decision paths while capturing the sometimes conflicting, sometimes synergistic
interactions of various components in the healthcare system. It can operate in
partially observable environments (in the case of missing observations or data)
by maintaining belief states about patient health status and functions as an
online agent that plans and re-plans. This framework was evaluated using real
patient data from an electronic health record. Such an AI framework easily
outperforms the current treatment-as-usual (TAU) case-rate/fee-for-service
models of healthcare (Cost per Unit Change: $189 vs. $497) while obtaining a
30-35% increase in patient outcomes. Tweaking certain model parameters further
enhances this advantage, obtaining roughly 50% more improvement for roughly
half the costs. Given careful design and problem formulation, an AI simulation
framework can approximate optimal decisions even in complex and uncertain
environments. Future work is described that outlines potential lines of
research and integration of machine learning algorithms for personalized
medicine."
"AI Researchers, Video Games Are Your Friends!","If you are an artificial intelligence researcher, you should look to video
games as ideal testbeds for the work you do. If you are a video game developer,
you should look to AI for the technology that makes completely new types of
games possible. This chapter lays out the case for both of these propositions.
It asks the question ""what can video games do for AI"", and discusses how in
particular general video game playing is the ideal testbed for artificial
general intelligence research. It then asks the question ""what can AI do for
video games"", and lays out a vision for what video games might look like if we
had significantly more advanced AI at our disposal. The chapter is based on my
keynote at IJCCI 2015, and is written in an attempt to be accessible to a broad
audience."
"Realizing an optimization approach inspired from Piagets theory on
  cognitive development","The objective of this paper is to introduce an artificial intelligence based
optimization approach, which is inspired from Piagets theory on cognitive
development. The approach has been designed according to essential processes
that an individual may experience while learning something new or improving his
/ her knowledge. These processes are associated with the Piagets ideas on an
individuals cognitive development. The approach expressed in this paper is a
simple algorithm employing swarm intelligence oriented tasks in order to
overcome single-objective optimization problems. For evaluating effectiveness
of this early version of the algorithm, test operations have been done via some
benchmark functions. The obtained results show that the approach / algorithm
can be an alternative to the literature in terms of single-objective
optimization. The authors have suggested the name: Cognitive Development
Optimization Algorithm (CoDOA) for the related intelligent optimization
approach."
Artificial Intelligence and Legal Liability,"A recent issue of a popular computing journal asked which laws would apply if
a self-driving car killed a pedestrian. This paper considers the question of
legal liability for artificially intelligent computer systems. It discusses
whether criminal liability could ever apply; to whom it might apply; and, under
civil law, whether an AI program is a product that is subject to product design
legislation or a service to which the tort of negligence applies. The issue of
sales warranties is also considered. A discussion of some of the practical
limitations that AI systems are subject to is also included."
"Institutional Metaphors for Designing Large-Scale Distributed AI versus
  AI Techniques for Running Institutions","Artificial Intelligence (AI) started out with an ambition to reproduce the
human mind, but, as the sheer scale of that ambition became manifest, it
quickly retreated into either studying specialized intelligent behaviours, or
proposing over-arching architectural concepts for interfacing specialized
intelligent behaviour components, conceived of as agents in a kind of
organization. This agent-based modeling paradigm, in turn, proves to have
interesting applications in understanding, simulating, and predicting the
behaviour of social and legal structures on an aggregate level. For these
reasons, this chapter examines a number of relevant cross-cutting concerns,
conceptualizations, modeling problems and design challenges in large-scale
distributed Artificial Intelligence, as well as in institutional systems, and
identifies potential grounds for novel advances."
"Verisimilar Percept Sequences Tests for Autonomous Driving Intelligent
  Agent Assessment","The autonomous car technology promises to replace human drivers with safer
driving systems. But although autonomous cars can become safer than human
drivers this is a long process that is going to be refined over time. Before
these vehicles are deployed on urban roads a minimum safety level must be
assured. Since the autonomous car technology is still under development there
is no standard methodology to evaluate such systems. It is important to
completely understand the technology that is being developed to design
efficient means to evaluate it. In this paper we assume safety-critical systems
reliability as a safety measure. We model an autonomous road vehicle as an
intelligent agent and we approach its evaluation from an artificial
intelligence perspective. Our focus is the evaluation of perception and
decision making systems and also to propose a systematic method to evaluate
their integration in the vehicle. We identify critical aspects of the data
dependency from the artificial intelligence state of the art models and we also
propose procedures to reproduce them."
Subjectivity Learning Theory towards Artificial General Intelligence,"The construction of artificial general intelligence (AGI) was a long-term
goal of AI research aiming to deal with the complex data in the real world and
make reasonable judgments in various cases like a human. However, the current
AI creations, referred to as ""Narrow AI"", are limited to a specific problem.
The constraints come from two basic assumptions of data, which are independent
and identical distributed samples and single-valued mapping between inputs and
outputs. We completely break these constraints and develop the subjectivity
learning theory for general intelligence. We assign the mathematical meaning
for the philosophical concept of subjectivity and build the data representation
of general intelligence. Under the subjectivity representation, then the global
risk is constructed as the new learning goal. We prove that subjectivity
learning holds a lower risk bound than traditional machine learning. Moreover,
we propose the principle of empirical global risk minimization (EGRM) as the
subjectivity learning process in practice, establish the condition of
consistency, and present triple variables for controlling the total risk bound.
The subjectivity learning is a novel learning theory for unconstrained real
data and provides a path to develop AGI."
"Artificial Intelligence Strategies for National Security and Safety
  Standards","Recent advances in artificial intelligence (AI) have lead to an explosion of
multimedia applications (e.g., computer vision (CV) and natural language
processing (NLP)) for different domains such as commercial, industrial, and
intelligence. In particular, the use of AI applications in a national security
environment is often problematic because the opaque nature of the systems leads
to an inability for a human to understand how the results came about. A
reliance on 'black boxes' to generate predictions and inform decisions is
potentially disastrous. This paper explores how the application of standards
during each stage of the development of an AI system deployed and used in a
national security environment would help enable trust. Specifically, we focus
on the standards outlined in Intelligence Community Directive 203 (Analytic
Standards) to subject machine outputs to the same rigorous standards as
analysis performed by humans."
"Moral Dilemmas for Artificial Intelligence: a position paper on an
  application of Compositional Quantum Cognition","Traditionally, the way one evaluates the performance of an Artificial
Intelligence (AI) system is via a comparison to human performance in specific
tasks, treating humans as a reference for high-level cognition. However, these
comparisons leave out important features of human intelligence: the capability
to transfer knowledge and make complex decisions based on emotional and
rational reasoning. These decisions are influenced by current inferences as
well as prior experiences, making the decision process strongly subjective and
apparently biased. In this context, a definition of compositional intelligence
is necessary to incorporate these features in future AI tests. Here, a concrete
implementation of this will be suggested, using recent developments in quantum
cognition, natural language and compositional meaning of sentences, thanks to
categorical compositional models of meaning."
BlackBox Toolkit: Intelligent Assistance to UI Design,"User Interface (UI) design is an creative process that involves considerable
reiteration and rework. Designers go through multiple iterations of different
prototyping fidelities to create a UI design. In this research, we propose to
modify the UI design process by assisting it with artificial intelligence (AI).
We propose to enable AI to perform repetitive tasks for the designer while
allowing the designer to take command of the creative process. This approach
makes the machine act as a black box that intelligently assists the designers
in creating UI design. We believe this approach would greatly benefit designers
in co-creating design solutions with AI."
Artificial Musical Intelligence: A Survey,"Computers have been used to analyze and create music since they were first
introduced in the 1950s and 1960s. Beginning in the late 1990s, the rise of the
Internet and large scale platforms for music recommendation and retrieval have
made music an increasingly prevalent domain of machine learning and artificial
intelligence research. While still nascent, several different approaches have
been employed to tackle what may broadly be referred to as ""musical
intelligence."" This article provides a definition of musical intelligence,
introduces a taxonomy of its constituent components, and surveys the wide range
of AI methods that can be, and have been, brought to bear in its pursuit, with
a particular emphasis on machine learning methods."
"Advancing the Research and Development of Assured Artificial
  Intelligence and Machine Learning Capabilities","Artificial intelligence (AI) and machine learning (ML) have become
increasingly vital in the development of novel defense and intelligence
capabilities across all domains of warfare. An adversarial AI (A2I) and
adversarial ML (AML) attack seeks to deceive and manipulate AI/ML models. It is
imperative that AI/ML models can defend against these attacks. A2I/AML defenses
will help provide the necessary assurance of these advanced capabilities that
use AI/ML models. The A2I Working Group (A2IWG) seeks to advance the research
and development of assured AI/ML capabilities via new A2I/AML defenses by
fostering a collaborative environment across the U.S. Department of Defense and
U.S. Intelligence Community. The A2IWG aims to identify specific challenges
that it can help solve or address more directly, with initial focus on three
topics: AI Trusted Robustness, AI System Security, and AI/ML Architecture
Vulnerabilities."
A Definition and a Test for Human-Level Artificial Intelligence,"Despite recent advances of AI research in many application-specific domains,
we do not know how to build a human-level artificial intelligence (HLAI). We
conjecture that learning from others' experience with the language is the
essential characteristic that distinguishes human intelligence from the rest.
Humans can update the action-value function with the verbal description as if
they experience states, actions, and corresponding rewards sequences firsthand.
In this paper, we present a classification of intelligence according to how
individual agents learn and propose a definition and a test for HLAI. The main
idea is that language acquisition without explicit rewards can be a sufficient
test for HLAI."
"A Neural Dynamic Model based on Activation Diffusion and a
  Micro-Explanation for Cognitive Operations","The neural mechanism of memory has a very close relation with the problem of
representation in artificial intelligence. In this paper a computational model
was proposed to simulate the network of neurons in brain and how they process
information. The model refers to morphological and electrophysiological
characteristics of neural information processing, and is based on the
assumption that neurons encode their firing sequence. The network structure,
functions for neural encoding at different stages, the representation of
stimuli in memory, and an algorithm to form a memory were presented. It also
analyzed the stability and recall rate for learning and the capacity of memory.
Because neural dynamic processes, one succeeding another, achieve a
neuron-level and coherent form by which information is represented and
processed, it may facilitate examination of various branches of Artificial
Intelligence, such as inference, problem solving, pattern recognition, natural
language processing and learning. The processes of cognitive manipulation
occurring in intelligent behavior have a consistent representation while all
being modeled from the perspective of computational neuroscience. Thus, the
dynamics of neurons make it possible to explain the inner mechanisms of
different intelligent behaviors by a unified model of cognitive architecture at
a micro-level."
Multisource AI Scorecard Table for System Evaluation,"The paper describes a Multisource AI Scorecard Table (MAST) that provides the
developer and user of an artificial intelligence (AI)/machine learning (ML)
system with a standard checklist focused on the principles of good analysis
adopted by the intelligence community (IC) to help promote the development of
more understandable systems and engender trust in AI outputs. Such a scorecard
enables a transparent, consistent, and meaningful understanding of AI tools
applied for commercial and government use. A standard is built on compliance
and agreement through policy, which requires buy-in from the stakeholders.
While consistency for testing might only exist across a standard data set, the
community requires discussion on verification and validation approaches which
can lead to interpretability, explainability, and proper use. The paper
explores how the analytic tradecraft standards outlined in Intelligence
Community Directive (ICD) 203 can provide a framework for assessing the
performance of an AI system supporting various operational needs. These include
sourcing, uncertainty, consistency, accuracy, and visualization. Three use
cases are presented as notional examples that support security for comparative
analysis."
"The future of human-AI collaboration: a taxonomy of design knowledge for
  hybrid intelligence systems","Recent technological advances, especially in the field of machine learning,
provide astonishing progress on the road towards artificial general
intelligence. However, tasks in current real-world business applications cannot
yet be solved by machines alone. We, therefore, identify the need for
developing socio-technological ensembles of humans and machines. Such systems
possess the ability to accomplish complex goals by combining human and
artificial intelligence to collectively achieve superior results and
continuously improve by learning from each other. Thus, the need for structured
design knowledge for those systems arises. Following a taxonomy development
method, this article provides three main contributions: First, we present a
structured overview of interdisciplinary research on the role of humans in the
machine learning pipeline. Second, we envision hybrid intelligence systems and
conceptualize the relevant dimensions for system design for the first time.
Finally, we offer useful guidance for system developers during the
implementation of such applications."
Chess AI: Competing Paradigms for Machine Intelligence,"Endgame studies have long served as a tool for testing human creativity and
intelligence. We find that they can serve as a tool for testing machine ability
as well. Two of the leading chess engines, Stockfish and Leela Chess Zero
(LCZero), employ significantly different methods during play. We use Plaskett's
Puzzle, a famous endgame study from the late 1970s, to compare the two engines.
Our experiments show that Stockfish outperforms LCZero on the puzzle. We
examine the algorithmic differences between the engines and use our
observations as a basis for carefully interpreting the test results. Drawing
inspiration from how humans solve chess problems, we ask whether machines can
possess a form of imagination. On the theoretical side, we describe how
Bellman's equation may be applied to optimize the probability of winning. To
conclude, we discuss the implications of our work on artificial intelligence
(AI) and artificial general intelligence (AGI), suggesting possible avenues for
future research."
"Scalar reward is not enough: A response to Silver, Singh, Precup and
  Sutton (2021)","The recent paper `""Reward is Enough"" by Silver, Singh, Precup and Sutton
posits that the concept of reward maximisation is sufficient to underpin all
intelligence, both natural and artificial. We contest the underlying assumption
of Silver et al. that such reward can be scalar-valued. In this paper we
explain why scalar rewards are insufficient to account for some aspects of both
biological and computational intelligence, and argue in favour of explicitly
multi-objective models of reward maximisation. Furthermore, we contend that
even if scalar reward functions can trigger intelligent behaviour in specific
cases, it is still undesirable to use this approach for the development of
artificial general intelligence due to unacceptable risks of unsafe or
unethical behaviour."
Efficient Adaptive Federated Optimization of Federated Learning for IoT,"The proliferation of the Internet of Things (IoT) and widespread use of
devices with sensing, computing, and communication capabilities have motivated
intelligent applications empowered by artificial intelligence. The classical
artificial intelligence algorithms require centralized data collection and
processing which are challenging in realistic intelligent IoT applications due
to growing data privacy concerns and distributed datasets. Federated Learning
(FL) has emerged as a distributed privacy-preserving learning framework that
enables IoT devices to train global model through sharing model parameters.
However, inefficiency due to frequent parameters transmissions significantly
reduce FL performance. Existing acceleration algorithms consist of two main
type including local update considering trade-offs between communication and
computation and parameter compression considering trade-offs between
communication and precision. Jointly considering these two trade-offs and
adaptively balancing their impacts on convergence have remained unresolved. To
solve the problem, this paper proposes a novel efficient adaptive federated
optimization (EAFO) algorithm to improve efficiency of FL, which minimizes the
learning error via jointly considering two variables including local update and
parameter compression and enables FL to adaptively adjust the two variables and
balance trade-offs among computation, communication and precision. The
experiment results illustrate that comparing with state-of-the-art algorithms,
the proposed EAFO can achieve higher accuracies faster."
"Towards Systems Education for Artificial Intelligence: A Course Practice
  in Intelligent Computing Architectures","With the rapid development of artificial intelligence (AI) community,
education in AI is receiving more and more attentions. There have been many AI
related courses in the respects of algorithms and applications, while not many
courses in system level are seriously taken into considerations. In order to
bridge the gap between AI and computing systems, we are trying to explore how
to conduct AI education from the perspective of computing systems. In this
paper, a course practice in intelligent computing architectures are provided to
demonstrate the system education in AI era. The motivation for this course
practice is first introduced as well as the learning orientations. The main
goal of this course aims to teach students for designing AI accelerators on
FPGA platforms. The elaborated course contents include lecture notes and
related technical materials. Especially several practical labs and projects are
detailed illustrated. Finally, some teaching experiences and effects are
discussed as well as some potential improvements in the future."
"Core and Periphery as Closed-System Precepts for Engineering General
  Intelligence","Engineering methods are centered around traditional notions of decomposition
and recomposition that rely on partitioning the inputs and outputs of
components to allow for component-level properties to hold after their
composition. In artificial intelligence (AI), however, systems are often
expected to influence their environments, and, by way of their environments, to
influence themselves. Thus, it is unclear if an AI system's inputs will be
independent of its outputs, and, therefore, if AI systems can be treated as
traditional components. This paper posits that engineering general intelligence
requires new general systems precepts, termed the core and periphery, and
explores their theoretical uses. The new precepts are elaborated using abstract
systems theory and the Law of Requisite Variety. By using the presented
material, engineers can better understand the general character of regulating
the outcomes of AI to achieve stakeholder needs and how the general systems
nature of embodiment challenges traditional engineering practice."
"Artificial Intelligence for Cybersecurity: Threats, Attacks and
  Mitigation","With the advent of the digital era, every day-to-day task is automated due to
technological advances. However, technology has yet to provide people with
enough tools and safeguards. As the internet connects more-and-more devices
around the globe, the question of securing the connected devices grows at an
even spiral rate. Data thefts, identity thefts, fraudulent transactions,
password compromises, and system breaches are becoming regular everyday news.
The surging menace of cyber-attacks got a jolt from the recent advancements in
Artificial Intelligence. AI is being applied in almost every field of different
sciences and engineering. The intervention of AI not only automates a
particular task but also improves efficiency by many folds. So it is evident
that such a scrumptious spread would be very appetizing to cybercriminals. Thus
the conventional cyber threats and attacks are now ``intelligent"" threats. This
article discusses cybersecurity and cyber threats along with both conventional
and intelligent ways of defense against cyber-attacks. Furthermore finally, end
the discussion with the potential prospects of the future of AI in
cybersecurity."
"Emergent collective intelligence from massive-agent cooperation and
  competition","Inspired by organisms evolving through cooperation and competition between
different populations on Earth, we study the emergence of artificial collective
intelligence through massive-agent reinforcement learning. To this end, We
propose a new massive-agent reinforcement learning environment, Lux, where
dynamic and massive agents in two teams scramble for limited resources and
fight off the darkness. In Lux, we build our agents through the standard
reinforcement learning algorithm in curriculum learning phases and leverage
centralized control via a pixel-to-pixel policy network. As agents co-evolve
through self-play, we observe several stages of intelligence, from the
acquisition of atomic skills to the development of group strategies. Since
these learned group strategies arise from individual decisions without an
explicit coordination mechanism, we claim that artificial collective
intelligence emerges from massive-agent cooperation and competition. We further
analyze the emergence of various learned strategies through metrics and
ablation studies, aiming to provide insights for reinforcement learning
implementations in massive-agent environments."
When Brain-inspired AI Meets AGI,"Artificial General Intelligence (AGI) has been a long-standing goal of
humanity, with the aim of creating machines capable of performing any
intellectual task that humans can do. To achieve this, AGI researchers draw
inspiration from the human brain and seek to replicate its principles in
intelligent machines. Brain-inspired artificial intelligence is a field that
has emerged from this endeavor, combining insights from neuroscience,
psychology, and computer science to develop more efficient and powerful AI
systems. In this article, we provide a comprehensive overview of brain-inspired
AI from the perspective of AGI. We begin with the current progress in
brain-inspired AI and its extensive connection with AGI. We then cover the
important characteristics for both human intelligence and AGI (e.g., scaling,
multimodality, and reasoning). We discuss important technologies toward
achieving AGI in current AI systems, such as in-context learning and prompt
tuning. We also investigate the evolution of AGI systems from both algorithmic
and infrastructural perspectives. Finally, we explore the limitations and
future of AGI."
"Mathematics of multi-agent learning systems at the interface of game
  theory and artificial intelligence","Evolutionary Game Theory (EGT) and Artificial Intelligence (AI) are two
fields that, at first glance, might seem distinct, but they have notable
connections and intersections. The former focuses on the evolution of behaviors
(or strategies) in a population, where individuals interact with others and
update their strategies based on imitation (or social learning). The more
successful a strategy is, the more prevalent it becomes over time. The latter,
meanwhile, is centered on machine learning algorithms and (deep) neural
networks. It is often from a single-agent perspective but increasingly involves
multi-agent environments, in which intelligent agents adjust their strategies
based on feedback and experience, somewhat akin to the evolutionary process yet
distinct in their self-learning capacities. In light of the key components
necessary to address real-world problems, including (i) learning and
adaptation, (ii) cooperation and competition, (iii) robustness and stability,
and altogether (iv) population dynamics of individual agents whose strategies
evolve, the cross-fertilization of ideas between both fields will contribute to
the advancement of mathematics of multi-agent learning systems, in particular,
to the nascent domain of ``collective cooperative intelligence'' bridging
evolutionary dynamics and multi-agent reinforcement learning."
A social path to human-like artificial intelligence,"Traditionally, cognitive and computer scientists have viewed intelligence
solipsistically, as a property of unitary agents devoid of social context.
Given the success of contemporary learning algorithms, we argue that the
bottleneck in artificial intelligence (AI) progress is shifting from data
assimilation to novel data generation. We bring together evidence showing that
natural intelligence emerges at multiple scales in networks of interacting
agents via collective living, social relationships and major evolutionary
transitions, which contribute to novel data generation through mechanisms such
as population pressures, arms races, Machiavellian selection, social learning
and cumulative culture. Many breakthroughs in AI exploit some of these
processes, from multi-agent structures enabling algorithms to master complex
games like Capture-The-Flag and StarCraft II, to strategic communication in
Diplomacy and the shaping of AI data streams by other AIs. Moving beyond a
solipsistic view of agency to integrate these mechanisms suggests a path to
human-like compounding innovation through ongoing novel data generation."
"Lifelong learning challenges in the era of artificial intelligence: a
  computational thinking perspective","The rapid advancement of artificial intelligence (AI) has brought significant
challenges to the education and workforce skills required to take advantage of
AI for human-AI collaboration in the workplace. As AI continues to reshape
industries and job markets, the need to define how AI literacy can be
considered in lifelong learning has become increasingly critical (Cetindamar et
al., 2022; Laupichler et al., 2022; Romero et al., 2023). Like any new
technology, AI is the subject of both hopes and fears, and what it entails
today presents major challenges (Cugurullo \& Acheampong, 2023; Villani et al.,
2018). It also raises profound questions about our own humanity. Will the
machine surpass the intelligence of the humans who designed it? What will be
the relationship between so-called AI and our human intelligences? How could
human-AI collaboration be regulated in a way that serves the Sustainable
Development Goals (SDGs)? This paper provides a review of the challenges of
lifelong learning in the era of AI from a computational thinking, critical
thinking, and creative competencies perspective, highlighting the implications
for management and leadership in organizations."
MathPartner: An Artificial Intelligence Cloud Service,"In a broad sense, artificial intelligence is a service to find a solution to
complex intellectual problems. In this sense, the MathPartner service provides
artificial intelligence that allows us to formulate questions and receive
answers to questions formulated in a mathematical language. For mathematicians
and physicists today, such a language is \LaTeX. The MathPartner service uses a
dialect of \LaTeX, which is called Mathpar. The service is a cloud-based
computer algebra system and provides users with the opportunity to solve many
mathematical problems. In this publication, we focus only on a small class of
extremum problems, which are widely applied in economics, management,
logistics, and in many engineering fields. In particular, we consider the
shortest path problem and discuss an algorithm that is based on the tropical
mathematics. The ability to work with many types of classical and tropical
algebras, which are freely available to users, is an important distinguishing
feature of this intelligent tool for symbolic-numerical calculations. We also
consider the use of the simplex algorithm for solving optimization problems."
"You Can't Get There From Here: Redefining Information Science to address
  our sociotechnical futures","Current definitions of Information Science are inadequate to comprehensively
describe the nature of its field of study and for addressing the problems that
are arising from intelligent technologies. The ubiquitous rise of artificial
intelligence applications and their impact on society demands the field of
Information Science acknowledge the sociotechnical nature of these
technologies. Previous definitions of Information Science over the last six
decades have inadequately addressed the environmental, human, and social
aspects of these technologies. This perspective piece advocates for an expanded
definition of Information Science that fully includes the sociotechnical
impacts information has on the conduct of research in this field. Proposing an
expanded definition of Information Science that includes the sociotechnical
aspects of this field should stimulate both conversation and widen the
interdisciplinary lens necessary to address how intelligent technologies may be
incorporated into society and our lives more fairly."
"NGENT: Next-Generation AI Agents Must Integrate Multi-Domain Abilities
  to Achieve Artificial General Intelligence","This paper argues that the next generation of AI agent (NGENT) should
integrate across-domain abilities to advance toward Artificial General
Intelligence (AGI). Although current AI agents are effective in specialized
tasks such as robotics, role-playing, and tool-using, they remain confined to
narrow domains. We propose that future AI agents should synthesize the
strengths of these specialized systems into a unified framework capable of
operating across text, vision, robotics, reinforcement learning, emotional
intelligence, and beyond. This integration is not only feasible but also
essential for achieving the versatility and adaptability that characterize
human intelligence. The convergence of technologies across AI domains, coupled
with increasing user demand for cross-domain capabilities, suggests that such
integration is within reach. Ultimately, the development of these versatile
agents is a critical step toward realizing AGI. This paper explores the
rationale for this shift, potential pathways for achieving it."
"Using artificial intelligence for data reduction in mechanical
  engineering","In this paper artificial neural networks and support vector machines are used
to reduce the amount of vibration data that is required to estimate the Time
Domain Average of a gear vibration signal. Two models for estimating the time
domain average of a gear vibration signal are proposed. The models are tested
on data from an accelerated gear life test rig. Experimental results indicate
that the required data for calculating the Time Domain Average of a gear
vibration signal can be reduced by up to 75% when the proposed models are
implemented."
On the Use of Skeletons when Learning in Bayesian Networks,"In this paper, we present a heuristic operator which aims at simultaneously
optimizing the orientations of all the edges in an intermediate Bayesian
network structure during the search process. This is done by alternating
between the space of directed acyclic graphs (DAGs) and the space of skeletons.
The found orientations of the edges are based on a scoring function rather than
on induced conditional independences. This operator can be used as an extension
to commonly employed search strategies. It is evaluated in experiments with
artificial and real-world data."
"Developing an App to interpret Chest X-rays to support the diagnosis of
  respiratory pathology with Artificial Intelligence","In this paper we present our work to improve access to diagnosis in remote
areas where good quality medical services may be lacking. We develop new
Machine Learning methodologies for deployment onto mobile devices to help the
early diagnosis of a number of life-threatening conditions using X-ray images.
By using the latest developments in fast and portable Artificial Intelligence
environments, we develop a smartphone app using an Artificial Neural Network to
assist physicians in their diagnostic."
Latent Semantic Search and Information Extraction Architecture,"The motivation, concept, design and implementation of latent semantic search
for search engines have limited semantic search, entity extraction and property
attribution features, have insufficient accuracy and response time of latent
search, may impose privacy concerns and the search results are unavailable in
offline mode for robotic search operations. The alternative suggestion involves
autonomous search engine with adaptive storage consumption, configurable search
scope and latent search response time with built-in options for entity
extraction and property attribution available as open source platform for
mobile, desktop and server solutions. The suggested architecture attempts to
implement artificial general intelligence (AGI) principles as long as
autonomous behaviour constrained by limited resources is concerned, and it is
applied for specific task of enabling Web search for artificial agents
implementing the AGI."
Gomoku: analysis of the game and of the player Wine,"Gomoku, also known as five in a row, is a classical board game, ideally
suited for quickly testing novel Artificial Intelligence (AI) techniques. With
the aim of facilitating a developer willing to write a new Gomoku player, in
this report we present an analysis of the main game concepts and strategies,
which is wider and deeper than existing ones. Moreover, after discussing the
general structure of an artificial player, we present and analyse a strong
Gomoku player, named Wine, the code of which is freely available on the
Internet and which is an excelent example of how a modern player is organised."
"Language and Intelligence, Artificial vs. Natural or What Can and What
  Cannot AI Do with NL?","In this talk, I argue that there are certain pragmatic features of natural
language (that I will call 'productivity' and 'malleability', on top of
syntactical generativity and semantical compositionality), which are not only
hard, but even impossible to capture in an artificial language used by an AI
system, and the reason for this is to be found in certain deep, metaphysical
differences between artificial and natural intelligence, accounting for the
differences in their respective processes of concept-formation."
Grid cells and their potential application in AI,"Since their Nobel Prize winning discovery in 2005, grid cells have been
studied extensively by neuroscientists. Their multi-scale periodic firing rates
tiling the environment as the animal moves around has been shown as critical
for path integration. Multiple experiments have shown that grid cells also fire
for other representations such as olfactory, attention mechanisms, imagined
movement, and concept organization potentially acting as a form of neural
recycling and showing the possible brain mechanism for cognitive maps that
Tolman envisioned in 1948. Grid cell integration into artificial neural
networks may enable more robust, generalized, and smarter computers. In this
paper we give an overview of grid cell research since their discovery, their
role in neuroscience and cognitive science, and possible future directions of
artificial intelligence research."
"Inductive Models for Artificial Intelligence Systems are Insufficient
  without Good Explanations","This paper discusses the limitations of machine learning (ML), particularly
deep artificial neural networks (ANNs), which are effective at approximating
complex functions but often lack transparency and explanatory power. It
highlights the `problem of induction' : the philosophical issue that past
observations may not necessarily predict future events, a challenge that ML
models face when encountering new, unseen data. The paper argues for the
importance of not just making predictions but also providing good explanations,
a feature that current models often fail to deliver. It suggests that for AI to
progress, we must seek models that offer insights and explanations, not just
predictions."
Error-margin Analysis for Hidden Neuron Activation Labels,"Understanding how high-level concepts are represented within artificial
neural networks is a fundamental challenge in the field of artificial
intelligence. While existing literature in explainable AI emphasizes the
importance of labeling neurons with concepts to understand their functioning,
they mostly focus on identifying what stimulus activates a neuron in most
cases, this corresponds to the notion of recall in information retrieval. We
argue that this is only the first-part of a two-part job, it is imperative to
also investigate neuron responses to other stimuli, i.e., their precision. We
call this the neuron labels error margin."
ASI as the New God: Technocratic Theocracy,"As Artificial General Intelligence edges closer to reality, Artificial
Superintelligence does too. This paper argues that ASI's unparalleled
capabilities might lead people to attribute godlike infallibility to it,
resulting in a cognitive bias toward unquestioning acceptance of its decisions.
By drawing parallels between ASI and divine attributes such as omnipotence,
omniscience, and omnipresence, this analysis highlights the risks of conflating
technological advancement with moral and ethical superiority. Such dynamics
could engender a technocratic theocracy, where decision-making is abdicated to
ASI, undermining human agency and critical thinking."
An Integrated Framework for Learning and Reasoning,"Learning and reasoning are both aspects of what is considered to be
intelligence. Their studies within AI have been separated historically,
learning being the topic of machine learning and neural networks, and reasoning
falling under classical (or symbolic) AI. However, learning and reasoning are
in many ways interdependent. This paper discusses the nature of some of these
interdependencies and proposes a general framework called FLARE, that combines
inductive learning using prior knowledge together with reasoning in a
propositional setting. Several examples that test the framework are presented,
including classical induction, many important reasoning protocols and two
simple expert systems."
Variations of the Turing Test in the Age of Internet and Virtual Reality,"Inspired by Hofstadter's Coffee-House Conversation (1982) and by the science
fiction short story SAM by Schattschneider (1988), we propose and discuss
criteria for non-mechanical intelligence. Firstly, we emphasize the practical
need for such tests in view of massively multiuser online role-playing games
(MMORPGs) and virtual reality systems like Second Life. Secondly, we
demonstrate Second Life as a useful framework for implementing (some iterations
of) that test."
"Modeling and Verification of a Multi-Agent Argumentation System using
  NuSMV","Autonomous intelligent agent research is a domain situated at the forefront
of artificial intelligence. Interest-based negotiation (IBN) is a form of
negotiation in which agents exchange information about their underlying goals,
with a view to improve the likelihood and quality of a offer. In this paper we
model and verify a multi-agent argumentation scenario of resource sharing
mechanism to enable resource sharing in a distributed system. We use IBN in our
model wherein agents express their interests to the others in the society to
gain certain resources."
"A Case Study in Knowledge Discovery and Elicitation in an Intelligent
  Tutoring Application","Most successful Bayesian network (BN) applications to datehave been built
through knowledge elicitation from experts.This is difficult and time
consuming, which has lead to recentinterest in automated methods for learning
BNs from data. We present a case study in the construction of a BN in
anintelligent tutoring application, specifically decimal misconceptions.
Wedescribe the BN construction using expert elicitation and then investigate
how certainexisting automated knowledge discovery methods might support the BN
knowledge engineering process."
Risk Agoras: Dialectical Argumentation for Scientific Reasoning,"We propose a formal framework for intelligent systems which can reason about
scientific domains, in particular about the carcinogenicity of chemicals, and
we study its properties. Our framework is grounded in a philosophy of
scientific enquiry and discourse, and uses a model of dialectical
argumentation. The formalism enables representation of scientific uncertainty
and conflict in a manner suitable for qualitative reasoning about the domain."
A Knowledge Acquisition Tool for Bayesian-Network Troubleshooters,"This paper describes a domain-specific knowledge acquisition tool for
intelligent automated troubleshooters based on Bayesian networks. No Bayesian
network knowledge is required to use the tool, and troubleshooting information
can be specified as natural and intuitive as possible. Probabilities can be
specified in the direction that is most natural to the domain expert. Thus, the
knowledge acquisition efficiently removes the traditional knowledge acquisition
bottleneck of Bayesian networks."
Network Engineering for Complex Belief Networks,"Like any large system development effort, the construction of a complex
belief network model requires systems engineering to manage the design and
construction process. We propose a rapid prototyping approach to network
engineering. We describe criteria for identifying network modules and the use
of ""stubs"" to represent not-yet-constructed modules. We propose an object
oriented representation for belief networks which captures the semantics of the
problem in addition to conditional independencies and probabilities. Methods
for evaluating complex belief network models are discussed. The ideas are
illustrated with examples from a large belief network construction problem in
the military intelligence domain."
On Non-monotonic Conditional Reasoning,"This note is concerned with a formal analysis of the problem of non-monotonic
reasoning in intelligent systems, especially when the uncertainty is taken into
account in a quantitative way. A firm connection between logic and probability
is established by introducing conditioning notions by means of formal
structures that do not rely on quantitative measures. The associated
conditional logic, compatible with conditional probability evaluations, is
non-monotonic relative to additional evidence. Computational aspects of
conditional probability logic are mentioned. The importance of this development
lies on its role to provide a conceptual basis for various forms of evidence
combination and on its significance to unify multi-valued and non-monotonic
logics"
Bayesian Inference in Model-Based Machine Vision,"This is a preliminary version of visual interpretation integrating multiple
sensors in SUCCESSOR, an intelligent, model-based vision system. We pursue a
thorough integration of hierarchical Bayesian inference with comprehensive
physical representation of objects and their relations in a system for
reasoning with geometry, surface materials and sensor models in machine vision.
Bayesian inference provides a framework for accruing_ probabilities to rank
order hypotheses."
"An Application of Non-Monotonic Probabilistic Reasoning to Air Force
  Threat Correlation","Current approaches to expert systems' reasoning under uncertainty fail to
capture the iterative revision process characteristic of intelligent human
reasoning. This paper reports on a system, called the Non-monotonic
Probabilist, or NMP (Cohen, et al., 1985). When its inferences result in
substantial conflict, NMP examines and revises the assumptions underlying the
inferences until conflict is reduced to acceptable levels. NMP has been
implemented in a demonstration computer-based system, described below, which
supports threat correlation and in-flight route replanning by Air Force pilots."
Energetics of the brain and AI,"Does the energy requirements for the human brain give energy constraints that
give reason to doubt the feasibility of artificial intelligence? This report
will review some relevant estimates of brain bioenergetics and analyze some of
the methods of estimating brain emulation energy requirements. Turning to AI,
there are reasons to believe the energy requirements for de novo AI to have
little correlation with brain (emulation) energy requirements since cost could
depend merely of the cost of processing higher-level representations rather
than billions of neural firings. Unless one thinks the human way of thinking is
the most optimal or most easily implementable way of achieving software
intelligence, we should expect de novo AI to make use of different, potentially
very compressed and fast, processes."
"Towards Effective Human-AI Collaboration in GUI-Based Interactive Task
  Learning Agents","We argue that a key challenge in enabling usable and useful interactive task
learning for intelligent agents is to facilitate effective Human-AI
collaboration. We reflect on our past 5 years of efforts on designing,
developing and studying the SUGILITE system, discuss the issues on
incorporating recent advances in AI with HCI principles in mixed-initiative
interactions and multi-modal interactions, and summarize the lessons we
learned. Lastly, we identify several challenges and opportunities, and describe
our ongoing work"
"Incorporating planning intelligence into deep learning: A planning
  support tool for street network design","Deep learning applications in shaping ad hoc planning proposals are limited
by the difficulty in integrating professional knowledge about cities with
artificial intelligence. We propose a novel, complementary use of deep neural
networks and planning guidance to automate street network generation that can
be context-aware, example-based and user-guided. The model tests suggest that
the incorporation of planning knowledge (e.g., road junctions and neighborhood
types) in the model training leads to a more realistic prediction of street
configurations. Furthermore, the new tool provides both professional and lay
users an opportunity to systematically and intuitively explore benchmark
proposals for comparisons and further evaluations."
"Concepts, Properties and an Approach for Compositional Generalization","Compositional generalization is the capacity to recognize and imagine a large
amount of novel combinations from known components. It is a key in human
intelligence, but current neural networks generally lack such ability. This
report connects a series of our work for compositional generalization, and
summarizes an approach. The first part contains concepts and properties. The
second part looks into a machine learning approach. The approach uses
architecture design and regularization to regulate information of
representations. This report focuses on basic ideas with intuitive and
illustrative explanations. We hope this work would be helpful to clarify
fundamentals of compositional generalization and lead to advance artificial
intelligence."
Symbolic Computation in Software Science: My Personal View,"In this note, I develop my personal view on the scope and relevance of
symbolic computation in software science. For this, I discuss the interaction
and differences between symbolic computation, software science, automatic
programming, mathematical knowledge management, artificial intelligence,
algorithmic intelligence, numerical computation, and machine learning. In the
discussion of these notions, I allow myself to refer also to papers (1982,
1985, 2001, 2003, 2013) of mine in which I expressed my views on these areas at
early stages of some of these fields."
Frontiers in Evolutionary Computation: A Workshop Report,"In July of 2021, the Santa Fe Institute hosted a workshop on evolutionary
computation as part of its Foundations of Intelligence in Natural and
Artificial Systems project. This project seeks to advance the field of
artificial intelligence by promoting interdisciplinary research on the nature
of intelligence. The workshop brought together computer scientists and
biologists to share their insights about the nature of evolution and the future
of evolutionary computation. In this report, we summarize each of the talks and
the subsequent discussions. We also draw out a number of key themes and
identify important frontiers for future research."
A Literature Survey of Recent Advances in Chatbots,"Chatbots are intelligent conversational computer systems designed to mimic
human conversation to enable automated online guidance and support. The
increased benefits of chatbots led to their wide adoption by many industries in
order to provide virtual assistance to customers. Chatbots utilise methods and
algorithms from two Artificial Intelligence domains: Natural Language
Processing and Machine Learning. However, there are many challenges and
limitations in their application. In this survey we review recent advances on
chatbots, where Artificial Intelligence and Natural Language processing are
used. We highlight the main challenges and limitations of current work and make
recommendations for future research investigation."
"Knowledge Engineering in the Long Game of Artificial Intelligence: The
  Case of Speech Acts","This paper describes principles and practices of knowledge engineering that
enable the development of holistic language-endowed intelligent agents that can
function across domains and applications, as well as expand their ontological
and lexical knowledge through lifelong learning. For illustration, we focus on
dialog act modeling, a task that has been widely pursued in linguistics,
cognitive modeling, and statistical natural language processing. We describe an
integrative approach grounded in the OntoAgent knowledge-centric cognitive
architecture and highlight the limitations of past approaches that isolate
dialog from other agent functionalities."
Intelligent Explorations of the String Theory Landscape,"The goal of identifying the Standard Model of particle physics and its
extensions within string theory has been one of the principal driving forces in
string phenomenology. Recently, the incorporation of artificial intelligence in
string theory and certain theoretical advancements have brought to light
unexpected solutions to mathematical hurdles that have so far hindered progress
in this direction. In this review we focus on model building efforts in the
context of the $E_8\times E_8$ heterotic string compactified on smooth
Calabi-Yau threefolds and discuss several areas in which machine learning is
expected to make a difference."
"Proceedings of Principle and practice of data and Knowledge Acquisition
  Workshop 2022 (PKAW 2022)","Over the past two decades, PKAW has provided a forum for researchers and
practitioners to discuss the state-of-the-arts in the area of knowledge
acquisition and machine intelligence (MI, also Artificial Intelligence, AI).
PKAW2022 will continue the above focus and welcome the contributions on the
multi-disciplinary approach of human and big data-driven knowledge acquisition,
as well as AI techniques and applications."
Survey of Consciousness Theory from Computational Perspective,"Human consciousness has been a long-lasting mystery for centuries, while
machine intelligence and consciousness is an arduous pursuit. Researchers have
developed diverse theories for interpreting the consciousness phenomenon in
human brains from different perspectives and levels. This paper surveys several
main branches of consciousness theories originating from different subjects
including information theory, quantum physics, cognitive psychology, physiology
and computer science, with the aim of bridging these theories from a
computational perspective. It also discusses the existing evaluation metrics of
consciousness and possibility for current computational models to be conscious.
Breaking the mystery of consciousness can be an essential step in building
general artificial intelligence with computing machines."
"NUTS, NARS, and Speech","To investigate whether ""Intelligence is the capacity of an
information-processing system to adapt to its environment while operating with
insufficient knowledge and resources"", we look at utilising the non axiomatic
reasoning system (NARS) for speech recognition. This article presents NUTS:
raNdom dimensionality redUction non axiomaTic reasoning few Shot learner for
perception. NUTS consists of naive dimensionality reduction, some
pre-processing, and then non axiomatic reasoning (NARS). With only 2 training
examples NUTS performs similarly to the Whisper Tiny model for discrete word
identification."
"First Analysis of the EU Artifical Intelligence Act: Towards a Global
  Standard for Trustworthy AI?","The EU Artificial Intelligence Act (AI Act) came into force in the European
Union (EU) on 1 August 2024. It is a key piece of legislation both for the
citizens at the heart of AI technologies and for the industry active in the
internal market. The AI Act imposes progressive compliance on organisations -
both private and public - involved in the global value chain of AI systems and
models marketed and used in the EU. While the Act is unprecedented on an
international scale in terms of its horizontal and binding regulatory scope,
its global appeal in support of trustworthy AI is one of its major challenges."
Reducing Diversity to Generate Hierarchical Archetypes,"The Artificial Intelligence field seldom address the development of a
fundamental building piece: a framework, methodology or algorithm to
automatically build hierarchies of abstractions. This is a key requirement in
order to build intelligent behaviour, as recent neuroscience studies clearly
expose. In this paper we present a primitive-based framework to automatically
generate hierarchies of constructive archetypes, as a theory of how to generate
hierarchies of abstractions. We assume the existence of a primitive with very
specific characteristics, and we develop our framework over it. We prove the
effectiveness of our framework through mathematical definitions and proofs.
Finally, we give a few insights about potential uses of our framework and the
expected results."
Neural networks consisting of DNA,"Neural networks based on soft and biological matter constitute an interesting
potential alternative to traditional implementations based on electric
circuits. DNA is a particularly promising system in this context due its
natural ability to store information. In recent years, researchers have started
to construct neural networks that are based on DNA. In this chapter, I provide
a very basic introduction to the concept of DNA neural networks, aiming at an
audience that is not familiar with biochemistry."
"User Simulation in the Era of Generative AI: User Modeling, Synthetic
  Data Generation, and System Evaluation","User simulation is an emerging interdisciplinary topic with multiple critical
applications in the era of Generative AI. It involves creating an intelligent
agent that mimics the actions of a human user interacting with an AI system,
enabling researchers to model and analyze user behaviour, generate synthetic
data for training, and evaluate interactive AI systems in a controlled and
reproducible manner. User simulation has profound implications for diverse
fields and plays a vital role in the pursuit of Artificial General
Intelligence. This paper provides an overview of user simulation, highlighting
its key applications, connections to various disciplines, and outlining future
research directions to advance this increasingly important technology."
"Assessing AI Adoption and Digitalization in SMEs: A Framework for
  Implementation","The primary objective of this research is to examine the current state of
digitalization and the integration of artificial intelligence (AI) within small
and medium-sized enterprises (SMEs) in Italy. There is a significant gap
between SMEs and large corporations in their use of AI, with SMEs facing
numerous barriers to adoption. This study identifies critical drivers and
obstacles to achieving intelligent transformation, proposing a framework model
to address key challenges and provide actionable guidelines"
LLM Reasoner and Automated Planner: A new NPC approach,"In domains requiring intelligent agents to emulate plausible human-like
behaviour, such as formative simulations, traditional techniques like behaviour
trees encounter significant challenges. Large Language Models (LLMs), despite
not always yielding optimal solutions, usually offer plausible and human-like
responses to a given problem. In this paper, we exploit this capability and
propose a novel architecture that integrates an LLM for decision-making with a
classical automated planner that can generate sound plans for that decision.
The combination aims to equip an agent with the ability to make decisions in
various situations, even if they were not anticipated during the design phase."
"A Survey of Theory of Mind in Large Language Models: Evaluations,
  Representations, and Safety Risks","Theory of Mind (ToM), the ability to attribute mental states to others and
predict their behaviour, is fundamental to social intelligence. In this paper,
we survey studies evaluating behavioural and representational ToM in Large
Language Models (LLMs), identify important safety risks from advanced LLM ToM
capabilities, and suggest several research directions for effective evaluation
and mitigation of these risks."
"Gas Station of the Future: A Perspective on AI/ML and IoT in Retail
  Downstream","The gas station of the future is poised to transform from a simple fuel
dispensing center into an intelligent retail hub, driven by advancements in
Artificial Intelligence (AI), Machine Learning (ML), and the Internet of Things
(IoT). This paper explores how technology is reshaping the retail downstream
sector while briefly addressing the upstream and midstream segments. By
leveraging AI/ML for predictive analytics, dynamic pricing, personalized
customer engagement, and IoT for real-time monitoring and automation, the
future gas station will redefine the fuel retail experience. Additionally, this
paper incorporates statistics, AI/ML core technical concepts, mathematical
formulations, case studies, and a proposed framework for a fully autonomous gas
station."
Algorithms and Complexity Results for Persuasive Argumentation,"The study of arguments as abstract entities and their interaction as
introduced by Dung (Artificial Intelligence 177, 1995) has become one of the
most active research branches within Artificial Intelligence and Reasoning. A
main issue for abstract argumentation systems is the selection of acceptable
sets of arguments. Value-based argumentation, as introduced by Bench-Capon (J.
Logic Comput. 13, 2003), extends Dung's framework. It takes into account the
relative strength of arguments with respect to some ranking representing an
audience: an argument is subjectively accepted if it is accepted with respect
to some audience, it is objectively accepted if it is accepted with respect to
all audiences. Deciding whether an argument is subjectively or objectively
accepted, respectively, are computationally intractable problems. In fact, the
problems remain intractable under structural restrictions that render the main
computational problems for non-value-based argumentation systems tractable. In
this paper we identify nontrivial classes of value-based argumentation systems
for which the acceptance problems are polynomial-time tractable. The classes
are defined by means of structural restrictions in terms of the underlying
graphical structure of the value-based system. Furthermore we show that the
acceptance problems are intractable for two classes of value-based systems that
where conjectured to be tractable by Dunne (Artificial Intelligence 171, 2007)."
"""Dave...I can assure you...that it's going to be all right..."" -- A
  definition, case for, and survey of algorithmic assurances in human-autonomy
  trust relationships","People who design, use, and are affected by autonomous artificially
intelligent agents want to be able to \emph{trust} such agents -- that is, to
know that these agents will perform correctly, to understand the reasoning
behind their actions, and to know how to use them appropriately. Many
techniques have been devised to assess and influence human trust in
artificially intelligent agents. However, these approaches are typically ad
hoc, and have not been formally related to each other or to formal trust
models. This paper presents a survey of \emph{algorithmic assurances}, i.e.
programmed components of agent operation that are expressly designed to
calibrate user trust in artificially intelligent agents. Algorithmic assurances
are first formally defined and classified from the perspective of formally
modeled human-artificially intelligent agent trust relationships. Building on
these definitions, a synthesis of research across communities such as machine
learning, human-computer interaction, robotics, e-commerce, and others reveals
that assurance algorithms naturally fall along a spectrum in terms of their
impact on an agent's core functionality, with seven notable classes ranging
from integral assurances (which impact an agent's core functionality) to
supplemental assurances (which have no direct effect on agent performance).
Common approaches within each of these classes are identified and discussed;
benefits and drawbacks of different approaches are also investigated."
"Hows and Whys of Artificial Intelligence for Public Sector Decisions:
  Explanation and Evaluation","Evaluation has always been a key challenge in the development of artificial
intelligence (AI) based software, due to the technical complexity of the
software artifact and, often, its embedding in complex sociotechnical
processes. Recent advances in machine learning (ML) enabled by deep neural
networks has exacerbated the challenge of evaluating such software due to the
opaque nature of these ML-based artifacts. A key related issue is the
(in)ability of such systems to generate useful explanations of their outputs,
and we argue that the explanation and evaluation problems are closely linked.
The paper models the elements of a ML-based AI system in the context of public
sector decision (PSD) applications involving both artificial and human
intelligence, and maps these elements against issues in both evaluation and
explanation, showing how the two are related. We consider a number of common
PSD application patterns in the light of our model, and identify a set of key
issues connected to explanation and evaluation in each case. Finally, we
propose multiple strategies to promote wider adoption of AI/ML technologies in
PSD, where each is distinguished by a focus on different elements of our model,
allowing PSD policy makers to adopt an approach that best fits their context
and concerns."
Directions for Explainable Knowledge-Enabled Systems,"Interest in the field of Explainable Artificial Intelligence has been growing
for decades and has accelerated recently. As Artificial Intelligence models
have become more complex, and often more opaque, with the incorporation of
complex machine learning techniques, explainability has become more critical.
Recently, researchers have been investigating and tackling explainability with
a user-centric focus, looking for explanations to consider trustworthiness,
comprehensibility, explicit provenance, and context-awareness. In this chapter,
we leverage our survey of explanation literature in Artificial Intelligence and
closely related fields and use these past efforts to generate a set of
explanation types that we feel reflect the expanded needs of explanation for
today's artificial intelligence applications. We define each type and provide
an example question that would motivate the need for this style of explanation.
We believe this set of explanation types will help future system designers in
their generation and prioritization of requirements and further help generate
explanations that are better aligned to users' and situational needs."
"Mapping the Landscape of Artificial Intelligence Applications against
  COVID-19","COVID-19, the disease caused by the SARS-CoV-2 virus, has been declared a
pandemic by the World Health Organization, which has reported over 18 million
confirmed cases as of August 5, 2020. In this review, we present an overview of
recent studies using Machine Learning and, more broadly, Artificial
Intelligence, to tackle many aspects of the COVID-19 crisis. We have identified
applications that address challenges posed by COVID-19 at different scales,
including: molecular, by identifying new or existing drugs for treatment;
clinical, by supporting diagnosis and evaluating prognosis based on medical
imaging and non-invasive measures; and societal, by tracking both the epidemic
and the accompanying infodemic using multiple data sources. We also review
datasets, tools, and resources needed to facilitate Artificial Intelligence
research, and discuss strategic considerations related to the operational
implementation of multidisciplinary partnerships and open science. We highlight
the need for international cooperation to maximize the potential of AI in this
and future pandemics."
"Decolonial AI: Decolonial Theory as Sociotechnical Foresight in
  Artificial Intelligence","This paper explores the important role of critical science, and in particular
of post-colonial and decolonial theories, in understanding and shaping the
ongoing advances in artificial intelligence. Artificial Intelligence (AI) is
viewed as amongst the technological advances that will reshape modern societies
and their relations. Whilst the design and deployment of systems that
continually adapt holds the promise of far-reaching positive change, they
simultaneously pose significant risks, especially to already vulnerable
peoples. Values and power are central to this discussion. Decolonial theories
use historical hindsight to explain patterns of power that shape our
intellectual, political, economic, and social world. By embedding a decolonial
critical approach within its technical practice, AI communities can develop
foresight and tactics that can better align research and technology development
with established ethical principles, centring vulnerable peoples who continue
to bear the brunt of negative impacts of innovation and scientific progress. We
highlight problematic applications that are instances of coloniality, and using
a decolonial lens, submit three tactics that can form a decolonial field of
artificial intelligence: creating a critical technical practice of AI, seeking
reverse tutelage and reverse pedagogies, and the renewal of affective and
political communities. The years ahead will usher in a wave of new scientific
breakthroughs and technologies driven by AI research, making it incumbent upon
AI communities to strengthen the social contract through ethical foresight and
the multiplicity of intellectual perspectives available to us; ultimately
supporting future technologies that enable greater well-being, with the goal of
beneficence and justice for all."
A Normative approach to Attest Digital Discrimination,"Digital discrimination is a form of discrimination whereby users are
automatically treated unfairly, unethically or just differently based on their
personal data by a machine learning (ML) system. Examples of digital
discrimination include low-income neighbourhood's targeted with high-interest
loans or low credit scores, and women being undervalued by 21% in online
marketing. Recently, different techniques and tools have been proposed to
detect biases that may lead to digital discrimination. These tools often
require technical expertise to be executed and for their results to be
interpreted. To allow non-technical users to benefit from ML, simpler notions
and concepts to represent and reason about digital discrimination are needed.
In this paper, we use norms as an abstraction to represent different situations
that may lead to digital discrimination. In particular, we formalise
non-discrimination norms in the context of ML systems and propose an algorithm
to check whether ML systems violate these norms."
Making Responsible AI the Norm rather than the Exception,"This report prepared by the Montreal AI Ethics Institute provides
recommendations in response to the National Security Commission on Artificial
Intelligence (NSCAI) Key Considerations for Responsible Development and
Fielding of Artificial Intelligence document. The report centres on the idea
that Responsible AI should be made the Norm rather than an Exception. It does
so by utilizing the guiding principles of: (1) alleviating friction in existing
workflows, (2) empowering stakeholders to get buy-in, and (3) conducting an
effective translation of abstract standards into actionable engineering
practices. After providing some overarching comments on the document from the
NSCAI, the report dives into the primary contribution of an actionable
framework to help operationalize the ideas presented in the document from the
NSCAI. The framework consists of: (1) a learning, knowledge, and information
exchange (LKIE), (2) the Three Ways of Responsible AI, (3) an
empirically-driven risk-prioritization matrix, and (4) achieving the right
level of complexity. All components reinforce each other to move from
principles to practice in service of making Responsible AI the norm rather than
the exception."
"Levels of explainable artificial intelligence for human-aligned
  conversational explanations","Over the last few years there has been rapid research growth into eXplainable
Artificial Intelligence (XAI) and the closely aligned Interpretable Machine
Learning (IML). Drivers for this growth include recent legislative changes and
increased investments by industry and governments, along with increased concern
from the general public. People are affected by autonomous decisions every day
and the public need to understand the decision-making process to accept the
outcomes. However, the vast majority of the applications of XAI/IML are focused
on providing low-level `narrow' explanations of how an individual decision was
reached based on a particular datum. While important, these explanations rarely
provide insights into an agent's: beliefs and motivations; hypotheses of other
(human, animal or AI) agents' intentions; interpretation of external cultural
expectations; or, processes used to generate its own explanation. Yet all of
these factors, we propose, are essential to providing the explanatory depth
that people require to accept and trust the AI's decision-making. This paper
aims to define levels of explanation and describe how they can be integrated to
create a human-aligned conversational explanation system. In so doing, this
paper will survey current approaches and discuss the integration of different
technologies to achieve these levels with Broad eXplainable Artificial
Intelligence (Broad-XAI), and thereby move towards high-level `strong'
explanations."
Quantum Artificial Intelligence for the Science of Climate Change,"Climate change has become one of the biggest global problems increasingly
compromising the Earth's habitability. Recent developments such as the
extraordinary heat waves in California & Canada, and the devastating floods in
Germany point to the role of climate change in the ever-increasing frequency of
extreme weather. Numerical modelling of the weather and climate have seen
tremendous improvements in the last five decades, yet stringent limitations
remain to be overcome. Spatially and temporally localized forecasting is the
need of the hour for effective adaptation measures towards minimizing the loss
of life and property. Artificial Intelligence-based methods are demonstrating
promising results in improving predictions, but are still limited by the
availability of requisite hardware and software required to process the vast
deluge of data at a scale of the planet Earth. Quantum computing is an emerging
paradigm that has found potential applicability in several fields. In this
opinion piece, we argue that new developments in Artificial Intelligence
algorithms designed for quantum computers - also known as Quantum Artificial
Intelligence (QAI) - may provide the key breakthroughs necessary to furthering
the science of climate change. The resultant improvements in weather and
climate forecasts are expected to cascade to numerous societal benefits."
"Advancing Artificial Intelligence and Machine Learning in the U.S.
  Government Through Improved Public Competitions","In the last two years, the U.S. government has emphasized the importance of
accelerating artificial intelligence (AI) and machine learning (ML) within the
government and across the nation. In particular, the National Artificial
Intelligence Initiative Act of 2020, which became law on January 1, 2021,
provides for a coordinated program across the entire federal government to
accelerate AI research and application. The U.S. government can benefit from
public artificial intelligence and machine learning challenges through the
development of novel algorithms and participation in experiential training.
Although the public, private, and non-profit sectors have a history of
leveraging crowdsourcing initiatives to generate novel solutions to difficult
problems and engage stakeholders, interest in public competitions has waned in
recent years as a result of at least three major factors: (1) a lack of
high-quality, high-impact data; (2) a narrow engagement focus on specialized
groups; and (3) insufficient operationalization of challenge results. Herein we
identify common issues and recommend approaches to increase the effectiveness
of challenges. To address these barriers, enabling the use of public
competitions for accelerating AI and ML practice, the U.S. government must
leverage methods that protect sensitive data while enabling modelling, enable
easier participation, empower deployment of validated models, and incentivize
engagement from broad sections of the population."
"Explainable Artificial Intelligence Methods in Combating Pandemics: A
  Systematic Review","Despite the myriad peer-reviewed papers demonstrating novel Artificial
Intelligence (AI)-based solutions to COVID-19 challenges during the pandemic,
few have made significant clinical impact. The impact of artificial
intelligence during the COVID-19 pandemic was greatly limited by lack of model
transparency. This systematic review examines the use of Explainable Artificial
Intelligence (XAI) during the pandemic and how its use could overcome barriers
to real-world success. We find that successful use of XAI can improve model
performance, instill trust in the end-user, and provide the value needed to
affect user decision-making. We introduce the reader to common XAI techniques,
their utility, and specific examples of their application. Evaluation of XAI
results is also discussed as an important step to maximize the value of
AI-based clinical decision support systems. We illustrate the classical,
modern, and potential future trends of XAI to elucidate the evolution of novel
XAI techniques. Finally, we provide a checklist of suggestions during the
experimental design process supported by recent publications. Common challenges
during the implementation of AI solutions are also addressed with specific
examples of potential solutions. We hope this review may serve as a guide to
improve the clinical impact of future AI-based solutions."
AI Approaches in Processing and Using Data in Personalized Medicine,"In modern dynamic constantly developing society, more and more people suffer
from chronic and serious diseases and doctors and patients need special and
sophisticated medical and health support. Accordingly, prominent health
stakeholders have recognized the importance of development of such services to
make patients life easier. Such support requires the collection of huge amount
of patients complex data like clinical, environmental, nutritional, daily
activities, variety of data from smart wearable devices, data from clothing
equipped with sensors etc. Holistic patients data must be properly aggregated,
processed, analyzed, and presented to the doctors and caregivers to recommend
adequate treatment and actions to improve patients health related parameters
and general wellbeing. Advanced artificial intelligence techniques offer the
opportunity to analyze such big data, consume them, and derive new knowledge to
support personalized medical decisions. New approaches like those based on
advanced machine learning, federated learning, transfer learning, explainable
artificial intelligence open new paths for more quality use of health and
medical data in future. In this paper, we will present some crucial aspects and
characteristic examples in the area of application of a range of artificial
intelligence approaches in personalized medical decisions."
Current and Near-Term AI as a Potential Existential Risk Factor,"There is a substantial and ever-growing corpus of evidence and literature
exploring the impacts of Artificial intelligence (AI) technologies on society,
politics, and humanity as a whole. A separate, parallel body of work has
explored existential risks to humanity, including but not limited to that
stemming from unaligned Artificial General Intelligence (AGI). In this paper,
we problematise the notion that current and near-term artificial intelligence
technologies have the potential to contribute to existential risk by acting as
intermediate risk factors, and that this potential is not limited to the
unaligned AGI scenario. We propose the hypothesis that certain
already-documented effects of AI can act as existential risk factors,
magnifying the likelihood of previously identified sources of existential risk.
Moreover, future developments in the coming decade hold the potential to
significantly exacerbate these risk factors, even in the absence of artificial
general intelligence. Our main contribution is a (non-exhaustive) exposition of
potential AI risk factors and the causal relationships between them, focusing
on how AI can affect power dynamics and information security. This exposition
demonstrates that there exist causal pathways from AI systems to existential
risks that do not presuppose hypothetical future AI capabilities."
"OAK4XAI: Model towards Out-Of-Box eXplainable Artificial Intelligence
  for Digital Agriculture","Recent machine learning approaches have been effective in Artificial
Intelligence (AI) applications. They produce robust results with a high level
of accuracy. However, most of these techniques do not provide
human-understandable explanations for supporting their results and decisions.
They usually act as black boxes, and it is not easy to understand how decisions
have been made. Explainable Artificial Intelligence (XAI), which has received
much interest recently, tries to provide human-understandable explanations for
decision-making and trained AI models. For instance, in digital agriculture,
related domains often present peculiar or input features with no link to
background knowledge. The application of the data mining process on
agricultural data leads to results (knowledge), which are difficult to explain.
In this paper, we propose a knowledge map model and an ontology design as an
XAI framework (OAK4XAI) to deal with this issue. The framework does not only
consider the data analysis part of the process, but it takes into account the
semantics aspect of the domain knowledge via an ontology and a knowledge map
model, provided as modules of the framework. Many ongoing XAI studies aim to
provide accurate and verbalizable accounts for how given feature values
contribute to model decisions. The proposed approach, however, focuses on
providing consistent information and definitions of concepts, algorithms, and
values involved in the data mining models. We built an Agriculture Computing
Ontology (AgriComO) to explain the knowledge mined in agriculture. AgriComO has
a well-designed structure and includes a wide range of concepts and
transformations suitable for agriculture and computing domains."
"Gathering Strength, Gathering Storms: The One Hundred Year Study on
  Artificial Intelligence (AI100) 2021 Study Panel Report","In September 2021, the ""One Hundred Year Study on Artificial Intelligence""
project (AI100) issued the second report of its planned long-term periodic
assessment of artificial intelligence (AI) and its impact on society. It was
written by a panel of 17 study authors, each of whom is deeply rooted in AI
research, chaired by Michael Littman of Brown University. The report, entitled
""Gathering Strength, Gathering Storms,"" answers a set of 14 questions probing
critical areas of AI development addressing the major risks and dangers of AI,
its effects on society, its public perception and the future of the field. The
report concludes that AI has made a major leap from the lab to people's lives
in recent years, which increases the urgency to understand its potential
negative effects. The questions were developed by the AI100 Standing Committee,
chaired by Peter Stone of the University of Texas at Austin, consisting of a
group of AI leaders with expertise in computer science, sociology, ethics,
economics, and other disciplines."
"Consumer acceptance of the use of artificial intelligence in online
  shopping: evidence from Hungary","The rapid development of technology has drastically changed the way consumers
do their shopping. The volume of global online commerce has significantly been
increasing partly due to the recent COVID-19 crisis that has accelerated the
expansion of e-commerce. A growing number of webshops integrate Artificial
Intelligence (AI), state-of-the-art technology into their stores to improve
customer experience, satisfaction and loyalty. However, little research has
been done to verify the process of how consumers adopt and use AI-powered
webshops. Using the technology acceptance model (TAM) as a theoretical
background, this study addresses the question of trust and consumer acceptance
of Artificial Intelligence in online retail. An online survey in Hungary was
conducted to build a database of 439 respondents for this study. To analyse
data, structural equation modelling (SEM) was used. After the respecification
of the initial theoretical model, a nested model, which was also based on TAM,
was developed and tested. The widely used TAM was found to be a suitable
theoretical model for investigating consumer acceptance of the use of
Artificial Intelligence in online shopping. Trust was found to be one of the
key factors influencing consumer attitudes towards Artificial Intelligence.
Perceived usefulness as the other key factor in attitudes and behavioural
intention was found to be more important than the perceived ease of use. These
findings offer valuable implications for webshop owners to increase customer
acceptance"
Emerging AI Technologies Inspiring the Next Generation of E-textiles,"The smart textile and wearables sector is looking towards advancing
technologies to meet both industry, consumer and new emerging innovative
textile application demands, within a fast paced textile industry. In parallel
inspiration based on the biological neural workings of the human brain is
driving the next generation of artificial intelligence. Artificial intelligence
inspired hardware (neuromorphic computing) and software modules mimicking the
processing capabilities and properties of neural networks and the human nervous
system are taking shape. The textile sector needs to actively look at such
emerging and new technologies taking inspiration from their workings and
processing methods in order to stimulate new and innovative embedded
intelligence advancements in the etextile world. This emerging next generation
of Artificial intelligence(AI) is rapidly gaining interest across varying
industries (textile, medical, automotive, aerospace, military). How such
properties can inspire and drive advancements within the etextiles sector needs
to be considered. This paper will provide an insight into current
nanotechnology and artificial intelligence advancements in the etextiles domain
before focusing specifically on the future vision and direction around the
potential application of neuromorphic computing and spiking neural network
inspired AI technologies within the textile sector. We investigate the core
architectural elements of artificial neural networks, neuromorphic computing
and how such neuroscience inspired technologies could impact and inspire change
and new research developments within the e-textile sector."
"A Review of the Ethics of Artificial Intelligence and its Applications
  in the United States","This study is focused on the ethics of Artificial Intelligence and its
application in the United States, the paper highlights the impact AI has in
every sector of the US economy and multiple facets of the technological space
and the resultant effect on entities spanning businesses, government, academia,
and civil society. There is a need for ethical considerations as these entities
are beginning to depend on AI for delivering various crucial tasks, which
immensely influence their operations, decision-making, and interactions with
each other. The adoption of ethical principles, guidelines, and standards of
work is therefore required throughout the entire process of AI development,
deployment, and usage to ensure responsible and ethical AI practices. Our
discussion explores eleven fundamental 'ethical principles' structured as
overarching themes. These encompass Transparency, Justice, Fairness, Equity,
Non- Maleficence, Responsibility, Accountability, Privacy, Beneficence,
Freedom, Autonomy, Trust, Dignity, Sustainability, and Solidarity. These
principles collectively serve as a guiding framework, directing the ethical
path for the responsible development, deployment, and utilization of artificial
intelligence (AI) technologies across diverse sectors and entities within the
United States. The paper also discusses the revolutionary impact of AI
applications, such as Machine Learning, and explores various approaches used to
implement AI ethics. This examination is crucial to address the growing
concerns surrounding the inherent risks associated with the widespread use of
artificial intelligence."
"Path To Gain Functional Transparency In Artificial Intelligence With
  Meaningful Explainability","Artificial Intelligence (AI) is rapidly integrating into various aspects of
our daily lives, influencing decision-making processes in areas such as
targeted advertising and matchmaking algorithms. As AI systems become
increasingly sophisticated, ensuring their transparency and explainability
becomes crucial. Functional transparency is a fundamental aspect of algorithmic
decision-making systems, allowing stakeholders to comprehend the inner workings
of these systems and enabling them to evaluate their fairness and accuracy.
However, achieving functional transparency poses significant challenges that
need to be addressed. In this paper, we propose a design for user-centered
compliant-by-design transparency in transparent systems. We emphasize that the
development of transparent and explainable AI systems is a complex and
multidisciplinary endeavor, necessitating collaboration among researchers from
diverse fields such as computer science, artificial intelligence, ethics, law,
and social science. By providing a comprehensive understanding of the
challenges associated with transparency in AI systems and proposing a
user-centered design framework, we aim to facilitate the development of AI
systems that are accountable, trustworthy, and aligned with societal values."
"Interpretable Geoscience Artificial Intelligence (XGeoS-AI): Application
  to Demystify Image Recognition","As Earth science enters the era of big data, artificial intelligence (AI) not
only offers great potential for solving geoscience problems, but also plays a
critical role in accelerating the understanding of the complex, interactive,
and multiscale processes of Earth's behavior. As geoscience AI models are
progressively utilized for significant predictions in crucial situations,
geoscience researchers are increasingly demanding their interpretability and
versatility. This study proposes an interpretable geoscience artificial
intelligence (XGeoS-AI) framework to unravel the mystery of image recognition
in the Earth sciences, and its effectiveness and versatility is demonstrated by
taking computed tomography (CT) image recognition as an example. Inspired by
the mechanism of human vision, the proposed XGeoS-AI framework generates a
threshold value from a local region within the whole image to complete the
recognition. Different kinds of artificial intelligence (AI) methods, such as
Support Vector Regression (SVR), Multilayer Perceptron (MLP), Convolutional
Neural Network (CNN), can be adopted as the AI engines of the proposed XGeoS-AI
framework to efficiently complete geoscience image recognition tasks.
Experimental results demonstrate that the effectiveness, versatility, and
heuristics of the proposed framework have great potential in solving geoscience
image recognition problems. Interpretable AI should receive more and more
attention in the field of the Earth sciences, which is the key to promoting
more rational and wider applications of AI in the field of Earth sciences. In
addition, the proposed interpretable framework may be the forerunner of
technological innovation in the Earth sciences."
"From Google Gemini to OpenAI Q* (Q-Star): A Survey of Reshaping the
  Generative Artificial Intelligence (AI) Research Landscape","This comprehensive survey explored the evolving landscape of generative
Artificial Intelligence (AI), with a specific focus on the transformative
impacts of Mixture of Experts (MoE), multimodal learning, and the speculated
advancements towards Artificial General Intelligence (AGI). It critically
examined the current state and future trajectory of generative Artificial
Intelligence (AI), exploring how innovations like Google's Gemini and the
anticipated OpenAI Q* project are reshaping research priorities and
applications across various domains, including an impact analysis on the
generative AI research taxonomy. It assessed the computational challenges,
scalability, and real-world implications of these technologies while
highlighting their potential in driving significant progress in fields like
healthcare, finance, and education. It also addressed the emerging academic
challenges posed by the proliferation of both AI-themed and AI-generated
preprints, examining their impact on the peer-review process and scholarly
communication. The study highlighted the importance of incorporating ethical
and human-centric methods in AI development, ensuring alignment with societal
norms and welfare, and outlined a strategy for future AI research that focuses
on a balanced and conscientious use of MoE, multimodality, and AGI in
generative AI."
"The Artificial Intelligence Ontology: LLM-assisted construction of AI
  concept hierarchies","The Artificial Intelligence Ontology (AIO) is a systematization of artificial
intelligence (AI) concepts, methodologies, and their interrelations. Developed
via manual curation, with the additional assistance of large language models
(LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a
comprehensive framework that encompasses both technical and ethical aspects of
AI technologies. The primary audience for AIO includes AI researchers,
developers, and educators seeking standardized terminology and concepts within
the AI domain. The ontology is structured around six top-level branches:
Networks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to
support the modular composition of AI methods and facilitate a deeper
understanding of deep learning architectures and ethical considerations in AI.
  AIO's development utilized the Ontology Development Kit (ODK) for its
creation and maintenance, with its content being dynamically updated through
AI-driven curation support. This approach not only ensures the ontology's
relevance amidst the fast-paced advancements in AI but also significantly
enhances its utility for researchers, developers, and educators by simplifying
the integration of new AI concepts and methodologies.
  The ontology's utility is demonstrated through the annotation of AI methods
data in a catalog of AI research publications and the integration into the
BioPortal ontology resource, highlighting its potential for cross-disciplinary
research. The AIO ontology is open source and is available on GitHub
(https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal
(https://bioportal.bioontology.org/ontologies/AIO)."
ShapG: new feature importance method based on the Shapley value,"With wide application of Artificial Intelligence (AI), it has become
particularly important to make decisions of AI systems explainable and
transparent. In this paper, we proposed a new Explainable Artificial
Intelligence (XAI) method called ShapG (Explanations based on Shapley value for
Graphs) for measuring feature importance. ShapG is a model-agnostic global
explanation method. At the first stage, it defines an undirected graph based on
the dataset, where nodes represent features and edges are added based on
calculation of correlation coefficients between features. At the second stage,
it calculates an approximated Shapley value by sampling the data taking into
account this graph structure. The sampling approach of ShapG allows to
calculate the importance of features efficiently, i.e. to reduce computational
complexity. Comparison of ShapG with other existing XAI methods shows that it
provides more accurate explanations for two examined datasets. We also compared
other XAI methods developed based on cooperative game theory with ShapG in
running time, and the results show that ShapG exhibits obvious advantages in
its running time, which further proves efficiency of ShapG. In addition,
extensive experiments demonstrate a wide range of applicability of the ShapG
method for explaining complex models. We find ShapG an important tool in
improving explainability and transparency of AI systems and believe it can be
widely used in various fields."
Study on the Helpfulness of Explainable Artificial Intelligence,"Explainable Artificial Intelligence (XAI) is essential for building advanced
machine learning-powered applications, especially in critical domains such as
medical diagnostics or autonomous driving. Legal, business, and ethical
requirements motivate using effective XAI, but the increasing number of
different methods makes it challenging to pick the right ones. Further, as
explanations are highly context-dependent, measuring the effectiveness of XAI
methods without users can only reveal a limited amount of information,
excluding human factors such as the ability to understand it. We propose to
evaluate XAI methods via the user's ability to successfully perform a proxy
task, designed such that a good performance is an indicator for the explanation
to provide helpful information. In other words, we address the helpfulness of
XAI for human decision-making. Further, a user study on state-of-the-art
methods was conducted, showing differences in their ability to generate trust
and skepticism and the ability to judge the rightfulness of an AI decision
correctly. Based on the results, we highly recommend using and extending this
approach for more objective-based human-centered user studies to measure XAI
performance in an end-to-end fashion."
"Facing Identity: The Formation and Performance of Identity via
  Face-Based Artificial Intelligence Technologies","How is identity constructed and performed in the digital via face-based
artificial intelligence technologies? While questions of identity on the
textual Internet have been thoroughly explored, the Internet has progressed to
a multimedia form that not only centers the visual, but specifically the face.
At the same time, a wealth of scholarship has and continues to center the
topics of surveillance and control through facial recognition technologies
(FRTs), which have extended the logics of the racist pseudoscience of
physiognomy. Much less work has been devoted to understanding how such
face-based artificial intelligence technologies have influenced the formation
and performance of identity. This literature review considers how such
technologies interact with faciality, which entails the construction of what a
face may represent or signify, along axes of identity such as race, gender, and
sexuality. In grappling with recent advances in AI such as image generation and
deepfakes, I propose that we are now in an era of ""post-facial"" technologies
that build off our existing culture of facility while eschewing the analog
face, complicating our relationship with identity vis-a-vis the face. Drawing
from previous frameworks of identity play in the digital, as well as trans
practices that have historically played with or transgressed the boundaries of
identity classification, we can develop concepts adequate for analyzing digital
faciality and identity given the current landscape of post-facial artificial
intelligence technologies that allow users to interface with the digital in an
entirely novel manner. To ground this framework of transgression, I conclude by
proposing an interview study with VTubers -- online streamers who perform using
motion-captured avatars instead of their real-life faces -- to gain qualitative
insight on how these sociotechnical experiences."
AI Readiness in Healthcare through Storytelling XAI,"Artificial Intelligence is rapidly advancing and radically impacting everyday
life, driven by the increasing availability of computing power. Despite this
trend, the adoption of AI in real-world healthcare is still limited. One of the
main reasons is the trustworthiness of AI models and the potential hesitation
of domain experts with model predictions. Explainable Artificial Intelligence
(XAI) techniques aim to address these issues. However, explainability can mean
different things to people with different backgrounds, expertise, and goals. To
address the target audience with diverse needs, we develop storytelling XAI. In
this research, we have developed an approach that combines multi-task
distillation with interpretability techniques to enable audience-centric
explainability. Using multi-task distillation allows the model to exploit the
relationships between tasks, potentially improving interpretability as each
task supports the other leading to an enhanced interpretability from the
perspective of a domain expert. The distillation process allows us to extend
this research to large deep models that are highly complex. We focus on both
model-agnostic and model-specific methods of interpretability, supported by
textual justification of the results in healthcare through our use case. Our
methods increase the trust of both the domain experts and the machine learning
experts to enable a responsible AI."
"AI-Driven Non-Invasive Detection and Staging of Steatosis in Fatty Liver
  Disease Using a Novel Cascade Model and Information Fusion Techniques","Non-alcoholic fatty liver disease (NAFLD) is one of the most widespread liver
disorders on a global scale, posing a significant threat of progressing to more
severe conditions like nonalcoholic steatohepatitis (NASH), liver fibrosis,
cirrhosis, and hepatocellular carcinoma. Diagnosing and staging NAFLD presents
challenges due to its non-specific symptoms and the invasive nature of liver
biopsies. Our research introduces a novel artificial intelligence cascade model
employing ensemble learning and feature fusion techniques. We developed a
non-invasive, robust, and reliable diagnostic artificial intelligence tool that
utilizes anthropometric and laboratory parameters, facilitating early detection
and intervention in NAFLD progression. Our novel artificial intelligence
achieved an 86% accuracy rate for the NASH steatosis staging task (non-NASH,
steatosis grade 1, steatosis grade 2, and steatosis grade 3) and an impressive
96% AUC-ROC for distinguishing between NASH (steatosis grade 1, grade 2, and
grade3) and non-NASH cases, outperforming current state-of-the-art models. This
notable improvement in diagnostic performance underscores the potential
application of artificial intelligence in the early diagnosis and treatment of
NAFLD, leading to better patient outcomes and a reduced healthcare burden
associated with advanced liver disease."
"What Do We Want From Explainable Artificial Intelligence (XAI)? -- A
  Stakeholder Perspective on XAI and a Conceptual Model Guiding
  Interdisciplinary XAI Research","Previous research in Explainable Artificial Intelligence (XAI) suggests that
a main aim of explainability approaches is to satisfy specific interests,
goals, expectations, needs, and demands regarding artificial systems (we call
these stakeholders' desiderata) in a variety of contexts. However, the
literature on XAI is vast, spreads out across multiple largely disconnected
disciplines, and it often remains unclear how explainability approaches are
supposed to achieve the goal of satisfying stakeholders' desiderata. This paper
discusses the main classes of stakeholders calling for explainability of
artificial systems and reviews their desiderata. We provide a model that
explicitly spells out the main concepts and relations necessary to consider and
investigate when evaluating, adjusting, choosing, and developing explainability
approaches that aim to satisfy stakeholders' desiderata. This model can serve
researchers from the variety of different disciplines involved in XAI as a
common ground. It emphasizes where there is interdisciplinary potential in the
evaluation and the development of explainability approaches."
"Artificial Utopia: Simulation and Intelligent Agents for a Democratised
  Future","Prevailing top-down systems in politics and economics struggle to keep pace
with the pressing challenges of the 21st century, such as climate change,
social inequality and conflict. Bottom-up democratisation and participatory
approaches in politics and economics are increasingly seen as promising
alternatives to confront and overcome these issues, often with utopian
overtones, as proponents believe they may dramatically reshape political,
social and ecological futures for the better and in contrast to contemporary
authoritarian tendencies across various countries. Institutional specifics and
the associated collective human behavior or culture remains little understood
and debated, however. In this article, I propose a novel research agenda
focusing on utopian democratisation efforts with formal and computational
methods as well as with artificial intelligence - I call this agenda Artificial
Utopia. Artificial Utopias provide safe testing grounds for new political ideas
and economic policies in-silico with reduced risk of negative consequences as
compared to testing ideas in real-world contexts. An increasing number of
advanced simulation and intelligence methods, that aim at representing human
cognition and collective decision-making in more realistic ways, could benefit
this process. This includes agent-based modelling, reinforcement learning,
large language models and more. I clarify what some of these simulation
approaches can contribute to the study of Artificial Utopias with the help of
two institutional examples: the citizen assembly and the democratic firm."
Hierarchical principles of embodied reinforcement learning: A review,"Cognitive Psychology and related disciplines have identified several critical
mechanisms that enable intelligent biological agents to learn to solve complex
problems. There exists pressing evidence that the cognitive mechanisms that
enable problem-solving skills in these species build on hierarchical mental
representations. Among the most promising computational approaches to provide
comparable learning-based problem-solving abilities for artificial agents and
robots is hierarchical reinforcement learning. However, so far the existing
computational approaches have not been able to equip artificial agents with
problem-solving abilities that are comparable to intelligent animals, including
human and non-human primates, crows, or octopuses. Here, we first survey the
literature in Cognitive Psychology, and related disciplines, and find that many
important mental mechanisms involve compositional abstraction, curiosity, and
forward models. We then relate these insights with contemporary hierarchical
reinforcement learning methods, and identify the key machine intelligence
approaches that realise these mechanisms. As our main result, we show that all
important cognitive mechanisms have been implemented independently in isolated
computational architectures, and there is simply a lack of approaches that
integrate them appropriately. We expect our results to guide the development of
more sophisticated cognitively inspired hierarchical methods, so that future
artificial agents achieve a problem-solving performance on the level of
intelligent animals."
"JAMIP: an artificial-intelligence aided data-driven infrastructure for
  computational materials informatics","Materials informatics has emerged as a promisingly new paradigm for
accelerating materials discovery and design. It exploits the intelligent power
of machine learning methods in massive materials data from experiments or
simulations to seek for new materials, functionality, principles, etc.
Developing specialized facility to generate, collect, manage, learn and mine
large-scale materials data is crucial to materials informatics. We herein
developed an artificial-intelligence-aided data-driven infrastructure named
Jilin Artificial-intelligence aided Materials-design Integrated Package
(JAMIP), which is an open-source Python framework to meet the research
requirements of computational materials informatics. It is integrated by
materials production factory, high-throughput first-principles calculations
engine, automatic tasks submission and monitoring progress, data extraction,
management and storage system, and artificial intelligence machine learning
based data mining functions. We have integrated specific features such as
inorganic crystal structure prototype database to facilitate high-throughput
calculations and essential modules associated with machine learning studies of
functional materials. We demonstrated how our developed code is useful in
exploring materials informatics of optoelectronic semiconductors by taking
halide perovskites as typical case. By obeying the principles of automation,
extensibility, reliability and intelligence, the JAMIP code is a promisingly
powerful tool contributing to the fast-growing field of computational materials
informatics."
"On the Efficiency of Ethics as a Governing Tool for Artificial
  Intelligence","The 4th Industrial Revolution is the culmination of the digital age.
Nowadays, technologies such as robotics, nanotechnology, genetics, and
artificial intelligence promise to transform our world and the way we live.
Artificial Intelligence Ethics and Safety is an emerging research field that
has been gaining popularity in recent years. Several private, public and
non-governmental organizations have published guidelines proposing ethical
principles for regulating the use and development of autonomous intelligent
systems. Meta-analyses of the AI Ethics research field point to convergence on
certain principles that supposedly govern the AI industry. However, little is
known about the effectiveness of this form of Ethics. In this paper, we would
like to conduct a critical analysis of the current state of AI Ethics and
suggest that this form of governance based on principled ethical guidelines is
not sufficient to norm the AI industry and its developers. We believe that
drastic changes are necessary, both in the training processes of professionals
in the fields related to the development of software and intelligent systems
and in the increased regulation of these professionals and their industry. To
this end, we suggest that law should benefit from recent contributions from
bioethics, to make the contributions of AI ethics to governance explicit in
legal terms."
"SAI: Solving AI Tasks with Systematic Artificial Intelligence in
  Communication Network","In the rapid development of artificial intelligence, solving complex AI tasks
is a crucial technology in intelligent mobile networks. Despite the good
performance of specialized AI models in intelligent mobile networks, they are
unable to handle complicated AI tasks. To address this challenge, we propose
Systematic Artificial Intelligence (SAI), which is a framework designed to
solve AI tasks by leveraging Large Language Models (LLMs) and JSON-format
intent-based input to connect self-designed model library and database.
Specifically, we first design a multi-input component, which simultaneously
integrates Large Language Models (LLMs) and JSON-format intent-based inputs to
fulfill the diverse intent requirements of different users. In addition, we
introduce a model library module based on model cards which employ model cards
to pairwise match between different modules for model composition. Model cards
contain the corresponding model's name and the required performance metrics.
Then when receiving user network requirements, we execute each subtask for
multiple selected model combinations and provide output based on the execution
results and LLM feedback. By leveraging the language capabilities of LLMs and
the abundant AI models in the model library, SAI can complete numerous complex
AI tasks in the communication network, achieving impressive results in network
optimization, resource allocation, and other challenging tasks."
"Large language models for artificial general intelligence (AGI): A
  survey of foundational principles and approaches","Generative artificial intelligence (AI) systems based on large-scale
pretrained foundation models (PFMs) such as vision-language models, large
language models (LLMs), diffusion models and vision-language-action (VLA)
models have demonstrated the ability to solve complex and truly non-trivial AI
problems in a wide variety of domains and contexts. Multimodal large language
models (MLLMs), in particular, learn from vast and diverse data sources,
allowing rich and nuanced representations of the world and, thereby, providing
extensive capabilities, including the ability to reason, engage in meaningful
dialog; collaborate with humans and other agents to jointly solve complex
problems; and understand social and emotional aspects of humans. Despite this
impressive feat, the cognitive abilities of state-of-the-art LLMs trained on
large-scale datasets are still superficial and brittle. Consequently, generic
LLMs are severely limited in their generalist capabilities. A number of
foundational problems -- embodiment, symbol grounding, causality and memory --
are required to be addressed for LLMs to attain human-level general
intelligence. These concepts are more aligned with human cognition and provide
LLMs with inherent human-like cognitive properties that support the realization
of physically-plausible, semantically meaningful, flexible and more
generalizable knowledge and intelligence. In this work, we discuss the
aforementioned foundational issues and survey state-of-the art approaches for
implementing these concepts in LLMs. Specifically, we discuss how the
principles of embodiment, symbol grounding, causality and memory can be
leveraged toward the attainment of artificial general intelligence (AGI) in an
organic manner."
"Transforming Student Evaluation with Adaptive Intelligence and
  Performance Analytics","The development in Artificial Intelligence (AI) offers transformative
potential for redefining student assessment methodologies. This paper aims to
establish the idea of the advancement of Artificial Intelligence (AI) and its
prospect in reshaping approaches to assessing students. It creates a system for
the evaluation of students performance using Artificial intelligence, and
particularly the Gemini API for the generation of questions, grading and report
on the students performances. This is to facilitate easy use of the tools in
creating, scheduling, and delivering assessments with minimal chances of
cheating through options such as full screen and time limit. There are formats
of questions in the system which comprises multiple choice, short answers and
descriptive questions, developed by Gemini. The most conspicuous feature is the
self-checking system whereby the user gets instant feedback for the correct
score that each of the students would have scored instantly with explanations
about wrong answers. Moreover, the platform has intelligent learning
progressions where the user will be able to monitor his/her performances to be
recommended a certain level of performance. It will allow students as well as
educators to have real-time analytics and feedback on what they are good at and
where they need to improve. Not only does it make the assessment easier, but it
also improves the levels of accuracy in grading and effectively strengthens a
data based learning process for students."
"Emotional Responses in Artificial Agent-Based Systems: Reflexivity and
  Adaptation in Artificial Life","The current work addresses a virtual environment with self-replicating agents
whose decisions are based on a form of ""somatic computation"" (soma - body) in
which basic emotional responses, taken in parallelism to actual living
organisms, are introduced as a way to provide the agents with greater reflexive
abilities. The work provides a contribution to the field of Artificial
Intelligence (AI) and Artificial Life (ALife) in connection to a
neurobiology-based cognitive framework for artificial systems and virtual
environments' simulations. The performance of the agents capable of emotional
responses is compared with that of self-replicating automata, and the
implications of research on emotions and AI, in connection to both virtual
agents as well as robots, is addressed regarding possible future directions and
applications."
Iris Codes Classification Using Discriminant and Witness Directions,"The main topic discussed in this paper is how to use intelligence for
biometric decision defuzzification. A neural training model is proposed and
tested here as a possible solution for dealing with natural fuzzification that
appears between the intra- and inter-class distribution of scores computed
during iris recognition tests. It is shown here that the use of proposed neural
network support leads to an improvement in the artificial perception of the
separation between the intra- and inter-class score distributions by moving
them away from each other."
Introduction to the SP theory of intelligence,"This article provides a brief introduction to the ""Theory of Intelligence""
and its realisation in the ""SP Computer Model"". The overall goal of the SP
programme of research, in accordance with long-established principles in
science, has been the simplification and integration of observations and
concepts across artificial intelligence, mainstream computing, mathematics, and
human learning, perception, and cognition. In broad terms, the SP system is a
brain-like system that takes in ""New"" information through its senses and stores
some or all of it as ""Old"" information. A central idea in the system is the
powerful concept of ""SP-multiple-alignment"", borrowed and adapted from
bioinformatics. This the key to the system's versatility in aspects of
intelligence, in the representation of diverse kinds of knowledge, and in the
seamless integration of diverse aspects of intelligence and diverse kinds of
knowledge, in any combination. There are many potential benefits and
applications of the SP system. It is envisaged that the system will be
developed as the ""SP Machine"", which will initially be a software virtual
machine, hosted on a high-performance computer, a vehicle for further research
and a step towards the development of an industrial-strength SP Machine."
The Challenge of Crafting Intelligible Intelligence,"Since Artificial Intelligence (AI) software uses techniques like deep
lookahead search and stochastic optimization of huge neural networks to fit
mammoth datasets, it often results in complex behavior that is difficult for
people to understand. Yet organizations are deploying AI algorithms in many
mission-critical settings. To trust their behavior, we must make AI
intelligible, either by using inherently interpretable models or by developing
new methods for explaining and controlling otherwise overwhelmingly complex
decisions using local approximation, vocabulary alignment, and interactive
explanation. This paper argues that intelligibility is essential, surveys
recent work on building such systems, and highlights key directions for
research."
"Losing Confidence in Quality: Unspoken Evolution of Computer Vision
  Services","Recent advances in artificial intelligence (AI) and machine learning (ML),
such as computer vision, are now available as intelligent services and their
accessibility and simplicity is compelling. Multiple vendors now offer this
technology as cloud services and developers want to leverage these advances to
provide value to end-users. However, there is no firm investigation into the
maintenance and evolution risks arising from use of these intelligent services;
in particular, their behavioural consistency and transparency of their
functionality. We evaluated the responses of three different intelligent
services (specifically computer vision) over 11 months using 3 different data
sets, verifying responses against the respective documentation and assessing
evolution risk. We found that there are: (1) inconsistencies in how these
services behave; (2) evolution risk in the responses; and (3) a lack of clear
communication that documents these risks and inconsistencies. We propose a set
of recommendations to both developers and intelligent service providers to
inform risk and assist maintainability."
From the Internet of Information to the Internet of Intelligence,"In the era of the Internet of information, we have gone through layering,
cross-layer, and cross-system design paradigms. Recently, the ``curse of
modeling"" and ``curse of dimensionality"" of the cross-system design paradigm
have resulted in the popularity of using artificial intelligence (AI) to
optimize the Internet of information. However, many significant research
challenges remain to be addressed for the AI approach, including the lack of
high-quality training data due to privacy and resources constraints in this
data-driven approach. To address these challenges, we need to take a look at
humans' cooperation in a larger time scale. To facilitate cooperation in modern
history, we have built three major technologies: ``grid of transportation"",
``grid of energy"", and ``the Internet of information"". In this paper, we argue
that the next cooperation paradigm could be the ``Internet of intelligence
(Intelligence-Net)"", where intelligence can be easily obtained like energy and
information, enabled by the recent advances in blockchain technology. We
present some recent advances in these areas, and discuss some open issues and
challenges that need to be addressed in the future."
Giving Up Control: Neurons as Reinforcement Learning Agents,"Artificial Intelligence has historically relied on planning, heuristics, and
handcrafted approaches designed by experts. All the while claiming to pursue
the creation of Intelligence. This approach fails to acknowledge that
intelligence emerges from the dynamics within a complex system. Neurons in the
brain are governed by local rules, where no single neuron, or group of neurons,
coordinates or controls the others. This local structure gives rise to the
appropriate dynamics in which intelligence can emerge. Populations of neurons
must compete with their neighbors for resources, inhibition, and activity
representation. At the same time, they must cooperate, so the population and
organism can perform high-level functions. To this end, we introduce modeling
neurons as reinforcement learning agents. Where each neuron may be viewed as an
independent actor, trying to maximize its own self-interest. By framing
learning in this way, we open the door to an entirely new approach to building
intelligent systems."
"An Argumentation-based Approach for Explaining Goal Selection in
  Intelligent Agents","During the first step of practical reasoning, i.e. deliberation or goals
selection, an intelligent agent generates a set of pursuable goals and then
selects which of them he commits to achieve. Explainable Artificial
Intelligence (XAI) systems, including intelligent agents, must be able to
explain their internal decisions. In the context of goals selection, agents
should be able to explain the reasoning path that leads them to select (or not)
a certain goal. In this article, we use an argumentation-based approach for
generating explanations about that reasoning path. Besides, we aim to enrich
the explanations with information about emerging conflicts during the selection
process and how such conflicts were resolved. We propose two types of
explanations: the partial one and the complete one and a set of explanatory
schemes to generate pseudo-natural explanations. Finally, we apply our proposal
to the cleaner world scenario."
Conceptualization and Framework of Hybrid Intelligence Systems,"As artificial intelligence (AI) systems are getting ubiquitous within our
society, issues related to its fairness, accountability, and transparency are
increasing rapidly. As a result, researchers are integrating humans with AI
systems to build robust and reliable hybrid intelligence systems. However, a
proper conceptualization of these systems does not underpin this rapid growth.
This article provides a precise definition of hybrid intelligence systems as
well as explains its relation with other similar concepts through our proposed
framework and examples from contemporary literature. The framework breakdowns
the relationship between a human and a machine in terms of the degree of
coupling and the directive authority of each party. Finally, we argue that all
AI systems are hybrid intelligence systems, so human factors need to be
examined at every stage of such systems' lifecycle."
"Finding the unicorn: Predicting early stage startup success through a
  hybrid intelligence method","Artificial intelligence is an emerging topic and will soon be able to perform
decisions better than humans. In more complex and creative contexts such as
innovation, however, the question remains whether machines are superior to
humans. Machines fail in two kinds of situations: processing and interpreting
soft information (information that cannot be quantified) and making predictions
in unknowable risk situations of extreme uncertainty. In such situations, the
machine does not have representative information for a certain outcome.
Thereby, humans are still the gold standard for assessing soft signals and make
use of intuition. To predict the success of startups, we, thus, combine the
complementary capabilities of humans and machines in a Hybrid Intelligence
method. To reach our aim, we follow a design science research approach to
develop a Hybrid Intelligence method that combines the strength of both machine
and collective intelligence to demonstrate its utility for predictions under
extreme uncertainty."
"Social Behaviour Understanding using Deep Neural Networks: Development
  of Social Intelligence Systems","With the rapid development in artificial intelligence, social computing has
evolved beyond social informatics toward the birth of social intelligence
systems. This paper, therefore, takes initiatives to propose a social behaviour
understanding framework with the use of deep neural networks for social and
behavioural analysis. The integration of information fusion, person and object
detection, social signal understanding, behaviour understanding, and context
understanding plays a harmonious role to elicit social behaviours. Three
systems, including depression detection, activity recognition and cognitive
impairment screening, are developed to evidently demonstrate the importance of
social intelligence. The study considerably contributes to the cumulative
development of social computing and health informatics. It also provides a
number of implications for academic bodies, healthcare practitioners, and
developers of socially intelligent agents."
"Edge service resource allocation strategy based on intelligent
  prediction","Artificial intelligence is one of the important technologies for industrial
applications, but it requires a lot of computing resources and sensor data to
support it. With the development of edge computing and the Internet of Things,
artificial intelligence are playing an increasingly important role in the field
of edge services. Therefore, how to make intelligent algorithms provide better
services and the development of the Internet of Things has become an
increasingly important topic. This paper focuses on the application of edge
service distribution strategy, and proposes an edge service distribution
strategy based on intelligent prediction, which reduces the bandwidth
consumption of edge service providers and minimizes the cost of edge service
providers. In addition, this article uses the real data provided by the Wangsu
Technology Company and an improved long and short term memory prediction method
to dynamically change the bandwidth, and achieves better optimization of
resources allocation comparing with actual industrial applications.The
simulation results show that our intelligent prediction can achieve good
results, and the mechanism can achieve higher resource utilization."
Video Intelligence as a component of a Global Security system,"This paper describes the evolution of our research from video analytics to a
global security system with focus on the video surveillance component. Indeed
video surveillance has evolved from a commodity security tool up to the most
efficient way of tracking perpetrators when terrorism hits our modern urban
centers. As number of cameras soars, one could expect the system to leverage
the huge amount of data carried through the video streams to provide fast
access to video evidences, actionable intelligence for monitoring real-time
events and enabling predictive capacities to assist operators in their
surveillance tasks. This research explores a hybrid platform for video
intelligence capture, automated data extraction, supervised Machine Learning
for intelligently assisted urban video surveillance; Extension to other
components of a global security system are discussed. Applying Knowledge
Management principles in this research helps with deep problem understanding
and facilitates the implementation of efficient information and experience
sharing decision support systems providing assistance to people on the field as
well as in operations centers. The originality of this work is also the
creation of ""common"" human-machine and machine to machine language and a
security ontology."
"Holistic Network Virtualization and Pervasive Network Intelligence for
  6G","In this tutorial paper, we look into the evolution and prospect of network
architecture and propose a novel conceptual architecture for the 6th generation
(6G) networks. The proposed architecture has two key elements, i.e., holistic
network virtualization and pervasive artificial intelligence (AI). The holistic
network virtualization consists of network slicing and digital twin, from the
aspects of service provision and service demand, respectively, to incorporate
service-centric and user-centric networking. The pervasive network intelligence
integrates AI into future networks from the perspectives of networking for AI
and AI for networking, respectively. Building on holistic network
virtualization and pervasive network intelligence, the proposed architecture
can facilitate three types of interplay, i.e., the interplay between digital
twin and network slicing paradigms, between model-driven and data-driven
methods for network management, and between virtualization and AI, to maximize
the flexibility, scalability, adaptivity, and intelligence for 6G networks. We
also identify challenges and open issues related to the proposed architecture.
By providing our vision, we aim to inspire further discussions and developments
on the potential architecture of 6G."
User-Like Bots for Cognitive Automation: A Survey,"Software bots have attracted increasing interest and popularity in both
research and society. Their contributions span automation, digital twins, game
characters with conscious-like behavior, and social media. However, there is
still a lack of intelligent bots that can adapt to web environments'
variability and dynamic nature. Unlike human users, they have difficulty
understanding and exploiting the affordances across multiple virtual
environments.
  Despite the hype, bots with human user-like cognition do not currently exist.
Chatbots, for instance, lack situational awareness on the digital platforms
where they operate, preventing them from enacting meaningful and autonomous
intelligent behavior similar to human users.
  In this survey, we aim to explore the role of cognitive architectures in
supporting efforts towards engineering software bots with advanced general
intelligence. We discuss how cognitive architectures can contribute to creating
intelligent software bots. Furthermore, we highlight key architectural
recommendations for the future development of autonomous, user-like cognitive
bots."
"A Survey of Generative AI for Intelligent Transportation Systems: Road
  Transportation Perspective","Intelligent transportation systems are vital for modern traffic management
and optimization, greatly improving traffic efficiency and safety. With the
rapid development of generative artificial intelligence (Generative AI)
technologies in areas like image generation and natural language processing,
generative AI has also played a crucial role in addressing key issues in
intelligent transportation systems (ITS), such as data sparsity, difficulty in
observing abnormal scenarios, and in modeling data uncertainty. In this review,
we systematically investigate the relevant literature on generative AI
techniques in addressing key issues in different types of tasks in ITS tailored
specifically for road transportation. First, we introduce the principles of
different generative AI techniques. Then, we classify tasks in ITS into four
types: traffic perception, traffic prediction, traffic simulation, and traffic
decision-making. We systematically illustrate how generative AI techniques
addresses key issues in these four different types of tasks. Finally, we
summarize the challenges faced in applying generative AI to intelligent
transportation systems, and discuss future research directions based on
different application scenarios."
"Real Sparks of Artificial Intelligence and the Importance of Inner
  Interpretability","The present paper looks at one of the most thorough articles on the
intelligence of GPT, research conducted by engineers at Microsoft. Although
there is a great deal of value in their work, I will argue that, for familiar
philosophical reasons, their methodology, !Blackbox Interpretability""#is
wrongheaded. But there is a better way. There is an exciting and emerging
discipline of !Inner Interpretability""#(and specifically Mechanistic
Interpretability) that aims to uncover the internal activations and weights of
models in order to understand what they represent and the algorithms they
implement. In my view, a crucial mistake in Black-box Interpretability is the
failure to appreciate that how processes are carried out matters when it comes
to intelligence and understanding. I can#t pretend to have a full story that
provides both necessary and sufficient conditions for being intelligent, but I
do think that Inner Interpretability dovetails nicely with plausible
philosophical views of what intelligence requires. So the conclusion is modest,
but the important point in my view is seeing how to get the research on the
right track. Towards the end of the paper, I will show how some of the
philosophical concepts can be used to further refine how Inner Interpretability
is approached, so the paper helps draw out a profitable, future two-way
exchange between Philosophers and Computer Scientists."
"Leveraging Hybrid Intelligence Towards Sustainable and Energy-Efficient
  Machine Learning","Hybrid intelligence aims to enhance decision-making, problem-solving, and
overall system performance by combining the strengths of both, human cognitive
abilities and artificial intelligence. With the rise of Large Language Models
(LLM), progressively participating as smart agents to accelerate machine
learning development, Hybrid Intelligence is becoming an increasingly important
topic for effective interaction between humans and machines. This paper
presents an approach to leverage Hybrid Intelligence towards sustainable and
energy-aware machine learning. When developing machine learning models, final
model performance commonly rules the optimization process while the efficiency
of the process itself is often neglected. Moreover, in recent times, energy
efficiency has become equally crucial due to the significant environmental
impact of complex and large-scale computational processes. The contribution of
this work covers the interactive inclusion of secondary knowledge sources
through Human-in-the-loop (HITL) and LLM agents to stress out and further
resolve inefficiencies in the machine learning development process."
"Machine Learning in Communications: A Road to Intelligent Transmission
  and Processing","Prior to the era of artificial intelligence and big data, wireless
communications primarily followed a conventional research route involving
problem analysis, model building and calibration, algorithm design and tuning,
and holistic and empirical verification. However, this methodology often
encountered limitations when dealing with large-scale and complex problems and
managing dynamic and massive data, resulting in inefficiencies and limited
performance of traditional communication systems and methods. As such, wireless
communications have embraced the revolutionary impact of artificial
intelligence and machine learning, giving birth to more adaptive, efficient,
and intelligent systems and algorithms. This technological shift opens a road
to intelligent information transmission and processing. This overview article
discusses the typical roles of machine learning in intelligent wireless
communications, as well as its features, challenges, and practical
considerations."
"Exploring User Acceptance Of Portable Intelligent Personal Assistants: A
  Hybrid Approach Using PLS-SEM And fsQCA","This research explores the factors driving user acceptance of Rabbit R1, a
newly developed portable intelligent personal assistant (PIPA) that aims to
redefine user interaction and control. The study extends the technology
acceptance model (TAM) by incorporating artificial intelligence-specific
factors (conversational intelligence, task intelligence, and perceived
naturalness), user interface design factors (simplicity in information design
and visual aesthetics), and user acceptance and loyalty. Using a purposive
sampling method, we gathered data from 824 users in the US and analyzed the
sample through partial least squares structural equation modeling (PLS-SEM) and
fuzzy set qualitative comparative analysis (fsQCA). The findings reveal that
all hypothesized relationships, including both direct and indirect effects, are
supported. Additionally, fsQCA supports the PLS-SEM findings and identifies
three configurations leading to high and low user acceptance. This research
enriches the literature and provides valuable insights for system designers and
marketers of PIPAs, guiding strategic decisions to foster widespread adoption
and long-term engagement."
Bringing AI To Edge: From Deep Learning's Perspective,"Edge computing and artificial intelligence (AI), especially deep learning for
nowadays, are gradually intersecting to build a novel system, called edge
intelligence. However, the development of edge intelligence systems encounters
some challenges, and one of these challenges is the \textit{computational gap}
between computation-intensive deep learning algorithms and less-capable edge
systems. Due to the computational gap, many edge intelligence systems cannot
meet the expected performance requirements. To bridge the gap, a plethora of
deep learning techniques and optimization methods are proposed in the past
years: light-weight deep learning models, network compression, and efficient
neural architecture search. Although some reviews or surveys have partially
covered this large body of literature, we lack a systematic and comprehensive
review to discuss all aspects of these deep learning techniques which are
critical for edge intelligence implementation. As various and diverse methods
which are applicable to edge systems are proposed intensively, a holistic
review would enable edge computing engineers and community to know the
state-of-the-art deep learning techniques which are instrumental for edge
intelligence and to facilitate the development of edge intelligence systems.
This paper surveys the representative and latest deep learning techniques that
are useful for edge intelligence systems, including hand-crafted models, model
compression, hardware-aware neural architecture search and adaptive deep
learning models. Finally, based on observations and simple experiments we
conducted, we discuss some future directions."
Artificial Immune Systems Tutorial,"The biological immune system is a robust, complex, adaptive system that
defends the body from foreign pathogens. It is able to categorize all cells (or
molecules) within the body as self-cells or non-self cells. It does this with
the help of a distributed task force that has the intelligence to take action
from a local and also a global perspective using its network of chemical
messengers for communication. There are two major branches of the immune
system. The innate immune system is an unchanging mechanism that detects and
destroys certain invading organisms, whilst the adaptive immune system responds
to previously unknown foreign cells and builds a response to them that can
remain in the body over a long period of time. This remarkable information
processing biological system has caught the attention of computer science in
recent years. A novel computational intelligence technique, inspired by
immunology, has emerged, called Artificial Immune Systems. Several concepts
from the immune have been extracted and applied for solution to real world
science and engineering problems. In this tutorial, we briefly describe the
immune system metaphors that are relevant to existing Artificial Immune Systems
methods. We will then show illustrative real-world problems suitable for
Artificial Immune Systems and give a step-by-step algorithm walkthrough for one
such problem. A comparison of the Artificial Immune Systems to other well-known
algorithms, areas for future work, tips & tricks and a list of resources will
round this tutorial off. It should be noted that as Artificial Immune Systems
is still a young and evolving field, there is not yet a fixed algorithm
template and hence actual implementations might differ somewhat from time to
time and from those examples given here."
Analysis of Algorithms and Partial Algorithms,"We present an alternative methodology for the analysis of algorithms, based
on the concept of expected discounted reward. This methodology naturally
handles algorithms that do not always terminate, so it can (theoretically) be
used with partial algorithms for undecidable problems, such as those found in
artificial general intelligence (AGI) and automated theorem proving. We mention
an approach to self-improving AGI enabled by this methodology.
  Aug 2017 addendum: This article was originally written with multiple
audiences in mind. It is really best put in the following terms. Goertzel,
Hutter, Legg, and others have developed a definition of an intelligence score
for a general abstract agent: expected lifetime reward in a random environment.
AIXI is generally the optimal agent according to this score, but there may be
reasons to analyze other agents and compare score values. If we want to use
this definition of intelligence in practice, perhaps we can start by analyzing
some simple agents. Common algorithms can be thought of as simple agents
(environment is input, reward is based on running time) so we take the goal of
applying the agent intelligence score to algorithms. That is, we want to find,
what are the IQ scores of algorithms? We can do some very simple analysis, but
the real answer is that even for simple algorithms, the intelligence score is
too difficult to work with in practice."
Explainable AI for Intelligence Augmentation in Multi-Domain Operations,"Central to the concept of multi-domain operations (MDO) is the utilization of
an intelligence, surveillance, and reconnaissance (ISR) network consisting of
overlapping systems of remote and autonomous sensors, and human intelligence,
distributed among multiple partners. Realising this concept requires
advancement in both artificial intelligence (AI) for improved distributed data
analytics and intelligence augmentation (IA) for improved human-machine
cognition. The contribution of this paper is threefold: (1) we map the
coalition situational understanding (CSU) concept to MDO ISR requirements,
paying particular attention to the need for assured and explainable AI to allow
robust human-machine decision-making where assets are distributed among
multiple partners; (2) we present illustrative vignettes for AI and IA in MDO
ISR, including human-machine teaming, dense urban terrain analysis, and
enhanced asset interoperability; (3) we appraise the state-of-the-art in
explainable AI in relation to the vignettes with a focus on human-machine
collaboration to achieve more rapid and agile coalition decision-making. The
union of these three elements is intended to show the potential value of a CSU
approach in the context of MDO ISR, grounded in three distinct use cases,
highlighting how the need for explainability in the multi-partner coalition
setting is key."
Hybrid Intelligence,"Research has a long history of discussing what is superior in predicting
certain outcomes: statistical methods or the human brain. This debate has
repeatedly been sparked off by the remarkable technological advances in the
field of artificial intelligence (AI), such as solving tasks like object and
speech recognition, achieving significant improvements in accuracy through
deep-learning algorithms (Goodfellow et al. 2016), or combining various methods
of computational intelligence, such as fuzzy logic, genetic algorithms, and
case-based reasoning (Medsker 2012). One of the implicit promises that underlie
these advancements is that machines will 1 day be capable of performing complex
tasks or may even supersede humans in performing these tasks. This triggers new
heated debates of when machines will ultimately replace humans (McAfee and
Brynjolfsson 2017). While previous research has proved that AI performs well in
some clearly defined tasks such as playing chess, playing Go or identifying
objects on images, it is doubted that the development of an artificial general
intelligence (AGI) which is able to solve multiple tasks at the same time can
be achieved in the near future (e.g., Russell and Norvig 2016). Moreover, the
use of AI to solve complex business problems in organizational contexts occurs
scarcely, and applications for AI that solve complex problems remain mainly in
laboratory settings instead of being implemented in practice. Since the road to
AGI is still a long one, we argue that the most likely paradigm for the
division of labor between humans and machines in the next decades is Hybrid
Intelligence. This concept aims at using the complementary strengths of human
intelligence and AI, so that they can perform better than each of the two could
separately (e.g., Kamar 2016)."
"Artificial Intelligence-Driven Customized Manufacturing Factory: Key
  Technologies, Applications, and Challenges","The traditional production paradigm of large batch production does not offer
flexibility towards satisfying the requirements of individual customers. A new
generation of smart factories is expected to support new multi-variety and
small-batch customized production modes. For that, Artificial Intelligence (AI)
is enabling higher value-added manufacturing by accelerating the integration of
manufacturing and information communication technologies, including computing,
communication, and control. The characteristics of a customized smart factory
are to include self-perception, operations optimization, dynamic
reconfiguration, and intelligent decision-making. The AI technologies will
allow manufacturing systems to perceive the environment, adapt to external
needs, and extract the processed knowledge, including business models, such as
intelligent production, networked collaboration, and extended service models.
  This paper focuses on the implementation of AI in customized manufacturing
(CM). The architecture of an AI-driven customized smart factory is presented.
Details of intelligent manufacturing devices, intelligent information
interaction, and the construction of a flexible manufacturing line are
showcased. The state-of-the-art AI technologies of potential use in CM, i.e.,
machine learning, multi-agent systems, Internet of Things, big data, and
cloud-edge computing are surveyed. The AI-enabled technologies in a customized
smart factory are validated with a case study of customized packaging. The
experimental results have demonstrated that the AI-assisted CM offers the
possibility of higher production flexibility and efficiency. Challenges and
solutions related to AI in CM are also discussed."
"Beyond Interpretable Benchmarks: Contextual Learning through Cognitive
  and Multimodal Perception","With state-of-the-art models achieving high performance on standard
benchmarks, contemporary research paradigms continue to emphasize general
intelligence as an enduring objective. However, this pursuit overlooks the
fundamental disparities between the high-level data perception abilities of
artificial and natural intelligence systems. This study questions the Turing
Test as a criterion of generally intelligent thought and contends that it is
misinterpreted as an attempt to anthropomorphize computer systems. Instead, it
emphasizes tacit learning as a cornerstone of general-purpose intelligence,
despite its lack of overt interpretability. This abstract form of intelligence
necessitates contextual cognitive attributes that are crucial for human-level
perception: generalizable experience, moral responsibility, and implicit
prioritization. The absence of these features yields undeniable perceptual
disparities and constrains the cognitive capacity of artificial systems to
effectively contextualize their environments. Additionally, this study
establishes that, despite extensive exploration of potential architecture for
future systems, little consideration has been given to how such models will
continuously absorb and adapt to contextual data. While conventional models may
continue to improve in benchmark performance, disregarding these contextual
considerations will lead to stagnation in human-like comprehension. Until
general intelligence can be abstracted from task-specific domains and systems
can learn implicitly from their environments, research standards should instead
prioritize the disciplines in which AI thrives."
Role of Morphogenetic Competency on Evolution,"The relationship between intelligence and evolution is bidirectional: while
evolution can help evolve intelligences, the degree of intelligence itself can
impact evolution (Baldwin, 1896). In the field of Evolutionary Computation, the
inverse relationship (impact of intelligence on evolution) is approached from
the perspective of organism level behaviour (Hinton, 1996). We extend these
ideas to the developmental (cellular morphogenetic) level in the context of an
expanded view of intelligence as not only the ability of a system to navigate
the three-dimensional world, but also as the ability to navigate other
arbitrary spaces (transcriptional, anatomical, physiological, etc.). Here, we
specifically focus on the intelligence of a minimal model of a system
navigating anatomical morphospace, and assess how the degree and manner of
problem solving competency during morphogenesis effects evolutionary dynamics.
To this end, we evolve populations of artificial embryos using a standard
genetic algorithm in silico. Artificial embryos were cellular collectives given
the capacity to undergo morphogenetic rearrangement (e.g., regulative
development) prior to selection within an evolutionary cycle. Results from our
model indicates that morphogenetic competency significantly alters evolutionary
dynamics, with evolution preferring to improve anatomical intelligence rather
than perfect the structural genes. These observations hint that evolution in
the natural world may be leveraging the problem solving competencies of cells
at multiple scales to boost evolvability and robustness to novel conditions. We
discuss implications of our results for the Developmental Biology and
Artificial Life communities."
Vision-Language Navigation with Embodied Intelligence: A Survey,"As a long-term vision in the field of artificial intelligence, the core goal
of embodied intelligence is to improve the perception, understanding, and
interaction capabilities of agents and the environment. Vision-language
navigation (VLN), as a critical research path to achieve embodied intelligence,
focuses on exploring how agents use natural language to communicate effectively
with humans, receive and understand instructions, and ultimately rely on visual
information to achieve accurate navigation. VLN integrates artificial
intelligence, natural language processing, computer vision, and robotics. This
field faces technical challenges but shows potential for application such as
human-computer interaction. However, due to the complex process involved from
language understanding to action execution, VLN faces the problem of aligning
visual information and language instructions, improving generalization ability,
and many other challenges. This survey systematically reviews the research
progress of VLN and details the research direction of VLN with embodied
intelligence. After a detailed summary of its system architecture and research
based on methods and commonly used benchmark datasets, we comprehensively
analyze the problems and challenges faced by current research and explore the
future development direction of this field, aiming to provide a practical
reference for researchers."
Creating Scalable AGI: the Open General Intelligence Framework,"Recent advancements in Artificial Intelligence (AI), particularly with Large
Language Models (LLMs), have led to significant progress in narrow tasks such
as image classification, language translation, coding, and writing. However,
these models face limitations in reliability and scalability due to their
siloed architectures, which are designed to handle only one data modality (data
type) at a time. This single modal approach hinders their ability to integrate
the complex set of data points required for real-world challenges and
problem-solving tasks like medical diagnosis, quality assurance, equipment
troubleshooting, and financial decision-making. Addressing these real-world
challenges requires a more capable Artificial General Intelligence (AGI)
system. Our primary contribution is the development of the Open General
Intelligence (OGI) framework, a novel systems architecture that serves as a
macro design reference for AGI. The OGI framework adopts a modular approach to
the design of intelligent systems, based on the premise that cognition must
occur across multiple specialized modules that can seamlessly operate as a
single system. OGI integrates these modules using a dynamic processing system
and a fabric interconnect, enabling real-time adaptability, multi-modal
integration, and scalable processing. The OGI framework consists of three key
components: (1) Overall Macro Design Guidance that directs operational design
and processing, (2) a Dynamic Processing System that controls routing, primary
goals, instructions, and weighting, and (3) Framework Areas, a set of
specialized modules that operate cohesively to form a unified cognitive system.
By incorporating known principles from human cognition into AI systems, the OGI
framework aims to overcome the challenges observed in today's intelligent
systems, paving the way for more holistic and context-aware problem-solving
capabilities."
Managing Inconsistent Intelligence,"In this paper we demonstrate that it is possible to manage intelligence in
constant time as a pre-process to information fusion through a series of
processes dealing with issues such as clustering reports, ranking reports with
respect to importance, extraction of prototypes from clusters and immediate
classification of newly arriving intelligence reports. These methods are used
when intelligence reports arrive which concerns different events which should
be handled independently, when it is not known a priori to which event each
intelligence report is related. We use clustering that runs as a back-end
process to partition the intelligence into subsets representing the events, and
in parallel, a fast classification that runs as a front-end process in order to
put the newly arriving intelligence into its correct information fusion
process."
"Modelling and Reasoning Techniques for Context Aware Computing in
  Intelligent Transportation System","The emergence of Internet of Things technology and recent advancement in
sensor networks enabled transportation systems to a new dimension called
Intelligent Transportation System. Due to increased usage of vehicles and
communication among entities in road traffic scenarios, the amount of raw data
generation in Intelligent Transportation System is huge. This raw data are to
be processed to infer contextual information and provide new services related
to different modes of road transport such as traffic signal management,
accident prediction, object detection etc. To understand the importance of
context, this article aims to study context awareness in the Intelligent
Transportation System. We present a review on prominent applications developed
in the literature concerning context awareness in the intelligent
transportation system. The objective of this research paper is to highlight
context and its features in ITS and to address the applicability of modelling
techniques and reasoning approaches in Intelligent Transportation System. Also
to shed light on impact of Internet of Things and machine learning in
Intelligent Transportation System development."
The E-Intelligence System,"Electronic Intelligence (ELINT), often known as E-Intelligence, is
intelligence obtained through electronic sensors. Other than personal
communications, ELINT intelligence is usually obtained. The goal is usually to
determine a target's capabilities, such as radar placement. Active or passive
sensors can be employed to collect data. A provided signal is analyzed and
contrasted to collected data for recognized signal types. The information may
be stored if the signal type is detected; it can be classed as new if no match
is found. ELINT collects and categorizes data. In a military setting (and
others that have adopted the usage, such as a business), intelligence helps an
organization make decisions that can provide them a strategic advantage over
the competition. The term ""intel"" is frequently shortened. The two main
subfields of signals intelligence (SIGINT) are ELINT and Communications
Intelligence (COMINT). The US Department of Defense specifies the
terminologies, and intelligence communities use the categories of data reviewed
worldwide."
Testing System Intelligence,"We discuss the adequacy of tests for intelligent systems and practical
problems raised by their implementation. We propose the replacement test as the
ability of a system to replace successfully another system performing a task in
a given context. We show how it can characterize salient aspects of human
intelligence that cannot be taken into account by the Turing test. We argue
that building intelligent systems passing the replacement test involves a
series of technical problems that are outside the scope of current AI. We
present a framework for implementing the proposed test and validating the
properties of the intelligent systems. We discuss the inherent limitations of
intelligent system validation and advocate new theoretical foundations for
extending existing rigorous test methods. We suggest that the replacement test,
based on the complementarity of skills between human and machine, can lead to a
multitude of intelligence concepts reflecting the ability to combine data-based
and symbolic knowledge to varying degrees."
"Deep Learning based Quasi-consciousness Training for Robot Intelligent
  Model","This paper explores a deep learning based robot intelligent model that
renders robots learn and reason for complex tasks. First, by constructing a
network of environmental factor matrix to stimulate the learning process of the
robot intelligent model, the model parameters must be subjected to coarse &
fine tuning to optimize the loss function for minimizing the loss score,
meanwhile robot intelligent model can fuse all previously known concepts
together to represent things never experienced before, which need robot
intelligent model can be generalized extensively. Secondly, in order to
progressively develop a robot intelligent model with primary consciousness,
every robot must be subjected to at least 1~3 years of special school for
training anthropomorphic behaviour patterns to understand and process complex
environmental information and make rational decisions. This work explores and
delivers the potential application of deep learning-based quasi-consciousness
training in the field of robot intelligent model."
Quantum Mathematics in Artificial Intelligence,"In the decade since 2010, successes in artificial intelligence have been at
the forefront of computer science and technology, and vector space models have
solidified a position at the forefront of artificial intelligence. At the same
time, quantum computers have become much more powerful, and announcements of
major advances are frequently in the news.
  The mathematical techniques underlying both these areas have more in common
than is sometimes realized. Vector spaces took a position at the axiomatic
heart of quantum mechanics in the 1930s, and this adoption was a key motivation
for the derivation of logic and probability from the linear geometry of vector
spaces. Quantum interactions between particles are modelled using the tensor
product, which is also used to express objects and operations in artificial
neural networks.
  This paper describes some of these common mathematical areas, including
examples of how they are used in artificial intelligence (AI), particularly in
automated reasoning and natural language processing (NLP). Techniques discussed
include vector spaces, scalar products, subspaces and implication, orthogonal
projection and negation, dual vectors, density matrices, positive operators,
and tensor products. Application areas include information retrieval,
categorization and implication, modelling word-senses and disambiguation,
inference in knowledge bases, and semantic composition.
  Some of these approaches can potentially be implemented on quantum hardware.
Many of the practical steps in this implementation are in early stages, and
some are already realized. Explaining some of the common mathematical tools can
help researchers in both AI and quantum computing further exploit these
overlaps, recognizing and exploring new directions along the way."
Brain-inspired and Self-based Artificial Intelligence,"The question ""Can machines think?"" and the Turing Test to assess whether
machines could achieve human-level intelligence is one of the roots of AI. With
the philosophical argument ""I think, therefore I am"", this paper challenge the
idea of a ""thinking machine"" supported by current AIs since there is no sense
of self in them. Current artificial intelligence is only seemingly intelligent
information processing and does not truly understand or be subjectively aware
of oneself and perceive the world with the self as human intelligence does. In
this paper, we introduce a Brain-inspired and Self-based Artificial
Intelligence (BriSe AI) paradigm. This BriSe AI paradigm is dedicated to
coordinating various cognitive functions and learning strategies in a
self-organized manner to build human-level AI models and robotic applications.
Specifically, BriSe AI emphasizes the crucial role of the Self in shaping the
future AI, rooted with a practical hierarchical Self framework, including
Perception and Learning, Bodily Self, Autonomous Self, Social Self, and
Conceptual Self. The hierarchical framework of the Self highlights self-based
environment perception, self-bodily modeling, autonomous interaction with the
environment, social interaction and collaboration with others, and even more
abstract understanding of the Self. Furthermore, the positive mutual promotion
and support among multiple levels of Self, as well as between Self and
learning, enhance the BriSe AI's conscious understanding of information and
flexible adaptation to complex environments, serving as a driving force
propelling BriSe AI towards real Artificial General Intelligence."
"Deep Learning for Code Intelligence: Survey, Benchmark and Toolkit","Code intelligence leverages machine learning techniques to extract knowledge
from extensive code corpora, with the aim of developing intelligent tools to
improve the quality and productivity of computer programming. Currently, there
is already a thriving research community focusing on code intelligence, with
efforts ranging from software engineering, machine learning, data mining,
natural language processing, and programming languages. In this paper, we
conduct a comprehensive literature review on deep learning for code
intelligence, from the aspects of code representation learning, deep learning
techniques, and application tasks. We also benchmark several state-of-the-art
neural models for code intelligence, and provide an open-source toolkit
tailored for the rapid prototyping of deep-learning-based code intelligence
models. In particular, we inspect the existing code intelligence models under
the basis of code representation learning, and provide a comprehensive overview
to enhance comprehension of the present state of code intelligence.
Furthermore, we publicly release the source code and data resources to provide
the community with a ready-to-use benchmark, which can facilitate the
evaluation and comparison of existing and future code intelligence models
(https://xcodemind.github.io). At last, we also point out several challenging
and promising directions for future research."
Polyethism in a colony of artificial ants,"We explore self-organizing strategies for role assignment in a foraging task
carried out by a colony of artificial agents. Our strategies are inspired by
various mechanisms of division of labor (polyethism) observed in eusocial
insects like ants, termites, or bees. Specifically we instantiate models of
caste polyethism and age or temporal polyethism to evaluated the benefits to
foraging in a dynamic environment. Our experiment is directly related to the
exploration/exploitation trade of in machine learning."
"Digital Genesis: Computers, Evolution and Artificial Life","The application of evolution in the digital realm, with the goal of creating
artificial intelligence and artificial life, has a history as long as that of
the digital computer itself. We illustrate the intertwined history of these
ideas, starting with the early theoretical work of John von Neumann and the
pioneering experimental work of Nils Aall Barricelli. We argue that
evolutionary thinking and artificial life will continue to play an integral
role in the future development of the digital world."
"Nanoscale artificial intelligence: creating artificial neural networks
  using autocatalytic reactions","A general methodology is proposed to engineer a system of interacting
components (particles) which is able to self-regulate their concentrations in
order to produce any prescribed output in response to a particular input. The
methodology is based on the mathematical equivalence between artificial neurons
in neural networks and species in autocatalytic reactions, and it specifies the
relationship between the artificial neural network's parameters and the rate
coefficients of the reactions between particle species. Such systems are
characterised by a high degree of robustness as they are able to reach the
desired output despite disturbances and perturbations of the concentrations of
the various species."
Toward Formalizing Teleportation of Pedagogical Artificial Agents,"Our paradigm for the use of artificial agents to teach requires among other
things that they persist through time in their interaction with human students,
in such a way that they ""teleport"" or ""migrate"" from an embodiment at one time
t to a different embodiment at later time t'. In this short paper, we report on
initial steps toward the formalization of such teleportation, in order to
enable an overseeing AI system to establish, mechanically, and verifiably, that
the human students in question will likely believe that the very same
artificial agent has persisted across such times despite the different
embodiments."
Cities as they could be: Artificial Life and Urban Systems,"The metaphor of cities as organisms has a long history in urban planning, and
a few urban modeling approaches have explicitly been linked to Artificial Life.
We propose in that paper to explore the extent of Artificial Life and
Artificial Intelligence application to urban issues, by constructing and
exploring a citation network of around 225,000 papers. It shows that most of
the literature is indeed application of methodologies and a rather strong
modularity of approaches. We finally develop ALife concepts which have a strong
potential for the development of new urban theories."
"Deep Convolutional Generative Modeling for Artificial Microstructure
  Development of Aluminum-Silicon Alloy","Machine learning which is a sub-domain of an Artificial Intelligence which is
finding various applications in manufacturing and material science sectors. In
the present study, Deep Generative Modeling which a type of unsupervised
machine learning technique has been adapted for the constructing the artificial
microstructure of Aluminium-Silicon alloy. Deep Generative Adversarial Networks
has been used for developing the artificial microstructure of the given
microstructure image dataset. The results obtained showed that the developed
models had learnt to replicate the lining near the certain images of the
microstructures."
"Accelerating drug discovery with Artificial: a whole-lab orchestration
  and scheduling system for self-driving labs","Self-driving labs are transforming drug discovery by enabling automated,
AI-guided experimentation, but they face challenges in orchestrating complex
workflows, integrating diverse instruments and AI models, and managing data
efficiently. Artificial addresses these issues with a comprehensive
orchestration and scheduling system that unifies lab operations, automates
workflows, and integrates AI-driven decision-making. By incorporating AI/ML
models like NVIDIA BioNeMo - which facilitates molecular interaction prediction
and biomolecular analysis - Artificial enhances drug discovery and accelerates
data-driven research. Through real-time coordination of instruments, robots,
and personnel, the platform streamlines experiments, enhances reproducibility,
and advances drug discovery."
Enhancing a Search Algorithm to Perform Intelligent Backtracking,"This paper illustrates how a Prolog program, using chronological backtracking
to find a solution in some search space, can be enhanced to perform intelligent
backtracking. The enhancement crucially relies on the impurity of Prolog that
allows a program to store information when a dead end is reached. To illustrate
the technique, a simple search program is enhanced.
  To appear in Theory and Practice of Logic Programming.
  Keywords: intelligent backtracking, dependency-directed backtracking,
backjumping, conflict-directed backjumping, nogood sets, look-back."
Modeling of Social Transitions Using Intelligent Systems,"In this study, we reproduce two new hybrid intelligent systems, involve three
prominent intelligent computing and approximate reasoning methods: Self
Organizing feature Map (SOM), Neruo-Fuzzy Inference System and Rough Set Theory
(RST),called SONFIS and SORST. We show how our algorithms can be construed as a
linkage of government-society interactions, where government catches various
states of behaviors: solid (absolute) or flexible. So, transition of society,
by changing of connectivity parameters (noise) from order to disorder is
inferred."
A theory of intelligence: networked problem solving in animal societies,"A society's single emergent, increasing intelligence arises partly from the
thermodynamic advantages of networking the innate intelligence of different
individuals, and partly from the accumulation of solved problems. Economic
growth is proportional to the square of the network entropy of a society's
population times the network entropy of the number of the society's solved
problems."
Intelligent Semantic Web Search Engines: A Brief Survey,"The World Wide Web (WWW) allows the people to share the information (data)
from the large database repositories globally. The amount of information grows
billions of databases. We need to search the information will specialize tools
known generically search engine. There are many of search engines available
today, retrieving meaningful information is difficult. However to overcome this
problem in search engines to retrieve meaningful information intelligently,
semantic web technologies are playing a major role. In this paper we present
survey on the search engine generations and the role of search engines in
intelligent web and semantic search technologies."
"Creating Intelligent Linking for Information Threading in Knowledge
  Networks","Informledge System (ILS) is a knowledge network with autonomous nodes and
intelligent links that integrate and structure the pieces of knowledge. In this
paper, we aim to put forward the link dynamics involved in intelligent
processing of information in ILS. There has been advancement in knowledge
management field which involve managing information in databases from a single
domain. ILS works with information from multiple domains stored in distributed
way in the autonomous nodes termed as Knowledge Network Node (KNN). Along with
the concept under consideration, KNNs store the processed information linking
concepts and processors leading to the appropriate processing of information."
An Intelligent Location Management approaches in GSM Mobile Network,"Location management refers to the problem of updating and searching the
current location of mobile nodes in a wireless network. To make it efficient,
the sum of update costs of location database must be minimized. Previous work
relying on fixed location databases is unable to fully exploit the knowledge of
user mobility patterns in the system so as to achieve this minimization. The
study presents an intelligent location management approach which has interacts
between intelligent information system and knowledge-base technologies, so we
can dynamically change the user patterns and reduce the transition between the
VLR and HLR. The study provides algorithms are ability to handle location
registration and call delivery"
Are there intelligent Turing machines?,"This paper introduces a new computing model based on the cooperation among
Turing machines called orchestrated machines. Like universal Turing machines,
orchestrated machines are also designed to simulate Turing machines but they
can also modify the original operation of the included Turing machines to
create a new layer of some kind of collective behavior. Using this new model we
can define some interested notions related to cooperation ability of Turing
machines such as the intelligence quotient or the emotional intelligence
quotient for Turing machines."
"A New Theoretical and Technological System of Imprecise-Information
  Processing","Imprecise-information processing will play an indispensable role in
intelligent systems, especially in the anthropomorphic intelligent systems (as
intelligent robots). A new theoretical and technological system of
imprecise-information processing has been founded in Principles of
Imprecise-Information Processing: A New Theoretical and Technological System[1]
which is different from fuzzy technology. The system has clear hierarchy and
rigorous structure, which results from the formation principle of imprecise
information and has solid mathematical and logical bases, and which has many
advantages beyond fuzzy technology. The system provides a technological
platform for relevant applications and lays a theoretical foundation for
further research."
"Acquisition and use of knowledge over a restricted domain by intelligent
  agents","This short paper provides a description of an architecture to acquisition and
use of knowledge by intelligent agents over a restricted domain of the Internet
Infrastructure. The proposed architecture is added to an intelligent agent
deployment model over a very useful server for Internet Autonomous System
administrators. Such servers, which are heavily dependent on arbitrary and
eventual updates of human beings, become unreliable. This is a position paper
that proposes three research questions that are still in progress."
Intelligent Processing in Vehicular Ad hoc Networks: a Survey,"The intelligent Processing technique is more and more attractive to
researchers due to its ability to deal with key problems in Vehicular Ad hoc
networks. However, several problems in applying intelligent processing
technologies in VANETs remain open. The existing applications are
comprehensively reviewed and discussed, and classified into different
categories in this paper. Their strategies, advantages/disadvantages, and
performances are elaborated. By generalizing different tactics in various
applications related to different scenarios of VANETs and evaluating their
performances, several promising directions for future research have been
suggested."
Turing Test Revisited: A Framework for an Alternative,"This paper aims to question the suitability of the Turing Test, for testing
machine intelligence, in the light of advances made in the last 60 years in
science, medicine, and philosophy of mind. While the main concept of the test
may seem sound and valid, a detailed analysis of what is required to pass the
test highlights a significant flow. Once the analysis of the test is presented,
a systematic approach is followed in analysing what is needed to devise a test
or tests for intelligent machines. The paper presents a plausible generic
framework based on categories of factors implied by subjective perception of
intelligence. An evaluative discussion concludes the paper highlighting some of
the unaddressed issues within this generic framework."
"Towards Intelligent Interactive Theatre: Drama Management as a way of
  Handling Performance","In this paper, we present a new modality for intelligent interactive
narratives within the theatre domain. We discuss the possibilities of using an
intelligent agent that serves as a drama manager and as an actor that plays a
character within the live theatre experience. We pose a set of research
challenges that arise from our analysis towards the implementation of such an
agent, as well as potential methodologies as a starting point to bridge the
gaps between current literature and the proposed modality."
Intelligent Hotel ROS-based Service Robot,"With the advances of artificial intelligence (AI) technology, many studies
and work have been carried out on how robots could replace human labor. In this
paper, we present a ROS based intelligence hotel robot, which simplifies the
check-in process. We use pioneer 3dx robot and considered different environment
settings. The robot combined with Hokuyo Lidar and Kinect Xbox camera, can plan
the routes accurately and reach rooms in different floors. In addition, we
added an intelligent voice system which provides an assistant for the
customers."
"Empathic Chatbot: Emotional Intelligence for Empathic Chatbot: Emotional
  Intelligence for Mental Health Well-being","Conversational chatbots are Artificial Intelligence (AI)-powered applications
that assist users with various tasks by responding in natural language and are
prevalent across different industries. Most of the chatbots that we encounter
on websites and digital assistants such as Alexa, Siri does not express empathy
towards the user, and their ability to empathise remains immature. Lack of
empathy towards the user is not critical for a transactional or interactive
chatbot, but the bots designed to support mental healthcare patients need to
understand the emotional state of the user and tailor the conversations. This
research explains the different types of emotional intelligence methodologies
adopted in the development of an empathic chatbot and how far they have been
adopted and succeeded."
Human-in-the-loop Machine Learning: A Macro-Micro Perspective,"Though technical advance of artificial intelligence and machine learning has
enabled many promising intelligent systems, many computing tasks are still not
able to be fully accomplished by machine intelligence. Motivated by the
complementary nature of human and machine intelligence, an emerging trend is to
involve humans in the loop of machine learning and decision-making. In this
paper, we provide a macro-micro review of human-in-the-loop machine learning.
We first describe major machine learning challenges which can be addressed by
human intervention in the loop. Then we examine closely the latest research and
findings of introducing humans into each step of the lifecycle of machine
learning. Finally, we analyze current research gaps and point out future
research directions."
"Designing an AI-Driven Talent Intelligence Solution: Exploring Big Data
  to extend the TOE Framework","AI has the potential to improve approaches to talent management enabling
dynamic provisions through implementing advanced automation. This study aims to
identify the new requirements for developing AI-oriented artifacts to address
talent management issues. Focusing on enhancing interactions between
professional assessment and planning attributes, the design artifact is an
intelligent employment automation solution for career guidance that is largely
dependent on a talent intelligent module and an individuals growth needs. A
design science method is adopted for conducting the experimental study with
structured machine learning techniques which is the primary element of a
comprehensive AI solution framework informed through a proposed moderation of
the technology-organization-environment theory."
Distributed Swarm Intelligence,"This paper presents the development of a distributed application that
facilitates the understanding and application of swarm intelligence in solving
optimization problems. The platform comprises a search space of customizable
random particles, allowing users to tailor the solution to their specific
needs. By leveraging the power of Ray distributed computing, the application
can support multiple users simultaneously, offering a flexible and scalable
solution. The primary objective of this project is to provide a user-friendly
platform that enhances the understanding and practical use of swarm
intelligence in problem-solving."
"From Conception to Deployment: Intelligent Stroke Prediction Framework
  using Machine Learning and Performance Evaluation","Stroke is the second leading cause of death worldwide. Machine learning
classification algorithms have been widely adopted for stroke prediction.
However, these algorithms were evaluated using different datasets and
evaluation metrics. Moreover, there is no comprehensive framework for stroke
data analytics. This paper proposes an intelligent stroke prediction framework
based on a critical examination of machine learning prediction algorithms in
the literature. The five most used machine learning algorithms for stroke
prediction are evaluated using a unified setup for objective comparison.
Comparative analysis and numerical results reveal that the Random Forest
algorithm is best suited for stroke prediction."
Intelligent Stress Assessment for e-Coaching,"This paper considers the adaptation of the e-coaching concept at times of
emergencies and disasters, through aiding the e-coaching with intelligent tools
for monitoring humans' affective state. The states such as anxiety, panic,
avoidance, and stress, if properly detected, can be mitigated using the
e-coaching tactic and strategy. In this work, we focus on a stress monitoring
assistant tool developed on machine learning techniques. We provide the results
of an experimental study using the proposed method."
"Enabling Intelligent Traffic Systems: A Deep Learning Method for
  Accurate Arabic License Plate Recognition","This paper introduces a novel two-stage framework for accurate Egyptian
Vehicle License Plate Recognition (EVLPR). The first stage employs image
processing techniques to reliably localize license plates, while the second
stage utilizes a custom-designed deep learning model for robust Arabic
character recognition. The proposed system achieves a remarkable 99.3% accuracy
on a diverse dataset, surpassing existing approaches. Its potential
applications extend to intelligent traffic management, including traffic
violation detection and parking optimization. Future research will focus on
enhancing the system's capabilities through architectural refinements, expanded
datasets, and addressing system dependencies."
"A Neural Model of Rule Discovery with Relatively Short-Term Sequence
  Memory","This report proposes a neural cognitive model for discovering regularities in
event sequences. In a fluid intelligence task, the subject is required to
discover regularities from relatively short-term memory of the first-seen task.
Some fluid intelligence tasks require discovering regularities in event
sequences. Thus, a neural network model was constructed to explain fluid
intelligence or regularity discovery in event sequences with relatively
short-term memory. The model was implemented and tested with delayed
match-to-sample tasks."
"Modular and Integrated AI Control Framework across Fiber and Wireless
  Networks for 6G","The rapid evolution of communication networks towards 6G increasingly
incorporates advanced AI-driven controls across various network segments to
achieve intelligent, zero-touch operation. This paper proposes a comprehensive
and modular framework for AI controllers, designed to be highly flexible and
adaptable for use across both fiber optical and radio networks. Building on the
principles established by the O-RAN Alliance for near-Real-Time RAN Intelligent
Controllers (near-RT RICs), our framework extends this AI-driven control into
the optical domain. Our approach addresses the critical need for a unified AI
control framework across diverse network transport technologies and domains,
enabling the development of intelligent, automated, and scalable 6G networks."
"Intelligent Resource Allocation Optimization for Cloud Computing via
  Machine Learning","With the rapid expansion of cloud computing applications, optimizing resource
allocation has become crucial for improving system performance and cost
efficiency. This paper proposes an intelligent resource allocation algorithm
that leverages deep learning (LSTM) for demand prediction and reinforcement
learning (DQN) for dynamic scheduling. By accurately forecasting computing
resource demands and enabling real-time adjustments, the proposed system
enhances resource utilization by 32.5%, reduces average response time by 43.3%,
and lowers operational costs by 26.6%. Experimental results in a production
cloud environment confirm that the method significantly improves efficiency
while maintaining high service quality. This study provides a scalable and
effective solution for intelligent cloud resource management, offering valuable
insights for future cloud optimization strategies."
Self-Regulated Artificial Ant Colonies on Digital Image Habitats,"Artificial life models, swarm intelligent and evolutionary computation
algorithms are usually built on fixed size populations. Some studies indicate
however that varying the population size can increase the adaptability of these
systems and their capability to react to changing environments. In this paper
we present an extended model of an artificial ant colony system designed to
evolve on digital image habitats. We will show that the present swarm can adapt
the size of the population according to the type of image on which it is
evolving and reacting faster to changing images, thus converging more rapidly
to the new desired regions, regulating the number of his image foraging agents.
Finally, we will show evidences that the model can be associated with the
Mathematical Morphology Watershed algorithm to improve the segmentation of
digital grey-scale images. KEYWORDS: Swarm Intelligence, Perception and Image
Processing, Pattern Recognition, Mathematical Morphology, Social Cognitive
Maps, Social Foraging, Self-Organization, Distributed Search."
Structured Learning Modulo Theories,"Modelling problems containing a mixture of Boolean and numerical variables is
a long-standing interest of Artificial Intelligence. However, performing
inference and learning in hybrid domains is a particularly daunting task. The
ability to model this kind of domains is crucial in ""learning to design"" tasks,
that is, learning applications where the goal is to learn from examples how to
perform automatic {\em de novo} design of novel objects. In this paper we
present Structured Learning Modulo Theories, a max-margin approach for learning
in hybrid domains based on Satisfiability Modulo Theories, which allows to
combine Boolean reasoning and optimization over continuous linear arithmetical
constraints. The main idea is to leverage a state-of-the-art generalized
Satisfiability Modulo Theory solver for implementing the inference and
separation oracles of Structured Output SVMs. We validate our method on
artificial and real world scenarios."
"Design of an Alarm System for Isfahan Ozone Level based on Artificial
  Intelligence Predictor Models","The ozone level prediction is an important task of air quality agencies of
modern cities. In this paper, we design an ozone level alarm system (OLP) for
Isfahan city and test it through the real word data from 1-1-2000 to 7-6-2011.
We propose a computer based system with three inputs and single output. The
inputs include three sensors of solar ultraviolet (UV), total solar radiation
(TSR) and total ozone (O3). And the output of the system is the predicted O3 of
the next day and the alarm massages. A developed artificial intelligence (AI)
algorithm is applied to determine the output, based on the inputs variables.
For this issue, AI models, including supervised brain emotional learning (BEL),
adaptive neuro-fuzzy inference system (ANFIS) and artificial neural networks
(ANNs), are compared in order to find the best model. The simulation of the
proposed system shows that it can be used successfully in prediction of major
cities ozone level."
"A Model of Pathways to Artificial Superintelligence Catastrophe for Risk
  and Decision Analysis","An artificial superintelligence (ASI) is artificial intelligence that is
significantly more intelligent than humans in all respects. While ASI does not
currently exist, some scholars propose that it could be created sometime in the
future, and furthermore that its creation could cause a severe global
catastrophe, possibly even resulting in human extinction. Given the high
stakes, it is important to analyze ASI risk and factor the risk into decisions
related to ASI research and development. This paper presents a graphical model
of major pathways to ASI catastrophe, focusing on ASI created via recursive
self-improvement. The model uses the established risk and decision analysis
modeling paradigms of fault trees and influence diagrams in order to depict
combinations of events and conditions that could lead to AI catastrophe, as
well as intervention options that could decrease risks. The events and
conditions include select aspects of the ASI itself as well as the human
process of ASI research, development, and management. Model structure is
derived from published literature on ASI risk. The model offers a foundation
for rigorous quantitative evaluation and decision making on the long-term risk
of ASI catastrophe."
Decoupling Learning Rules from Representations,"In the artificial intelligence field, learning often corresponds to changing
the parameters of a parameterized function. A learning rule is an algorithm or
mathematical expression that specifies precisely how the parameters should be
changed. When creating an artificial intelligence system, we must make two
decisions: what representation should be used (i.e., what parameterized
function should be used) and what learning rule should be used to search
through the resulting set of representable functions. Using most learning
rules, these two decisions are coupled in a subtle (and often unintentional)
way. That is, using the same learning rule with two different representations
that can represent the same sets of functions can result in two different
outcomes. After arguing that this coupling is undesirable, particularly when
using artificial neural networks, we present a method for partially decoupling
these two decisions for a broad class of learning rules that span unsupervised
learning, reinforcement learning, and supervised learning."
"Information Flow Theory (IFT) of Biologic and Machine Consciousness:
  Implications for Artificial General Intelligence and the Technological
  Singularity","The subjective experience of consciousness is at once familiar and yet deeply
mysterious. Strategies exploring the top-down mechanisms of conscious thought
within the human brain have been unable to produce a generalized explanatory
theory that scales through evolution and can be applied to artificial systems.
Information Flow Theory (IFT) provides a novel framework for understanding both
the development and nature of consciousness in any system capable of processing
information. In prioritizing the direction of information flow over information
computation, IFT produces a range of unexpected predictions. The purpose of
this manuscript is to introduce the basic concepts of IFT and explore the
manifold implications regarding artificial intelligence, superhuman
consciousness, and our basic perception of reality."
"Brain-Inspired Hardware for Artificial Intelligence: Accelerated
  Learning in a Physical-Model Spiking Neural Network","Future developments in artificial intelligence will profit from the existence
of novel, non-traditional substrates for brain-inspired computing. Neuromorphic
computers aim to provide such a substrate that reproduces the brain's
capabilities in terms of adaptive, low-power information processing. We present
results from a prototype chip of the BrainScaleS-2 mixed-signal neuromorphic
system that adopts a physical-model approach with a 1000-fold acceleration of
spiking neural network dynamics relative to biological real time. Using the
embedded plasticity processor, we both simulate the Pong arcade video game and
implement a local plasticity rule that enables reinforcement learning, allowing
the on-chip neural network to learn to play the game. The experiment
demonstrates key aspects of the employed approach, such as accelerated and
flexible learning, high energy efficiency and resilience to noise."
Scenarios and Recommendations for Ethical Interpretive AI,"Artificially intelligent systems, given a set of non-trivial ethical rules to
follow, will inevitably be faced with scenarios which call into question the
scope of those rules. In such cases, human reasoners typically will engage in
interpretive reasoning, where interpretive arguments are used to support or
attack claims that some rule should be understood a certain way. Artificially
intelligent reasoners, however, currently lack the ability to carry out
human-like interpretive reasoning, and we argue that bridging this gulf is of
tremendous importance to human-centered AI. In order to better understand how
future artificial reasoners capable of human-like interpretive reasoning must
be developed, we have collected a dataset of ethical rules, scenarios designed
to invoke interpretive reasoning, and interpretations of those scenarios. We
perform a qualitative analysis of our dataset, and summarize our findings in
the form of practical recommendations."
"Very simple statistical evidence that AlphaGo has exceeded human limits
  in playing GO game","Deep learning technology is making great progress in solving the challenging
problems of artificial intelligence, hence machine learning based on artificial
neural networks is in the spotlight again. In some areas, artificial
intelligence based on deep learning is beyond human capabilities. It seemed
extremely difficult for a machine to beat a human in a Go game, but AlphaGo has
shown to beat a professional player in the game. By looking at the statistical
distribution of the distance in which the Go stones are laid in succession, we
find a clear trace that Alphago has surpassed human abilities. The AlphaGo than
professional players and professional players than ordinary players shows the
laying of stones in the distance becomes more frequent. In addition, AlphaGo
shows a much more pronounced difference than that of ordinary players and
professional players."
Interpreting convolutional networks trained on textual data,"There have been many advances in the artificial intelligence field due to the
emergence of deep learning. In almost all sub-fields, artificial neural
networks have reached or exceeded human-level performance. However, most of the
models are not interpretable. As a result, it is hard to trust their decisions,
especially in life and death scenarios. In recent years, there has been a
movement toward creating explainable artificial intelligence, but most work to
date has concentrated on image processing models, as it is easier for humans to
perceive visual patterns. There has been little work in other fields like
natural language processing. In this paper, we train a convolutional model on
textual data and analyze the global logic of the model by studying its filter
values. In the end, we find the most important words in our corpus to our
models logic and remove the rest (95%). New models trained on just the 5% most
important words can achieve the same performance as the original model while
reducing training time by more than half. Approaches such as this will help us
to understand NLP models, explain their decisions according to their word
choices, and improve them by finding blind spots and biases."
"Artificial Intelligence: Research Impact on Key Industries; the
  Upper-Rhine Artificial Intelligence Symposium (UR-AI 2020)","The TriRhenaTech alliance presents a collection of accepted papers of the
cancelled tri-national 'Upper-Rhine Artificial Inteeligence Symposium' planned
for 13th May 2020 in Karlsruhe. The TriRhenaTech alliance is a network of
universities in the Upper-Rhine Trinational Metropolitan Region comprising of
the German universities of applied sciences in Furtwangen, Kaiserslautern,
Karlsruhe, and Offenburg, the Baden-Wuerttemberg Cooperative State University
Loerrach, the French university network Alsace Tech (comprised of 14 'grandes
\'ecoles' in the fields of engineering, architecture and management) and the
University of Applied Sciences and Arts Northwestern Switzerland. The
alliance's common goal is to reinforce the transfer of knowledge, research, and
technology, as well as the cross-border mobility of students."
Photonics for artificial intelligence and neuromorphic computing,"Research in photonic computing has flourished due to the proliferation of
optoelectronic components on photonic integration platforms. Photonic
integrated circuits have enabled ultrafast artificial neural networks,
providing a framework for a new class of information processing machines.
Algorithms running on such hardware have the potential to address the growing
demand for machine learning and artificial intelligence, in areas such as
medical diagnosis, telecommunications, and high-performance and scientific
computing. In parallel, the development of neuromorphic electronics has
highlighted challenges in that domain, in particular, related to processor
latency. Neuromorphic photonics offers sub-nanosecond latencies, providing a
complementary opportunity to extend the domain of artificial intelligence.
Here, we review recent advances in integrated photonic neuromorphic systems,
discuss current and future challenges, and outline the advances in science and
technology needed to meet those challenges."
A Survey on the Explainability of Supervised Machine Learning,"Predictions obtained by, e.g., artificial neural networks have a high
accuracy but humans often perceive the models as black boxes. Insights about
the decision making are mostly opaque for humans. Particularly understanding
the decision making in highly sensitive areas such as healthcare or fifinance,
is of paramount importance. The decision-making behind the black boxes requires
it to be more transparent, accountable, and understandable for humans. This
survey paper provides essential definitions, an overview of the different
principles and methodologies of explainable Supervised Machine Learning (SML).
We conduct a state-of-the-art survey that reviews past and recent explainable
SML approaches and classifies them according to the introduced definitions.
Finally, we illustrate principles by means of an explanatory case study and
discuss important future directions."
"Modular Object-Oriented Games: A Task Framework for Reinforcement
  Learning, Psychology, and Neuroscience","In recent years, trends towards studying simulated games have gained momentum
in the fields of artificial intelligence, cognitive science, psychology, and
neuroscience. The intersections of these fields have also grown recently, as
researchers increasing study such games using both artificial agents and human
or animal subjects. However, implementing games can be a time-consuming
endeavor and may require a researcher to grapple with complex codebases that
are not easily customized. Furthermore, interdisciplinary researchers studying
some combination of artificial intelligence, human psychology, and animal
neurophysiology face additional challenges, because existing platforms are
designed for only one of these domains. Here we introduce Modular
Object-Oriented Games, a Python task framework that is lightweight, flexible,
customizable, and designed for use by machine learning, psychology, and
neurophysiology researchers."
Making AI 'Smart': Bridging AI and Cognitive Science,"The last two decades have seen tremendous advances in Artificial
Intelligence. The exponential growth in terms of computation capabilities has
given us hope of developing humans like robots. The question is: are we there
yet? Maybe not. With the integration of cognitive science, the 'artificial'
characteristic of Artificial Intelligence (AI) might soon be replaced with
'smart'. This will help develop more powerful AI systems and simultaneously
gives us a better understanding of how the human brain works. We discuss the
various possibilities and challenges of bridging these two fields and how they
can benefit each other. We argue that the possibility of AI taking over human
civilization is low as developing such an advanced system requires a better
understanding of the human brain first."
Human-AI ecosystem with abrupt changes as a function of the composition,"The progressive advent of artificial intelligence machines may represent both
an opportunity or a threat. In order to have an idea of what is coming we
propose a model that simulate a Human-AI ecosystem. In particular we consider
systems where agents present biases, peer-to-peer interactions and also three
body interactions that are crucial and describe two humans interacting with an
artificial agent and two artificial intelligence agents interacting with a
human. We focus our analysis by exploring how the relative fraction of
artificial intelligence agents affect that ecosystem. We find evidence that for
suitable values of the interaction parameters, arbitrarily small changes in
such percentage may trigger dramatic changes for the system that can be either
in one of the two polarised states or in an undecided state."
"From Knowledge Augmentation to Multi-tasking: Towards Human-like
  Dialogue Systems","The goal of building dialogue agents that can converse with humans naturally
has been a long-standing dream of researchers since the early days of
artificial intelligence. The well-known Turing Test proposed to judge the
ultimate validity of an artificial intelligence agent on the
indistinguishability of its dialogues from humans'. It should come as no
surprise that human-level dialogue systems are very challenging to build. But,
while early effort on rule-based systems found limited success, the emergence
of deep learning enabled great advance on this topic.
  In this thesis, we focus on methods that address the numerous issues that
have been imposing the gap between artificial conversational agents and
human-level interlocutors. These methods were proposed and experimented with in
ways that were inspired by general state-of-the-art AI methodologies. But they
also targeted the characteristics that dialogue systems possess."
Detection of Fake Generated Scientific Abstracts,"The widespread adoption of Large Language Models and publicly available
ChatGPT has marked a significant turning point in the integration of Artificial
Intelligence into people's everyday lives. The academic community has taken
notice of these technological advancements and has expressed concerns regarding
the difficulty of discriminating between what is real and what is artificially
generated. Thus, researchers have been working on developing effective systems
to identify machine-generated text. In this study, we utilize the GPT-3 model
to generate scientific paper abstracts through Artificial Intelligence and
explore various text representation methods when combined with Machine Learning
models with the aim of identifying machine-written text. We analyze the models'
performance and address several research questions that rise during the
analysis of the results. By conducting this research, we shed light on the
capabilities and limitations of Artificial Intelligence generated text."
"Emergence of Symbols in Neural Networks for Semantic Understanding and
  Communication","The capacity to generate meaningful symbols and effectively employ them for
advanced cognitive processes, such as communication, reasoning, and planning,
constitutes a fundamental and distinctive aspect of human intelligence.
Existing deep neural networks still notably lag human capabilities in terms of
generating symbols for higher cognitive functions. Here, we propose a solution
(symbol emergence artificial network (SEA-net)) to endow neural networks with
the ability to create symbols, understand semantics, and achieve communication.
SEA-net generates symbols that dynamically configure the network to perform
specific tasks. These symbols capture compositional semantic information that
allows the system to acquire new functions purely by symbolic manipulation or
communication. In addition, these self-generated symbols exhibit an intrinsic
structure resembling that of natural language, suggesting a common framework
underlying the generation and understanding of symbols in both human brains and
artificial neural networks. We believe that the proposed framework will be
instrumental in producing more capable systems that can synergize the strengths
of connectionist and symbolic approaches for artificial intelligence (AI)."
"Development of an intelligent system for the detection of corona virus
  using artificial neural network","This paper presents the development of an intelligent system for the
detection of coronavirus using artificial neural network. This was done after
series of literature review which indicated that high fever accounts for 87.9%
of the COVID-19 symptoms. 683 temperature data of COVID-19 patients at >= 38C^o
were collected from Colliery hospital Enugu, Nigeria and used to train an
artificial neural network detective model for the detection of COVID-19. The
reference model generated was used converted into Verilog codes using Hardware
Description Language (HDL) and then burn into a Field Programming Gate Array
(FPGA) controller using FPGA tool in Matlab. The performance of the model when
evaluated using confusion matrix, regression and means square error (MSE)
showed that the regression value is 0.967; the accuracy is 97% and then MSE is
0.00100Mu. These results all implied that the new detection system for is
reliable and very effective for the detection of COVID-19."
"Software Metadata Classification based on Generative Artificial
  Intelligence","This paper presents a novel approach to enhance the performance of binary
code comment quality classification models through the application of
Generative Artificial Intelligence (AI). By leveraging the OpenAI API, a
dataset comprising 1239 newly generated code-comment pairs, extracted from
various GitHub repositories and open-source projects, has been labelled as
""Useful"" or ""Not Useful"", and integrated into the existing corpus of 9048 pairs
in the C programming language. Employing a cutting-edge Large Language Model
Architecture, the generated dataset demonstrates notable improvements in model
accuracy. Specifically, when incorporated into the Support Vector Machine (SVM)
model, a 6% increase in precision is observed, rising from 0.79 to 0.85.
Additionally, the Artificial Neural Network (ANN) model exhibits a 1.5%
increase in recall, climbing from 0.731 to 0.746. This paper sheds light on the
potential of Generative AI in augmenting code comment quality classification
models. The results affirm the effectiveness of this methodology, indicating
its applicability in broader contexts within software development and quality
assurance domains. The findings underscore the significance of integrating
generative techniques to advance the accuracy and efficacy of machine learning
models in practical software engineering scenarios."
"A Philosophical Introduction to Language Models -- Part I: Continuity
  With Classic Debates","Large language models like GPT-4 have achieved remarkable proficiency in a
broad spectrum of language-based tasks, some of which are traditionally
associated with hallmarks of human intelligence. This has prompted ongoing
disagreements about the extent to which we can meaningfully ascribe any kind of
linguistic or cognitive competence to language models. Such questions have deep
philosophical roots, echoing longstanding debates about the status of
artificial neural networks as cognitive models. This article -- the first part
of two companion papers -- serves both as a primer on language models for
philosophers, and as an opinionated survey of their significance in relation to
classic debates in the philosophy cognitive science, artificial intelligence,
and linguistics. We cover topics such as compositionality, language
acquisition, semantic competence, grounding, world models, and the transmission
of cultural knowledge. We argue that the success of language models challenges
several long-held assumptions about artificial neural networks. However, we
also highlight the need for further empirical investigation to better
understand their internal mechanisms. This sets the stage for the companion
paper (Part II), which turns to novel empirical methods for probing the inner
workings of language models, and new philosophical questions prompted by their
latest developments."
"Can a Machine be Conscious? Towards Universal Criteria for Machine
  Consciousness","As artificially intelligent systems become more anthropomorphic and
pervasive, and their potential impact on humanity more urgent, discussions
about the possibility of machine consciousness have significantly intensified,
and it is sometimes seen as 'the holy grail'. Many concerns have been voiced
about the ramifications of creating an artificial conscious entity. This is
compounded by a marked lack of consensus around what constitutes consciousness
and by an absence of a universal set of criteria for determining consciousness.
By going into depth on the foundations and characteristics of consciousness, we
propose five criteria for determining whether a machine is conscious, which can
also be applied more generally to any entity. This paper aims to serve as a
primer and stepping stone for researchers of consciousness, be they in
philosophy, computer science, medicine, or any other field, to further pursue
this holy grail of philosophy, neuroscience and artificial intelligence."
"Beyond traditional Magnetic Resonance processing with Artificial
  Intelligence","Smart signal processing approaches using Artificial Intelligence are gaining
momentum in NMR applications. In this study, we demonstrate that AI offers new
opportunities beyond tasks addressed by traditional techniques. We developed
and trained several artificial neural networks in our new toolbox Magnetic
Resonance with Artificial intelligence (MR-Ai) to solve three ""impossible""
problems: quadrature detection using only Echo (or Anti-Echo) modulation from
the traditional Echo/Anti-Echo scheme; accessing uncertainty of signal
intensity at each point in a spectrum processed by any given method; and
defining a reference-free score for quantitative access of NMR spectrum
quality. Our findings highlight the potential of AI techniques to revolutionize
NMR processing and analysis."
Artificial intelligence and the internal processes of creativity,"Artificial intelligence (AI) systems capable of generating creative outputs
are reshaping our understanding of creativity. This shift presents an
opportunity for creativity researchers to reevaluate the key components of the
creative process. In particular, the advanced capabilities of AI underscore the
importance of studying the internal processes of creativity. This paper
explores the neurobiological machinery that underlies these internal processes
and describes the experiential component of creativity. It is concluded that
although the products of artificial and human creativity can be similar, the
internal processes are different. The paper also discusses how AI may
negatively affect the internal processes of human creativity, such as the
development of skills, the integration of knowledge, and the diversity of
ideas."
Application of Artificial Intelligence (AI) in Civil Engineering,"Hard computing generally deals with precise data, which provides ideal
solutions to problems. However, in the civil engineering field, amongst other
disciplines, that is not always the case as real-world systems are continuously
changing. Here lies the need to explore soft computing methods and artificial
intelligence to solve civil engineering shortcomings. The integration of
advanced computational models, including Artificial Neural Networks (ANNs),
Fuzzy Logic, Genetic Algorithms (GAs), and Probabilistic Reasoning, has
revolutionized the domain of civil engineering. These models have significantly
advanced diverse sub-fields by offering innovative solutions and improved
analysis capabilities. Sub-fields such as: slope stability analysis, bearing
capacity, water quality and treatment, transportation systems, air quality,
structural materials, etc. ANNs predict non-linearities and provide accurate
estimates. Fuzzy logic uses an efficient decision-making process to provide a
more precise assessment of systems. Lastly, while GAs optimizes models (based
on evolutionary processes) for better outcomes, probabilistic reasoning lowers
their statistical uncertainties."
"Neuroplasticity in Artificial Intelligence -- An Overview and
  Inspirations on Drop In & Out Learning","Artificial Intelligence (AI) has achieved new levels of performance and
spread in public usage with the rise of deep neural networks (DNNs). Initially
inspired by human neurons and their connections, NNs have become the foundation
of AI models for many advanced architectures. However, some of the most
integral processes in the human brain, particularly neurogenesis and
neuroplasticity in addition to the more spread neuroapoptosis have largely been
ignored in DNN architecture design. Instead, contemporary AI development
predominantly focuses on constructing advanced frameworks, such as large
language models, which retain a static structure of neural connections during
training and inference. In this light, we explore how neurogenesis,
neuroapoptosis, and neuroplasticity can inspire future AI advances.
Specifically, we examine analogous activities in artificial NNs, introducing
the concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and
structural pruning for neuroapoptosis. We additionally suggest neuroplasticity
combining the two for future large NNs in ``life-long learning'' settings
following the biological inspiration. We conclude by advocating for greater
research efforts in this interdisciplinary domain and identifying promising
directions for future exploration."
"Artificial Intelligence and Systems Theory: Applied to Cooperative
  Robots","This paper describes an approach to the design of a population of cooperative
robots based on concepts borrowed from Systems Theory and Artificial
Intelligence. The research has been developed under the SocRob project, carried
out by the Intelligent Systems Laboratory at the Institute for Systems and
Robotics - Instituto Superior Tecnico (ISR/IST) in Lisbon. The acronym of the
project stands both for ""Society of Robots"" and ""Soccer Robots"", the case study
where we are testing our population of robots. Designing soccer robots is a
very challenging problem, where the robots must act not only to shoot a ball
towards the goal, but also to detect and avoid static (walls, stopped robots)
and dynamic (moving robots) obstacles. Furthermore, they must cooperate to
defeat an opposing team. Our past and current research in soccer robotics
includes cooperative sensor fusion for world modeling, object recognition and
tracking, robot navigation, multi-robot distributed task planning and
coordination, including cooperative reinforcement learning in cooperative and
adversarial environments, and behavior-based architectures for real time task
execution of cooperating robot teams."
"Applying the Negative Selection Algorithm for Merger and Acquisition
  Target Identification","In this paper, we propose a new methodology based on the Negative Selection
Algorithm that belongs to the field of Computational Intelligence,
specifically, Artificial Immune Systems to identify takeover targets. Although
considerable research based on customary statistical techniques and some
contemporary Computational Intelligence techniques have been devoted to
identify takeover targets, most of the existing studies are based upon multiple
previous mergers and acquisitions. Contrary to previous research, the novelty
of this proposal lies in its ability to suggest takeover targets for novice
firms that are at the beginning of their merger and acquisition spree. We first
discuss the theoretical perspective and then provide a case study with details
for practical implementation, both capitalizing from unique generalization
capabilities of artificial immune systems algorithms."
"On-line Planning and Scheduling: An Application to Controlling Modular
  Printers","We present a case study of artificial intelligence techniques applied to the
control of production printing equipment. Like many other real-world
applications, this complex domain requires high-speed autonomous
decision-making and robust continual operation. To our knowledge, this work
represents the first successful industrial application of embedded
domain-independent temporal planning. Our system handles execution failures and
multi-objective preferences. At its heart is an on-line algorithm that combines
techniques from state-space planning and partial-order scheduling. We suggest
that this general architecture may prove useful in other applications as more
intelligent systems operate in continual, on-line settings. Our system has been
used to drive several commercial prototypes and has enabled a new product
architecture for our industrial partner. When compared with state-of-the-art
off-line planners, our system is hundreds of times faster and often finds
better plans. Our experience demonstrates that domain-independent AI planning
based on heuristic search can flexibly handle time, resources, replanning, and
multiple objectives in a high-speed practical application without requiring
hand-coded control knowledge."
"Using Thought-Provoking Children's Questions to Drive Artificial
  Intelligence Research","We propose to use thought-provoking children's questions (TPCQs), namely
Highlights BrainPlay questions, as a new method to drive artificial
intelligence research and to evaluate the capabilities of general-purpose AI
systems. These questions are designed to stimulate thought and learning in
children, and they can be used to do the same thing in AI systems, while
demonstrating the system's reasoning capabilities to the evaluator. We
introduce the TPCQ task, which which takes a TPCQ question as input and
produces as output (1) answers to the question and (2) learned generalizations.
We discuss how BrainPlay questions stimulate learning. We analyze 244 BrainPlay
questions, and we report statistics on question type, question class, answer
cardinality, answer class, types of knowledge needed, and types of reasoning
needed. We find that BrainPlay questions span many aspects of intelligence.
Because the answers to BrainPlay questions and the generalizations learned from
them are often highly open-ended, we suggest using human judges for evaluation."
"Mathematical Foundations for Designing and Development of Intelligent
  Systems of Information Analysis","This article is an attempt to combine different ways of working with sets of
objects and their classes for designing and development of artificial
intelligent systems (AIS) of analysis information, using object-oriented
programming (OOP). This paper contains analysis of basic concepts of OOP and
their relation with set theory and artificial intelligence (AI). Process of
sets and multisets creation from different sides, in particular mathematical
set theory, OOP and AI is considered. Definition of object and its properties,
homogeneous and inhomogeneous classes of objects, set of objects, multiset of
objects and constructive methods of their creation and classification are
proposed. In addition, necessity of some extension of existing OOP tools for
the purpose of practical implementation AIS of analysis information, using
proposed approach, is shown."
"The Challenge of Non-Technical Loss Detection using Artificial
  Intelligence: A Survey","Detection of non-technical losses (NTL) which include electricity theft,
faulty meters or billing errors has attracted increasing attention from
researchers in electrical engineering and computer science. NTLs cause
significant harm to the economy, as in some countries they may range up to 40%
of the total electricity distributed. The predominant research direction is
employing artificial intelligence to predict whether a customer causes NTL.
This paper first provides an overview of how NTLs are defined and their impact
on economies, which include loss of revenue and profit of electricity providers
and decrease of the stability and reliability of electrical power grids. It
then surveys the state-of-the-art research efforts in a up-to-date and
comprehensive review of algorithms, features and data sets used. It finally
identifies the key scientific and engineering challenges in NTL detection and
suggests how they could be addressed in the future."
"Designing a Safe Autonomous Artificial Intelligence Agent based on Human
  Self-Regulation","There is a growing focus on how to design safe artificial intelligent (AI)
agents. As systems become more complex, poorly specified goals or control
mechanisms may cause AI agents to engage in unwanted and harmful outcomes. Thus
it is necessary to design AI agents that follow initial programming intentions
as the program grows in complexity. How to specify these initial intentions has
also been an obstacle to designing safe AI agents. Finally, there is a need for
the AI agent to have redundant safety mechanisms to ensure that any programming
errors do not cascade into major problems. Humans are autonomous intelligent
agents that have avoided these problems and the present manuscript argues that
by understanding human self-regulation and goal setting, we may be better able
to design safe AI agents. Some general principles of human self-regulation are
outlined and specific guidance for AI design is given."
SenseNet: 3D Objects Database and Tactile Simulator,"The majority of artificial intelligence research, as it relates from which to
biological senses has been focused on vision. The recent explosion of machine
learning and in particular, dee p learning, can be partially attributed to the
release of high quality data sets for algorithm s from which to model the world
on. Thus, most of these datasets are comprised of images. We believe that
focusing on sensorimotor systems and tactile feedback will create algorithms
that better mimic human intelligence. Here we present SenseNet: a collection of
tactile simulators and a large scale dataset of 3D objects for manipulation.
SenseNet was created for the purpose of researching and training Artificial
Intelligences (AIs) to interact with the environment via sensorimotor neural
systems and tactile feedback. We aim to accelerate that same explosion in image
processing, but for the domain of tactile feedback and sensorimotor research.
We hope that SenseNet can offer researchers in both the machine learning and
computational neuroscience communities brand new opportunities and avenues to
explore."
Ethically Aligned Opportunistic Scheduling for Productive Laziness,"In artificial intelligence (AI) mediated workforce management systems (e.g.,
crowdsourcing), long-term success depends on workers accomplishing tasks
productively and resting well. This dual objective can be summarized by the
concept of productive laziness. Existing scheduling approaches mostly focus on
efficiency but overlook worker wellbeing through proper rest. In order to
enable workforce management systems to follow the IEEE Ethically Aligned Design
guidelines to prioritize worker wellbeing, we propose a distributed
Computational Productive Laziness (CPL) approach in this paper. It
intelligently recommends personalized work-rest schedules based on local data
concerning a worker's capabilities and situational factors to incorporate
opportunistic resting and achieve superlinear collective productivity without
the need for explicit coordination messages. Extensive experiments based on a
real-world dataset of over 5,000 workers demonstrate that CPL enables workers
to spend 70% of the effort to complete 90% of the tasks on average, providing
more ethically aligned scheduling than existing approaches."
"The Role of Artificial Intelligence (AI) in Adaptive eLearning System
  (AES) Content Formation: Risks and Opportunities involved","Artificial Intelligence (AI) plays varying roles in supporting both existing
and emerging technologies. In the area of Learning and Tutoring, it plays key
role in Intelligent Tutoring Systems (ITS). The fusion of ITS with Adaptive
Hypermedia and Multimedia (AHAM) form the backbone of Adaptive eLearning
Systems (AES) which provides personalized experiences to learners. This
experience is important because it facilitates the accurate delivery of the
learning modules in specific to the learner capacity and readiness. AES types
vary, with Adaptive Web Based eLearning Systems (AWBES) being the popular type
because of wider access offered by the web technology.The retrieval and
aggregation of contents for any eLearning system is critical whichis determined
by the relevance of learning material to the needs of the learner.In this
paper, we discuss components of AES, role of AI in AES content aggregation,
possible risks and available opportunities."
"A Conceptual Bio-Inspired Framework for the Evolution of Artificial
  General Intelligence","In this work, a conceptual bio-inspired parallel and distributed learning
framework for the emergence of general intelligence is proposed, where agents
evolve through environmental rewards and learn throughout their lifetime
without supervision, i.e., self-learning through embodiment. The chosen control
mechanism for agents is a biologically plausible neuron model based on spiking
neural networks. Network topologies become more complex through evolution,
i.e., the topology is not fixed, while the synaptic weights of the networks
cannot be inherited, i.e., newborn brains are not trained and have no innate
knowledge of the environment. What is subject to the evolutionary process is
the network topology, the type of neurons, and the type of learning. This
process ensures that controllers that are passed through the generations have
the intrinsic ability to learn and adapt during their lifetime in mutable
environments. We envision that the described approach may lead to the emergence
of the simplest form of artificial general intelligence."
A system of different layers of abstraction for artificial intelligence,"The field of artificial intelligence (AI) represents an enormous endeavour of
humankind that is currently transforming our societies down to their very
foundations. Its task, building truly intelligent systems, is underpinned by a
vast array of subfields ranging from the development of new electronic
components to mathematical formulations of highly abstract and complex
reasoning. This breadth of subfields renders it often difficult to understand
how they all fit together into a bigger picture and hides the multi-faceted,
multi-layered conceptual structure that in a sense can be said to be what AI
truly is. In this perspective we propose a system of five levels/layers of
abstraction that underpin many AI implementations. We further posit that each
layer is subject to a complexity-performance trade-off whilst different layers
are interlocked with one another in a control-complexity trade-off. This
overview provides a conceptual map that can help to identify how and where
innovation should be targeted in order to achieve different levels of
functionality, assure them for safety, optimise performance under various
operating constraints and map the opportunity space for social and economic
exploitation."
Responsive Planning and Recognition for Closed-Loop Interaction,"Many intelligent systems currently interact with others using at least one of
fixed communication inputs or preset responses, resulting in rigid interaction
experiences and extensive efforts developing a variety of scenarios for the
system. Fixed inputs limit the natural behavior of the user in order to
effectively communicate, and preset responses prevent the system from adapting
to the current situation unless it was specifically implemented. Closed-loop
interaction instead focuses on dynamic responses that account for what the user
is currently doing based on interpretations of their perceived activity. Agents
employing closed-loop interaction can also monitor their interactions to ensure
that the user responds as expected. We introduce a closed-loop interactive
agent framework that integrates planning and recognition to predict what the
user is trying to accomplish and autonomously decide on actions to take in
response to these predictions. Based on a recent demonstration of such an
assistive interactive agent in a turn-based simulated game, we also discuss new
research challenges that are not present in the areas of artificial
intelligence planning or recognition alone."
Datamorphic Testing: A Methodology for Testing AI Applications,"With the rapid growth of the applications of machine learning (ML) and other
artificial intelligence (AI) techniques, adequate testing has become a
necessity to ensure their quality. This paper identifies the characteristics of
AI applications that distinguish them from traditional software, and analyses
the main difficulties in applying existing testing methods. Based on this
analysis, we propose a new method called datamorphic testing and illustrate the
method with an example of testing face recognition applications. We also report
an experiment with four real industrial application systems of face recognition
to validate the proposed approach."
"Artificial versus Biological Intelligence in the Cosmos: Clues from a
  Stochastic Analysis of the Drake Equation","The Drake equation has been used many times to estimate the number of
observable civilizations in the Galaxy. However, the uncertainty of the outcome
is so great that any individual result is of limited use, as predictions can
range from a handful of observable civilizations in the observable universe to
tens of millions per Milky Way-sized galaxy. A statistical investigation shows
that the Drake equation, despite its uncertainties, delivers robust predictions
of the likelihood that the prevalent form of intelligence in the universe is
artificial rather than biological. The likelihood of artificial intelligence
far exceeds the likelihood of biological intelligence in all cases
investigated. This conclusion is contingent upon a limited number of plausible
assumptions. The significance of this outcome in explaining the Fermi paradox
is discussed."
"The Unreasonable Effectiveness of Deep Learning in Artificial
  Intelligence","Deep learning networks have been trained to recognize speech, caption
photographs and translate text between languages at high levels of performance.
Although applications of deep learning networks to real world problems have
become ubiquitous, our understanding of why they are so effective is lacking.
These empirical results should not be possible according to sample complexity
in statistics and non-convex optimization theory. However, paradoxes in the
training and effectiveness of deep learning networks are being investigated and
insights are being found in the geometry of high-dimensional spaces. A
mathematical theory of deep learning would illuminate how they function, allow
us to assess the strengths and weaknesses of different network architectures
and lead to major improvements. Deep learning has provided natural ways for
humans to communicate with digital devices and is foundational for building
artificial general intelligence. Deep learning was inspired by the architecture
of the cerebral cortex and insights into autonomy and general intelligence may
be found in other brain regions that are essential for planning and survival,
but major breakthroughs will be needed to achieve these goals."
"Morphological Computation and Learning to Learn In Natural Intelligent
  Systems And AI","At present, artificial intelligence in the form of machine learning is making
impressive progress, especially the field of deep learning (DL) [1]. Deep
learning algorithms have been inspired from the beginning by nature,
specifically by the human brain, in spite of our incomplete knowledge about its
brain function. Learning from nature is a two-way process as discussed in
[2][3][4], computing is learning from neuroscience, while neuroscience is
quickly adopting information processing models. The question is, what can the
inspiration from computational nature at this stage of the development
contribute to deep learning and how much models and experiments in machine
learning can motivate, justify and lead research in neuroscience and cognitive
science and to practical applications of artificial intelligence."
Artificial intelligence in space,"In the next coming years, space activities are expected to undergo a radical
transformation with the emergence of new satellite systems or new services
which will incorporate the contributions of artificial intelligence and machine
learning defined as covering a wide range of innovations from autonomous
objects with their own decision-making power to increasingly sophisticated
services exploiting very large volumes of information from space. This chapter
identifies some of the legal and ethical challenges linked to its use. These
legal and ethical challenges call for solutions which the international
treaties in force are not sufficient to determine and implement. For this
reason, a legal methodology must be developed that makes it possible to link
intelligent systems and services to a system of rules applicable thereto. It
discusses existing legal AI-based tools amenable for making space law
actionable, interoperable and machine readable for future compliance tools."
"Turbulence on the Global Economy influenced by Artificial Intelligence
  and Foreign Policy Inefficiencies","It is said that Data and Information are the new oil. One, who handles the
data, handles the emerging future of the global economy. Complex algorithms and
intelligence-based filter programs are utilized to manage, store, handle and
maneuver vast amounts of data for the fulfillment of specific purposes. This
paper seeks to find the bridge between artificial intelligence and its impact
on the international policy implementation in the light of geopolitical
influence, global economy and the future of labor markets. We hypothesize that
the distortion in the labor markets caused by artificial intelligence can be
mitigated by a collaborative international foreign policy on the deployment of
AI in the industrial circles. We, in this paper, then proceed to propose a
disposition for the essentials of AI-based foreign policy and implementation,
while asking questions such as 'could AI become the real Invisible Hand
discussed by economists?'."
"Inertial Sensing Meets Artificial Intelligence: Opportunity or
  Challenge?","The inertial navigation system (INS) has been widely used to provide
self-contained and continuous motion estimation in intelligent transportation
systems. Recently, the emergence of chip-level inertial sensors has expanded
the relevant applications from positioning, navigation, and mobile mapping to
location-based services, unmanned systems, and transportation big data.
Meanwhile, benefit from the emergence of big data and the improvement of
algorithms and computing power, artificial intelligence (AI) has become a
consensus tool that has been successfully applied in various fields. This
article reviews the research on using AI technology to enhance inertial sensing
from various aspects, including sensor design and selection, calibration and
error modeling, navigation and motion-sensing algorithms, multi-sensor
information fusion, system evaluation, and practical application. Based on the
over 30 representative articles selected from the nearly 300 related
publications, this article summarizes the state of the art, advantages, and
challenges on each aspect. Finally, it summarizes nine advantages and nine
challenges of AI-enhanced inertial sensing and then points out future research
directions."
AI Lifecycle Models Need To Be Revised. An Exploratory Study in Fintech,"Tech-leading organizations are embracing the forthcoming artificial
intelligence revolution. Intelligent systems are replacing and cooperating with
traditional software components. Thus, the same development processes and
standards in software engineering ought to be complied in artificial
intelligence systems. This study aims to understand the processes by which
artificial intelligence-based systems are developed and how state-of-the-art
lifecycle models fit the current needs of the industry. We conducted an
exploratory case study at ING, a global bank with a strong European base. We
interviewed 17 people with different roles and from different departments
within the organization. We have found that the following stages have been
overlooked by previous lifecycle models: data collection, feasibility study,
documentation, model monitoring, and model risk assessment. Our work shows that
the real challenges of applying Machine Learning go much beyond sophisticated
learning algorithms - more focus is needed on the entire lifecycle. In
particular, regardless of the existing development tools for Machine Learning,
we observe that they are still not meeting the particularities of this field."
"Understanding understanding: a renormalization group inspired model of
  (artificial) intelligence","This paper is about the meaning of understanding in scientific and in
artificial intelligent systems. We give a mathematical definition of the
understanding, where, contrary to the common wisdom, we define the probability
space on the input set, and we treat the transformation made by an intelligent
actor not as a loss of information, but instead a reorganization of the
information in the framework of a new coordinate system. We introduce,
following the ideas of physical renormalization group, the notions of relevant
and irrelevant parameters, and discuss, how the different AI tasks can be
interpreted along these concepts, and how the process of learning can be
described. We show, how scientific understanding fits into this framework, and
demonstrate, what is the difference between a scientific task and pattern
recognition. We also introduce a measure of relevance, which is useful for
performing lossy compression."
"From Artificial Intelligence to Brain Intelligence: The basis learning
  and memory algorithm for brain-like intelligence","The algorithm of brain learning and memory is still undetermined. The
backpropagation algorithm of artificial neural networks was thought not
suitable for brain cortex, and there is a lack of algorithm for memory engram.
We designed a brain version of backpropagation algorithm, which are
biologically plausible and could be implemented with virtual neurons to
complete image classification task. An encoding algorithm that can
automatically allocate engram cells is proposed, which is an algorithm
implementation for memory engram theory, and could simulate how hippocampus
achieve fast associative memory. The role of the LTP and LTD in the cerebellum
is also explained in algorithm level. Our results proposed a method for the
brain to deploy backpropagation algorithm, and sparse coding method for memory
engram theory."
Grounding Artificial Intelligence in the Origins of Human Behavior,"Recent advances in Artificial Intelligence (AI) have revived the quest for
agents able to acquire an open-ended repertoire of skills. However, although
this ability is fundamentally related to the characteristics of human
intelligence, research in this field rarely considers the processes that may
have guided the emergence of complex cognitive capacities during the evolution
of the species.
  Research in Human Behavioral Ecology (HBE) seeks to understand how the
behaviors characterizing human nature can be conceived as adaptive responses to
major changes in the structure of our ecological niche. In this paper, we
propose a framework highlighting the role of environmental complexity in
open-ended skill acquisition, grounded in major hypotheses from HBE and recent
contributions in Reinforcement learning (RL). We use this framework to
highlight fundamental links between the two disciplines, as well as to identify
feedback loops that bootstrap ecological complexity and create promising
research directions for AI researchers."
Artificial Intelligence enabled Smart Learning,"Artificial Intelligence (AI) is a discipline of computer science that deals
with machine intelligence. It is essential to bring AI into the context of
learning because it helps in analysing the enormous amounts of data that is
collected from individual students, teachers and academic staff. The major
priorities of implementing AI in education are making innovative use of
existing digital technologies for learning, and teaching practices that
significantly improve traditional educational methods. The main problem with
traditional learning is that it cannot be suited to every student in class.
Some students may grasp the concepts well, while some may have difficulties in
understanding them and some may be more auditory or visual learners. The World
Bank report on education has indicated that the learning gap created by this
problem causes many students to drop out (World Development Report, 2018).
Personalised learning has been able to solve this grave problem."
Symbolic Behaviour in Artificial Intelligence,"The ability to use symbols is the pinnacle of human intelligence, but has yet
to be fully replicated in machines. Here we argue that the path towards
symbolically fluent artificial intelligence (AI) begins with a reinterpretation
of what symbols are, how they come to exist, and how a system behaves when it
uses them. We begin by offering an interpretation of symbols as entities whose
meaning is established by convention. But crucially, something is a symbol only
for those who demonstrably and actively participate in this convention. We then
outline how this interpretation thematically unifies the behavioural traits
humans exhibit when they use symbols. This motivates our proposal that the
field place a greater emphasis on symbolic behaviour rather than particular
computational mechanisms inspired by more restrictive interpretations of
symbols. Finally, we suggest that AI research explore social and cultural
engagement as a tool to develop the cognitive machinery necessary for symbolic
behaviour to emerge. This approach will allow for AI to interpret something as
symbolic on its own rather than simply manipulate things that are only symbols
to human onlookers, and thus will ultimately lead to AI with more human-like
symbolic fluency."
"Monte Carlo Tree Search: A Review of Recent Modifications and
  Applications","Monte Carlo Tree Search (MCTS) is a powerful approach to designing
game-playing bots or solving sequential decision problems. The method relies on
intelligent tree search that balances exploration and exploitation. MCTS
performs random sampling in the form of simulations and stores statistics of
actions to make more educated choices in each subsequent iteration. The method
has become a state-of-the-art technique for combinatorial games, however, in
more complex games (e.g. those with high branching factor or real-time ones),
as well as in various practical domains (e.g. transportation, scheduling or
security) an efficient MCTS application often requires its problem-dependent
modification or integration with other techniques. Such domain-specific
modifications and hybrid approaches are the main focus of this survey. The last
major MCTS survey has been published in 2012. Contributions that appeared since
its release are of particular interest for this review."
"Reviewing continual learning from the perspective of human-level
  intelligence","Humans' continual learning (CL) ability is closely related to Stability
Versus Plasticity Dilemma that describes how humans achieve ongoing learning
capacity and preservation for learned information. The notion of CL has always
been present in artificial intelligence (AI) since its births. This paper
proposes a comprehensive review of CL. Different from previous reviews that
mainly focus on the catastrophic forgetting phenomenon in CL, this paper
surveys CL from a more macroscopic perspective based on the Stability Versus
Plasticity mechanism. Analogous to biological counterpart, ""smart"" AI agents
are supposed to i) remember previously learned information (information
retrospection); ii) infer on new information continuously (information
prospection:); iii) transfer useful information (information transfer), to
achieve high-level CL. According to the taxonomy, evaluation metrics,
algorithms, applications as well as some open issues are then introduced. Our
main contributions concern i) rechecking CL from the level of artificial
general intelligence; ii) providing a detailed and extensive overview on CL
topics; iii) presenting some novel ideas on the potential development of CL."
"Artificial Intelligence for Suicide Assessment using Audiovisual Cues: A
  Review","Death by suicide is the seventh leading death cause worldwide. The recent
advancement in Artificial Intelligence (AI), specifically AI applications in
image and voice processing, has created a promising opportunity to
revolutionize suicide risk assessment. Subsequently, we have witnessed
fast-growing literature of research that applies AI to extract audiovisual
non-verbal cues for mental illness assessment. However, the majority of the
recent works focus on depression, despite the evident difference between
depression symptoms and suicidal behavior and non-verbal cues. This paper
reviews recent works that study suicide ideation and suicide behavior detection
through audiovisual feature analysis, mainly suicidal voice/speech acoustic
features analysis and suicidal visual cues. Automatic suicide assessment is a
promising research direction that is still in the early stages. Accordingly,
there is a lack of large datasets that can be used to train machine learning
and deep learning models proven to be effective in other, similar tasks."
Learning Robust Real-Time Cultural Transmission without Human Data,"Cultural transmission is the domain-general social skill that allows agents
to acquire and use information from each other in real-time with high fidelity
and recall. In humans, it is the inheritance process that powers cumulative
cultural evolution, expanding our skills, tools and knowledge across
generations. We provide a method for generating zero-shot, high recall cultural
transmission in artificially intelligent agents. Our agents succeed at
real-time cultural transmission from humans in novel contexts without using any
pre-collected human data. We identify a surprisingly simple set of ingredients
sufficient for generating cultural transmission and develop an evaluation
methodology for rigorously assessing it. This paves the way for cultural
evolution as an algorithm for developing artificial general intelligence."
"A Perspective on Robotic Telepresence and Teleoperation using Cognition:
  Are we there yet?","Telepresence and teleoperation robotics have attracted a great amount of
attention in the last 10 years. With the Artificial Intelligence (AI)
revolution already being started, we can see a wide range of robotic
applications being realized. Intelligent robotic systems are being deployed
both in industrial and domestic environments. Telepresence is the idea of being
present in a remote location virtually or via robotic avatars. Similarly, the
idea of operating a robot from a remote location for various tasks is called
teleoperation. These technologies find significant application in health care,
education, surveillance, disaster recovery, and corporate/government sectors.
But question still remains about their maturity, security and safety levels. We
also need to think about enhancing the user experience and trust in such
technologies going into the next generation of computing."
"Artificial Intelligence in Vehicular Wireless Networks: A Case Study
  Using ns-3","Artificial intelligence (AI) techniques have emerged as a powerful approach
to make wireless networks more efficient and adaptable. In this paper we
present an ns-3 simulation framework, able to implement AI algorithms for the
optimization of wireless networks. Our pipeline consists of: (i) a new
geometry-based mobility-dependent channel model for V2X; (ii) all the layers of
a 5G-NR-compliant protocol stack, based on the ns3-mmwave module; (iii) a new
application to simulate V2X data transmission, and (iv) a new intelligent
entity for the control of the network via AI. Thanks to its flexible and
modular design, researchers can use this tool to implement, train, and evaluate
their own algorithms in a realistic and controlled environment. We test the
behavior of our framework in a Predictive Quality of Service (PQoS) scenario,
where AI functionalities are implemented using Reinforcement Learning (RL), and
demonstrate that it promotes better network optimization compared to baseline
solutions that do not implement AI."
Consent as a Foundation for Responsible Autonomy,"This paper focuses on a dynamic aspect of responsible autonomy, namely, to
make intelligent agents be responsible at run time. That is, it considers
settings where decision making by agents impinges upon the outcomes perceived
by other agents. For an agent to act responsibly, it must accommodate the
desires and other attitudes of its users and, through other agents, of their
users.
  The contribution of this paper is twofold. First, it provides a conceptual
analysis of consent, its benefits and misuses, and how understanding consent
can help achieve responsible autonomy. Second, it outlines challenges for AI
(in particular, for agents and multiagent systems) that merit investigation to
form as a basis for modeling consent in multiagent systems and applying consent
to achieve responsible autonomy."
"Evaluating Human-like Explanations for Robot Actions in Reinforcement
  Learning Scenarios","Explainable artificial intelligence is a research field that tries to provide
more transparency for autonomous intelligent systems. Explainability has been
used, particularly in reinforcement learning and robotic scenarios, to better
understand the robot decision-making process. Previous work, however, has been
widely focused on providing technical explanations that can be better
understood by AI practitioners than non-expert end-users. In this work, we make
use of human-like explanations built from the probability of success to
complete the goal that an autonomous robot shows after performing an action.
These explanations are intended to be understood by people who have no or very
little experience with artificial intelligence methods. This paper presents a
user trial to study whether these explanations that focus on the probability an
action has of succeeding in its goal constitute a suitable explanation for
non-expert end-users. The results obtained show that non-expert participants
rate robot explanations that focus on the probability of success higher and
with less variance than technical explanations generated from Q-values, and
also favor counterfactual explanations over standalone explanations."
"Choose, not Hoard: Information-to-Model Matching for Artificial
  Intelligence in O-RAN","Open Radio Access Network (O-RAN) is an emerging paradigm, whereby
virtualized network infrastructure elements from different vendors communicate
via open, standardized interfaces. A key element therein is the RAN Intelligent
Controller (RIC), an Artificial Intelligence (AI)-based controller.
Traditionally, all data available in the network has been used to train a
single AI model to be used at the RIC. This paper introduces, discusses, and
evaluates the creation of multiple AI model instances at different RICs,
leveraging information from some (or all) locations for their training. This
brings about a flexible relationship between gNBs, the AI models used to
control them, and the data such models are trained with. Experiments with
real-world traces show how using multiple AI model instances that choose
training data from specific locations improve the performance of traditional
approaches following the hoarding strategy."
"Exploring the Constraints on Artificial General Intelligence: A
  Game-Theoretic No-Go Theorem","The emergence of increasingly sophisticated artificial intelligence (AI)
systems have sparked intense debate among researchers, policymakers, and the
public due to their potential to surpass human intelligence and capabilities in
all domains. In this paper, I propose a game-theoretic framework that captures
the strategic interactions between a human agent and a potential superhuman
machine agent. I identify four key assumptions: Strategic Unpredictability,
Access to Machine's Strategy, Rationality, and Superhuman Machine. The main
result of this paper is an impossibility theorem: these four assumptions are
inconsistent when taken together, but relaxing any one of them results in a
consistent set of assumptions. Two straightforward policy recommendations
follow: first, policymakers should control access to specific human data to
maintain Strategic Unpredictability; and second, they should grant select AI
researchers access to superhuman machine research to ensure Access to Machine's
Strategy holds. My analysis contributes to a better understanding of the
context that can shape the theoretical development of superhuman AI."
"Unified, User and Task (UUT) Centered Artificial Intelligence for
  Metaverse Edge Computing","The Metaverse can be considered the extension of the present-day web, which
integrates the physical and virtual worlds, delivering hyper-realistic user
experiences. The inception of the Metaverse brings forth many ecosystem
services such as content creation, social entertainment, in-world value
transfer, intelligent traffic, healthcare. These services are compute-intensive
and require computation offloading onto a Metaverse edge computing server
(MECS). Existing Metaverse edge computing approaches do not efficiently and
effectively handle resource allocation to ensure a fluid, seamless and
hyper-realistic Metaverse experience required for Metaverse ecosystem services.
Therefore, we introduce a new Metaverse-compatible, Unified, User and Task
(UUT) centered artificial intelligence (AI)- based mobile edge computing (MEC)
paradigm, which serves as a concept upon which future AI control algorithms
could be built to develop a more user and task-focused MEC."
Empathetic AI for Empowering Resilience in Games,"Failure and resilience are important aspects of gameplay. This is especially
important for serious and competitive games, where players need to adapt and
cope with failure frequently. In such situations, emotion regulation -- the
active process of modulating ones' emotions to cope and adapt to challenging
situations -- becomes essential. It is one of the prominent aspects of human
intelligence and promotes mental health and well-being. While there has been
work on developing artificial emotional regulation assistants to help users
cope with emotion regulation in the field of Intelligent Tutoring systems,
little is done to incorporate such systems or ideas into (serious) video games.
In this paper, we introduce a data-driven 6-phase approach to establish
empathetic artificial intelligence (EAI), which operates on raw chat log data
to detect key affective states, identify common sequences and emotion
regulation strategies and generalizes these to make them applicable for
intervention systems."
Towards Trust of Explainable AI in Thyroid Nodule Diagnosis,"The ability to explain the prediction of deep learning models to end-users is
an important feature to leverage the power of artificial intelligence (AI) for
the medical decision-making process, which is usually considered
non-transparent and challenging to comprehend. In this paper, we apply
state-of-the-art eXplainable artificial intelligence (XAI) methods to explain
the prediction of the black-box AI models in the thyroid nodule diagnosis
application. We propose new statistic-based XAI methods, namely Kernel Density
Estimation and Density map, to explain the case of no nodule detected. XAI
methods' performances are considered under a qualitative and quantitative
comparison as feedback to improve the data quality and the model performance.
Finally, we survey to assess doctors' and patients' trust in XAI explanations
of the model's decisions on thyroid nodule images."
AI-Generated Content (AIGC): A Survey,"To address the challenges of digital intelligence in the digital economy,
artificial intelligence-generated content (AIGC) has emerged. AIGC uses
artificial intelligence to assist or replace manual content generation by
generating content based on user-inputted keywords or requirements. The
development of large model algorithms has significantly strengthened the
capabilities of AIGC, which makes AIGC products a promising generative tool and
adds convenience to our lives. As an upstream technology, AIGC has unlimited
potential to support different downstream applications. It is important to
analyze AIGC's current capabilities and shortcomings to understand how it can
be best utilized in future applications. Therefore, this paper provides an
extensive overview of AIGC, covering its definition, essential conditions,
cutting-edge capabilities, and advanced features. Moreover, it discusses the
benefits of large-scale pre-trained models and the industrial chain of AIGC.
Furthermore, the article explores the distinctions between auxiliary generation
and automatic generation within AIGC, providing examples of text generation.
The paper also examines the potential integration of AIGC with the Metaverse.
Lastly, the article highlights existing issues and suggests some future
directions for application."
"A optimization framework for herbal prescription planning based on deep
  reinforcement learning","Treatment planning for chronic diseases is a critical task in medical
artificial intelligence, particularly in traditional Chinese medicine (TCM).
However, generating optimized sequential treatment strategies for patients with
chronic diseases in different clinical encounters remains a challenging issue
that requires further exploration. In this study, we proposed a TCM herbal
prescription planning framework based on deep reinforcement learning for
chronic disease treatment (PrescDRL). PrescDRL is a sequential herbal
prescription optimization model that focuses on long-term effectiveness rather
than achieving maximum reward at every step, thereby ensuring better patient
outcomes. We constructed a high-quality benchmark dataset for sequential
diagnosis and treatment of diabetes and evaluated PrescDRL against this
benchmark. Our results showed that PrescDRL achieved a higher curative effect,
with the single-step reward improving by 117% and 153% compared to doctors.
Furthermore, PrescDRL outperformed the benchmark in prescription prediction,
with precision improving by 40.5% and recall improving by 63%. Overall, our
study demonstrates the potential of using artificial intelligence to improve
clinical intelligent diagnosis and treatment in TCM."
Working Memory Capacity of ChatGPT: An Empirical Study,"Working memory is a critical aspect of both human intelligence and artificial
intelligence, serving as a workspace for the temporary storage and manipulation
of information. In this paper, we systematically assess the working memory
capacity of ChatGPT, a large language model developed by OpenAI, by examining
its performance in verbal and spatial n-back tasks under various conditions.
Our experiments reveal that ChatGPT has a working memory capacity limit
strikingly similar to that of humans. Furthermore, we investigate the impact of
different instruction strategies on ChatGPT's performance and observe that the
fundamental patterns of a capacity limit persist. From our empirical findings,
we propose that n-back tasks may serve as tools for benchmarking the working
memory capacity of large language models and hold potential for informing
future efforts aimed at enhancing AI working memory."
Critical Appraisal of Artificial Intelligence-Mediated Communication,"Over the last two decades, technology use in language learning and teaching
has significantly advanced and is now referred to as Computer-Assisted Language
Learning (CALL). Recently, the integration of Artificial Intelligence (AI) into
CALL has brought about a significant shift in the traditional approach to
language education both inside and outside the classroom. In line with this
book's scope, I explore the advantages and disadvantages of AI-mediated
communication in language education. I begin with a brief review of AI in
education. I then introduce the ICALL and give a critical appraisal of the
potential of AI-powered automatic speech recognition (ASR), Machine Translation
(MT), Intelligent Tutoring Systems (ITSs), AI-powered chatbots, and Extended
Reality (XR). In conclusion, I argue that it is crucial for language teachers
to engage in CALL teacher education and professional development to keep up
with the ever-evolving technology landscape and improve their teaching
effectiveness."
"New Era of Artificial Intelligence in Education: Towards a Sustainable
  Multifaceted Revolution","The recent high performance of ChatGPT on several standardized academic tests
has thrust the topic of artificial intelligence (AI) into the mainstream
conversation about the future of education. As deep learning is poised to shift
the teaching paradigm, it is essential to have a clear understanding of its
effects on the current education system to ensure sustainable development and
deployment of AI-driven technologies at schools and universities. This research
aims to investigate the potential impact of AI on education through review and
analysis of the existing literature across three major axes: applications,
advantages, and challenges. Our review focuses on the use of artificial
intelligence in collaborative teacher--student learning, intelligent tutoring
systems, automated assessment, and personalized learning. We also report on the
potential negative aspects, ethical issues, and possible future routes for AI
implementation in education. Ultimately, we find that the only way forward is
to embrace the new technology, while implementing guardrails to prevent its
abuse."
"Integrated Sensing-Communication-Computation for Edge Artificial
  Intelligence","Edge artificial intelligence (AI) has been a promising solution towards 6G to
empower a series of advanced techniques such as digital twins, holographic
projection, semantic communications, and auto-driving, for achieving
intelligence of everything. The performance of edge AI tasks, including edge
learning and edge AI inference, depends on the quality of three highly coupled
processes, i.e., sensing for data acquisition, computation for information
extraction, and communication for information transmission. However, these
three modules need to compete for network resources for enhancing their own
quality-of-services. To this end, integrated sensing-communication-computation
(ISCC) is of paramount significance for improving resource utilization as well
as achieving the customized goals of edge AI tasks. By investigating the
interplay among the three modules, this article presents various kinds of ISCC
schemes for federated edge learning tasks and edge AI inference tasks in both
application and physical layers."
"Perceptions of the Fourth Industrial Revolution and Artificial
  Intelligence Impact on Society","The Fourth Industrial Revolution, particularly Artificial Intelligence (AI),
has had a profound impact on society, raising concerns about its implications
and ethical considerations. The emergence of text generative AI tools like
ChatGPT has further intensified concerns regarding ethics, security, privacy,
and copyright. This study aims to examine the perceptions of individuals in
different information flow categorizations toward AI. The results reveal key
themes in participant-supplied definitions of AI and the fourth industrial
revolution, emphasizing the replication of human intelligence, machine
learning, automation, and the integration of digital technologies. Participants
expressed concerns about job replacement, privacy invasion, and inaccurate
information provided by AI. However, they also recognized the benefits of AI,
such as solving complex problems and increasing convenience. Views on
government involvement in shaping the fourth industrial revolution varied, with
some advocating for strict regulations and others favoring support and
development. The anticipated changes brought by the fourth industrial
revolution include automation, potential job impacts, increased social
disconnect, and reliance on technology. Understanding these perceptions is
crucial for effectively managing the challenges and opportunities associated
with AI in the evolving digital landscape."
"Incorporating Neuro-Inspired Adaptability for Continual Learning in
  Artificial Intelligence","Continual learning aims to empower artificial intelligence (AI) with strong
adaptability to the real world. For this purpose, a desirable solution should
properly balance memory stability with learning plasticity, and acquire
sufficient compatibility to capture the observed distributions. Existing
advances mainly focus on preserving memory stability to overcome catastrophic
forgetting, but remain difficult to flexibly accommodate incremental changes as
biological intelligence (BI) does. By modeling a robust Drosophila learning
system that actively regulates forgetting with multiple learning modules, here
we propose a generic approach that appropriately attenuates old memories in
parameter distributions to improve learning plasticity, and accordingly
coordinates a multi-learner architecture to ensure solution compatibility.
Through extensive theoretical and empirical validation, our approach not only
clearly enhances the performance of continual learning, especially over
synaptic regularization methods in task-incremental settings, but also
potentially advances the understanding of neurological adaptive mechanisms,
serving as a novel paradigm to progress AI and BI together."
Intelligence as a Measure of Consciousness,"Evaluating artificial systems for signs of consciousness is increasingly
becoming a pressing concern, and a rigorous psychometric measurement framework
may be of crucial importance in evaluating large language models in this
regard. Most prominent theories of consciousness, both scientific and
metaphysical, argue for different kinds of information coupling as a necessary
component of human-like consciousness. By comparing information coupling in
human and animal brains, human cognitive development, emergent abilities, and
mental representation development to analogous phenomena in large language
models, I argue that psychometric measures of intelligence, such as the
g-factor or IQ, indirectly approximate the extent of conscious experience.
Based on a broader source of both scientific and metaphysical theories of
consciousness, I argue that all systems possess a degree of consciousness
ascertainable psychometrically and that psychometric measures of intelligence
may be used to gauge relative similarities of conscious experiences across
disparate systems, be they artificial or human."
"Evolution-Bootstrapped Simulation: Artificial or Human Intelligence:
  Which Came First?","Humans have created artificial intelligence (AI), not the other way around.
This statement is deceptively obvious. In this note, we decided to challenge
this statement as a small, lighthearted Gedankenexperiment. We ask a simple
question: in a world driven by evolution by natural selection, would neural
networks or humans be likely to evolve first? We compare the
Solomonoff--Kolmogorov--Chaitin complexity of the two and find neural networks
(even LLMs) to be significantly simpler than humans. Further, we claim that it
is unnecessary for any complex human-made equipment to exist for there to be
neural networks. Neural networks may have evolved as naturally occurring
objects before humans did as a form of chemical reaction-based or enzyme-based
computation. Now that we know that neural networks can pass the Turing test and
suspect that they may be capable of superintelligence, we ask whether the
natural evolution of neural networks could lead from pure evolution by natural
selection to what we call evolution-bootstrapped simulation. The evolution of
neural networks does not involve irreducible complexity; would easily allow
irreducible complexity to exist in the evolution-bootstrapped simulation; is a
falsifiable scientific hypothesis; and is independent of / orthogonal to the
issue of intelligent design."
"Deep Learning Approaches for Improving Question Answering Systems in
  Hepatocellular Carcinoma Research","In recent years, advancements in natural language processing (NLP) have been
fueled by deep learning techniques, particularly through the utilization of
powerful computing resources like GPUs and TPUs. Models such as BERT and GPT-3,
trained on vast amounts of data, have revolutionized language understanding and
generation. These pre-trained models serve as robust bases for various tasks
including semantic understanding, intelligent writing, and reasoning, paving
the way for a more generalized form of artificial intelligence. NLP, as a vital
application of AI, aims to bridge the gap between humans and computers through
natural language interaction. This paper delves into the current landscape and
future prospects of large-scale model-based NLP, focusing on the
question-answering systems within this domain. Practical cases and developments
in artificial intelligence-driven question-answering systems are analyzed to
foster further exploration and research in the realm of large-scale NLP."
"AI-Driven Anonymization: Protecting Personal Data Privacy While
  Leveraging Machine Learning","The development of artificial intelligence has significantly transformed
people's lives. However, it has also posed a significant threat to privacy and
security, with numerous instances of personal information being exposed online
and reports of criminal attacks and theft. Consequently, the need to achieve
intelligent protection of personal information through machine learning
algorithms has become a paramount concern. Artificial intelligence leverages
advanced algorithms and technologies to effectively encrypt and anonymize
personal data, enabling valuable data analysis and utilization while
safeguarding privacy. This paper focuses on personal data privacy protection
and the promotion of anonymity as its core research objectives. It achieves
personal data privacy protection and detection through the use of machine
learning's differential privacy protection algorithm. The paper also addresses
existing challenges in machine learning related to privacy and personal data
protection, offers improvement suggestions, and analyzes factors impacting
datasets to enable timely personal data privacy detection and protection."
Quantum Natural Language Processing,"Language processing is at the heart of current developments in artificial
intelligence, and quantum computers are becoming available at the same time.
This has led to great interest in quantum natural language processing, and
several early proposals and experiments.
  This paper surveys the state of this area, showing how NLP-related techniques
have been used in quantum language processing. We examine the art of word
embeddings and sequential models, proposing some avenues for future
investigation and discussing the tradeoffs present in these directions. We also
highlight some recent methods to compute attention in transformer models, and
perform grammatical parsing. We also introduce a new quantum design for the
basic task of text encoding (representing a string of characters in memory),
which has not been addressed in detail before.
  Quantum theory has contributed toward quantifying uncertainty and explaining
""What is intelligence?"" In this context, we argue that ""hallucinations"" in
modern artificial intelligence systems are a misunderstanding of the way facts
are conceptualized: language can express many plausible hypotheses, of which
only a few become actual."
"Research and application of artificial intelligence based webshell
  detection model: A literature review","Webshell, as the ""culprit"" behind numerous network attacks, is one of the
research hotspots in the field of cybersecurity. However, the complexity,
stealthiness, and confusing nature of webshells pose significant challenges to
the corresponding detection schemes. With the rise of Artificial Intelligence
(AI) technology, researchers have started to apply different intelligent
algorithms and neural network architectures to the task of webshell detection.
However, the related research still lacks a systematic and standardized
methodological process, which is confusing and redundant. Therefore, following
the development timeline, we carefully summarize the progress of relevant
research in this field, dividing it into three stages: Start Stage, Initial
Development Stage, and In-depth Development Stage. We further elaborate on the
main characteristics and core algorithms of each stage. In addition, we analyze
the pain points and challenges that still exist in this field and predict the
future development trend of this field from our point of view. To the best of
our knowledge, this is the first review that details the research related to
AI-based webshell detection. It is also hoped that this paper can provide
detailed technical information for more researchers interested in AI-based
webshell detection tasks."
"Exploring Teachers' Perception of Artificial Intelligence: The
  Socio-emotional Deficiency as Opportunities and Challenges in Human-AI
  Complementarity in K-12 Education","In schools, teachers play a multitude of roles, serving as educators,
counselors, decision-makers, and members of the school community. With recent
advances in artificial intelligence (AI), there is increasing discussion about
how AI can assist, complement, and collaborate with teachers. To pave the way
for better teacher-AI complementary relationships in schools, our study aims to
expand the discourse on teacher-AI complementarity by seeking educators'
perspectives on the potential strengths and limitations of AI across a spectrum
of responsibilities. Through a mixed method using a survey with 100 elementary
school teachers in South Korea and in-depth interviews with 12 teachers, our
findings indicate that teachers anticipate AI's potential to complement human
teachers by automating administrative tasks and enhancing personalized learning
through advanced intelligence. Interestingly, the deficit of AI's
socio-emotional capabilities has been perceived as both challenges and
opportunities. Overall, our study demonstrates the nuanced perception of
teachers and different levels of expectations over their roles, challenging the
need for decisions about AI adoption tailored to educators' preferences and
concerns."
Trustworthy Artificial Intelligence in the Context of Metrology,"We review research at the National Physical Laboratory (NPL) in the area of
trustworthy artificial intelligence (TAI), and more specifically trustworthy
machine learning (TML), in the context of metrology, the science of
measurement. We describe three broad themes of TAI: technical, socio-technical
and social, which play key roles in ensuring that the developed models are
trustworthy and can be relied upon to make responsible decisions. From a
metrology perspective we emphasise uncertainty quantification (UQ), and its
importance within the framework of TAI to enhance transparency and trust in the
outputs of AI systems. We then discuss three research areas within TAI that we
are working on at NPL, and examine the certification of AI systems in terms of
adherence to the characteristics of TAI."
Artifical intelligence and inherent mathematical difficulty,"This paper explores the relationship of artificial intelligence to the task
of resolving open questions in mathematics. We first present an updated version
of a traditional argument that limitative results from computability and
complexity theory show that proof discovery is an inherently difficult problem.
We then illustrate how several recent applications of artificial
intelligence-inspired methods -- respectively involving automated theorem
proving, SAT-solvers, and large language models -- do indeed raise novel
questions about the nature of mathematical proof. We also argue that the
results obtained by such techniques do not tell against our basic argument.
This is so because they are embodiments of brute force search and are thus
capable of deciding only statements of low logical complexity."
"A theory of understanding for artificial intelligence: composability,
  catalysts, and learning","Understanding is a crucial yet elusive concept in artificial intelligence
(AI). This work proposes a framework for analyzing understanding based on the
notion of composability. Given any subject (e.g., a person or an AI), we
suggest characterizing its understanding of an object in terms of its ability
to process (compose) relevant inputs into satisfactory outputs from the
perspective of a verifier. This highly universal framework can readily apply to
non-human subjects, such as AIs, non-human animals, and institutions. Further,
we propose methods for analyzing the inputs that enhance output quality in
compositions, which we call catalysts. We show how the structure of a subject
can be revealed by analyzing its components that act as catalysts and argue
that a subject's learning ability can be regarded as its ability to compose
inputs into its inner catalysts. Finally we examine the importance of learning
ability for AIs to attain general intelligence. Our analysis indicates that
models capable of generating outputs that can function as their own catalysts,
such as language models, establish a foundation for potentially overcoming
existing limitations in AI understanding."
Neuro-Symbolic AI for Military Applications,"Artificial Intelligence (AI) plays a significant role in enhancing the
capabilities of defense systems, revolutionizing strategic decision-making, and
shaping the future landscape of military operations. Neuro-Symbolic AI is an
emerging approach that leverages and augments the strengths of neural networks
and symbolic reasoning. These systems have the potential to be more impactful
and flexible than traditional AI systems, making them well-suited for military
applications. This paper comprehensively explores the diverse dimensions and
capabilities of Neuro-Symbolic AI, aiming to shed light on its potential
applications in military contexts. We investigate its capacity to improve
decision-making, automate complex intelligence analysis, and strengthen
autonomous systems. We further explore its potential to solve complex tasks in
various domains, in addition to its applications in military contexts. Through
this exploration, we address ethical, strategic, and technical considerations
crucial to the development and deployment of Neuro-Symbolic AI in military and
civilian applications. Contributing to the growing body of research, this study
represents a comprehensive exploration of the extensive possibilities offered
by Neuro-Symbolic AI."
"Easydiagnos: a framework for accurate feature selection for automatic
  diagnosis in smart healthcare","The rapid advancements in artificial intelligence (AI) have revolutionized
smart healthcare, driving innovations in wearable technologies, continuous
monitoring devices, and intelligent diagnostic systems. However, security,
explainability, robustness, and performance optimization challenges remain
critical barriers to widespread adoption in clinical environments. This
research presents an innovative algorithmic method using the Adaptive Feature
Evaluator (AFE) algorithm to improve feature selection in healthcare datasets
and overcome problems. AFE integrating Genetic Algorithms (GA), Explainable
Artificial Intelligence (XAI), and Permutation Combination Techniques (PCT),
the algorithm optimizes Clinical Decision Support Systems (CDSS), thereby
enhancing predictive accuracy and interpretability. The proposed method is
validated across three diverse healthcare datasets using six distinct machine
learning algorithms, demonstrating its robustness and superiority over
conventional feature selection techniques. The results underscore the
transformative potential of AFE in smart healthcare, enabling personalized and
transparent patient care. Notably, the AFE algorithm, when combined with a
Multi-layer Perceptron (MLP), achieved an accuracy of up to 98.5%, highlighting
its capability to improve clinical decision-making processes in real-world
healthcare applications."
MAC Revivo: Artificial Intelligence Paves the Way,"The vast adoption of Wi-Fi and/or Bluetooth capabilities in Internet of
Things (IoT) devices, along with the rapid growth of deployed smart devices,
has caused significant interference and congestion in the industrial,
scientific, and medical (ISM) bands. Traditional Wi-Fi Medium Access Control
(MAC) design faces significant challenges in managing increasingly complex
wireless environments while ensuring network Quality of Service (QoS)
performance. This paper explores the potential integration of advanced
Artificial Intelligence (AI) methods into the design of Wi-Fi MAC protocols. We
propose AI-MAC, an innovative approach that employs machine learning algorithms
to dynamically adapt to changing network conditions, optimize channel access,
mitigate interference, and ensure deterministic latency. By intelligently
predicting and managing interference, AI-MAC aims to provide a robust solution
for next generation of Wi-Fi networks, enabling seamless connectivity and
enhanced QoS. Our experimental results demonstrate that AI-MAC significantly
reduces both interference and latency, paving the way for more reliable and
efficient wireless communications in the increasingly crowded ISM band."
AI-Driven Innovations in Modern Cloud Computing,"The world has witnessed rapid technological transformation, past couple of
decades and with Advent of Cloud computing the landscape evolved exponentially
leading to efficient and scalable application development. Now, the past couple
of years the digital ecosystem has brought in numerous innovations with
integration of Artificial Intelligence commonly known as AI. This paper
explores how AI and cloud computing intersect to deliver transformative
capabilities for modernizing applications by providing services and
infrastructure. Harnessing the combined potential of both AI & Cloud
technologies, technology providers can now exploit intelligent resource
management, predictive analytics, automated deployment & scaling with enhanced
security leading to offering innovative solutions to their customers.
Furthermore, by leveraging such technologies of cloud & AI businesses can reap
rich rewards in the form of reducing operational costs and improving service
delivery. This paper further addresses challenges associated such as data
privacy concerns and how it can be mitigated with robust AI governance
frameworks."
Artificial Intelligence Ecosystem for Automating Self-Directed Teaching,"This research introduces an innovative artificial intelligence-driven
educational concept designed to optimize self-directed learning through
personalized course delivery and automated teaching assistance. The system
leverages fine-tuned AI models to create an adaptive learning environment that
encompasses customized roadmaps, automated presentation generation, and
three-dimensional modeling for complex concept visualization. By integrating
real-time virtual assistance for doubt resolution, the platform addresses the
immediate educational needs of learners while promoting autonomous learning
practices. This study explores the psychological advantages of self-directed
learning and demonstrates how AI automation can enhance educational outcomes
through personalized content delivery and interactive support mechanisms. The
research contributes to the growing field of educational technology by
presenting a comprehensive framework that combines automated content
generation, visual learning aids, and intelligent tutoring to create an
efficient, scalable solution for modern educational needs. Preliminary findings
suggest that this approach not only accommodates diverse learning styles but
also strengthens student engagement and knowledge retention through its
emphasis on self-paced, independent learning methodologies."
Explainable Artificial Intelligence for Medical Applications: A Review,"The continuous development of artificial intelligence (AI) theory has
propelled this field to unprecedented heights, owing to the relentless efforts
of scholars and researchers. In the medical realm, AI takes a pivotal role,
leveraging robust machine learning (ML) algorithms. AI technology in medical
imaging aids physicians in X-ray, computed tomography (CT) scans, and magnetic
resonance imaging (MRI) diagnoses, conducts pattern recognition and disease
prediction based on acoustic data, delivers prognoses on disease types and
developmental trends for patients, and employs intelligent health management
wearable devices with human-computer interaction technology to name but a few.
While these well-established applications have significantly assisted in
medical field diagnoses, clinical decision-making, and management,
collaboration between the medical and AI sectors faces an urgent challenge: How
to substantiate the reliability of decision-making? The underlying issue stems
from the conflict between the demand for accountability and result transparency
in medical scenarios and the black-box model traits of AI. This article reviews
recent research grounded in explainable artificial intelligence (XAI), with an
emphasis on medical practices within the visual, audio, and multimodal
perspectives. We endeavour to categorise and synthesise these practices, aiming
to provide support and guidance for future researchers and healthcare
professionals."
From Words to Workflows: Automating Business Processes,"As businesses increasingly rely on automation to streamline operations, the
limitations of Robotic Process Automation (RPA) have become apparent,
particularly its dependence on expert knowledge and inability to handle complex
decision-making tasks. Recent advancements in Artificial Intelligence (AI),
particularly Generative AI (GenAI) and Large Language Models (LLMs), have paved
the way for Intelligent Automation (IA), which integrates cognitive
capabilities to overcome the shortcomings of RPA. This paper introduces
Text2Workflow, a novel method that automatically generates workflows from
natural language user requests. Unlike traditional automation approaches,
Text2Workflow offers a generalized solution for automating any business
process, translating user inputs into a sequence of executable steps
represented in JavaScript Object Notation (JSON) format. Leveraging the
decision-making and instruction-following capabilities of LLMs, this method
provides a scalable, adaptable framework that enables users to visualize and
execute workflows with minimal manual intervention. This research outlines the
Text2Workflow methodology and its broader implications for automating complex
business processes."
"Intelligent Electric Power Steering: Artificial Intelligence Integration
  Enhances Vehicle Safety and Performance","Electric Power Steering (EPS) systems utilize electric motors to aid users in
steering their vehicles, which provide additional precise control and reduced
energy consumption compared to traditional hydraulic systems. EPS technology
provides safety,control and efficiency.. This paper explains the integration of
Artificial Intelligence (AI) into Electric Power Steering (EPS) systems,
focusing on its role in enhancing the safety, and adaptability across diverse
driving conditions. We explore significant development in AI-driven EPS,
including predictive control algorithms, adaptive torque management systems,
and data-driven diagnostics. The paper presents case studies of AI applications
in EPS, such as Lane centering control (LCC), Automated Parking Systems, and
Autonomous Vehicle Steering, while considering the challenges, limitations, and
future prospects of this technology. This article discusses current
developments in AI-driven EPS, emphasizing on the benefits of improved safety,
adaptive control, and predictive maintenance. Challenges in integrating AI in
EPS systems. This paper addresses cybersecurity risks, ethical concerns, and
technical limitations,, along with next steps for research and implementation
in autonomous, and connected vehicles."
"Advancing Generative Artificial Intelligence and Large Language Models
  for Demand Side Management with Internet of Electric Vehicles","Generative artificial intelligence, particularly through large language
models (LLMs), is poised to transform energy optimization and demand side
management (DSM) within microgrids. This paper explores the integration of LLMs
into energy management, emphasizing their roles in automating the optimization
of DSM strategies with Internet of electric vehicles. We investigate challenges
and solutions associated with DSM and explore the new opportunities presented
by leveraging LLMs. Then, we propose an innovative solution that enhances LLMs
with retrieval-augmented generation for automatic problem formulation, code
generation, and customizing optimization. We present a case study to
demonstrate the effectiveness of our proposed solution in charging scheduling
and optimization for electric vehicles, highlighting our solution's significant
advancements in energy efficiency and user adaptability. This work underscores
the potential of LLMs for energy optimization and fosters a new era of
intelligent DSM solutions."
"EdgeMark: An Automation and Benchmarking System for Embedded Artificial
  Intelligence Tools","The integration of artificial intelligence (AI) into embedded devices, a
paradigm known as embedded artificial intelligence (eAI) or tiny machine
learning (TinyML), is transforming industries by enabling intelligent data
processing at the edge. However, the many tools available in this domain leave
researchers and developers wondering which one is best suited to their needs.
This paper provides a review of existing eAI tools, highlighting their
features, trade-offs, and limitations. Additionally, we introduce EdgeMark, an
open-source automation system designed to streamline the workflow for deploying
and benchmarking machine learning (ML) models on embedded platforms. EdgeMark
simplifies model generation, optimization, conversion, and deployment while
promoting modularity, reproducibility, and scalability. Experimental
benchmarking results showcase the performance of widely used eAI tools,
including TensorFlow Lite Micro (TFLM), Edge Impulse, Ekkono, and Renesas eAI
Translator, across a wide range of models, revealing insights into their
relative strengths and weaknesses. The findings provide guidance for
researchers and developers in selecting the most suitable tools for specific
application requirements, while EdgeMark lowers the barriers to adoption of eAI
technologies."
"The Society of HiveMind: Multi-Agent Optimization of Foundation Model
  Swarms to Unlock the Potential of Collective Intelligence","Multi-agent systems address issues of accessibility and scalability of
artificial intelligence (AI) foundation models, which are often represented by
large language models. We develop a framework - the ""Society of HiveMind""
(SOHM) - that orchestrates the interaction between multiple AI foundation
models, imitating the observed behavior of animal swarms in nature by following
modern evolutionary theories. On the one hand, we find that the SOHM provides a
negligible benefit on tasks that mainly require real-world knowledge. On the
other hand, we remark a significant improvement on tasks that require intensive
logical reasoning, indicating that multi-agent systems are capable of
increasing the reasoning capabilities of the collective compared to the
individual agents. Our findings demonstrate the potential of combining a
multitude of diverse AI foundation models to form an artificial swarm
intelligence capable of self-improvement through interactions with a given
environment."
"KHAIT: K-9 Handler Artificial Intelligence Teaming for Collaborative
  Sensemaking","In urban search and rescue (USAR) operations, communication between handlers
and specially trained canines is crucial but often complicated by challenging
environments and the specific behaviors canines are trained to exhibit when
detecting a person. Since a USAR canine often works out of sight of the
handler, the handler lacks awareness of the canine's location and situation,
known as the 'sensemaking gap.' In this paper, we propose KHAIT, a novel
approach to close the sensemaking gap and enhance USAR effectiveness by
integrating object detection-based Artificial Intelligence (AI) and Augmented
Reality (AR). Equipped with AI-powered cameras, edge computing, and AR
headsets, KHAIT enables precise and rapid object detection from a canine's
perspective, improving survivor localization. We evaluate this approach in a
real-world USAR environment, demonstrating an average survival allocation time
decrease of 22%, enhancing the speed and accuracy of operations."
"Aportes para el cumplimiento del Reglamento (UE) 2024/1689 en robtica
  y sistemas autnomos","Cybersecurity in robotics stands out as a key aspect within Regulation (EU)
2024/1689, also known as the Artificial Intelligence Act, which establishes
specific guidelines for intelligent and automated systems. A fundamental
distinction in this regulatory framework is the difference between robots with
Artificial Intelligence (AI) and those that operate through automation systems
without AI, since the former are subject to stricter security requirements due
to their learning and autonomy capabilities. This work analyzes cybersecurity
tools applicable to advanced robotic systems, with special emphasis on the
protection of knowledge bases in cognitive architectures. Furthermore, a list
of basic tools is proposed to guarantee the security, integrity, and resilience
of these systems, and a practical case is presented, focused on the analysis of
robot knowledge management, where ten evaluation criteria are defined to ensure
compliance with the regulation and reduce risks in human-robot interaction
(HRI) environments."
"Rethinking industrial artificial intelligence: a unified foundation
  framework","Recent advancements in industrial artificial intelligence (AI) are reshaping
the industry by driving smarter manufacturing, predictive maintenance, and
intelligent decision-making. However, existing approaches often focus primarily
on algorithms and models while overlooking the importance of systematically
integrating domain knowledge, data, and models to develop more comprehensive
and effective AI solutions. Therefore, the effective development and deployment
of industrial AI require a more comprehensive and systematic approach. To
address this gap, this paper reviews previous research, rethinks the role of
industrial AI, and proposes a unified industrial AI foundation framework
comprising three core modules: the knowledge module, data module, and model
module. These modules help to extend and enhance the industrial AI methodology
platform, supporting various industrial applications. In addition, a case study
on rotating machinery diagnosis is presented to demonstrate the effectiveness
of the proposed framework, and several future directions are highlighted for
the development of the industrial AI foundation framework."
"Towards an Evaluation Framework for Explainable Artificial Intelligence
  Systems for Health and Well-being","The integration of Artificial Intelligence in the development of computer
systems presents a new challenge: make intelligent systems explainable to
humans. This is especially vital in the field of health and well-being, where
transparency in decision support systems enables healthcare professionals to
understand and trust automated decisions and predictions. To address this need,
tools are required to guide the development of explainable AI systems. In this
paper, we introduce an evaluation framework designed to support the development
of explainable AI systems for health and well-being. Additionally, we present a
case study that illustrates the application of the framework in practice. We
believe that our framework can serve as a valuable tool not only for developing
explainable AI systems in healthcare but also for any AI system that has a
significant impact on individuals."
Towards Regulated Deep Learning,"Regulation of Multi-Agent Systems (MAS) and Declarative Electronic
Institutions (DEIs) was a multidisciplinary research topic of the past decade
involving (Physical and Software) Agents and Law since the beginning, but
recently evolved towards News-claimed Robot Lawyer since 2016. One of these
first proposals of restricting the behaviour of Software Agents was Electronic
Institutions.However, with the recent reformulation of Artificial Neural
Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal
issues regarding the use of DL has raised concerns in the Artificial
Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly
addressed, we propose the Regulation of Artificial Neural Networks as
Agent-based Training of a special type of regulated Artificial Neural Network
that we call Institutional Neural Network (INN).The main purpose of this paper
is to bring attention to Artificial Teaching (AT) and to give a tentative
answer showing a proof-of-concept implementation of Regulated Deep Learning
(RDL). This paper introduces the former concept and provide $I^*$, a language
previously used to model declaratively and extend Electronic Institutions, as a
means to regulate the execution of Artificial Neural Networks and their
interactions with Artificial Teachers (ATs)"
"Edge Intelligence: Architectures, Challenges, and Applications","Edge intelligence refers to a set of connected systems and devices for data
collection, caching, processing, and analysis in locations close to where data
is captured based on artificial intelligence. The aim of edge intelligence is
to enhance the quality and speed of data processing and protect the privacy and
security of the data. Although recently emerged, spanning the period from 2011
to now, this field of research has shown explosive growth over the past five
years. In this paper, we present a thorough and comprehensive survey on the
literature surrounding edge intelligence. We first identify four fundamental
components of edge intelligence, namely edge caching, edge training, edge
inference, and edge offloading, based on theoretical and practical results
pertaining to proposed and deployed systems. We then aim for a systematic
classification of the state of the solutions by examining research results and
observations for each of the four components and present a taxonomy that
includes practical problems, adopted techniques, and application goals. For
each category, we elaborate, compare and analyse the literature from the
perspectives of adopted techniques, objectives, performance, advantages and
drawbacks, etc. This survey article provides a comprehensive introduction to
edge intelligence and its application areas. In addition, we summarise the
development of the emerging research field and the current state-of-the-art and
discuss the important open issues and possible theoretical and technical
solutions."
Intelligent Radio Signal Processing: A Survey,"Intelligent signal processing for wireless communications is a vital task in
modern wireless systems, but it faces new challenges because of network
heterogeneity, diverse service requirements, a massive number of connections,
and various radio characteristics. Owing to recent advancements in big data and
computing technologies, artificial intelligence (AI) has become a useful tool
for radio signal processing and has enabled the realization of intelligent
radio signal processing. This survey covers four intelligent signal processing
topics for the wireless physical layer, including modulation classification,
signal detection, beamforming, and channel estimation. In particular, each
theme is presented in a dedicated section, starting with the most fundamental
principles, followed by a review of up-to-date studies and a summary. To
provide the necessary background, we first present a brief overview of AI
techniques such as machine learning, deep learning, and federated learning.
Finally, we highlight a number of research challenges and future directions in
the area of intelligent radio signal processing. We expect this survey to be a
good source of information for anyone interested in intelligent radio signal
processing, and the perspectives we provide therein will stimulate many more
novel ideas and contributions in the future."
"The emergence of Explainability of Intelligent Systems: Delivering
  Explainable and Personalised Recommendations for Energy Efficiency","The recent advances in artificial intelligence namely in machine learning and
deep learning, have boosted the performance of intelligent systems in several
ways. This gave rise to human expectations, but also created the need for a
deeper understanding of how intelligent systems think and decide. The concept
of explainability appeared, in the extent of explaining the internal system
mechanics in human terms. Recommendation systems are intelligent systems that
support human decision making, and as such, they have to be explainable in
order to increase user trust and improve the acceptance of recommendations. In
this work, we focus on a context-aware recommendation system for energy
efficiency and develop a mechanism for explainable and persuasive
recommendations, which are personalized to user preferences and habits. The
persuasive facts either emphasize on the economical saving prospects (Econ) or
on a positive ecological impact (Eco) and explanations provide the reason for
recommending an energy saving action. Based on a study conducted using a
Telegram bot, different scenarios have been validated with actual data and
human feedback. Current results show a total increase of 19\% on the
recommendation acceptance ratio when both economical and ecological persuasive
facts are employed. This revolutionary approach on recommendation systems,
demonstrates how intelligent recommendations can effectively encourage energy
saving behavior."
"On the independence between phenomenal consciousness and computational
  intelligence","Consciousness and intelligence are properties commonly understood as
dependent by folk psychology and society in general. The term artificial
intelligence and the kind of problems that it managed to solve in the recent
years has been shown as an argument to establish that machines experience some
sort of consciousness. Following the analogy of Russell, if a machine is able
to do what a conscious human being does, the likelihood that the machine is
conscious increases. However, the social implications of this analogy are
catastrophic. Concretely, if rights are given to entities that can solve the
kind of problems that a neurotypical person can, does the machine have
potentially more rights that a person that has a disability? For example, the
autistic syndrome disorder spectrum can make a person unable to solve the kind
of problems that a machine solves. We believe that the obvious answer is no, as
problem solving does not imply consciousness. Consequently, we will argue in
this paper how phenomenal consciousness and, at least, computational
intelligence are independent and why machines do not possess phenomenal
consciousness, although they can potentially develop a higher computational
intelligence that human beings. In order to do so, we try to formulate an
objective measure of computational intelligence and study how it presents in
human beings, animals and machines. Analogously, we study phenomenal
consciousness as a dichotomous variable and how it is distributed in humans,
animals and machines. As phenomenal consciousness and computational
intelligence are independent, this fact has critical implications for society
that we also analyze in this work."
"A Survey on Intelligent Computation Offloading and Pricing Strategy in
  UAV-Enabled MEC Network: Challenges and Research Directions","The Mobile Network Operator (MNO) must select how to delegate Mobile Device
(MD) queries to its Mobile Edge Computing (MEC) server in order to maximize the
overall benefit of admitted requests with varying latency needs. Unmanned
Aerial Vehicles (UAVs) and Artificial Intelligent (AI) can increase MNO
performance because of their flexibility in deployment, high mobility of UAV,
and efficiency of AI algorithms. There is a trade-off between the cost incurred
by the MD and the profit received by the MNO. Intelligent computing offloading
to UAV-enabled MEC, on the other hand, is a promising way to bridge the gap
between MDs' limited processing resources, as well as the intelligent
algorithms that are utilized for computation offloading in the UAV-MEC network
and the high computing demands of upcoming applications. This study looks at
some of the research on the benefits of computation offloading process in the
UAV-MEC network, as well as the intelligent models that are utilized for
computation offloading. In addition, this article examines several intelligent
pricing techniques in different structures in the UAV-MEC network. Finally,
this work highlights some important open research issues and future research
directions of Artificial Intelligent (AI) in computation offloading and
applying intelligent pricing strategies in the UAV-MEC network."
"Towards a Framework for Visual Intelligence in Service Robotics:
  Epistemic Requirements and Gap Analysis","A key capability required by service robots operating in real-world, dynamic
environments is that of Visual Intelligence, i.e., the ability to use their
vision system, reasoning components and background knowledge to make sense of
their environment. In this paper, we analyze the epistemic requirements for
Visual Intelligence, both in a top-down fashion, using existing frameworks for
human-like Visual Intelligence in the literature, and from the bottom up, based
on the errors emerging from object recognition trials in a real-world robotic
scenario. Finally, we use these requirements to evaluate current knowledge
bases for Service Robotics and to identify gaps in the support they provide for
Visual Intelligence. These gaps provide the basis of a research agenda for
developing more effective knowledge representations for Visual Intelligence."
"Beware the evolving 'intelligent' web service! An integration
  architecture tactic to guard AI-first components","Intelligent services provide the power of AI to developers via simple RESTful
API endpoints, abstracting away many complexities of machine learning. However,
most of these intelligent services-such as computer vision-continually learn
with time. When the internals within the abstracted 'black box' become hidden
and evolve, pitfalls emerge in the robustness of applications that depend on
these evolving services. Without adapting the way developers plan and construct
projects reliant on intelligent services, significant gaps and risks result in
both project planning and development. Therefore, how can software engineers
best mitigate software evolution risk moving forward, thereby ensuring that
their own applications maintain quality? Our proposal is an architectural
tactic designed to improve intelligent service-dependent software robustness.
The tactic involves creating an application-specific benchmark dataset
baselined against an intelligent service, enabling evolutionary behaviour
changes to be mitigated. A technical evaluation of our implementation of this
architecture demonstrates how the tactic can identify 1,054 cases of
substantial confidence evolution and 2,461 cases of substantial changes to
response label sets using a dataset consisting of 331 images that evolve when
sent to a service."
Wide Area Network Intelligence with Application to Multimedia Service,"Network intelligence is a discipline that builds on the capabilities of
network systems to act intelligently by the usage of network resources for
delivering high-quality services in a changing environment. Wide area network
intelligence is a class of network intelligence in wide area network which
covers the core and the edge of Internet. In this paper, we propose a system
based on machine learning for wide area network intelligence. The whole system
consists of a core machine for pre-training and many terminal machines to
accomplish faster responses. Each machine is one of dual-hemisphere models
which are made of left and right hemispheres. The left hemisphere is used to
improve latency by terminal response and the right hemisphere is used to
improve communication by data generation. In an application on multimedia
service, the proposed model is superior to the latest deep feed forward neural
network in the data center with respect to the accuracy, latency and
communication. Evaluation shows scalable improvement with regard to the number
of terminal machines. Evaluation also shows the cost of improvement is longer
learning time."
Robust Design of Federated Learning for Edge-Intelligent Networks,"Mass data traffics, low-latency wireless services and advanced artificial
intelligence (AI) technologies have driven the emergence of a new paradigm for
wireless networks, namely edge-intelligent networks, which are more efficient
and flexible than traditional cloud-intelligent networks. Considering users'
privacy, model sharing-based federated learning (FL) that migrates model
parameters but not private data from edge devices to a central cloud is
particularly attractive for edge-intelligent networks. Due to multiple rounds
of iterative updating of high-dimensional model parameters between base station
(BS) and edge devices, the communication reliability is a critical issue of FL
for edge-intelligent networks. We reveal the impacts of the errors generated
during model broadcast and model aggregation via wireless channels caused by
channel fading, interference and noise on the accuracy of FL, especially when
there exists channel uncertainty. To alleviate the impacts, we propose a robust
FL algorithm for edge-intelligent networks with channel uncertainty, which is
formulated as a worst-case optimization problem with joint device selection and
transceiver design. Finally, simulation results validate the robustness and
effectiveness of the proposed algorithm."
Naive Few-Shot Learning: Uncovering the fluid intelligence of machines,"In this paper, we aimed to help bridge the gap between human fluid
intelligence - the ability to solve novel tasks without prior training - and
the performance of deep neural networks, which typically require extensive
prior training. An essential cognitive component for solving intelligence
tests, which in humans are used to measure fluid intelligence, is the ability
to identify regularities in sequences. This motivated us to construct a
benchmark task, which we term \textit{sequence consistency evaluation} (SCE),
whose solution requires the ability to identify regularities in sequences.
Given the proven capabilities of deep networks, their ability to solve such
tasks after extensive training is expected. Surprisingly, however, we show that
naive (randomly initialized) deep learning models that are trained on a
\textit{single} SCE with a \textit{single} optimization step can still solve
non-trivial versions of the task relatively well. We extend our findings to
solve, without any prior training, real-world anomaly detection tasks in the
visual and auditory modalities. These results demonstrate the fluid-intelligent
computational capabilities of deep networks. We discuss the implications of our
work for constructing fluid-intelligent machines."
Scenarios and branch points to future machine intelligence,"We discuss scenarios and branch points to four major possible consequences
regarding future machine intelligence; 1) the singleton scenario where the
first and only super-intelligence acquires a decisive strategic advantage, 2)
the multipolar scenario where the singleton scenario is not technically denied
but political or other factors in human society or multi-agent interactions
between the intelligent agents prevent a single agent from gaining a decisive
strategic advantage, 3) the ecosystem scenario where the singleton scenario is
denied and many autonomous intelligent agents operate in such a way that they
are interdependent and virtually unstoppable, and 4) the upper-bound scenario
where cognitive capabilities that can be achieved by human-designed intelligent
agents or their descendants are inherently limited to the sub-human level. We
identify six major constraints that can form branch points to these scenarios;
(1) constraints on autonomy, (2) constraints on the ability to improve
self-structure, (3) constraints related to thermodynamic efficiency, (4)
constraints on updating physical infrastructure, (5) constraints on relative
advantage, and (6) constraints on locality."
"Unsupervised speech intelligibility assessment with utterance level
  alignment distance between teacher and learner Wav2Vec-2.0 representations","Speech intelligibility is crucial in language learning for effective
communication. Thus, to develop computer-assisted language learning systems,
automatic speech intelligibility detection (SID) is necessary. Most of the
works have assessed the intelligibility in a supervised manner considering
manual annotations, which requires cost and time; hence scalability is limited.
To overcome these, this work proposes an unsupervised approach for SID. The
proposed approach considers alignment distance computed with dynamic-time
warping (DTW) between teacher and learner representation sequence as a measure
to separate intelligible versus non-intelligible speech. We obtain the feature
sequence using current state-of-the-art self-supervised representations from
Wav2Vec-2.0. We found the detection accuracies as 90.37\%, 92.57\% and 96.58\%,
respectively, with three alignment distance measures -- mean absolute error,
mean squared error and cosine distance (equal to one minus cosine similarity)."
"Intelligent Parsing: An Automated Parsing Framework for Extracting
  Design Semantics from E-commerce Creatives","In the industrial e-commerce landscape, creative designs such as banners and
posters are ubiquitous. Extracting structured semantic information from
creative e-commerce design materials (manuscripts crafted by designers) to
obtain design semantics represents a core challenge in the realm of intelligent
design. In this paper, we propose a comprehensive automated framework for
intelligently parsing creative materials. This framework comprises material
recognition, preprocess, smartname, and label layers. The material recognition
layer consolidates various detection and recognition interfaces, covering
business aspects including detection of auxiliary areas within creative
materials and layer-level detection, alongside label identification.
Algorithmically, it encompasses a variety of coarse-to-fine methods such as
Cascade RCNN, GFL, and other models. The preprocess layer involves filtering
creative layers and grading creative materials. The smartname layer achieves
intelligent naming for creative materials, while the label layer covers
multi-level tagging for creative materials, enabling tagging at different
hierarchical levels. Intelligent parsing constitutes a complete parsing
framework that significantly aids downstream processes such as intelligent
creation, creative optimization, and material library construction. Within the
practical business applications at Suning, it markedly enhances the exposure,
circulation, and click-through rates of creative materials, expediting the
closed-loop production of creative materials and yielding substantial benefits."
"Autonomous Threat Hunting: A Future Paradigm for AI-Driven Threat
  Intelligence","The evolution of cybersecurity has spurred the emergence of autonomous threat
hunting as a pivotal paradigm in the realm of AI-driven threat intelligence.
This review navigates through the intricate landscape of autonomous threat
hunting, exploring its significance and pivotal role in fortifying cyber
defense mechanisms. Delving into the amalgamation of artificial intelligence
(AI) and traditional threat intelligence methodologies, this paper delineates
the necessity and evolution of autonomous approaches in combating contemporary
cyber threats. Through a comprehensive exploration of foundational AI-driven
threat intelligence, the review accentuates the transformative influence of AI
and machine learning on conventional threat intelligence practices. It
elucidates the conceptual framework underpinning autonomous threat hunting,
spotlighting its components, and the seamless integration of AI algorithms
within threat hunting processes.. Insightful discussions on challenges
encompassing scalability, interpretability, and ethical considerations in
AI-driven models enrich the discourse. Moreover, through illuminating case
studies and evaluations, this paper showcases real-world implementations,
underscoring success stories and lessons learned by organizations adopting
AI-driven threat intelligence. In conclusion, this review consolidates key
insights, emphasizing the substantial implications of autonomous threat hunting
for the future of cybersecurity. It underscores the significance of continual
research and collaborative efforts in harnessing the potential of AI-driven
approaches to fortify cyber defenses against evolving threats."
AI-enhanced Collective Intelligence,"Current societal challenges exceed the capacity of humans operating either
alone or collectively. As AI evolves, its role within human collectives will
vary from an assistive tool to a participatory member. Humans and AI possess
complementary capabilities that, together, can surpass the collective
intelligence of either humans or AI in isolation. However, the interactions in
human-AI systems are inherently complex, involving intricate processes and
interdependencies. This review incorporates perspectives from complex network
science to conceptualize a multilayer representation of human-AI collective
intelligence, comprising cognition, physical, and information layers. Within
this multilayer network, humans and AI agents exhibit varying characteristics;
humans differ in diversity from surface-level to deep-level attributes, while
AI agents range in degrees of functionality and anthropomorphism. We explore
how agents' diversity and interactions influence the system's collective
intelligence and analyze real-world instances of AI-enhanced collective
intelligence. We conclude by considering potential challenges and future
developments in this field."
"Visual-O1: Understanding Ambiguous Instructions via Multi-modal
  Multi-turn Chain-of-thoughts Reasoning","As large-scale models evolve, language instructions are increasingly utilized
in multi-modal tasks. Due to human language habits, these instructions often
contain ambiguities in real-world scenarios, necessitating the integration of
visual context or common sense for accurate interpretation. However, even
highly intelligent large models exhibit significant performance limitations on
ambiguous instructions, where weak reasoning abilities of disambiguation can
lead to catastrophic errors. To address this issue, this paper proposes
Visual-O1, a multi-modal multi-turn chain-of-thought reasoning framework. It
simulates human multi-modal multi-turn reasoning, providing instantial
experience for highly intelligent models or empirical experience for generally
intelligent models to understand ambiguous instructions. Unlike traditional
methods that require models to possess high intelligence to understand long
texts or perform lengthy complex reasoning, our framework does not
significantly increase computational overhead and is more general and
effective, even for generally intelligent models. Experiments show that our
method not only significantly enhances the performance of models of different
intelligence levels on ambiguous instructions but also improves their
performance on general datasets. Our work highlights the potential of
artificial intelligence to work like humans in real-world scenarios with
uncertainty and ambiguity. We will release our data and code."
"A Contemporary Overview: Trends and Applications of Large Language
  Models on Mobile Devices","With the rapid development of large language models (LLMs), which possess
powerful natural language processing and generation capabilities, LLMs are
poised to provide more natural and personalized user experiences. Their
deployment on mobile devices is gradually becoming a significant trend in the
field of intelligent devices. LLMs have demonstrated tremendous potential in
applications such as voice assistants, real-time translation, and intelligent
recommendations. Advancements in hardware technologies (such as neural network
accelerators) and network infrastructure (such as 5G) have enabled efficient
local inference and low-latency intelligent responses on mobile devices. This
reduces reliance on cloud computing while enhancing data privacy and security.
Developers can easily integrate LLM functionalities through open APIs and SDKs,
enabling the creation of more innovative intelligent applications. The
widespread use of LLMs not only enhances the intelligence of mobile devices but
also fosters the integrated innovation of fields like augmented reality (AR)
and the Internet of Things (IoT). This trend is expected to drive the
development of the next generation of mobile intelligent applications."
Autonomous robots and the SP theory of intelligence,"This article is about how the ""SP theory of intelligence"" and its realisation
in the ""SP machine"" (both outlined in the article) may help to solve
computer-related problems in the design of autonomous robots, meaning robots
that do not depend on external intelligence or power supplies, are mobile, and
are designed to exhibit as much human-like intelligence as possible. The
article is about: how to increase the computational and energy efficiency of
computers and reduce their bulk; how to achieve human-like versatility in
intelligence; and likewise for human-like adaptability in intelligence. The SP
system has potential for substantial gains in computational and energy
efficiency and reductions in the bulkiness of computers: by reducing the size
of data to be processed; by exploiting statistical information that the system
gathers; and via an updated version of Donald Hebb's concept of a ""cell
assembly"". Towards human-like versatility in intelligence, the SP system has
strengths in unsupervised learning, natural language processing, pattern
recognition, information retrieval, several kinds of reasoning, planning,
problem solving, and more, with seamless integration amongst structures and
functions. The SP system's strengths in unsupervised learning and other aspects
of intelligence may help to achieve human-like adaptability in intelligence
via: the learning of natural language; learning to see; building 3D models of
objects and of a robot's surroundings; learning regularities in the workings of
a robot and in the robot's environment; exploration and play; learning major
skills; and secondary forms of learning. Also discussed are: how the SP system
may process parallel streams of information; generalisation of knowledge,
correction of over-generalisations, and learning from dirty data; how to cut
the cost of learning; and reinforcements, motivations, goals, and
demonstration."
A World of Views: A World of Interacting Post-human Intelligences,"What would a human hundreds or thousands times more intelligent than the
brightest human ever born be like? We must admit we can hardly guess. A human
being of such intelligence will be so radically different from us that it can
hardly, if at all, be recognized as human. If we had to go back along the
evolutionary tree to identify a creature 1000 times less intelligent than the
average contemporary human, we will have to go really far back. Would it be a
kind of a lizard? An insect perhaps? Considering this, how can we possibly
aspire to have a grasp of something a thousand times more intelligent than us?
When it comes to intelligence, even the very attempt to quantify it is highly
misleading. Now if we attend to a seemingly adjacent question, what would a
machine with such capacity for intelligence be like? Just coming up with an
approximate metaphor requires a huge stretch of the imagination, meaning that
almost anything goes... What would a society of such super intelligent agents,
be they human, machines or an amalgam of both, be like? Well, here we are
transported into the realm of pure speculation. Technological Singularity is
referred to as the event of artificial intelligence surpassing the intelligence
of humans and shortly after augmenting itself far beyond that. It is no wonder
that the mathematical concept of singularity has become the symbol of an event
so disruptive and so far reaching that it is impossible to conceptually or even
metaphorically grasp, much less to predict."
Compression Represents Intelligence Linearly,"There is a belief that learning to compress well will lead to intelligence.
Recently, language modeling has been shown to be equivalent to compression,
which offers a compelling rationale for the success of large language models
(LLMs): the development of more advanced language models is essentially
enhancing compression which facilitates intelligence. Despite such appealing
discussions, little empirical evidence is present for the interplay between
compression and intelligence. In this work, we examine their relationship in
the context of LLMs, treating LLMs as data compressors. Given the abstract
concept of ""intelligence"", we adopt the average downstream benchmark scores as
a surrogate, specifically targeting intelligence related to knowledge and
commonsense, coding, and mathematical reasoning. Across 12 benchmarks, our
study brings together 31 public LLMs that originate from diverse organizations.
Remarkably, we find that LLMs' intelligence -- reflected by average benchmark
scores -- almost linearly correlates with their ability to compress external
text corpora. These results provide concrete evidence supporting the belief
that superior compression indicates greater intelligence. Furthermore, our
findings suggest that compression efficiency, as an unsupervised metric derived
from raw text corpora, serves as a reliable evaluation measure that is linearly
associated with the model capabilities. We open-source our compression datasets
as well as our data collection pipelines to facilitate future researchers to
assess compression properly."
"Detecting Fake News on Social Media: A Novel Reliability Aware
  Machine-Crowd Hybrid Intelligence-Based Method","Fake news on social media platforms poses a significant threat to societal
systems, underscoring the urgent need for advanced detection methods. The
existing detection methods can be divided into machine intelligence-based,
crowd intelligence-based, and hybrid intelligence-based methods. Among them,
hybrid intelligence-based methods achieve the best performance but fail to
consider the reliability issue in detection. In light of this, we propose a
novel Reliability Aware Hybrid Intelligence (RAHI) method for fake news
detection. Our method comprises three integral modules. The first module
employs a Bayesian deep learning model to capture the inherent reliability
within machine intelligence. The second module uses an Item Response Theory
(IRT)-based user response aggregation to account for the reliability in crowd
intelligence. The third module introduces a new distribution fusion mechanism,
which takes the distributions derived from both machine and crowd intelligence
as input, and outputs a fused distribution that provides predictions along with
the associated reliability. The experiments on the Weibo dataset demonstrate
the advantages of our method. This study contributes to the research field with
a novel RAHI-based method, and the code is shared at
https://github.com/Kangwei-g/RAHI. This study has practical implications for
three key stakeholders: internet users, online platform managers, and the
government."
Classifying Signals with Local Classifiers,"This paper deals with the problem of classifying signals. The new method for
building so called local classifiers and local features is presented. The
method is a combination of the lifting scheme and the support vector machines.
Its main aim is to produce effective and yet comprehensible classifiers that
would help in understanding processes hidden behind classified signals. To
illustrate the method we present the results obtained on an artificial and a
real dataset."
"The Future of Scientific Simulations: from Artificial Life to Artificial
  Cosmogenesis","This philosophical paper explores the relation between modern scientific
simulations and the future of the universe. We argue that a simulation of an
entire universe will result from future scientific activity. This requires us
to tackle the challenge of simulating open-ended evolution at all levels in a
single simulation. The simulation should encompass not only biological
evolution, but also physical evolution (a level below) and cultural evolution
(a level above). The simulation would allow us to probe what would happen if we
would ""replay the tape of the universe"" with the same or different laws and
initial conditions. We also distinguish between real-world and artificial-world
modelling. Assuming that intelligent life could indeed simulate an entire
universe, this leads to two tentative hypotheses. Some authors have argued that
we may already be in a simulation run by an intelligent entity. Or, if such a
simulation could be made real, this would lead to the production of a new
universe. This last direction is argued with a careful speculative
philosophical approach, emphasizing the imperative to find a solution to the
heat death problem in cosmology. The reader is invited to consult Annex 1 for
an overview of the logical structure of this paper. -- Keywords: far future,
future of science, ALife, simulation, realization, cosmology, heat death,
fine-tuning, physical eschatology, cosmological natural selection, cosmological
artificial selection, artificial cosmogenesis, selfish biocosm hypothesis,
meduso-anthropic principle, developmental singularity hypothesis, role of
intelligent life."
"I'm sorry to say, but your understanding of image processing
  fundamentals is absolutely wrong","The ongoing discussion whether modern vision systems have to be viewed as
visually-enabled cognitive systems or cognitively-enabled vision systems is
groundless, because perceptual and cognitive faculties of vision are separate
components of human (and consequently, artificial) information processing
system modeling."
Computational Understanding and Manipulation of Symmetries,"For natural and artificial systems with some symmetry structure,
computational understanding and manipulation can be achieved without learning
by exploiting the algebraic structure. Here we describe this algebraic
coordinatization method and apply it to permutation puzzles. Coordinatization
yields a structural understanding, not just solutions for the puzzles."
Une architecture cognitive et affective orient{}e interaction,"In this paper, we present CAIO, a Cognitive and Affective
Interaction-Oriented architecture for social human-robot interactions (HRI),
allowing robots to reason on mental states (including emotions), and to act
physically, emotionally and verbally. We also present a short scenario and
implementation on a Nao robot."
Conscious enactive computation,"This paper looks at recent debates in the enactivist literature on
computation and consciousness in order to assess major obstacles to building
artificial conscious agents. We consider a proposal from Villalobos and
Dewhurst (2018) for enactive computation on the basis of organizational
closure. We attempt to improve the argument by reflecting on the closed paths
through state space taken by finite state automata. This motivates a defense
against Clark's recent criticisms of ""extended consciousness"", and perhaps a
new perspective on living with machines."
Localizing Catastrophic Forgetting in Neural Networks,"Artificial neural networks (ANNs) suffer from catastrophic forgetting when
trained on a sequence of tasks. While this phenomenon was studied in the past,
there is only very limited recent research on this phenomenon. We propose a
method for determining the contribution of individual parameters in an ANN to
catastrophic forgetting. The method is used to analyze an ANNs response to
three different continual learning scenarios."
A Precis of Language Models are not Models of Language,"Natural Language Processing is one of the leading application areas in the
current resurgence of Artificial Intelligence, spearheaded by Artificial Neural
Networks. We show that despite their many successes at performing linguistic
tasks, Large Neural Language Models are ill-suited as comprehensive models of
natural language. The wider implication is that, in spite of the often
overbearing optimism about AI, modern neural models do not represent a
revolution in our understanding of cognition."
Neural Networks,"We review the theory of neural networks, as it has emerged in the last ten
years or so within the physics community, emphasizing questions of biological
relevance over those of importance in mathematical statistics and machine
learning theory."
A Dynamical Systems Approach for Static Evaluation in Go,"In the paper arguments are given why the concept of static evaluation has the
potential to be a useful extension to Monte Carlo tree search. A new concept of
modeling static evaluation through a dynamical system is introduced and
strengths and weaknesses are discussed. The general suitability of this
approach is demonstrated."
The Search for Computational Intelligence,"We define and explore in simulation several rules for the local evolution of
generative rules for 1D and 2D cellular automata. Our implementation uses
strategies from conceptual blending. We discuss potential applications to
modelling social dynamics."
"From Seed AI to Technological Singularity via Recursively Self-Improving
  Software","Software capable of improving itself has been a dream of computer scientists
since the inception of the field. In this work we provide definitions for
Recursively Self-Improving software, survey different types of self-improving
software, review the relevant literature, analyze limits on computation
restricting recursive self-improvement and introduce RSI Convergence Theory
which aims to predict general behavior of RSI systems. Finally, we address
security implications from self-improving intelligent software."
Fuzzy Clustering Data Given in the Ordinal Scale,"A fuzzy clustering algorithm for multidimensional data is proposed in this
article. The data is described by vectors whose components are linguistic
variables defined in an ordinal scale. The obtained results confirm the
efficiency of the proposed approach."
"Transferrable Plausibility Model - A Probabilistic Interpretation of
  Mathematical Theory of Evidence","This paper suggests a new interpretation of the Dempster-Shafer theory in
terms of probabilistic interpretation of plausibility. A new rule of
combination of independent evidence is shown and its preservation of
interpretation is demonstrated."
Can Machines Think in Radio Language?,"People can think in auditory, visual and tactile forms of language, so can
machines principally. But is it possible for them to think in radio language?
According to a first principle presented for general intelligence, i.e. the
principle of language's relativity, the answer may give an exceptional solution
for robot astronauts to talk with each other in space exploration."
"Modeling car-following behavior on urban expressways in Shanghai: A
  naturalistic driving study","Five car-following models were calibrated, validated and cross-compared. The
intelligent driver model performed best among the evaluated models.
Considerable behavioral differences between different drivers were found.
Calibrated model parameters may not be numerically equivalent with observed
ones."
How to define co-occurrence in different domains of study?,"This position paper presents a comparative study of co-occurrences. Some
similarities and differences in the definition exist depending on the research
domain (e.g. linguistics, NLP, computer science). This paper discusses these
points, and deals with the methodological aspects in order to identify
co-occurrences in a multidisciplinary paradigm."
Unpredictability of AI,"The young field of AI Safety is still in the process of identifying its
challenges and limitations. In this paper, we formally describe one such
impossibility result, namely Unpredictability of AI. We prove that it is
impossible to precisely and consistently predict what specific actions a
smarter-than-human intelligent system will take to achieve its objectives, even
if we know terminal goals of the system. In conclusion, impact of
Unpredictability on AI Safety is discussed."
Modern Techniques for Ancient Games,"Games potentially provide a wealth of knowledge about our shared cultural
past and the development of human civilisation, but our understanding of early
games is incomplete and often based on unreliable reconstructions. This paper
describes the Digital Ludeme Project, a five-year research project currently
underway that aims to address such issues using modern computational
techniques."
"Using reinforcement learning to design an AI assistantfor a satisfying
  co-op experience","In this project, we designed an intelligent assistant player for the
single-player game Space Invaders with the aim to provide a satisfying co-op
experience. The agent behaviour was designed using reinforcement learning
techniques and evaluated based on several criteria. We validate the hypothesis
that an AI-driven computer player can provide a satisfying co-op experience."
"Challenges of Artificial Intelligence -- From Machine Learning and
  Computer Vision to Emotional Intelligence","Artificial intelligence (AI) has become a part of everyday conversation and
our lives. It is considered as the new electricity that is revolutionizing the
world. AI is heavily invested in both industry and academy. However, there is
also a lot of hype in the current AI debate. AI based on so-called deep
learning has achieved impressive results in many problems, but its limits are
already visible. AI has been under research since the 1940s, and the industry
has seen many ups and downs due to over-expectations and related
disappointments that have followed.
  The purpose of this book is to give a realistic picture of AI, its history,
its potential and limitations. We believe that AI is a helper, not a ruler of
humans. We begin by describing what AI is and how it has evolved over the
decades. After fundamentals, we explain the importance of massive data for the
current mainstream of artificial intelligence. The most common representations
for AI, methods, and machine learning are covered. In addition, the main
application areas are introduced. Computer vision has been central to the
development of AI. The book provides a general introduction to computer vision,
and includes an exposure to the results and applications of our own research.
Emotions are central to human intelligence, but little use has been made in AI.
We present the basics of emotional intelligence and our own research on the
topic. We discuss super-intelligence that transcends human understanding,
explaining why such achievement seems impossible on the basis of present
knowledge,and how AI could be improved. Finally, a summary is made of the
current state of AI and what to do in the future. In the appendix, we look at
the development of AI education, especially from the perspective of contents at
our own university."
"Spatial Intelligence of a Self-driving Car and Rule-Based Decision
  Making","In this paper we show how rule-based decision making can be combined with
traditional motion planning techniques to achieve human-like behavior of a
self-driving vehicle in complex traffic situations. We give and discuss
examples of decision rules in autonomous driving. We draw on these examples to
illustrate that developing techniques for spatial awareness of robots is an
exciting activity which deserves more attention from spatial reasoning
community that it had received so far."
Implement services for business scenarios by combining basic emulators,"This article mainly introduces how to use various basic emulators to form a
combined emulator in the Jiutian Intelligence Network Simulation Platform to
realize simulation service functions in different business scenarios. Among
them, the combined emulator is included. The business scenarios include
different practical applications such as multi-objective antenna optimization,
high traffic of business, CSI (channel state information) compression feedback,
etc."
"Inference of Abstraction for a Unified Account of Symbolic Reasoning
  from Data","Inspired by empirical work in neuroscience for Bayesian approaches to brain
function, we give a unified probabilistic account of various types of symbolic
reasoning from data. We characterise them in terms of formal logic using the
classical consequence relation, an empirical consequence relation, maximal
consistent sets, maximal possible sets and maximum likelihood estimation. The
theory gives new insights into reasoning towards human-like machine
intelligence."
Driving behavior recognition via self-discovery learning,"Autonomous driving systems require a deep understanding of human driving
behaviors to achieve higher intelligence and safety.Despite advancements in
deep learning, challenges such as long-tail distribution due to scarce samples
and confusion from similar behaviors hinder effective driving behavior
detection.Existing methods often fail to address sample confusion adequately,
as datasets frequently contain ambiguous samples that obscure unique semantic
information."
"Definition and properties to assess multi-agent environments as social
  intelligence tests","Social intelligence in natural and artificial systems is usually measured by
the evaluation of associated traits or tasks that are deemed to represent some
facets of social behaviour. The amalgamation of these traits is then used to
configure the intuitive notion of social intelligence. Instead, in this paper
we start from a parametrised definition of social intelligence as the expected
performance in a set of environments with several agents, and we assess and
derive tests from it. This definition makes several dependencies explicit: (1)
the definition depends on the choice (and weight) of environments and agents,
(2) the definition may include both competitive and cooperative behaviours
depending on how agents and rewards are arranged into teams, (3) the definition
mostly depends on the abilities of other agents, and (4) the actual difference
between social intelligence and general intelligence (or other abilities)
depends on these choices. As a result, we address the problem of converting
this definition into a more precise one where some fundamental properties
ensuring social behaviour (such as action and reward dependency and
anticipation on competitive/cooperative behaviours) are met as well as some
other more instrumental properties (such as secernment, boundedness, symmetry,
validity, reliability, efficiency), which are convenient to convert the
definition into a practical test. From the definition and the formalised
properties, we take a look at several representative multi-agent environments,
tests and games to see whether they meet these properties."
"Single photon in hierarchical architecture for physical reinforcement
  learning: Photon intelligence","Understanding and using natural processes for intelligent functionalities,
referred to as natural intelligence, has recently attracted interest from a
variety of fields, including post-silicon computing for artificial intelligence
and decision making in the behavioural sciences. In a past study, we
successfully used the wave-particle duality of single photons to solve the
two-armed bandit problem, which constitutes the foundation of reinforcement
learning and decision making. In this study, we propose and confirm a
hierarchical architecture for single-photon-based reinforcement learning and
decision making that verifies the scalability of the principle. Specifically,
the four-armed bandit problem is solved given zero prior knowledge in a
two-layer hierarchical architecture, where polarization is autonomously adapted
in order to effect adequate decision making using single-photon measurements.
In the hierarchical structure, the notion of layer-dependent decisions emerges.
The optimal solutions in the coarse layer and in the fine layer, however,
conflict with each other in some contradictive problems. We show that while
what we call a tournament strategy resolves such contradictions, the
probabilistic nature of single photons allows for the direct location of the
optimal solution even for contradictive problems, hence manifesting the
exploration ability of single photons. This study provides insights into photon
intelligence in hierarchical architectures for future artificial intelligence
as well as the potential of natural processes for intelligent functionalities."
"Edge Graph Intelligence: Reciprocally Empowering Edge Networks with
  Graph Intelligence","Recent years have witnessed a thriving growth of computing facilities
connected at the network edge, cultivating edge networks as a fundamental
infrastructure for supporting miscellaneous intelligent services.Meanwhile,
Artificial Intelligence (AI) frontiers have extrapolated to the graph domain
and promoted Graph Intelligence (GI). Given the inherent relation between
graphs and networks, the interdiscipline of graph learning and edge networks,
i.e., Edge GI or EGI, has revealed a novel interplay between them -- GI aids in
optimizing edge networks, while edge networks facilitate GI model deployment.
Driven by this delicate closed-loop, EGI is recognized as a promising solution
to fully unleash the potential of edge computing power and is garnering growing
attention. Nevertheless, research on EGI remains nascent, and there is a
soaring demand within both the communications and AI communities for a
dedicated venue to share recent advancements. To this end, this paper promotes
the concept of EGI, explores its scope and core principles, and conducts a
comprehensive survey concerning recent research efforts on this emerging field.
Specifically, this paper introduces and discusses: 1) fundamentals of edge
computing and graph learning,2) emerging techniques centering on the closed
loop between graph intelligence and edge networks, and 3) open challenges and
research opportunities of future EGI. By bridging the gap across communication,
networking, and graph learning areas, we believe that this survey can garner
increased attention, foster meaningful discussions, and inspire further
research ideas in EGI."
"Research and Design on Intelligent Recognition of Unordered Targets for
  Robots Based on Reinforcement Learning","In the field of robot target recognition research driven by artificial
intelligence (AI), factors such as the disordered distribution of targets, the
complexity of the environment, the massive scale of data, and noise
interference have significantly restricted the improvement of target
recognition accuracy. Against the backdrop of the continuous iteration and
upgrading of current AI technologies, to meet the demand for accurate
recognition of disordered targets by intelligent robots in complex and
changeable scenarios, this study innovatively proposes an AI - based
intelligent robot disordered target recognition method using reinforcement
learning. This method processes the collected target images with the bilateral
filtering algorithm, decomposing them into low - illumination images and
reflection images. Subsequently, it adopts differentiated AI strategies,
compressing the illumination images and enhancing the reflection images
respectively, and then fuses the two parts of images to generate a new image.
On this basis, this study deeply integrates deep learning, a core AI
technology, with the reinforcement learning algorithm. The enhanced target
images are input into a deep reinforcement learning model for training,
ultimately enabling the AI - based intelligent robot to efficiently recognize
disordered targets. Experimental results show that the proposed method can not
only significantly improve the quality of target images but also enable the AI
- based intelligent robot to complete the recognition task of disordered
targets with higher efficiency and accuracy, demonstrating extremely high
application value and broad development prospects in the field of AI robots."
"EduPlanner: LLM-Based Multi-Agent Systems for Customized and Intelligent
  Instructional Design","Large Language Models (LLMs) have significantly advanced smart education in
the Artificial General Intelligence (AGI) era. A promising application lies in
the automatic generalization of instructional design for curriculum and
learning activities, focusing on two key aspects: (1) Customized Generation:
generating niche-targeted teaching content based on students' varying learning
abilities and states, and (2) Intelligent Optimization: iteratively optimizing
content based on feedback from learning effectiveness or test scores.
Currently, a single large LLM cannot effectively manage the entire process,
posing a challenge for designing intelligent teaching plans. To address these
issues, we developed EduPlanner, an LLM-based multi-agent system comprising an
evaluator agent, an optimizer agent, and a question analyst, working in
adversarial collaboration to generate customized and intelligent instructional
design for curriculum and learning activities. Taking mathematics lessons as
our example, EduPlanner employs a novel Skill-Tree structure to accurately
model the background mathematics knowledge of student groups, personalizing
instructional design for curriculum and learning activities according to
students' knowledge levels and learning abilities. Additionally, we introduce
the CIDDP, an LLM-based five-dimensional evaluation module encompassing
clarity, Integrity, Depth, Practicality, and Pertinence, to comprehensively
assess mathematics lesson plan quality and bootstrap intelligent optimization.
Experiments conducted on the GSM8K and Algebra datasets demonstrate that
EduPlanner excels in evaluating and optimizing instructional design for
curriculum and learning activities. Ablation studies further validate the
significance and effectiveness of each component within the framework. Our code
is publicly available at https://github.com/Zc0812/Edu_Planner"
"Linguistics Computation, Automatic Model Generation, and Intensions","Techniques are presented for defining models of computational linguistics
theories. The methods of generalized diagrams that were developed by this
author for modeling artificial intelligence planning and reasoning are shown to
be applicable to models of computation of linguistics theories. It is shown
that for extensional and intensional interpretations, models can be generated
automatically which assign meaning to computations of linguistics theories for
natural languages.
  Keywords: Computational Linguistics, Reasoning Models, G-diagrams For Models,
Dynamic Model Implementation, Linguistics and Logics For Artificial
Intelligence"
Dynamic Backtracking,"Because of their occasional need to return to shallow points in a search
tree, existing backtracking methods can sometimes erase meaningful progress
toward solving a search problem. In this paper, we present a method by which
backtrack points can be moved deeper in the search space, thereby avoiding this
difficulty. The technique developed is a variant of dependency-directed
backtracking that uses only polynomial space while still providing useful
control information and retaining the completeness guarantees provided by
earlier approaches."
Applying GSAT to Non-Clausal Formulas,"In this paper we describe how to modify GSAT so that it can be applied to
non-clausal formulas. The idea is to use a particular ``score'' function which
gives the number of clauses of the CNF conversion of a formula which are false
under a given truth assignment. Its value is computed in linear time, without
constructing the CNF conversion itself. The proposed methodology applies to
most of the variants of GSAT proposed so far."
On Planning while Learning,"This paper introduces a framework for Planning while Learning where an agent
is given a goal to achieve in an environment whose behavior is only partially
known to the agent. We discuss the tractability of various plan-design
processes. We show that for a large natural class of Planning while Learning
systems, a plan can be presented and verified in a reasonable time. However,
coming up algorithmically with a plan, even for simple classes of systems is
apparently intractable. We emphasize the role of off-line plan-design
processes, and show that, in most natural cases, the verification (projection)
part can be carried out in an efficient algorithmic manner."
Decision-Theoretic Foundations for Causal Reasoning,"We present a definition of cause and effect in terms of decision-theoretic
primitives and thereby provide a principled foundation for causal reasoning.
Our definition departs from the traditional view of causation in that causal
assertions may vary with the set of decisions available. We argue that this
approach provides added clarity to the notion of cause. Also in this paper, we
examine the encoding of causal relationships in directed acyclic graphs. We
describe a special class of influence diagrams, those in canonical form, and
show its relationship to Pearl's representation of cause and effect. Finally,
we show how canonical form facilitates counterfactual reasoning."
Statistical Feature Combination for the Evaluation of Game Positions,"This article describes an application of three well-known statistical methods
in the field of game-tree search: using a large number of classified Othello
positions, feature weights for evaluation functions with a
game-phase-independent meaning are estimated by means of logistic regression,
Fisher's linear discriminant, and the quadratic discriminant function for
normally distributed features. Thereafter, the playing strengths are compared
by means of tournaments between the resulting versions of a world-class Othello
program. In this application, logistic regression - which is used here for the
first time in the context of game playing - leads to better results than the
other approaches."
Rule-based Machine Learning Methods for Functional Prediction,"We describe a machine learning method for predicting the value of a
real-valued function, given the values of multiple input variables. The method
induces solutions from samples in the form of ordered disjunctive normal form
(DNF) decision rules. A central objective of the method and representation is
the induction of compact, easily interpretable solutions. This rule-based
decision model can be extended to search efficiently for similar cases prior to
approximating function values. Experimental results on real-world data
demonstrate that the new techniques are competitive with existing machine
learning and statistical methods and can sometimes yield superior regression
performance."
"Well-Founded Semantics for Extended Logic Programs with Dynamic
  Preferences","The paper describes an extension of well-founded semantics for logic programs
with two types of negation. In this extension information about preferences
between rules can be expressed in the logical language and derived dynamically.
This is achieved by using a reserved predicate symbol and a naming technique.
Conflicts among rules are resolved whenever possible on the basis of derived
preference information. The well-founded conclusions of prioritized logic
programs can be computed in polynomial time. A legal reasoning example
illustrates the usefulness of the approach."
Quantum Computing and Phase Transitions in Combinatorial Search,"We introduce an algorithm for combinatorial search on quantum computers that
is capable of significantly concentrating amplitude into solutions for some NP
search problems, on average. This is done by exploiting the same aspects of
problem structure as used by classical backtrack methods to avoid unproductive
search choices. This quantum algorithm is much more likely to find solutions
than the simple direct use of quantum parallelism. Furthermore, empirical
evaluation on small problems shows this quantum algorithm displays the same
phase transition behavior, and at the same location, as seen in many previously
studied classical search methods. Specifically, difficult problem instances are
concentrated near the abrupt change from underconstrained to overconstrained
problems."
Mean Field Theory for Sigmoid Belief Networks,"We develop a mean field theory for sigmoid belief networks based on ideas
from statistical mechanics. Our mean field theory provides a tractable
approximation to the true probability distribution in these networks; it also
yields a lower bound on the likelihood of evidence. We demonstrate the utility
of this framework on a benchmark problem in statistical pattern
recognition---the classification of handwritten digits."
Improved Use of Continuous Attributes in C4.5,"A reported weakness of C4.5 in domains with continuous attributes is
addressed by modifying the formation and evaluation of tests on continuous
attributes. An MDL-inspired penalty is applied to such tests, eliminating some
of them from consideration and altering the relative desirability of all tests.
Empirical trials show that the modifications lead to smaller decision trees
with higher predictive accuracies. Results also confirm that a new version of
C4.5 incorporating these changes is superior to recent approaches that use
global discretization and that construct small trees with multi-interval
splits."
Active Learning with Statistical Models,"For many types of machine learning algorithms, one can compute the
statistically `optimal' way to select training data. In this paper, we review
how optimal data selection techniques have been used with feedforward neural
networks. We then show how the same principles may be used to select data for
two alternative, statistically-based learning architectures: mixtures of
Gaussians and locally weighted regression. While the techniques for neural
networks are computationally expensive and approximate, the techniques for
mixtures of Gaussians and locally weighted regression are both efficient and
accurate. Empirically, we observe that the optimality criterion sharply
decreases the number of training examples the learner needs in order to achieve
good performance."
A Divergence Critic for Inductive Proof,"Inductive theorem provers often diverge. This paper describes a simple
critic, a computer program which monitors the construction of inductive proofs
attempting to identify diverging proof attempts. Divergence is recognized by
means of a ``difference matching'' procedure. The critic then proposes lemmas
and generalizations which ``ripple'' these differences away so that the proof
can go through without divergence. The critic enables the theorem prover Spike
to prove many theorems completely automatically from the definitions alone."
Learning First-Order Definitions of Functions,"First-order learning involves finding a clause-form definition of a relation
from examples of the relation and relevant background information. In this
paper, a particular first-order learning system is modified to customize it for
finding definitions of functional relations. This restriction leads to faster
learning times and, in some cases, to definitions that have higher predictive
accuracy. Other first-order learning systems might benefit from similar
specialization."
Lifeworld Analysis,"We argue that the analysis of agent/environment interactions should be
extended to include the conventions and invariants maintained by agents
throughout their activity. We refer to this thicker notion of environment as a
lifeworld and present a partial set of formal tools for describing structures
of lifeworlds and the ways in which they computationally simplify activity. As
one specific example, we apply the tools to the analysis of the Toast system
and show how versions of the system with very different control structures in
fact implement a common control structure together with different conventions
for encoding task state in the positions or states of objects in the
environment."
A Complete Classification of Tractability in RCC-5,"We investigate the computational properties of the spatial algebra RCC-5
which is a restricted version of the RCC framework for spatial reasoning. The
satisfiability problem for RCC-5 is known to be NP-complete but not much is
known about its approximately four billion subclasses. We provide a complete
classification of satisfiability for all these subclasses into polynomial and
NP-complete respectively. In the process, we identify all maximal tractable
subalgebras which are four in total."
Eight Maximal Tractable Subclasses of Allen's Algebra with Metric Time,"This paper combines two important directions of research in temporal
resoning: that of finding maximal tractable subclasses of Allen's interval
algebra, and that of reasoning with metric temporal information. Eight new
maximal tractable subclasses of Allen's interval algebra are presented, some of
them subsuming previously reported tractable algebras. The algebras allow for
metric temporal constraints on interval starting or ending points, using the
recent framework of Horn DLRs. Two of the algebras can express the notion of
sequentiality between intervals, being the first such algebras admitting both
qualitative and metric time."
"Defining Relative Likelihood in Partially-Ordered Preferential
  Structures","Starting with a likelihood or preference order on worlds, we extend it to a
likelihood ordering on sets of worlds in a natural way, and examine the
resulting logic. Lewis earlier considered such a notion of relative likelihood
in the context of studying counterfactuals, but he assumed a total preference
order on worlds. Complications arise when examining partial orders that are not
present for total orders. There are subtleties involving the exact approach to
lifting the order on worlds to an order on sets of worlds. In addition, the
axiomatization of the logic of relative likelihood in the case of partial
orders gives insight into the connection between relative likelihood and
default reasoning."
Representation Theory for Default Logic,"Default logic can be regarded as a mechanism to represent families of belief
sets of a reasoning agent. As such, it is inherently second-order. In this
paper, we study the problem of representability of a family of theories as the
set of extensions of a default theory. We give a complete solution to the
representability by means of normal default theories. We obtain partial results
on representability by arbitrary default theories. We construct examples of
denumerable families of non-including theories that are not representable. We
also study the concept of equivalence between default theories."
Mixing Metaphors,"Mixed metaphors have been neglected in recent metaphor research. This paper
suggests that such neglect is short-sighted. Though mixing is a more complex
phenomenon than straight metaphors, the same kinds of reasoning and knowledge
structures are required. This paper provides an analysis of both parallel and
serial mixed metaphors within the framework of an AI system which is already
capable of reasoning about straight metaphorical manifestations and argues that
the processes underlying mixing are central to metaphorical meaning. Therefore,
any theory of metaphors must be able to account for mixing."
Fuzzy Approaches to Abductive Inference,"This paper proposes two kinds of fuzzy abductive inference in the framework
of fuzzy rule base. The abductive inference processes described here depend on
the semantic of the rule. We distinguish two classes of interpretation of a
fuzzy rule, certainty generation rules and possible generation rules. In this
paper we present the architecture of abductive inference in the first class of
interpretation. We give two kinds of problem that we can resolve by using the
proposed models of inference."
Exact Phase Transitions in Random Constraint Satisfaction Problems,"In this paper we propose a new type of random CSP model, called Model RB,
which is a revision to the standard Model B. It is proved that phase
transitions from a region where almost all problems are satisfiable to a region
where almost all problems are unsatisfiable do exist for Model RB as the number
of variables approaches infinity. Moreover, the critical values at which the
phase transitions occur are also known exactly. By relating the hardness of
Model RB to Model B, it is shown that there exist a lot of hard instances in
Model RB."
Entrenchment Relations: A Uniform Approach to Nonmonotonicity,"We show that Gabbay's nonmonotonic consequence relations can be reduced to a
new family of relations, called entrenchment relations. Entrenchment relations
provide a direct generalization of epistemic entrenchment and expectation
ordering introduced by Gardenfors and Makinson for the study of belief revision
and expectation inference, respectively."
A Tableau Calculus for Pronoun Resolution,"We present a tableau calculus for reasoning in fragments of natural language.
We focus on the problem of pronoun resolution and the way in which it
complicates automated theorem proving for natural language processing. A method
for explicitly manipulating contextual information during deduction is
proposed, where pronouns are resolved against this context during deduction. As
a result, pronoun resolution and deduction can be interleaved in such a way
that pronouns are only resolved if this is licensed by a deduction rule; this
helps us to avoid the combinatorial complexity of total pronoun disambiguation."
Generalized Qualitative Probability: Savage revisited,"Preferences among acts are analyzed in the style of L. Savage, but as
partially ordered. The rationality postulates considered are weaker than
Savage's on three counts. The Sure Thing Principle is derived in this setting.
The postulates are shown to lead to a characterization of generalized
qualitative probability that includes and blends both traditional qualitative
probability and the ranked structures used in logical approaches."
Bayesian Information Extraction Network,"Dynamic Bayesian networks (DBNs) offer an elegant way to integrate various
aspects of language in one model. Many existing algorithms developed for
learning and inference in DBNs are applicable to probabilistic language
modeling. To demonstrate the potential of DBNs for natural language processing,
we employ a DBN in an information extraction task. We show how to assemble
wealth of emerging linguistic instruments for shallow parsing, syntactic and
semantic tagging, morphological decomposition, named entity recognition etc. in
order to incrementally build a robust information extraction system. Our method
outperforms previously published results on an established benchmark domain."
"Information Compression by Multiple Alignment, Unification and Search as
  a Unifying Principle in Computing and Cognition","This article presents an overview of the idea that ""information compression
by multiple alignment, unification and search"" (ICMAUS) may serve as a unifying
principle in computing (including mathematics and logic) and in such aspects of
human cognition as the analysis and production of natural language, fuzzy
pattern recognition and best-match information retrieval, concept hierarchies
with inheritance of attributes, probabilistic reasoning, and unsupervised
inductive learning. The ICMAUS concepts are described together with an outline
of the SP61 software model in which the ICMAUS concepts are currently realised.
A range of examples is presented, illustrated with output from the SP61 model."
Using Artificial Intelligence for Model Selection,"We apply the optimization algorithm Adaptive Simulated Annealing (ASA) to the
problem of analyzing data on a large population and selecting the best model to
predict that an individual with various traits will have a particular disease.
We compare ASA with traditional forward and backward regression on computer
simulated data. We find that the traditional methods of modeling are better for
smaller data sets whereas a numerically stable ASA seems to perform better on
larger and more complicated data sets."
Memory As A Monadic Control Construct In Problem-Solving,"Recent advances in programming languages study and design have established a
standard way of grounding computational systems representation in category
theory. These formal results led to a better understanding of issues of control
and side-effects in functional and imperative languages. This framework can be
successfully applied to the investigation of the performance of Artificial
Intelligence (AI) inference and cognitive systems. In this paper, we delineate
a categorical formalisation of memory as a control structure driving
performance in inference systems. Abstracting away control mechanisms from
three widely used representations of memory in cognitive systems (scripts,
production rules and clusters) we explain how categorical triples capture the
interaction between learning and problem-solving."
Defensive forecasting,"We consider how to make probability forecasts of binary labels. Our main
mathematical result is that for any continuous gambling strategy used for
detecting disagreement between the forecasts and the actual labels, there
exists a forecasting strategy whose forecasts are ideal as far as this gambling
strategy is concerned. A forecasting strategy obtained in this way from a
gambling strategy demonstrating a strong law of large numbers is simplified and
studied empirically."
Relation Variables in Qualitative Spatial Reasoning,"We study an alternative to the prevailing approach to modelling qualitative
spatial reasoning (QSR) problems as constraint satisfaction problems. In the
standard approach, a relation between objects is a constraint whereas in the
alternative approach it is a variable. The relation-variable approach greatly
simplifies integration and implementation of QSR. To substantiate this point,
we discuss several QSR algorithms from the literature which in the
relation-variable approach reduce to the customary constraint propagation
algorithm enforcing generalised arc-consistency."
Adaptation Knowledge Discovery from a Case Base,"In case-based reasoning, the adaptation step depends in general on
domain-dependent knowledge, which motivates studies on adaptation knowledge
acquisition (AKA). CABAMAKA is an AKA system based on principles of knowledge
discovery from databases. This system explores the variations within the case
base to elicit adaptation knowledge. It has been successfully tested in an
application of case-based decision support to breast cancer treatment."
Dependency Parsing with Dynamic Bayesian Network,"Exact parsing with finite state automata is deemed inappropriate because of
the unbounded non-locality languages overwhelmingly exhibit. We propose a way
to structure the parsing task in order to make it amenable to local
classification methods. This allows us to build a Dynamic Bayesian Network
which uncovers the syntactic dependency structure of English sentences.
Experiments with the Wall Street Journal demonstrate that the model
successfully learns from labeled data."
Neural Networks with c-NOT Gated Nodes,"We try to design a quantum neural network with qubits instead of classical
neurons with deterministic states, and also with quantum operators replacing
teh classical action potentials. With our choice of gates interconnecting teh
neural lattice, it appears that the state of the system behaves in ways
reflecting both the strengths of coupling between neurons as well as initial
conditions. We find that depending whether there is a threshold for emission
from excited to ground state, the system shows either aperiodic oscillations or
coherent ones with periodicity depending on the strength of coupling."
Arabic Speech Recognition System using CMU-Sphinx4,"In this paper we present the creation of an Arabic version of Automated
Speech Recognition System (ASR). This system is based on the open source
Sphinx-4, from the Carnegie Mellon University. Which is a speech recognition
system based on discrete hidden Markov models (HMMs). We investigate the
changes that must be made to the model to adapt Arabic voice recognition.
  Keywords: Speech recognition, Acoustic model, Arabic language, HMMs,
CMUSphinx-4, Artificial intelligence."
Symbolic sensors,"This paper deals with sensors which compute and report linguistic assessments
of their values.Such sensors, called symbolic sensors are a natural extension
of smart ones when working with control systems which use artificial
intelligence based technics. After having recalled the smart sensor concepts,
this paper introduces the symbolic sensor ones. Links between the physical
world and the symbolic one are described. It is then shown how Zadeh's
approximate reasoning theory provides a smart way to implement symbolic
sensors. Finally, since the symbolic sensor is a general component, a
functional adaptation to the measurement context is proposed."
Artificial Intelligence for Conflict Management,"Militarised conflict is one of the risks that have a significant impact on
society. Militarised Interstate Dispute (MID) is defined as an outcome of
interstate interactions, which result on either peace or conflict. Effective
prediction of the possibility of conflict between states is an important
decision support tool for policy makers. In a previous research, neural
networks (NNs) have been implemented to predict the MID. Support Vector
Machines (SVMs) have proven to be very good prediction techniques and are
introduced for the prediction of MIDs in this study and compared to neural
networks. The results show that SVMs predict MID better than NNs while NNs give
more consistent and easy to interpret sensitivity analysis than SVMs."
A Novel Model of Working Set Selection for SMO Decomposition Methods,"In the process of training Support Vector Machines (SVMs) by decomposition
methods, working set selection is an important technique, and some exciting
schemes were employed into this field. To improve working set selection, we
propose a new model for working set selection in sequential minimal
optimization (SMO) decomposition methods. In this model, it selects B as
working set without reselection. Some properties are given by simple proof, and
experiments demonstrate that the proposed method is in general faster than
existing methods."
Feature Dynamic Bayesian Networks,"Feature Markov Decision Processes (PhiMDPs) are well-suited for learning
agents in general environments. Nevertheless, unstructured (Phi)MDPs are
limited to relatively simple environments. Structured MDPs like Dynamic
Bayesian Networks (DBNs) are used for large-scale real-world problems. In this
article I extend PhiMDP to PhiDBN. The primary contribution is to derive a cost
criterion that allows to automatically extract the most relevant features from
the environment, leading to the ""best"" DBN representation. I discuss all
building blocks required for a complete general learning algorithm."
Breaking Value Symmetry,"Symmetry is an important factor in solving many constraint satisfaction
problems. One common type of symmetry is when we have symmetric values. In a
recent series of papers, we have studied methods to break value symmetries. Our
results identify computational limits on eliminating value symmetry. For
instance, we prove that pruning all symmetric values is NP-hard in general.
Nevertheless, experiments show that much value symmetry can be broken in
practice. These results may be useful to researchers in planning, scheduling
and other areas as value symmetry occurs in many different domains."
Symmetry Breaking Using Value Precedence,"We present a comprehensive study of the use of value precedence constraints
to break value symmetry. We first give a simple encoding of value precedence
into ternary constraints that is both efficient and effective at breaking
symmetry. We then extend value precedence to deal with a number of
generalizations like wreath value and partial interchangeability. We also show
that value precedence is closely related to lexicographical ordering. Finally,
we consider the interaction between value precedence and symmetry breaking
constraints for variable symmetries."
Stochastic Constraint Programming,"To model combinatorial decision problems involving uncertainty and
probability, we introduce stochastic constraint programming. Stochastic
constraint programs contain both decision variables (which we can set) and
stochastic variables (which follow a probability distribution). They combine
together the best features of traditional constraint satisfaction, stochastic
integer programming, and stochastic satisfiability. We give a semantics for
stochastic constraint programs, and propose a number of complete algorithms and
approximation procedures. Finally, we discuss a number of extensions of
stochastic constraint programming to relax various assumptions like the
independence between stochastic variables, and compare with other approaches
for decision making under uncertainty."
"Decompositions of All Different, Global Cardinality and Related
  Constraints","We show that some common and important global constraints like ALL-DIFFERENT
and GCC can be decomposed into simple arithmetic constraints on which we
achieve bound or range consistency, and in some cases even greater pruning.
These decompositions can be easily added to new solvers. They also provide
other constraints with access to the state of the propagator by sharing of
variables. Such sharing can be used to improve propagation between constraints.
We report experiments with our decomposition in a pseudo-Boolean solver."
"Reasoning about soft constraints and conditional preferences: complexity
  results and approximation techniques","Many real life optimization problems contain both hard and soft constraints,
as well as qualitative conditional preferences. However, there is no single
formalism to specify all three kinds of information. We therefore propose a
framework, based on both CP-nets and soft constraints, that handles both hard
and soft constraints as well as conditional preferences efficiently and
uniformly. We study the complexity of testing the consistency of preference
statements, and show how soft constraints can faithfully approximate the
semantics of conditional preference statements whilst improving the
computational complexity"
Multiset Ordering Constraints,"We identify a new and important global (or non-binary) constraint. This
constraint ensures that the values taken by two vectors of variables, when
viewed as multisets, are ordered. This constraint is useful for a number of
different applications including breaking symmetry and fuzzy constraint
satisfaction. We propose and implement an efficient linear time algorithm for
enforcing generalised arc consistency on such a multiset ordering constraint.
Experimental results on several problem domains show considerable promise."
Pattern Recognition Theory of Mind,"I propose that pattern recognition, memorization and processing are key
concepts that can be a principle set for the theoretical modeling of the mind
function. Most of the questions about the mind functioning can be answered by a
descriptive modeling and definitions from these principles. An understandable
consciousness definition can be drawn based on the assumption that a pattern
recognition system can recognize its own patterns of activity. The principles,
descriptive modeling and definitions can be a basis for theoretical and applied
research on cognitive sciences, particularly at artificial intelligence
studies."
How to Complete an Interactive Configuration Process?,"When configuring customizable software, it is useful to provide interactive
tool-support that ensures that the configuration does not breach given
constraints.
  But, when is a configuration complete and how can the tool help the user to
complete it?
  We formalize this problem and relate it to concepts from non-monotonic
reasoning well researched in Artificial Intelligence. The results are
interesting for both practitioners and theoreticians. Practitioners will find a
technique facilitating an interactive configuration process and experiments
supporting feasibility of the approach. Theoreticians will find links between
well-known formal concepts and a concrete practical application."
A fuzzified BRAIN algorithm for learning DNF from incomplete data,"Aim of this paper is to address the problem of learning Boolean functions
from training data with missing values. We present an extension of the BRAIN
algorithm, called U-BRAIN (Uncertainty-managing Batch Relevance-based
Artificial INtelligence), conceived for learning DNF Boolean formulas from
partial truth tables, possibly with uncertain values or missing bits.
  Such an algorithm is obtained from BRAIN by introducing fuzzy sets in order
to manage uncertainty. In the case where no missing bits are present, the
algorithm reduces to the original BRAIN."
Symmetry within Solutions,"We define the concept of an internal symmetry. This is a symmety within a
solution of a constraint satisfaction problem. We compare this to solution
symmetry, which is a mapping between different solutions of the same problem.
We argue that we may be able to exploit both types of symmetry when finding
solutions. We illustrate the potential of exploiting internal symmetries on two
benchmark domains: Van der Waerden numbers and graceful graphs. By identifying
internal symmetries we are able to extend the state of the art in both cases."
Integrating multiple sources to answer questions in Algebraic Topology,"We present in this paper an evolution of a tool from a user interface for a
concrete Computer Algebra system for Algebraic Topology (the Kenzo system), to
a front-end allowing the interoperability among different sources for
computation and deduction. The architecture allows the system not only to
interface several systems, but also to make them cooperate in shared
calculations."
Faithfulness in Chain Graphs: The Gaussian Case,"This paper deals with chain graphs under the classic
Lauritzen-Wermuth-Frydenberg interpretation. We prove that the regular Gaussian
distributions that factorize with respect to a chain graph $G$ with $d$
parameters have positive Lebesgue measure with respect to $\mathbb{R}^d$,
whereas those that factorize with respect to $G$ but are not faithful to it
have zero Lebesgue measure with respect to $\mathbb{R}^d$. This means that, in
the measure-theoretic sense described, almost all the regular Gaussian
distributions that factorize with respect to $G$ are faithful to it."
AI 3D Cybug Gaming,"In this short paper I briefly discuss 3D war Game based on artificial
intelligence concepts called AI WAR. Going in to the details, I present the
importance of CAICL language and how this language is used in AI WAR. Moreover
I also present a designed and implemented 3D War Cybug for AI WAR using CAICL
and discus the implemented strategy to defeat its enemies during the game life."
Covered Clause Elimination,"Generalizing the novel clause elimination procedures developed in [M. Heule,
M. J\""arvisalo, and A. Biere. Clause elimination procedures for CNF formulas.
In Proc. LPAR-17, volume 6397 of LNCS, pages 357-371. Springer, 2010.], we
introduce explicit (CCE), hidden (HCCE), and asymmetric (ACCE) variants of a
procedure that eliminates covered clauses from CNF formulas. We show that these
procedures are more effective in reducing CNF formulas than the respective
variants of blocked clause elimination, and may hence be interesting as new
preprocessing/simplification techniques for SAT solving."
Use of Python and Phoenix-M Interface in Robotics,"In this paper I will show how to use Python programming with a computer
interface such as Phoenix-M 1 to drive simple robots. In my quest towards
Artificial Intelligence(AI) I am experimenting with a lot of different
possibilities in Robotics. This one will try to mimic the working of a simple
insect's nervous system using hard wiring and some minimal software usage. This
is the precursor to my advanced robotics and AI integration where I plan to use
a new paradigm of AI based on Machine Learning and Self Consciousness via
Knowledge Feedback and Update Process."
Solving the Satisfiability Problem Through Boolean Networks,"In this paper we present a new approach to solve the satisfiability problem
(SAT), based on boolean networks (BN). We define a mapping between a SAT
instance and a BN, and we solve SAT problem by simulating the BN dynamics. We
prove that BN fixed points correspond to the SAT solutions. The mapping
presented allows to develop a new class of algorithms to solve SAT. Moreover,
this new approach suggests new ways to combine symbolic and connectionist
computation and provides a general framework for local search algorithms."
Teraflop-scale Incremental Machine Learning,"We propose a long-term memory design for artificial general intelligence
based on Solomonoff's incremental machine learning methods. We use R5RS Scheme
and its standard library with a few omissions as the reference machine. We
introduce a Levin Search variant based on Stochastic Context Free Grammar
together with four synergistic update algorithms that use the same grammar as a
guiding probability distribution of programs. The update algorithms include
adjusting production probabilities, re-using previous solutions, learning
programming idioms and discovery of frequent subprograms. Experiments with two
training sequences demonstrate that our approach to incremental learning is
effective."
"Semantic Similarity in a Taxonomy: An Information-Based Measure and its
  Application to Problems of Ambiguity in Natural Language","This article presents a measure of semantic similarity in an IS-A taxonomy
based on the notion of shared information content. Experimental evaluation
against a benchmark set of human similarity judgments demonstrates that the
measure performs better than the traditional edge-counting approach. The
article presents algorithms that take advantage of taxonomic similarity in
resolving syntactic and semantic ambiguity, along with experimental results
demonstrating their effectiveness."
Variational Cumulant Expansions for Intractable Distributions,"Intractable distributions present a common difficulty in inference within the
probabilistic knowledge representation framework and variational methods have
recently been popular in providing an approximate solution. In this article, we
describe a perturbational approach in the form of a cumulant expansion which,
to lowest order, recovers the standard Kullback-Leibler variational bound.
Higher-order terms describe corrections on the variational approach without
incurring much further computational cost. The relationship to other
perturbational approaches such as TAP is also elucidated. We demonstrate the
method on a particular class of undirected graphical models, Boltzmann
machines, for which our simulation results confirm improved accuracy and
enhanced stability during learning."
Solving Highly Constrained Search Problems with Quantum Computers,"A previously developed quantum search algorithm for solving 1-SAT problems in
a single step is generalized to apply to a range of highly constrained k-SAT
problems. We identify a bound on the number of clauses in satisfiability
problems for which the generalized algorithm can find a solution in a constant
number of steps as the number of variables increases. This performance
contrasts with the linear growth in the number of steps required by the best
classical algorithms, and the exponential number required by classical and
quantum methods that ignore the problem structure. In some cases, the algorithm
can also guarantee that insoluble problems in fact have no solutions, unlike
previously proposed quantum search algorithms."
Evolutionary Algorithms for Reinforcement Learning,"There are two distinct approaches to solving reinforcement learning problems,
namely, searching in value function space and searching in policy space.
Temporal difference methods and evolutionary algorithms are well-known examples
of these approaches. Kaelbling, Littman and Moore recently provided an
informative survey of temporal difference methods. This article focuses on the
application of evolutionary algorithms to the reinforcement learning problem,
emphasizing alternative policy representations, credit assignment methods, and
problem-specific genetic operators. Strengths and weaknesses of the
evolutionary approach to reinforcement learning are presented, along with a
survey of representative applications."
Reasoning about Minimal Belief and Negation as Failure,"We investigate the problem of reasoning in the propositional fragment of
MBNF, the logic of minimal belief and negation as failure introduced by
Lifschitz, which can be considered as a unifying framework for several
nonmonotonic formalisms, including default logic, autoepistemic logic,
circumscription, epistemic queries, and logic programming. We characterize the
complexity and provide algorithms for reasoning in propositional MBNF. In
particular, we show that entailment in propositional MBNF lies at the third
level of the polynomial hierarchy, hence it is harder than reasoning in all the
above mentioned propositional formalisms for nonmonotonic reasoning. We also
prove the exact correspondence between negation as failure in MBNF and negative
introspection in Moore's autoepistemic logic."
"Planning Graph as a (Dynamic) CSP: Exploiting EBL, DDB and other CSP
  Search Techniques in Graphplan","This paper reviews the connections between Graphplan's planning-graph and the
dynamic constraint satisfaction problem and motivates the need for adapting CSP
search techniques to the Graphplan algorithm. It then describes how explanation
based learning, dependency directed backtracking, dynamic variable ordering,
forward checking, sticky values and random-restart search strategies can be
adapted to Graphplan. Empirical results are provided to demonstrate that these
augmentations improve Graphplan's performance significantly (up to 1000x
speedups) on several benchmark problems. Special attention is paid to the
explanation-based learning and dependency directed backtracking techniques as
they are empirically found to be most useful in improving the performance of
Graphplan."
"On Deducing Conditional Independence from d-Separation in Causal Graphs
  with Feedback (Research Note)","Pearl and Dechter (1996) claimed that the d-separation criterion for
conditional independence in acyclic causal networks also applies to networks of
discrete variables that have feedback cycles, provided that the variables of
the system are uniquely determined by the random disturbances. I show by
example that this is not true in general. Some condition stronger than
uniqueness is needed, such as the existence of a causal dynamics guaranteed to
lead to the unique solution."
"Nonapproximability Results for Partially Observable Markov Decision
  Processes","We show that for several variations of partially observable Markov decision
processes, polynomial-time algorithms for finding control policies are unlikely
to or simply don't have guarantees of finding policies within a constant factor
or a constant summand of optimal. Here ""unlikely"" means ""unless some complexity
classes collapse,"" where the collapses considered are P=NP, P=PSPACE, or P=EXP.
Until or unless these collapses are shown to hold, any control-policy designer
must choose between such performance guarantees and efficient computation."
Mean Field Methods for a Special Class of Belief Networks,"The chief aim of this paper is to propose mean-field approximations for a
broad class of Belief networks, of which sigmoid and noisy-or networks can be
seen as special cases. The approximations are based on a powerful mean-field
theory suggested by Plefka. We show that Saul, Jaakkola and Jordan' s approach
is the first order approximation in Plefka's approach, via a variational
derivation. The application of Plefka's theory to belief networks is not
computationally tractable. To tackle this problem we propose new approximations
based on Taylor series. Small scale experiments show that the proposed schemes
are attractive."
Planning by Rewriting,"Domain-independent planning is a hard combinatorial problem. Taking into
account plan quality makes the task even more difficult. This article
introduces Planning by Rewriting (PbR), a new paradigm for efficient
high-quality domain-independent planning. PbR exploits declarative
plan-rewriting rules and efficient local search techniques to transform an
easy-to-generate, but possibly suboptimal, initial plan into a high-quality
plan. In addition to addressing the issues of planning efficiency and plan
quality, this framework offers a new anytime planning algorithm. We have
implemented this planner and applied it to several existing domains. The
experimental results show that the PbR approach provides significant savings in
planning effort while generating high-quality plans."
"Speeding Up the Convergence of Value Iteration in Partially Observable
  Markov Decision Processes","Partially observable Markov decision processes (POMDPs) have recently become
popular among many AI researchers because they serve as a natural model for
planning under uncertainty. Value iteration is a well-known algorithm for
finding optimal policies for POMDPs. It typically takes a large number of
iterations to converge. This paper proposes a method for accelerating the
convergence of value iteration. The method has been evaluated on an array of
benchmark problems and was found to be very effective: It enabled value
iteration to converge after only a few iterations on all the test problems."
"Optimizing Dialogue Management with Reinforcement Learning: Experiments
  with the NJFun System","Designing the dialogue policy of a spoken dialogue system involves many
nontrivial choices. This paper presents a reinforcement learning approach for
automatically optimizing a dialogue policy, which addresses the technical
challenges in applying reinforcement learning to a working dialogue system with
human users. We report on the design, construction and empirical evaluation of
NJFun, an experimental spoken dialogue system that provides users with access
to information about fun things to do in New Jersey. Our results show that by
optimizing its performance via reinforcement learning, NJFun measurably
improves system performance."
ATTac-2000: An Adaptive Autonomous Bidding Agent,"The First Trading Agent Competition (TAC) was held from June 22nd to July
8th, 2000. TAC was designed to create a benchmark problem in the complex domain
of e-marketplaces and to motivate researchers to apply unique approaches to a
common task. This article describes ATTac-2000, the first-place finisher in
TAC. ATTac-2000 uses a principled bidding strategy that includes several
elements of adaptivity. In addition to the success at the competition, isolated
empirical results are presented indicating the robustness and effectiveness of
ATTac-2000's adaptive strategy."
Efficient Methods for Qualitative Spatial Reasoning,"The theoretical properties of qualitative spatial reasoning in the RCC8
framework have been analyzed extensively. However, no empirical investigation
has been made yet. Our experiments show that the adaption of the algorithms
used for qualitative temporal reasoning can solve large RCC8 instances, even if
they are in the phase transition region -- provided that one uses the maximal
tractable subsets of RCC8 that have been identified by us. In particular, we
demonstrate that the orthogonal combination of heuristic methods is successful
in solving almost all apparently hard instances in the phase transition region
up to a certain size in reasonable time."
"Improving the Efficiency of Inductive Logic Programming Through the Use
  of Query Packs","Inductive logic programming, or relational learning, is a powerful paradigm
for machine learning or data mining. However, in order for ILP to become
practically useful, the efficiency of ILP systems must improve substantially.
To this end, the notion of a query pack is introduced: it structures sets of
similar queries. Furthermore, a mechanism is described for executing such query
packs. A complexity analysis shows that considerable efficiency improvements
can be achieved through the use of this query pack execution mechanism. This
claim is supported by empirical results obtained by incorporating support for
query pack execution in two existing learning systems."
Expert-Guided Subgroup Discovery: Methodology and Application,"This paper presents an approach to expert-guided subgroup discovery. The main
step of the subgroup discovery process, the induction of subgroup descriptions,
is performed by a heuristic beam search algorithm, using a novel parametrized
definition of rule quality which is analyzed in detail. The other important
steps of the proposed subgroup discovery process are the detection of
statistically significant properties of selected subgroups and subgroup
visualization: statistically significant properties are used to enrich the
descriptions of induced subgroups, while the visualization shows subgroup
properties in the form of distributions of the numbers of examples in the
subgroups. The approach is illustrated by the results obtained for a medical
problem of early detection of patient risk groups."
"An Architectural Approach to Ensuring Consistency in Hierarchical
  Execution","Hierarchical task decomposition is a method used in many agent systems to
organize agent knowledge. This work shows how the combination of a hierarchy
and persistent assertions of knowledge can lead to difficulty in maintaining
logical consistency in asserted knowledge. We explore the problematic
consequences of persistent assumptions in the reasoning process and introduce
novel potential solutions. Having implemented one of the possible solutions,
Dynamic Hierarchical Justification, its effectiveness is demonstrated with an
empirical analysis."
Learning to Coordinate Efficiently: A Model-based Approach,"In common-interest stochastic games all players receive an identical payoff.
Players participating in such games must learn to coordinate with each other in
order to receive the highest-possible value. A number of reinforcement learning
algorithms have been proposed for this problem, and some have been shown to
converge to good solutions in the limit. In this paper we show that using very
simple model-based algorithms, much better (i.e., polynomial) convergence rates
can be attained. Moreover, our model-based algorithms are guaranteed to
converge to the optimal value, unlike many of the existing algorithms."
"Temporal Decision Trees: Model-based Diagnosis of Dynamic Systems
  On-Board","The automatic generation of decision trees based on off-line reasoning on
models of a domain is a reasonable compromise between the advantages of using a
model-based approach in technical domains and the constraints imposed by
embedded applications. In this paper we extend the approach to deal with
temporal information. We introduce a notion of temporal decision tree, which is
designed to make use of relevant information as long as it is acquired, and we
present an algorithm for compiling such trees from a model-based reasoning
system."
"A New Technique for Combining Multiple Classifiers using The
  Dempster-Shafer Theory of Evidence","This paper presents a new classifier combination technique based on the
Dempster-Shafer theory of evidence. The Dempster-Shafer theory of evidence is a
powerful method for combining measures of evidence from different classifiers.
However, since each of the available methods that estimates the evidence of
classifiers has its own limitations, we propose here a new implementation which
adapts to training data so that the overall mean square error is minimized. The
proposed technique is shown to outperform most available classifier combination
methods when tested on three different classification problems."
Can We Learn to Beat the Best Stock,"A novel algorithm for actively trading stocks is presented. While traditional
expert advice and ""universal"" algorithms (as well as standard technical trading
heuristics) attempt to predict winners or trends, our approach relies on
predictable statistical relations between all pairs of stocks in the market.
Our empirical results on historical markets provide strong evidence that this
type of technical trading can ""beat the market"" and moreover, can beat the best
stock in the market. In doing so we utilize a new idea for smoothing critical
parameters in the context of expert learning."
Restricted Value Iteration: Theory and Algorithms,"Value iteration is a popular algorithm for finding near optimal policies for
POMDPs. It is inefficient due to the need to account for the entire belief
space, which necessitates the solution of large numbers of linear programs. In
this paper, we study value iteration restricted to belief subsets. We show
that, together with properly chosen belief subsets, restricted value iteration
yields near-optimal policies and we give a condition for determining whether a
given belief subset would bring about savings in space and time. We also apply
restricted value iteration to two interesting classes of POMDPs, namely
informative POMDPs and near-discernible POMDPs."
"Towards a Reliable Framework of Uncertainty-Based Group Decision Support
  System","This study proposes a framework of Uncertainty-based Group Decision Support
System (UGDSS). It provides a platform for multiple criteria decision analysis
in six aspects including (1) decision environment, (2) decision problem, (3)
decision group, (4) decision conflict, (5) decision schemes and (6) group
negotiation. Based on multiple artificial intelligent technologies, this
framework provides reliable support for the comprehensive manipulation of
applications and advanced decision approaches through the design of an
integrated multi-agents architecture."
"An Information Theoretic Representation of Agent Dynamics as Set
  Intersections","We represent agents as sets of strings. Each string encodes a potential
interaction with another agent or environment. We represent the total set of
dynamics between two agents as the intersection of their respective strings, we
prove complexity properties of player interactions using Algorithmic
Information Theory. We show how the proposed construction is compatible with
Universal Artificial Intelligence, in that the AIXI model can be seen as
universal with respect to interaction."
"Generalized Fast Approximate Energy Minimization via Graph Cuts:
  Alpha-Expansion Beta-Shrink Moves","We present alpha-expansion beta-shrink moves, a simple generalization of the
widely-used alpha-beta swap and alpha-expansion algorithms for approximate
energy minimization. We show that in a certain sense, these moves dominate both
alpha-beta-swap and alpha-expansion moves, but unlike previous generalizations
the new moves require no additional assumptions and are still solvable in
polynomial-time. We show promising experimental results with the new moves,
which we believe could be used in any context where alpha-expansions are
currently employed."
"On the Practical use of Variable Elimination in Constraint Optimization
  Problems: 'Still-life' as a Case Study","Variable elimination is a general technique for constraint processing. It is
often discarded because of its high space complexity. However, it can be
extremely useful when combined with other techniques. In this paper we study
the applicability of variable elimination to the challenging problem of finding
still-lifes. We illustrate several alternatives: variable elimination as a
stand-alone algorithm, interleaved with search, and as a source of good quality
lower bounds. We show that these techniques are the best known option both
theoretically and empirically. In our experiments we have been able to solve
the n=20 instance, which is far beyond reach with alternative approaches."
Reasoning about Action: An Argumentation - Theoretic Approach,"We present a uniform non-monotonic solution to the problems of reasoning
about action on the basis of an argumentation-theoretic approach. Our theory is
provably correct relative to a sensible minimisation policy introduced on top
of a temporal propositional logic. Sophisticated problem domains can be
formalised in our framework. As much attention of researchers in the field has
been paid to the traditional and basic problems in reasoning about actions such
as the frame, the qualification and the ramification problems, approaches to
these problems within our formalisation lie at heart of the expositions
presented in this paper."
Logical Hidden Markov Models,"Logical hidden Markov models (LOHMMs) upgrade traditional hidden Markov
models to deal with sequences of structured symbols in the form of logical
atoms, rather than flat characters.
  This note formally introduces LOHMMs and presents solutions to the three
central inference problems for LOHMMs: evaluation, most likely hidden state
sequence and parameter estimation. The resulting representation and algorithms
are experimentally evaluated on problems from the domain of bioinformatics."
mGPT: A Probabilistic Planner Based on Heuristic Search,"We describe the version of the GPT planner used in the probabilistic track of
the 4th International Planning Competition (IPC-4). This version, called mGPT,
solves Markov Decision Processes specified in the PPDDL language by extracting
and using different classes of lower bounds along with various heuristic-search
algorithms. The lower bounds are extracted from deterministic relaxations where
the alternative probabilistic effects of an action are mapped into different,
independent, deterministic actions. The heuristic-search algorithms use these
lower bounds for focusing the updates and delivering a consistent value
function over all states reachable from the initial state and the greedy
policy."
Optiplan: Unifying IP-based and Graph-based Planning,"The Optiplan planning system is the first integer programming-based planner
that successfully participated in the international planning competition. This
engineering note describes the architecture of Optiplan and provides the
integer programming formulation that enabled it to perform reasonably well in
the competition. We also touch upon some recent developments that make integer
programming encodings significantly more competitive."
PDDL2.1 - The Art of the Possible? Commentary on Fox and Long,"PDDL2.1 was designed to push the envelope of what planning algorithms can do,
and it has succeeded. It adds two important features: durative actions,which
take time (and may have continuous effects); and objective functions for
measuring the quality of plans. The concept of durative actions is flawed; and
the treatment of their semantics reveals too strong an attachment to the way
many contemporary planners work. Future PDDL innovators should focus on
producing a clean semantics for additions to the language, and let planner
implementers worry about coupling their algorithms to problems expressed in the
latest version of the language."
The Case for Durative Actions: A Commentary on PDDL2.1,"The addition of durative actions to PDDL2.1 sparked some controversy. Fox and
Long argued that actions should be considered as instantaneous, but can start
and stop processes. Ultimately, a limited notion of durative actions was
incorporated into the language. I argue that this notion is still impoverished,
and that the underlying philosophical position of regarding durative actions as
being a shorthand for a start action, process, and stop action ignores the
realities of modelling and execution for complex systems."
Generative Prior Knowledge for Discriminative Classification,"We present a novel framework for integrating prior knowledge into
discriminative classifiers. Our framework allows discriminative classifiers
such as Support Vector Machines (SVMs) to utilize prior knowledge specified in
the generative setting. The dual objective of fitting the data and respecting
prior knowledge is formulated as a bilevel program, which is solved
(approximately) via iterative application of second-order cone programming. To
test our approach, we consider the problem of using WordNet (a semantic
database of English language) to improve low-sample classification accuracy of
newsgroup categorization. WordNet is viewed as an approximate, but readily
available source of background knowledge, and our framework is capable of
utilizing it in a flexible way."
"Properties and Applications of Programs with Monotone and Convex
  Constraints","We study properties of programs with monotone and convex constraints. We
extend to these formalisms concepts and results from normal logic programming.
They include the notions of strong and uniform equivalence with their
characterizations, tight programs and Fages Lemma, program completion and loop
formulas. Our results provide an abstract account of properties of some recent
extensions of logic programming with aggregates, especially the formalism of
lparse programs. They imply a method to compute stable models of lparse
programs by means of off-the-shelf solvers of pseudo-boolean constraints, which
is often much faster than the smodels system."
Solving Factored MDPs with Hybrid State and Action Variables,"Efficient representations and solutions for large decision problems with
continuous and discrete variables are among the most important challenges faced
by the designers of automated decision support systems. In this paper, we
describe a novel hybrid factored Markov decision process (MDP) model that
allows for a compact representation of these problems, and a new hybrid
approximate linear programming (HALP) framework that permits their efficient
solutions. The central idea of HALP is to approximate the optimal value
function by a linear combination of basis functions and optimize its weights by
linear programming. We analyze both theoretical and computational aspects of
this approach, and demonstrate its scale-up potential on several hybrid
optimization problems."
Learning Symbolic Models of Stochastic Domains,"In this article, we work towards the goal of developing agents that can learn
to act in complex worlds. We develop a probabilistic, relational planning rule
representation that compactly models noisy, nondeterministic action effects,
and show how such rules can be effectively learned. Through experiments in
simple planning domains and a 3D simulated blocks world with realistic physics,
we demonstrate that this learning algorithm allows agents to effectively model
world dynamics."
Combining Spatial and Temporal Logics: Expressiveness vs. Complexity,"In this paper, we construct and investigate a hierarchy of spatio-temporal
formalisms that result from various combinations of propositional spatial and
temporal logics such as the propositional temporal logic PTL, the spatial
logics RCC-8, BRCC-8, S4u and their fragments. The obtained results give a
clear picture of the trade-off between expressiveness and computational
realisability within the hierarchy. We demonstrate how different combining
principles as well as spatial and temporal primitives can produce NP-, PSPACE-,
EXPSPACE-, 2EXPSPACE-complete, and even undecidable spatio-temporal logics out
of components that are at most NP- or PSPACE-complete."
The Power of Modeling - a Response to PDDL2.1,"In this commentary I argue that although PDDL is a very useful standard for
the planning competition, its design does not properly consider the issue of
domain modeling. Hence, I would not advocate its use in specifying planning
domains outside of the context of the planning competition. Rather, the field
needs to explore different approaches and grapple more directly with the
problem of effectively modeling and utilizing all of the diverse pieces of
knowledge we typically have about planning domains."
Auctions with Severely Bounded Communication,"We study auctions with severe bounds on the communication allowed: each
bidder may only transmit t bits of information to the auctioneer. We consider
both welfare- and profit-maximizing auctions under this communication
restriction. For both measures, we determine the optimal auction and show that
the loss incurred relative to unconstrained auctions is mild. We prove
non-surprising properties of these kinds of auctions, e.g., that in optimal
mechanisms bidders simply report the interval in which their valuation lies in,
as well as some surprising properties, e.g., that asymmetric auctions are
better than symmetric ones and that multi-round auctions reduce the
communication complexity only by a linear factor."
Marvin: A Heuristic Search Planner with Online Macro-Action Learning,"This paper describes Marvin, a planner that competed in the Fourth
International Planning Competition (IPC 4). Marvin uses
action-sequence-memoisation techniques to generate macro-actions, which are
then used during search for a solution plan. We provide an overview of its
architecture and search behaviour, detailing the algorithms used. We also
empirically demonstrate the effectiveness of its features in various planning
domains; in particular, the effects on performance due to the use of
macro-actions, the novel features of its search behaviour, and the native
support of ADL and Derived Predicates."
Discovering Classes of Strongly Equivalent Logic Programs,"In this paper we apply computer-aided theorem discovery technique to discover
theorems about strongly equivalent logic programs under the answer set
semantics. Our discovered theorems capture new classes of strongly equivalent
logic programs that can lead to new program simplification rules that preserve
strong equivalence. Specifically, with the help of computers, we discovered
exact conditions that capture the strong equivalence between a rule and the
empty set, between two rules, between two rules and one of the two rules,
between two rules and another rule, and between three rules and two of the
three rules."
Phase Transition for Random Quantified XOR-Formulas,"The QXORSAT problem is the quantified version of the satisfiability problem
XORSAT in which the connective exclusive-or is used instead of the usual or. We
study the phase transition associated with random QXORSAT instances. We give a
description of this phase transition in the case of one alternation of
quantifiers, thus performing an advanced practical and theoretical study on the
phase transition of a quantified roblem."
"Probabilistic Planning via Heuristic Forward Search and Weighted Model
  Counting","We present a new algorithm for probabilistic planning with no observability.
Our algorithm, called Probabilistic-FF, extends the heuristic forward-search
machinery of Conformant-FF to problems with probabilistic uncertainty about
both the initial state and action effects. Specifically, Probabilistic-FF
combines Conformant-FFs techniques with a powerful machinery for weighted model
counting in (weighted) CNFs, serving to elegantly define both the search space
and the heuristic function. Our evaluation of Probabilistic-FF shows its fine
scalability in a range of probabilistic domains, constituting a several orders
of magnitude improvement over previous results in this area. We use a
problematic case to point out the main open issue to be addressed by further
research."
The Complexity of Planning Problems With Simple Causal Graphs,"We present three new complexity results for classes of planning problems with
simple causal graphs. First, we describe a polynomial-time algorithm that uses
macros to generate plans for the class 3S of planning problems with binary
state variables and acyclic causal graphs. This implies that plan generation
may be tractable even when a planning problem has an exponentially long minimal
solution. We also prove that the problem of plan existence for planning
problems with multi-valued variables and chain causal graphs is NP-hard.
Finally, we show that plan existence for planning problems with binary state
variables and polytree causal graphs is NP-complete."
A Bayesian Model for Plan Recognition in RTS Games applied to StarCraft,"The task of keyhole (unobtrusive) plan recognition is central to adaptive
game AI. ""Tech trees"" or ""build trees"" are the core of real-time strategy (RTS)
game strategic (long term) planning. This paper presents a generic and simple
Bayesian model for RTS build tree prediction from noisy observations, which
parameters are learned from replays (game logs). This unsupervised machine
learning approach involves minimal work for the game developers as it leverage
players' data (com- mon in RTS). We applied it to StarCraft1 and showed that it
yields high quality and robust predictions, that can feed an adaptive AI."
Reasoning about Unreliable Actions,"We analyse the philosopher Davidson's semantics of actions, using a strongly
typed logic with contexts given by sets of partial equations between the
outcomes of actions. This provides a perspicuous and elegant treatment of
reasoning about action, analogous to Reiter's work on artificial intelligence.
We define a sequent calculus for this logic, prove cut elimination, and give a
semantics based on fibrations over partial cartesian categories: we give a
structure theory for such fibrations. The existence of lax comma objects is
necessary for the proof of cut elimination, and we give conditions on the
domain fibration of a partial cartesian category for such comma objects to
exist."
Three new sensitivity analysis methods for influence diagrams,"Performing sensitivity analysis for influence diagrams using the decision
circuit framework is particularly convenient, since the partial derivatives
with respect to every parameter are readily available [Bhattacharjya and
Shachter, 2007; 2008]. In this paper we present three non-linear sensitivity
analysis methods that utilize this partial derivative information and therefore
do not require re-evaluating the decision situation multiple times.
Specifically, we show how to efficiently compare strategies in decision
situations, perform sensitivity to risk aversion and compute the value of
perfect hedging [Seyller, 2008]."
Distribution over Beliefs for Memory Bounded Dec-POMDP Planning,"We propose a new point-based method for approximate planning in Dec-POMDP
which outperforms the state-of-the-art approaches in terms of solution quality.
It uses a heuristic estimation of the prior probability of beliefs to choose a
bounded number of policy trees: this choice is formulated as a combinatorial
optimisation problem minimising the error induced by pruning."
BEEM : Bucket Elimination with External Memory,"A major limitation of exact inference algorithms for probabilistic graphical
models is their extensive memory usage, which often puts real-world problems
out of their reach. In this paper we show how we can extend inference
algorithms, particularly Bucket Elimination, a special case of cluster (join)
tree decomposition, to utilize disk memory. We provide the underlying ideas and
show promising empirical results of exactly solving large problems not solvable
before."
Solving Hybrid Influence Diagrams with Deterministic Variables,"We describe a framework and an algorithm for solving hybrid influence
diagrams with discrete, continuous, and deterministic chance variables, and
discrete and continuous decision variables. A continuous chance variable in an
influence diagram is said to be deterministic if its conditional distributions
have zero variances. The solution algorithm is an extension of Shenoy's fusion
algorithm for discrete influence diagrams. We describe an extended
Shenoy-Shafer architecture for propagation of discrete, continuous, and utility
potentials in hybrid influence diagrams that include deterministic chance
variables. The algorithm and framework are illustrated by solving two small
examples."
"A Delayed Column Generation Strategy for Exact k-Bounded MAP Inference
  in Markov Logic Networks","The paper introduces k-bounded MAP inference, a parameterization of MAP
inference in Markov logic networks. k-Bounded MAP states are MAP states with at
most k active ground atoms of hidden (non-evidence) predicates. We present a
novel delayed column generation algorithm and provide empirical evidence that
the algorithm efficiently computes k-bounded MAP states for meaningful
real-world graph matching problems. The underlying idea is that, instead of
solving one large optimization problem, it is often more efficient to tackle
several small ones."
"Comparative Analysis of Probabilistic Models for Activity Recognition
  with an Instrumented Walker","Rollating walkers are popular mobility aids used by older adults to improve
balance control. There is a need to automatically recognize the activities
performed by walker users to better understand activity patterns, mobility
issues and the context in which falls are more likely to happen. We design and
compare several techniques to recognize walker related activities. A
comprehensive evaluation with control subjects and walker users from a
retirement community is presented."
Confounding Equivalence in Causal Inference,"The paper provides a simple test for deciding, from a given causal diagram,
whether two sets of variables have the same bias-reducing potential under
adjustment. The test requires that one of the following two conditions holds:
either (1) both sets are admissible (i.e., satisfy the back-door criterion) or
(2) the Markov boundaries surrounding the manipulated variable(s) are identical
in both sets. Applications to covariate selection and model testing are
discussed."
"Characterizing the Set of Coherent Lower Previsions with a Finite Number
  of Constraints or Vertices","The standard coherence criterion for lower previsions is expressed using an
infinite number of linear constraints. For lower previsions that are
essentially defined on some finite set of gambles on a finite possibility
space, we present a reformulation of this criterion that only uses a finite
number of constraints. Any such lower prevision is coherent if it lies within
the convex polytope defined by these constraints. The vertices of this polytope
are the extreme coherent lower previsions for the given set of gambles. Our
reformulation makes it possible to compute them. We show how this is done and
illustrate the procedure and its results."
Dynamic programming in in uence diagrams with decision circuits,"Decision circuits perform efficient evaluation of influence diagrams,
building on the ad- vances in arithmetic circuits for belief net- work
inference [Darwiche, 2003; Bhattachar- jya and Shachter, 2007]. We show how
even more compact decision circuits can be con- structed for dynamic
programming in influ- ence diagrams with separable value functions and
conditionally independent subproblems. Once a decision circuit has been
constructed based on the diagram's ""global"" graphical structure, it can be
compiled to exploit ""lo- cal"" structure for efficient evaluation and sen-
sitivity analysis."
Modeling Events with Cascades of Poisson Processes,"We present a probabilistic model of events in continuous time in which each
event triggers a Poisson process of successor events. The ensemble of observed
events is thereby modeled as a superposition of Poisson processes. Efficient
inference is feasible under this model with an EM algorithm. Moreover, the EM
algorithm can be implemented as a distributed algorithm, permitting the model
to be applied to very large datasets. We apply these techniques to the modeling
of Twitter messages and the revision history of Wikipedia."
Bayesian Model Averaging Using the k-best Bayesian Network Structures,"We study the problem of learning Bayesian network structures from data. We
develop an algorithm for finding the k-best Bayesian network structures. We
propose to compute the posterior probabilities of hypotheses of interest by
Bayesian model averaging over the k-best Bayesian networks. We present
empirical results on structural discovery over several real and synthetic data
sets and show that the method outperforms the model selection method and the
state of-the-art MCMC methods."
Truthful Feedback for Sanctioning Reputation Mechanisms,"For product rating environments, similar to that of Amazon Reviews, it has
been shown that the truthful elicitation of feedback is possible through
mechanisms which pay buyer reports contingent on the reports of other buyers.
We study whether similar mechanisms can be designed for reputation mechanisms
at online auction sites where the buyers' experiences are partially determined
by a strategic seller. We show that this is impossible for the basic setting.
However, introducing a small prior belief that the seller is a cooperative
commitment player leads to a payment scheme with a truthful perfect Bayesian
equilibrium."
Solving Multistage Influence Diagrams using Branch-and-Bound Search,"A branch-and-bound approach to solving influ- ence diagrams has been
previously proposed in the literature, but appears to have never been
implemented and evaluated - apparently due to the difficulties of computing
effective bounds for the branch-and-bound search. In this paper, we describe
how to efficiently compute effective bounds, and we develop a practical
implementa- tion of depth-first branch-and-bound search for influence diagram
evaluation that outperforms existing methods for solving influence diagrams
with multiple stages."
Multiple faults diagnosis using causal graph,"This work proposes to put up a tool for diagnosing multi faults based on
model using techniques of detection and localization inspired from the
community of artificial intelligence and that of automatic. The diagnostic
procedure to be integrated into the supervisory system must therefore be
provided with explanatory features. Techniques based on causal reasoning are a
pertinent approach for this purpose. Bond graph modeling is used to describe
the cause effect relationship between process variables. Experimental results
are presented and discussed in order to compare performance of causal graph
technique and classic methods inspired from artificial intelligence (DX) and
control theory (FDI)."
"Development of knowledge Base Expert System for Natural treatment of
  Diabetes disease","The development of expert system for treatment of Diabetes disease by using
natural methods is new information technology derived from Artificial
Intelligent research using ESTA (Expert System Text Animation) System. The
proposed expert system contains knowledge about various methods of natural
treatment methods (Massage, Herbal/Proper Nutrition, Acupuncture, Gems) for
Diabetes diseases of Human Beings. The system is developed in the ESTA (Expert
System shell for Text Animation) which is Visual Prolog 7.3 Application. The
knowledge for the said system will be acquired from domain experts, texts and
other related sources."
Constraint Processing in Lifted Probabilistic Inference,"First-order probabilistic models combine representational power of
first-order logic with graphical models. There is an ongoing effort to design
lifted inference algorithms for first-order probabilistic models. We analyze
lifted inference from the perspective of constraint processing and, through
this viewpoint, we analyze and compare existing approaches and expose their
advantages and limitations. Our theoretical results show that the wrong choice
of constraint processing method can lead to exponential increase in
computational complexity. Our empirical tests confirm the importance of
constraint processing in lifted inference. This is the first theoretical and
empirical study of constraint processing in lifted inference."
Deterministic POMDPs Revisited,"We study a subclass of POMDPs, called Deterministic POMDPs, that is
characterized by deterministic actions and observations. These models do not
provide the same generality of POMDPs yet they capture a number of interesting
and challenging problems, and permit more efficient algorithms. Indeed, some of
the recent work in planning is built around such assumptions mainly by the
quest of amenable models more expressive than the classical deterministic
models. We provide results about the fundamental properties of Deterministic
POMDPs, their relation with AND/OR search problems and algorithms, and their
computational complexity."
AND/OR Importance Sampling,"The paper introduces AND/OR importance sampling for probabilistic graphical
models. In contrast to importance sampling, AND/OR importance sampling caches
samples in the AND/OR space and then extracts a new sample mean from the stored
samples. We prove that AND/OR importance sampling may have lower variance than
importance sampling; thereby providing a theoretical justification for
preferring it over importance sampling. Our empirical evaluation demonstrates
that AND/OR importance sampling is far more accurate than importance sampling
in many cases."
Identifying reasoning patterns in games,"We present an algorithm that identifies the reasoning patterns of agents in a
game, by iteratively examining the graph structure of its Multi-Agent Influence
Diagram (MAID) representation. If the decision of an agent participates in no
reasoning patterns, then we can effectively ignore that decision for the
purpose of calculating a Nash equilibrium for the game. In some cases, this can
lead to exponential time savings in the process of equilibrium calculation.
Moreover, our algorithm can be used to enumerate the reasoning patterns in a
game, which can be useful for constructing more effective computerized agents
interacting with humans."
Complexity of Inference in Graphical Models,"It is well-known that inference in graphical models is hard in the worst
case, but tractable for models with bounded treewidth. We ask whether treewidth
is the only structural criterion of the underlying graph that enables tractable
inference. In other words, is there some class of structures with unbounded
treewidth in which inference is tractable? Subject to a combinatorial
hypothesis due to Robertson et al. (1994), we show that low treewidth is indeed
the only structural restriction that can ensure tractability. Thus, even for
the ""best case"" graph structure, there is no inference algorithm with
complexity polynomial in the treewidth."
Sampling First Order Logical Particles,"Approximate inference in dynamic systems is the problem of estimating the
state of the system given a sequence of actions and partial observations. High
precision estimation is fundamental in many applications like diagnosis,
natural language processing, tracking, planning, and robotics. In this paper we
present an algorithm that samples possible deterministic executions of a
probabilistic sequence. The algorithm takes advantage of a compact
representation (using first order logic) for actions and world states to
improve the precision of its estimation. Theoretical and empirical results show
that the algorithm's expected error is smaller than propositional sampling and
Sequential Monte Carlo (SMC) sampling techniques."
Improving Gradient Estimation by Incorporating Sensor Data,"An efficient policy search algorithm should estimate the local gradient of
the objective function, with respect to the policy parameters, from as few
trials as possible. Whereas most policy search methods estimate this gradient
by observing the rewards obtained during policy trials, we show, both
theoretically and empirically, that taking into account the sensor data as well
gives better gradient estimates and hence faster learning. The reason is that
rewards obtained during policy execution vary from trial to trial due to noise
in the environment; sensor data, which correlates with the noise, can be used
to partially correct for this variation, resulting in an estimatorwith lower
variance."
Explanation Trees for Causal Bayesian Networks,"Bayesian networks can be used to extract explanations about the observed
state of a subset of variables. In this paper, we explicate the desiderata of
an explanation and confront them with the concept of explanation proposed by
existing methods. The necessity of taking into account causal approaches when a
causal graph is available is discussed. We then introduce causal explanation
trees, based on the construction of explanation trees using the measure of
causal information ow (Ay and Polani, 2006). This approach is compared to
several other methods on known networks."
Model-Based Bayesian Reinforcement Learning in Large Structured Domains,"Model-based Bayesian reinforcement learning has generated significant
interest in the AI community as it provides an elegant solution to the optimal
exploration-exploitation tradeoff in classical reinforcement learning.
Unfortunately, the applicability of this type of approach has been limited to
small domains due to the high complexity of reasoning about the joint posterior
over model parameters. In this paper, we consider the use of factored
representations combined with online planning techniques, to improve
scalability of these methods. The main contribution of this paper is a Bayesian
framework for learning the structure and parameters of a dynamical system,
while also simultaneously planning a (near-)optimal sequence of actions."
Bounding Search Space Size via (Hyper)tree Decompositions,"This paper develops a measure for bounding the performance of AND/OR search
algorithms for solving a variety of queries over graphical models. We show how
drawing a connection to the recent notion of hypertree decompositions allows to
exploit determinism in the problem specification and produce tighter bounds. We
demonstrate on a variety of practical problem instances that we are often able
to improve upon existing bounds by several orders of magnitude."
New Techniques for Algorithm Portfolio Design,"We present and evaluate new techniques for designing algorithm portfolios. In
our view, the problem has both a scheduling aspect and a machine learning
aspect. Prior work has largely addressed one of the two aspects in isolation.
Building on recent work on the scheduling aspect of the problem, we present a
technique that addresses both aspects simultaneously and has attractive
theoretical guarantees. Experimentally, we show that this technique can be used
to improve the performance of state-of-the-art algorithms for Boolean
satisfiability, zero-one integer programming, and A.I. planning."
Refractor Importance Sampling,"In this paper we introduce Refractor Importance Sampling (RIS), an
improvement to reduce error variance in Bayesian network importance sampling
propagation under evidential reasoning. We prove the existence of a collection
of importance functions that are close to the optimal importance function under
evidential reasoning. Based on this theoretic result we derive the RIS
algorithm. RIS approaches the optimal importance function by applying localized
arc changes to minimize the divergence between the evidence-adjusted importance
function and the optimal importance function. The validity and performance of
RIS is empirically tested with a large setof synthetic Bayesian networks and
two real-world networks."
Inference for Multiplicative Models,"The paper introduces a generalization for known probabilistic models such as
log-linear and graphical models, called here multiplicative models. These
models, that express probabilities via product of parameters are shown to
capture multiple forms of contextual independence between variables, including
decision graphs and noisy-OR functions. An inference algorithm for
multiplicative models is provided and its correctness is proved. The complexity
analysis of the inference algorithm uses a more refined parameter than the
tree-width of the underlying graph, and shows the computational cost does not
exceed that of the variable elimination algorithm in graphical models. The
paper ends with examples where using the new models and algorithm is
computationally beneficial."
Large-Flip Importance Sampling,"We propose a new Monte Carlo algorithm for complex discrete distributions.
The algorithm is motivated by the N-Fold Way, which is an ingenious
event-driven MCMC sampler that avoids rejection moves at any specific state.
The N-Fold Way can however get ""trapped"" in cycles. We surmount this problem by
modifying the sampling process. This correction does introduce bias, but the
bias is subsequently corrected with a carefully engineered importance sampler."
Causal Reasoning in Graphical Time Series Models,"We propose a definition of causality for time series in terms of the effect
of an intervention in one component of a multivariate time series on another
component at some later point in time. Conditions for identifiability,
comparable to the back-door and front-door criteria, are presented and can also
be verified graphically. Computation of the causal effect is derived and
illustrated for the linear case."
Minimax regret based elicitation of generalized additive utilities,"We describe the semantic foundations for elicitation of generalized
additively independent (GAI) utilities using the minimax regret criterion, and
propose several new query types and strategies for this purpose. Computational
feasibility is obtained by exploiting the local GAI structure in the model. Our
results provide a practical approach for implementing preference-based
constrained configuration optimization as well as effective search in
multiattribute product databases."
Polynomial Constraints in Causal Bayesian Networks,"We use the implicitization procedure to generate polynomial equality
constraints on the set of distributions induced by local interventions on
variables governed by a causal Bayesian network with hidden variables. We show
how we may reduce the complexity of the implicitization problem and make the
problem tractable in certain causal Bayesian networks. We also show some
preliminary results on the algebraic structure of polynomial constraints. The
results have applications in distinguishing between causal models and in
testing causal models with combined observational and experimental data."
Accuracy Bounds for Belief Propagation,"The belief propagation (BP) algorithm is widely applied to perform
approximate inference on arbitrary graphical models, in part due to its
excellent empirical properties and performance. However, little is known
theoretically about when this algorithm will perform well. Using recent
analysis of convergence and stability properties in BP and new results on
approximations in binary systems, we derive a bound on the error in BP's
estimates for pairwise Markov random fields over discrete valued random
variables. Our bound is relatively simple to compute, and compares favorably
with a previous method of bounding the accuracy of BP."
What Counterfactuals Can Be Tested,"Counterfactual statements, e.g., ""my headache would be gone had I taken an
aspirin"" are central to scientific discourse, and are formally interpreted as
statements derived from ""alternative worlds"". However, since they invoke
hypothetical states of affairs, often incompatible with what is actually known
or observed, testing counterfactuals is fraught with conceptual and practical
difficulties. In this paper, we provide a complete characterization of
""testable counterfactuals,"" namely, counterfactual statements whose
probabilities can be inferred from physical experiments. We provide complete
procedures for discerning whether a given counterfactual is testable and, if
so, expressing its probability in terms of experimental data."
Improved Memory-Bounded Dynamic Programming for Decentralized POMDPs,"Memory-Bounded Dynamic Programming (MBDP) has proved extremely effective in
solving decentralized POMDPs with large horizons. We generalize the algorithm
and improve its scalability by reducing the complexity with respect to the
number of observations from exponential to polynomial. We derive error bounds
on solution quality with respect to this new approximation and analyze the
convergence behavior. To evaluate the effectiveness of the improvements, we
introduce a new, larger benchmark problem. Experimental results show that
despite the high complexity of decentralized POMDPs, scalable solution
techniques such as MBDP perform surprisingly well."
On the Robustness of Most Probable Explanations,"In Bayesian networks, a Most Probable Explanation (MPE) is a complete
variable instantiation with a highest probability given the current evidence.
In this paper, we discuss the problem of finding robustness conditions of the
MPE under single parameter changes. Specifically, we ask the question: How much
change in a single network parameter can we afford to apply while keeping the
MPE unchanged? We will describe a procedure, which is the first of its kind,
that computes this answer for each parameter in the Bayesian network variable
in time O(n exp(w)), where n is the number of network variables and w is its
treewidth."
Inequality Constraints in Causal Models with Hidden Variables,"We present a class of inequality constraints on the set of distributions
induced by local interventions on variables governed by a causal Bayesian
network, in which some of the variables remain unmeasured. We derive bounds on
causal effects that are not directly measured in randomized experiments. We
derive instrumental inequality type of constraints on nonexperimental
distributions. The results have applications in testing causal models with
observational or experimental data."
A new axiomatization for likelihood gambles,"This paper studies a new and more general axiomatization than one presented
previously for preference on likelihood gambles. Likelihood gambles describe
actions in a situation where a decision maker knows multiple probabilistic
models and a random sample generated from one of those models but does not know
prior probability of models. This new axiom system is inspired by Jensen's
axiomatization of probabilistic gambles. Our approach provides a new
perspective to the role of data in decision making under ambiguity. It avoids
one of the most controversial issue of Bayesian methodology namely the
assumption of prior probability."
Direct and Indirect Effects of Sequential Treatments,"In this paper we review the notion of direct causal effect as introduced by
Pearl (2001). We show how it can be formulated without counterfactuals, using
intervention indicators instead. This allows to consider the natural direct
effect as a special case of sequential treatments discussed by Dawid and
Didelez (2005) which immediately yields conditions for identifiability as well
as a graphical way of checking identifiability. The results are contrasted with
the criteria given by Pearl (2001) and Robins (2003)."
From influence diagrams to multi-operator cluster DAGs,"There exist several architectures to solve influence diagrams using local
computations, such as the Shenoy-Shafer, the HUGIN, or the Lazy Propagation
architectures. They all extend usual variable elimination algorithms thanks to
the use of so-called 'potentials'. In this paper, we introduce a new
architecture, called the Multi-operator Cluster DAG architecture, which can
produce decompositions with an improved constrained induced-width, and
therefore induce potentially exponential gains. Its principle is to benefit
from the composite nature of influence diagrams, instead of using uniform
potentials, in order to better analyze the problem structure."
Approximate Separability for Weak Interaction in Dynamic Systems,"One approach to monitoring a dynamic system relies on decomposition of the
system into weakly interacting subsystems. An earlier paper introduced a notion
of weak interaction called separability, and showed that it leads to exact
propagation of marginals for prediction. This paper addresses two questions
left open by the earlier paper: can we define a notion of approximate
separability that occurs naturally in practice, and do separability and
approximate separability lead to accurate monitoring? The answer to both
questions is afirmative. The paper also analyzes the structure of approximately
separable decompositions, and provides some explanation as to why these models
perform well."
Identifying the Relevant Nodes Without Learning the Model,"We propose a method to identify all the nodes that are relevant to compute
all the conditional probability distributions for a given set of nodes. Our
method is simple, effcient, consistent, and does not require learning a
Bayesian network first. Therefore, our method can be applied to
high-dimensional databases, e.g. gene expression databases."
Belief Update in CLG Bayesian Networks With Lazy Propagation,"In recent years Bayesian networks (BNs) with a mixture of continuous and
discrete variables have received an increasing level of attention. We present
an architecture for exact belief update in Conditional Linear Gaussian BNs (CLG
BNs). The architecture is an extension of lazy propagation using operations of
Lauritzen & Jensen [6] and Cowell [2]. By decomposing clique and separator
potentials into sets of factors, the proposed architecture takes advantage of
independence and irrelevance properties induced by the structure of the graph
and the evidence. The resulting benefits are illustrated by examples. Results
of a preliminary empirical performance evaluation indicate a significant
potential of the proposed architecture."
Reasoning about Uncertainty in Metric Spaces,"We set up a model for reasoning about metric spaces with belief theoretic
measures. The uncertainty in these spaces stems from both probability and
metric. To represent both aspect of uncertainty, we choose an expected distance
function as a measure of uncertainty. A formal logical system is constructed
for the reasoning about expected distance. Soundness and completeness are shown
for this logic. For reasoning on product metric space with uncertainty, a new
metric is defined and shown to have good properties."
Stratified Analysis of `Probabilities of Causation',"This paper proposes new formulas for the probabilities of causation difined
by Pearl (2000). Tian and Pearl (2000a, 2000b) showed how to bound the
quantities of the probabilities of causation from experimental and
observational data, under the minimal assumptions about the data-generating
process. We derive narrower bounds than Tian-Pearl bounds by making use of the
covariate information measured in experimental and observational studies. In
addition, we provide identifiable case under no-prevention assumption and
discuss the covariate selection problem from the viewpoint of estimation
accuracy. These results are helpful in providing more evidence for public
policy assessment and dicision making problems."
Bayesian Inference for Gaussian Mixed Graph Models,"We introduce priors and algorithms to perform Bayesian inference in Gaussian
models defined by acyclic directed mixed graphs. Such a class of graphs,
composed of directed and bi-directed edges, is a representation of conditional
independencies that is closed under marginalization and arises naturally from
causal models which allow for unmeasured confounding. Monte Carlo methods and a
variational approximation for such models are presented. Our algorithms for
Bayesian inference allow the evaluation of posterior distributions for several
quantities of interest, including causal effects that are not identifiable from
data alone but could otherwise be inferred where informative prior knowledge
about confounding is available."
Identification of Conditional Interventional Distributions,"The subject of this paper is the elucidation of effects of actions from
causal assumptions represented as a directed graph, and statistical knowledge
given as a probability distribution. In particular, we are interested in
predicting conditional distributions resulting from performing an action on a
set of variables and, subsequently, taking measurements of another set. We
provide a necessary and sufficient graphical condition for the cases where such
distributions can be uniquely computed from the available information, as well
as an algorithm which performs this computation whenever the condition holds.
Furthermore, we use our results to prove completeness of do-calculus [Pearl,
1995] for the same identification problem."
Rule Based Expert System for Cerebral Palsy Diagnosis,"The use of Artificial Intelligence is finding prominence not only in core
computer areas, but also in cross disciplinary areas including medical
diagnosis. In this paper, we present a rule based Expert System used in
diagnosis of Cerebral Palsy. The expert system takes user input and depending
on the symptoms of the patient, diagnoses if the patient is suffering from
Cerebral Palsy. The Expert System also classifies the Cerebral Palsy as mild,
moderate or severe based on the presented symptoms."
Cost Sensitive Reachability Heuristics for Handling State Uncertainty,"While POMDPs provide a general platform for non-deterministic conditional
planning under a variety of quality metrics they have limited scalability. On
the other hand, non-deterministic conditional planners scale very well, but
many lack the ability to optimize plan quality metrics. We present a novel
generalization of planning graph based heuristics that helps conditional
planners both scale and generate high quality plans when using actions with
nonuniform costs. We make empirical comparisons with two state of the art
planners to show the benefit of our techniques."
"Prediction, Expectation, and Surprise: Methods, Designs, and Study of a
  Deployed Traffic Forecasting Service","We present research on developing models that forecast traffic flow and
congestion in the Greater Seattle area. The research has led to the deployment
of a service named JamBayes, that is being actively used by over 2,500 users
via smartphones and desktop versions of the system. We review the modeling
effort and describe experiments probing the predictive accuracy of the models.
Finally, we present research on building models that can identify current and
future surprises, via efforts on modeling and forecasting unexpected
situations."
Belief Updating and Learning in Semi-Qualitative Probabilistic Networks,"This paper explores semi-qualitative probabilistic networks (SQPNs) that
combine numeric and qualitative information. We first show that exact
inferences with SQPNs are NPPP-Complete. We then show that existing qualitative
relations in SQPNs (plus probabilistic logic and imprecise assessments) can be
dealt effectively through multilinear programming. We then discuss learning: we
consider a maximum likelihood method that generates point estimates given a
SQPN and empirical data, and we describe a Bayesian-minded method that employs
the Imprecise Dirichlet Model to generate set-valued estimates."
Common Voting Rules as Maximum Likelihood Estimators,"Voting is a very general method of preference aggregation. A voting rule
takes as input every voter's vote (typically, a ranking of the alternatives),
and produces as output either just the winning alternative or a ranking of the
alternatives. One potential view of voting is the following. There exists a
'correct' outcome (winner/ranking), and each voter's vote corresponds to a
noisy perception of this correct outcome. If we are given the noise model, then
for any vector of votes, we can"
On Bayesian Network Approximation by Edge Deletion,"We consider the problem of deleting edges from a Bayesian network for the
purpose of simplifying models in probabilistic inference. In particular, we
propose a new method for deleting network edges, which is based on the evidence
at hand. We provide some interesting bounds on the KL-divergence between
original and approximate networks, which highlight the impact of given evidence
on the quality of approximation and shed some light on good and bad candidates
for edge deletion. We finally demonstrate empirically the promise of the
proposed edge deletion technique as a basis for approximate inference."
Exploiting Evidence in Probabilistic Inference,"We define the notion of compiling a Bayesian network with evidence and
provide a specific approach for evidence-based compilation, which makes use of
logical processing. The approach is practical and advantageous in a number of
application areas-including maximum likelihood estimation, sensitivity
analysis, and MAP computations-and we provide specific empirical results in the
domain of genetic linkage analysis. We also show that the approach is
applicable for networks that do not contain determinism, and show that it
empirically subsumes the performance of the quickscore algorithm when applied
to noisy-or networks."
Local Markov Property for Models Satisfying Composition Axiom,"The local Markov condition for a DAG to be an independence map of a
probability distribution is well known. For DAGs with latent variables,
represented as bi-directed edges in the graph, the local Markov property may
invoke exponential number of conditional independencies. This paper shows that
the number of conditional independence relations required may be reduced if the
probability distributions satisfy the composition axiom. In certain types of
graphs, only linear number of conditional independencies are required. The
result has applications in testing linear structural equation models with
correlated errors."
"Approximate Inference Algorithms for Hybrid Bayesian Networks with
  Discrete Constraints","In this paper, we consider Hybrid Mixed Networks (HMN) which are Hybrid
Bayesian Networks that allow discrete deterministic information to be modeled
explicitly in the form of constraints. We present two approximate inference
algorithms for HMNs that integrate and adjust well known algorithmic principles
such as Generalized Belief Propagation, Rao-Blackwellised Importance Sampling
and Constraint Propagation to address the complexity of modeling and reasoning
in HMNs. We demonstrate the performance of our approximate inference algorithms
on randomly generated HMNs."
Metrics for Markov Decision Processes with Infinite State Spaces,"We present metrics for measuring state similarity in Markov decision
processes (MDPs) with infinitely many states, including MDPs with continuous
state spaces. Such metrics provide a stable quantitative analogue of the notion
of bisimulation for MDPs, and are suitable for use in MDP approximation. We
show that the optimal value function associated with a discounted infinite
horizon planning task varies continuously with respect to our metric distances."
"Unstructuring User Preferences: Efficient Non-Parametric Utility
  Revelation","Tackling the problem of ordinal preference revelation and reasoning, we
propose a novel methodology for generating an ordinal utility function from a
set of qualitative preference statements. To the best of our knowledge, our
proposal constitutes the first nonparametric solution for this problem that is
both efficient and semantically sound. Our initial experiments provide strong
evidence for practical effectiveness of our approach."
"The Graphical Identification for Total Effects by using Surrogate
  Variables","Consider the case where cause-effect relationships between variables can be
described as a directed acyclic graph and the corresponding linear structural
equation model. This paper provides graphical identifiability criteria for
total effects by using surrogate variables in the case where it is difficult to
observe a treatment/response variable. The results enable us to judge from
graph structure whether a total effect can be identified through the
observation of surrogate variables."
The Relationship Between AND/OR Search and Variable Elimination,"In this paper we compare search and inference in graphical models through the
new framework of AND/OR search. Specifically, we compare Variable Elimination
(VE) and memoryintensive AND/OR Search (AO) and place algorithms such as
graph-based backjumping and no-good and good learning, as well as Recursive
Conditioning [7] and Value Elimination [2] within the AND/OR search framework."
Point-Based POMDP Algorithms: Improved Analysis and Implementation,"Existing complexity bounds for point-based POMDP value iteration algorithms
focus either on the curse of dimensionality or the curse of history. We derive
a new bound that relies on both and uses the concept of discounted
reachability; our conclusions may help guide future algorithm design. We also
discuss recent improvements to our (point-based) heuristic search value
iteration algorithm. Our new implementation calculates tighter initial bounds,
avoids solving linear programs, and makes more effective use of sparsity."
"Qualitative Decision Making Under Possibilistic Uncertainty: Toward more
  discriminating criteria","The aim of this paper is to propose a generalization of previous approaches
in qualitative decision making. Our work is based on the binary possibilistic
utility (PU), which is a possibilistic counterpart of Expected Utility (EU).We
first provide a new axiomatization of PU and study its relation with the
lexicographic aggregation of pessimistic and optimistic utilities. Then we
explain the reasons of the coarseness of qualitative decision criteria.
Finally, thanks to a redefinition of possibilistic lotteries and mixtures, we
present the refined binary possibilistic utility, which is more discriminating
than previously proposed criteria."
"Generating Markov Equivalent Maximal Ancestral Graphs by Single Edge
  Replacement","Maximal ancestral graphs (MAGs) are used to encode conditional independence
relations in DAG models with hidden variables. Different MAGs may represent the
same set of conditional independences and are called Markov equivalent. This
paper considers MAGs without undirected edges and shows conditions under which
an arrow in a MAG can be reversed or interchanged with a bi-directed edge so as
to yield a Markov equivalent MAG."
Reasoning about Agent Programs using ATL-like Logics,"We propose a variant of Alternating-time Temporal Logic (ATL) grounded in the
agents' operational know-how, as defined by their libraries of abstract plans.
Inspired by ATLES, a variant itself of ATL, it is possible in our logic to
explicitly refer to ""rational"" strategies for agents developed under the
Belief-Desire-Intention agent programming paradigm. This allows us to express
and verify properties of BDI systems using ATL-type logical frameworks."
Metrics for Finite Markov Decision Processes,"We present metrics for measuring the similarity of states in a finite Markov
decision process (MDP). The formulation of our metrics is based on the notion
of bisimulation for MDPs, with an aim towards solving discounted infinite
horizon reinforcement learning tasks. Such metrics can be used to aggregate
states, as well as to better structure other value function approximators
(e.g., memory-based or nearest-neighbor approximators). We provide bounds that
relate our metric distances to the optimal values of states in the given MDP."
Dynamic Programming for Structured Continuous Markov Decision Problems,"We describe an approach for exploiting structure in Markov Decision Processes
with continuous state variables. At each step of the dynamic programming, the
state space is dynamically partitioned into regions where the value function is
the same throughout the region. We first describe the algorithm for piecewise
constant representations. We then extend it to piecewise linear
representations, using techniques from POMDPs to represent and reason about
linear surfaces efficiently. We show that for complex, structured problems, our
approach exploits the natural structure so that optimal solutions can be
computed efficiently."
Region-Based Incremental Pruning for POMDPs,"We present a major improvement to the incremental pruning algorithm for
solving partially observable Markov decision processes. Our technique targets
the cross-sum step of the dynamic programming (DP) update, a key source of
complexity in POMDP algorithms. Instead of reasoning about the whole belief
space when pruning the cross-sums, our algorithm divides the belief space into
smaller regions and performs independent pruning in each region. We evaluate
the benefits of the new technique both analytically and experimentally, and
show that it produces very significant performance gains. The results
contribute to the scalability of POMDP algorithms to domains that cannot be
handled by the best existing techniques."
A Unified framework for order-of-magnitude confidence relations,"The aim of this work is to provide a unified framework for ordinal
representations of uncertainty lying at the crosswords between possibility and
probability theories. Such confidence relations between events are commonly
found in monotonic reasoning, inconsistency management, or qualitative decision
theory. They start either from probability theory, making it more qualitative,
or from possibility theory, making it more expressive. We show these two trends
converge to a class of genuine probability theories. We provide
characterization results for these useful tools that preserve the qualitative
nature of possibility rankings, while enjoying the power of expressivity of
additive representations."
"Mixtures of Deterministic-Probabilistic Networks and their AND/OR Search
  Space","The paper introduces mixed networks, a new framework for expressing and
reasoning with probabilistic and deterministic information. The framework
combines belief networks with constraint networks, defining the semantics and
graphical representation. We also introduce the AND/OR search space for
graphical models, and develop a new linear space search algorithm. This
provides the basis for understanding the benefits of processing the constraint
information separately, resulting in the pruning of the search space. When the
constraint part is tractable or has a small number of solutions, using the
mixed representation can be exponentially more effective than using pure belief
networks which odel constraints as conditional probability tables."
Compact Value-Function Representations for Qualitative Preferences,"We consider the challenge of preference elicitation in systems that help
users discover the most desirable item(s) within a given database. Past work on
preference elicitation focused on structured models that provide a factored
representation of users' preferences. Such models require less information to
construct and support efficient reasoning algorithms. This paper makes two
substantial contributions to this area: (1) Strong representation theorems for
factored value functions. (2) A methodology that utilizes our representation
results to address the problem of optimal item selection."
"Selection of Identifiability Criteria for Total Effects by using Path
  Diagrams","Pearl has provided the back door criterion, the front door criterion and the
conditional instrumental variable (IV) method as identifiability criteria for
total effects. In some situations, these three criteria can be applied to
identifying total effects simultaneously. For the purpose of increasing
estimating accuracy, this paper compares the three ways of identifying total
effects in terms of the asymptotic variance, and concludes that in some
situations the superior of them can be recognized directly from the graph
structure."
Identifying Conditional Causal Effects,"This paper concerns the assessment of the effects of actions from a
combination of nonexperimental data and causal assumptions encoded in the form
of a directed acyclic graph in which some variables are presumed to be
unobserved. We provide a procedure that systematically identifies cause effects
between two sets of variables conditioned on some other variables, in time
polynomial in the number of variables in the graph. The identifiable
conditional causal effects are expressed in terms of the observed joint
distribution."
Heuristic Search Value Iteration for POMDPs,"We present a novel POMDP planning algorithm called heuristic search value
iteration (HSVI).HSVI is an anytime algorithm that returns a policy and a
provable bound on its regret with respect to the optimal policy. HSVI gets its
power by combining two well-known techniques: attention-focusing search
heuristics and piecewise linear convex representations of the value function.
HSVI's soundness and convergence have been proven. On some benchmark problems
from the literature, HSVI displays speedups of greater than 100 with respect to
other state-of-the-art POMDP value iteration algorithms. We also apply HSVI to
a new rover exploration problem 10 times larger than most POMDP problems in the
literature."
Robustness of Causal Claims,"A causal claim is any assertion that invokes causal relationships between
variables, for example that a drug has a certain effect on preventing a
disease. Causal claims are established through a combination of data and a set
of causal assumptions called a causal model. A claim is robust when it is
insensitive to violations of some of the causal assumptions embodied in the
model. This paper gives a formal definition of this notion of robustness and
establishes a graphical condition for quantifying the degree of robustness of a
given causal claim. Algorithms for computing the degree of robustness are also
presented."
"An improvement direction for filter selection techniques using
  information theory measures and quadratic optimization","Filter selection techniques are known for their simplicity and efficiency.
However this kind of methods doesn't take into consideration the features
inter-redundancy. Consequently the un-removed redundant features remain in the
final classification model, giving lower generalization performance. In this
paper we propose to use a mathematical optimization method that reduces
inter-features redundancy and maximize relevance between each feature and the
target variable."
Join-graph based cost-shifting schemes,"We develop several algorithms taking advantage of two common approaches for
bounding MPE queries in graphical models: minibucket elimination and
message-passing updates for linear programming relaxations. Both methods are
quite similar, and offer useful perspectives for the other; our hybrid
approaches attempt to balance the advantages of each. We demonstrate the power
of our hybrid algorithms through extensive empirical evaluation. Most notably,
a Branch and Bound search guided by the heuristic function calculated by one of
our new algorithms has recently won first place in the PASCAL2 inference
challenge."
A Maximum Likelihood Approach For Selecting Sets of Alternatives,"We consider the problem of selecting a subset of alternatives given noisy
evaluations of the relative strength of different alternatives. We wish to
select a k-subset (for a given k) that provides a maximum likelihood estimate
for one of several objectives, e.g., containing the strongest alternative.
Although this problem is NP-hard, we show that when the noise level is
sufficiently high, intuitive methods provide the optimal solution. We thus
generalize classical results about singling out one alternative and identifying
the hidden ranking of alternatives by strength. Extensive experiments show that
our methods perform well in practical settings."
"A Case Study in Complexity Estimation: Towards Parallel Branch-and-Bound
  over Graphical Models","We study the problem of complexity estimation in the context of parallelizing
an advanced Branch and Bound-type algorithm over graphical models. The
algorithm's pruning power makes load balancing, one crucial element of every
distributed system, very challenging. We propose using a statistical regression
model to identify and tackle disproportionally complex parallel subproblems,
the cause of load imbalance, ahead of time. The proposed model is evaluated and
analyzed on various levels and shown to yield robust predictions. We then
demonstrate its effectiveness for load balancing in practice."
The Complexity of Approximately Solving Influence Diagrams,"Influence diagrams allow for intuitive and yet precise description of complex
situations involving decision making under uncertainty. Unfortunately, most of
the problems described by influence diagrams are hard to solve. In this paper
we discuss the complexity of approximately solving influence diagrams. We do
not assume no-forgetting or regularity, which makes the class of problems we
address very broad. Remarkably, we show that when both the tree-width and the
cardinality of the variables are bounded the problem admits a fully
polynomial-time approximation scheme."
Belief Propagation for Structured Decision Making,"Variational inference algorithms such as belief propagation have had
tremendous impact on our ability to learn and use graphical models, and give
many insights for developing or understanding exact and approximate inference.
However, variational approaches have not been widely adoped for decision making
in graphical models, often formulated through influence diagrams and including
both centralized and decentralized (or multi-agent) decisions. In this work, we
present a general variational framework for solving structured cooperative
decision-making problems, use it to propose several belief propagation-like
algorithms, and analyze them both theoretically and empirically."
"An Approximate Solution Method for Large Risk-Averse Markov Decision
  Processes","Stochastic domains often involve risk-averse decision makers. While recent
work has focused on how to model risk in Markov decision processes using risk
measures, it has not addressed the problem of solving large risk-averse
formulations. In this paper, we propose and analyze a new method for solving
large risk-averse MDPs with hybrid continuous-discrete state spaces and
continuous action spaces. The proposed method iteratively improves a bound on
the value function using a linearity structure of the MDP. We demonstrate the
utility and properties of the method on a portfolio optimization problem."
"From imprecise probability assessments to conditional probabilities with
  quasi additive classes of conditioning events","In this paper, starting from a generalized coherent (i.e. avoiding uniform
loss) intervalvalued probability assessment on a finite family of conditional
events, we construct conditional probabilities with quasi additive classes of
conditioning events which are consistent with the given initial assessment.
Quasi additivity assures coherence for the obtained conditional probabilities.
In order to reach our goal we define a finite sequence of conditional
probabilities by exploiting some theoretical results on g-coherence. In
particular, we use solutions of a finite sequence of linear systems."
Multi-objective Influence Diagrams,"We describe multi-objective influence diagrams, based on a set of p
objectives, where utility values are vectors in Rp, and are typically only
partially ordered. These can still be solved by a variable elimination
algorithm, leading to a set of maximal values of expected utility. If the
Pareto ordering is used this set can often be prohibitively large. We consider
approximate representations of the Pareto set based on e-coverings, allowing
much larger problems to be solved. In addition, we define a method for
incorporating user tradeoffs, which also greatly improves the efficiency."
Verbalizing Ontologies in Controlled Baltic Languages,"Controlled natural languages (mostly English-based) recently have emerged as
seemingly informal supplementary means for OWL ontology authoring, if compared
to the formal notations that are used by professional knowledge engineers. In
this paper we present by examples controlled Latvian language that has been
designed to be compliant with the state of the art Attempto Controlled English.
We also discuss relation with controlled Lithuanian language that is being
designed in parallel."
Quantum Consciousness Soccer Simulator,"In cognitive sciences it is not uncommon to use various games effectively.
For example, in artificial intelligence, the RoboCup initiative was to set up
to catalyse research on the field of autonomous agent technology. In this
paper, we introduce a similar soccer simulation initiative to try to
investigate a model of human consciousness and a notion of reality in the form
of a cognitive problem. In addition, for example, the home pitch advantage and
the objective role of the supporters could be naturally described and discussed
in terms of this new soccer simulation model."
Gliders2012: Development and Competition Results,"The RoboCup 2D Simulation League incorporates several challenging features,
setting a benchmark for Artificial Intelligence (AI). In this paper we describe
some of the ideas and tools around the development of our team, Gliders2012. In
our description, we focus on the evaluation function as one of our central
mechanisms for action selection. We also point to a new framework for watching
log files in a web browser that we release for use and further development by
the RoboCup community. Finally, we also summarize results of the group and
final matches we played during RoboCup 2012, with Gliders2012 finishing 4th out
of 19 teams."
A Dataset for StarCraft AI \& an Example of Armies Clustering,"This paper advocates the exploration of the full state of recorded real-time
strategy (RTS) games, by human or robotic players, to discover how to reason
about tactics and strategy. We present a dataset of StarCraft games
encompassing the most of the games' state (not only player's orders). We
explain one of the possible usages of this dataset by clustering armies on
their compositions. This reduction of armies compositions to mixtures of
Gaussian allow for strategic reasoning at the level of the components. We
evaluated this clustering method by predicting the outcomes of battles based on
armies compositions' mixtures components"
A possibilistic handling of partially ordered information,"In a standard possibilistic logic, prioritized information are encoded by
means of weighted knowledge base. This paper proposes an extension of
possibilistic logic for dealing with partially ordered information. We Show
that all basic notions of standard possibilitic logic (sumbsumption, syntactic
and semantic inference, etc.) have natural couterparts when dealing with
partially ordered information. We also propose an algorithm which computes
possibilistic conclusions of a partial knowledge base of a partially ordered
knowlege base."
Structure-Based Causes and Explanations in the Independent Choice Logic,"This paper is directed towards combining Pearl's structural-model approach to
causal reasoning with high-level formalisms for reasoning about actions. More
precisely, we present a combination of Pearl's structural-model approach with
Poole's independent choice logic. We show how probabilistic theories in the
independent choice logic can be mapped to probabilistic causal models. This
mapping provides the independent choice logic with appealing concepts of
causality and explanation from the structural-model approach. We illustrate
this along Halpern and Pearl's sophisticated notions of actual cause,
explanation, and partial explanation. This mapping also adds first-order
modeling capabilities and explicit actions to the structural-model approach."
Probabilistic Reasoning about Actions in Nonmonotonic Causal Theories,"We present the language {m P}{cal C}+ for probabilistic reasoning about
actions, which is a generalization of the action language {cal C}+ that allows
to deal with probabilistic as well as nondeterministic effects of actions. We
define a formal semantics of {m P}{cal C}+ in terms of probabilistic
transitions between sets of states. Using a concept of a history and its belief
state, we then show how several important problems in reasoning about actions
can be concisely formulated in our formalism."
A Linear Belief Function Approach to Portfolio Evaluation,"By elaborating on the notion of linear belief functions (Dempster 1990; Liu
1996), we propose an elementary approach to knowledge representation for expert
systems using linear belief functions. We show how to use basic matrices to
represent market information and financial knowledge, including complete
ignorance, statistical observations, subjective speculations, distributional
assumptions, linear relations, and empirical asset pricing models. We then
appeal to Dempster's rule of combination to integrate the knowledge for
assessing an overall belief of portfolio performance, and updating the belief
by incorporating additional information. We use an example of three gold stocks
to illustrate the approach."
Policy-contingent abstraction for robust robot control,"This paper presents a scalable control algorithm that enables a deployed
mobile robot system to make high-level decisions under full consideration of
its probabilistic belief. Our approach is based on insights from the rich
literature of hierarchical controllers and hierarchical MDPs. The resulting
controller has been successfully deployed in a nursing facility near
Pittsburgh, PA. To the best of our knowledge, this work is a unique instance of
applying POMDPs to high-level robotic control problems."
"An Axiomatic Approach to Robustness in Search Problems with Multiple
  Scenarios","This paper is devoted to the search of robust solutions in state space graphs
when costs depend on scenarios. We first present axiomatic requirements for
preference compatibility with the intuitive idea of robustness.This leads us to
propose the Lorenz dominance rule as a basis for robustness analysis. Then,
after presenting complexity results about the determination of robust
solutions, we propose a new sophistication of A* specially designed to
determine the set of robust paths in a state space graph. The behavior of the
algorithm is illustrated on a small example. Finally, an axiomatic
justification of the refinement of robustness by an OWA criterion is provided."
"A constraint satisfaction approach to the robust spanning tree problem
  with interval data","Robust optimization is one of the fundamental approaches to deal with
uncertainty in combinatorial optimization. This paper considers the robust
spanning tree problem with interval data, which arises in a variety of
telecommunication applications. It proposes a constraint satisfaction approach
using a combinatorial lower bound, a pruning component that removes infeasible
and suboptimal edges, as well as a search strategy exploring the most uncertain
edges first. The resulting algorithm is shown to produce very dramatic
improvements over the mathematical programming approach of Yaman et al. and to
enlarge considerably the class of problems amenable to effective solutions"
Qualitative MDPs and POMDPs: An Order-Of-Magnitude Approximation,"We develop a qualitative theory of Markov Decision Processes (MDPs) and
Partially Observable MDPs that can be used to model sequential decision making
tasks when only qualitative information is available. Our approach is based
upon an order-of-magnitude approximation of both probabilities and utilities,
similar to epsilon-semantics. The result is a qualitative theory that has close
ties with the standard maximum-expected-utility theory and is amenable to
general planning techniques."
Generalized Instrumental Variables,"This paper concerns the assessment of direct causal effects from a
combination of: (i) non-experimental data, and (ii) qualitative domain
knowledge. Domain knowledge is encoded in the form of a directed acyclic graph
(DAG), in which all interactions are assumed linear, and some variables are
presumed to be unobserved. We provide a generalization of the well-known method
of Instrumental Variables, which allows its application to models with few
conditional independeces."
"Causes and Explanations in the Structural-Model Approach: Tractable
  Cases","In this paper, we continue our research on the algorithmic aspects of Halpern
and Pearl's causes and explanations in the structural-model approach. To this
end, we present new characterizations of weak causes for certain classes of
causal models, which show that under suitable restrictions deciding causes and
explanations is tractable. To our knowledge, these are the first explicit
tractability results for the structural-model approach."
"Statistical Decisions Using Likelihood Information Without Prior
  Probabilities","This paper presents a decision-theoretic approach to statistical inference
that satisfies the likelihood principle (LP) without using prior information.
Unlike the Bayesian approach, which also satisfies LP, we do not assume
knowledge of the prior distribution of the unknown parameter. With respect to
information that can be obtained from an experiment, our solution is more
efficient than Wald's minimax solution.However, with respect to information
assumed to be known before the experiment, our solution demands less input than
the Bayesian solution."
Reduction of Maximum Entropy Models to Hidden Markov Models,"We show that maximum entropy (maxent) models can be modeled with certain
kinds of HMMs, allowing us to construct maxent models with hidden variables,
hidden state sequences, or other characteristics. The models can be trained
using the forward-backward algorithm. While the results are primarily of
theoretical interest, unifying apparently unrelated concepts, we also give
experimental results for a maxent model with a hidden variable on a word
disambiguation task; the model outperforms standard techniques."
"Expectation Propogation for approximate inference in dynamic Bayesian
  networks","We describe expectation propagation for approximate inference in dynamic
Bayesian networks as a natural extension of Pearl s exact belief
propagation.Expectation propagation IS a greedy algorithm, converges IN many
practical cases, but NOT always.We derive a DOUBLE - loop algorithm, guaranteed
TO converge TO a local minimum OF a Bethe free energy.Furthermore, we show that
stable fixed points OF (damped) expectation propagation correspond TO local
minima OF this free energy, but that the converse need NOT be the CASE .We
illustrate the algorithms BY applying them TO switching linear dynamical
systems AND discuss implications FOR approximate inference IN general Bayesian
networks."
Coordinates: Probabilistic Forecasting of Presence and Availability,"We present methods employed in Coordinate, a prototype service that supports
collaboration and communication by learning predictive models that provide
forecasts of users s AND availability.We describe how data IS collected about
USER activity AND proximity FROM multiple devices, IN addition TO analysis OF
the content OF users, the time of day, and day of week. We review applications
of presence forecasting embedded in the Priorities application and then present
details of the Coordinate service that was informed by the earlier efforts."
"Efficient Nash Computation in Large Population Games with Bounded
  Influence","We introduce a general representation of large-population games in which each
player s influence ON the others IS centralized AND limited, but may otherwise
be arbitrary.This representation significantly generalizes the class known AS
congestion games IN a natural way.Our main results are provably correct AND
efficient algorithms FOR computing AND learning approximate Nash equilibria IN
this general framework."
Formalizing Scenario Analysis,"We propose a formal treatment of scenarios in the context of a dialectical
argumentation formalism for qualitative reasoning about uncertain propositions.
Our formalism extends prior work in which arguments for and against uncertain
propositions were presented and compared in interaction spaces called Agoras.
We now define the notion of a scenario in this framework and use it to define a
set of qualitative uncertainty labels for propositions across a collection of
scenarios. This work is intended to lead to a formal theory of scenarios and
scenario analysis."
Factored Particles for Scalable Monitoring,"Exact monitoring in dynamic Bayesian networks is intractable, so approximate
algorithms are necessary. This paper presents a new family of approximate
monitoring algorithms that combine the best qualities of the particle filtering
and Boyen-Koller methods. Our algorithms maintain an approximate representation
the belief state in the form of sets of factored particles, that correspond to
samples of clusters of state variables. Empirical results show that our
algorithms outperform both ordinary particle filtering and the Boyen-Koller
algorithm on large systems."
"Inference with Seperately Specified Sets of Probabilities in Credal
  Networks","We present new algorithms for inference in credal networks --- directed
acyclic graphs associated with sets of probabilities. Credal networks are here
interpreted as encoding strong independence relations among variables. We first
present a theory of credal networks based on separately specified sets of
probabilities. We also show that inference with polytrees is NP-hard in this
setting. We then introduce new techniques that reduce the computational effort
demanded by inference, particularly in polytrees, by exploring separability of
credal sets."
Asymptotic Model Selection for Naive Bayesian Networks,"We develop a closed form asymptotic formula to compute the marginal
likelihood of data given a naive Bayesian network model with two hidden states
and binary features. This formula deviates from the standard BIC score. Our
work provides a concrete example that the BIC score is generally not valid for
statistical models that belong to a stratified exponential family. This stands
in contrast to linear and curved exponential families, where the BIC score has
been proven to provide a correct approximation for the marginal likelihood."
Loopy Belief Propogation and Gibbs Measures,"We address the question of convergence in the loopy belief propagation (LBP)
algorithm. Specifically, we relate convergence of LBP to the existence of a
weak limit for a sequence of Gibbs measures defined on the LBP s associated
computation tree.Using tools FROM the theory OF Gibbs measures we develop
easily testable sufficient conditions FOR convergence.The failure OF
convergence OF LBP implies the existence OF multiple phases FOR the associated
Gibbs specification.These results give new insight INTO the mechanics OF the
algorithm."
Particle Filters in Robotics (Invited Talk),"This presentation will introduce the audience to a new, emerging body of
research on sequential Monte Carlo techniques in robotics. In recent years,
particle filters have solved several hard perceptual robotic problems. Early
successes were limited to low-dimensional problems, such as the problem of
robot localization in environments with known maps. More recently, researchers
have begun exploiting structural properties of robotic domains that have led to
successful particle filter applications in spaces with as many as 100,000
dimensions. The presentation will discuss specific tricks necessary to make
these techniques work in real - world domains,and also discuss open challenges
for researchers IN the UAI community."
On the Testable Implications of Causal Models with Hidden Variables,"The validity OF a causal model can be tested ONLY IF the model imposes
constraints ON the probability distribution that governs the generated data. IN
the presence OF unmeasured variables, causal models may impose two types OF
constraints : conditional independencies, AS READ through the d - separation
criterion, AND functional constraints, FOR which no general criterion IS
available.This paper offers a systematic way OF identifying functional
constraints AND, thus, facilitates the task OF testing causal models AS well AS
inferring such models FROM data."
Markov Chain Monte Carlo using Tree-Based Priors on Model Structure,"We present a general framework for defining priors on model structure and
sampling from the posterior using the Metropolis-Hastings algorithm. The key
idea is that structure priors are defined via a probability tree and that the
proposal mechanism for the Metropolis-Hastings algorithm operates by traversing
this tree, thereby defining a cheaply computable acceptance probability. We
have applied this approach to Bayesian net structure learning using a number of
priors and tree traversal strategies. Our results show that these must be
chosen appropriately for this approach to be successful."
A Calculus for Causal Relevance,"This paper presents a sound and completecalculus for causal relevance, based
onPearl's functional models semantics.The calculus consists of axioms and
rulesof inference for reasoning about causalrelevance relationships.We extend
the set of known axioms for causalrelevance with three new axioms, andintroduce
two new rules of inference forreasoning about specific subclasses
ofmodels.These subclasses give a more refinedcharacterization of causal models
than the one given in Halpern's axiomatizationof counterfactual
reasoning.Finally, we show how the calculus for causalrelevance can be used in
the task ofidentifying causal structure from non-observational data."
Instrumentality Tests Revisited,"An instrument is a random variable thatallows the identification of
parameters inlinear models when the error terms arenot uncorrelated.It is a
popular method used in economicsand the social sciences that reduces theproblem
of identification to the problemof finding the appropriate instruments.Few
years ago, Pearl introduced a necessarytest for instruments that allows the
researcher to discard those candidatesthat fail the test.In this paper, we make
a detailed study of Pearl's test and the general model forinstruments. The
results of this studyinclude a novel interpretation of Pearl'stest, a general
theory of instrumentaltests, and an affirmative answer to aprevious conjecture.
We also presentnew instrumentality tests for the casesof discrete and
continuous variables."
"Conditions Under Which Conditional Independence and Scoring Methods Lead
  to Identical Selection of Bayesian Network Models","It is often stated in papers tackling the task of inferring Bayesian network
structures from data that there are these two distinct approaches: (i) Apply
conditional independence tests when testing for the presence or otherwise of
edges; (ii) Search the model space using a scoring metric. Here I argue that
for complete data and a given node ordering this division is a myth, by showing
that cross entropy methods for checking conditional independence are
mathematically identical to methods based upon discriminating between models by
their overall goodness-of-fit logarithmic scores."
Causes and Explanations: A Structural-Model Approach --- Part 1: Causes,"We propose a new definition of actual causes, using structural equations to
model counterfactuals.We show that the definitions yield a plausible and
elegant account ofcausation that handles well examples which have caused
problems forother definitions and resolves major difficulties in the
traditionalaccount. In a companion paper, we show how the definition of
causality can beused to give an elegant definition of (causal) explanation."
Plausible reasoning from spatial observations,"This article deals with plausible reasoning from incomplete knowledge about
large-scale spatial properties. The availableinformation, consisting of a set
of pointwise observations,is extrapolated to neighbour points. We make use of
belief functions to represent the influence of the knowledge at a given point
to another point; the quantitative strength of this influence decreases when
the distance between both points increases. These influences arethen aggregated
using a variant of Dempster's rule of combination which takes into account the
relative dependence between observations."
Probabilistic Logic Programming under Inheritance with Overriding,"We present probabilistic logic programming under inheritance with overriding.
This approach is based on new notions of entailment for reasoning with
conditional constraints, which are obtained from the classical notion of
logical entailment by adding the principle of inheritance with overriding. This
is done by using recent approaches to probabilistic default reasoning with
conditional constraints. We analyze the semantic properties of the new
entailment relations. We also present algorithms for probabilistic logic
programming under inheritance with overriding, and program transformations for
an increased efficiency."
"Solving Influence Diagrams using HUGIN, Shafer-Shenoy and Lazy
  Propagation","In this paper we compare three different architectures for the evaluation of
influence diagrams: HUGIN, Shafer-Shenoy, and Lazy Evaluation architecture. The
computational complexity of the architectures are compared on the LImited
Memory Influence Diagram (LIMID): a diagram where only the requiste information
for the computation of the optimal policies are depicted. Because the requsite
information is explicitly represented in the LIMID the evaluation can take
advantage of it, and significant savings in computational can be obtained. In
this paper we show how the obtained savings is considerably increased when the
computations performed on the LIMID is according to the Lazy Evaluation scheme."
Direct and Indirect Effects,"The direct effect of one eventon another can be defined and measured
byholding constant all intermediate variables between the two.Indirect effects
present conceptual andpractical difficulties (in nonlinear models), because
they cannot be isolated by holding certain variablesconstant. This paper shows
a way of defining any path-specific effectthat does not invoke blocking the
remainingpaths.This permits the assessment of a more naturaltype of direct and
indirect effects, one thatis applicable in both linear and nonlinear models.
The paper establishesconditions under which such assessments can be estimated
consistentlyfrom experimental and nonexperimental data,and thus extends
path-analytic techniques tononlinear and nonparametric models."
Vector-space Analysis of Belief-state Approximation for POMDPs,"We propose a new approach to value-directed belief state approximation for
POMDPs. The value-directed model allows one to choose approximation methods for
belief state monitoring that have a small impact on decision quality. Using a
vector space analysis of the problem, we devise two new search procedures for
selecting an approximation scheme that have much better computational
properties than existing methods. Though these provide looser error bounds, we
show empirically that they have a similar impact on decision quality in
practice, and run up to two orders of magnitude more quickly."
A Mixed Graphical Model for Rhythmic Parsing,"A method is presented for the rhythmic parsing problem: Given a sequence of
observed musical note onset times, we estimate the corresponding notated rhythm
and tempo process. A graphical model is developed that represents the
simultaneous evolution of tempo and rhythm and relates these hidden quantities
to observations. The rhythm variables are discrete and the tempo and
observation variables are continuous. We show how to compute the globally most
likely configuration of the tempo and rhythm variables given an observation of
note onset times. Preliminary experiments are presented on a small data set. A
generalization to arbitrary conditional Gaussian distributions is outlined."
A Tractable POMDP for a Class of Sequencing Problems,"We consider a partially observable Markov decision problem (POMDP) that
models a class of sequencing problems. Although POMDPs are typically
intractable, our formulation admits tractable solution. Instead of maintaining
a value function over a high-dimensional set of belief states, we reduce the
state space to one of smaller dimension, in which grid-based dynamic
programming techniques are effective. We develop an error bound for the
resulting approximation, and discuss an application of the model to a problem
in targeted advertising."
Causal Discovery from Changes,"We propose a new method of discovering causal structures, based on the
detection of local, spontaneous changes in the underlying data-generating
model. We analyze the classes of structures that are equivalent relative to a
stream of distributions produced by local changes, and devise algorithms that
output graphical representations of these equivalence classes. We present
experimental results, using simulated data, and examine the errors associated
with detection of changes and recovery of structures."
Using Temporal Data for Making Recommendations,"We treat collaborative filtering as a univariate time series estimation
problem: given a user's previous votes, predict the next vote. We describe two
families of methods for transforming data to encode time order in ways amenable
to off-the-shelf classification and density estimation tools, and examine the
results of using these approaches on several real-world data sets. The
improvements in predictive accuracy we realize recommend the use of other
predictive algorithms that exploit the temporal order of data."
Perfect Tree-Like Markovian Distributions,"We show that if a strictly positive joint probability distribution for a set
of binary random variables factors according to a tree, then vertex separation
represents all and only the independence relations enclosed in the
distribution. The same result is shown to hold also for multivariate strictly
positive normal distributions. Our proof uses a new property of conditional
independence that holds for these two classes of probability distributions."
A Principled Analysis of Merging Operations in Possibilistic Logic,"Possibilistic logic offers a qualitative framework for representing pieces of
information associated with levels of uncertainty of priority. The fusion of
multiple sources information is discussed in this setting. Different classes of
merging operators are considered including conjunctive, disjunctive,
reinforcement, adaptive and averaging operators. Then we propose to analyse
these classes in terms of postulates. This is done by first extending the
postulate for merging classical bases to the case where priorites are avaialbe."
Approximately Optimal Monitoring of Plan Preconditions,"Monitoring plan preconditions can allow for replanning when a precondition
fails, generally far in advance of the point in the plan where the precondition
is relevant. However, monitoring is generally costly, and some precondition
failures have a very small impact on plan quality. We formulate a model for
optimal precondition monitoring, using partially-observable Markov decisions
processes, and describe methods for solving this model efficitively, though
approximately. Specifically, we show that the single-precondition monitoring
problem is generally tractable, and the multiple-precondition monitoring
policies can be efficitively approximated using single-precondition soultions."
"Stochastic Logic Programs: Sampling, Inference and Applications","Algorithms for exact and approximate inference in stochastic logic programs
(SLPs) are presented, based respectively, on variable elimination and
importance sampling. We then show how SLPs can be used to represent prior
distributions for machine learning, using (i) logic programs and (ii) Bayes net
structures as examples. Drawing on existing work in statistics, we apply the
Metropolis-Hasting algorithm to construct a Markov chain which samples from the
posterior distribution. A Prolog implementation for this is described. We also
discuss the possibility of constructing explicit representations of the
posterior."
"A Qualitative Linear Utility Theory for Spohn's Theory of Epistemic
  Beliefs","In this paper, we formulate a qualitative ""linear"" utility theory for
lotteries in which uncertainty is expressed qualitatively using a Spohnian
disbelief function. We argue that a rational decision maker facing an uncertain
decision problem in which the uncertainty is expressed qualitatively should
behave so as to maximize ""qualitative expected utility."" Our axiomatization of
the qualitative utility is similar to the axiomatization developed by von
Neumann and Morgenstern for probabilistic lotteries. We compare our results
with other recent results in qualitative decision making."
"Probabilistic Arc Consistency: A Connection between Constraint Reasoning
  and Probabilistic Reasoning","We document a connection between constraint reasoning and probabilistic
reasoning. We present an algorithm, called {em probabilistic arc consistency},
which is both a generalization of a well known algorithm for arc consistency
used in constraint reasoning, and a specialization of the belief updating
algorithm for singly-connected networks. Our algorithm is exact for singly-
connected constraint problems, but can work well as an approximation for
arbitrary problems. We briefly discuss some empirical results, and related
methods."
Causal Mechanism-based Model Construction,"We propose a framework for building graphical causal model that is based on
the concept of causal mechanisms. Causal models are intuitive for human users
and, more importantly, support the prediction of the effect of manipulation. We
describe an implementation of the proposed framework as an interactive model
construction module, ImaGeNIe, in SMILE (Structural Modeling, Inference, and
Learning Engine) and in GeNIe (SMILE's Windows user interface)."
Evaluating Influence Diagrams using LIMIDs,"We present a new approach to the solution of decision problems formulated as
influence diagrams. The approach converts the influence diagram into a simpler
structure, the LImited Memory Influence Diagram (LIMID), where only the
requisite information for the computation of optimal policies is depicted.
Because the requisite information is explicitly represented in the diagram, the
evaluation procedure can take advantage of it. In this paper we show how to
convert an influence diagram to a LIMID and describe the procedure for finding
an optimal strategy. Our approach can yield significant savings of memory and
computational time when compared to traditional methods."
Conversation as Action Under Uncertainty,"Conversations abound with uncetainties of various kinds. Treating
conversation as inference and decision making under uncertainty, we propose a
task independent, multimodal architecture for supporting robust continuous
spoken dialog called Quartet. We introduce four interdependent levels of
analysis, and describe representations, inference procedures, and decision
strategies for managing uncertainties within and between the levels. We
highlight the approach by reviewing interactions between a user and two spoken
dialog systems developed using the Quartet architecture: Prsenter, a prototype
system for navigating Microsoft PowerPoint presentations, and the Bayesian
Receptionist, a prototype system for dealing with tasks typically handled by
front desk receptionists at the Microsoft corporate campus."
Pivotal Pruning of Trade-offs in QPNs,"Qualitative probabilistic networks have been designed for probabilistic
reasoning in a qualitative way. Due to their coarse level of representation
detail, qualitative probabilistic networks do not provide for resolving
trade-offs and typically yield ambiguous results upon inference. We present an
algorithm for computing more insightful results for unresolved trade-offs. The
algorithm builds upon the idea of using pivots to zoom in on the trade-offs and
identifying the information that would serve to resolve them."
A Branch-and-Bound Algorithm for MDL Learning Bayesian Networks,"This paper extends the work in [Suzuki, 1996] and presents an efficient
depth-first branch-and-bound algorithm for learning Bayesian network
structures, based on the minimum description length (MDL) principle, for a
given (consistent) variable ordering. The algorithm exhaustively searches
through all network structures and guarantees to find the network with the best
MDL score. Preliminary experiments show that the algorithm is efficient, and
that the time complexity grows slowly with the sample size. The algorithm is
useful for empirically studying both the performance of suboptimal heuristic
search algorithms and the adequacy of the MDL principle in learning Bayesian
networks."
Probabilities of Causation: Bounds and Identification,"This paper deals with the problem of estimating the probability that one
event was a cause of another in a given scenario. Using structural-semantical
definitions of the probabilities of necessary or sufficient causation (or
both), we show how to optimally bound these quantities from data obtained in
experimental and observational studies, making minimal assumptions concerning
the data-generating process. In particular, we strengthen the results of Pearl
(1999) by weakening the data-generation assumptions and deriving theoretically
sharp bounds on the probabilities of causation. These results delineate
precisely how empirical data can be used both in settling questions of
attribution and in solving attribution-related problems of decision making."
An Application of Uncertain Reasoning to Requirements Engineering,"This paper examines the use of Bayesian Networks to tackle one of the tougher
problems in requirements engineering, translating user requirements into system
requirements. The approach taken is to model domain knowledge as Bayesian
Network fragments that are glued together to form a complete view of the domain
specific system requirements. User requirements are introduced as evidence and
the propagation of belief is used to determine what are the appropriate system
requirements as indicated by user requirements. This concept has been
demonstrated in the development of a system specification and the results are
presented here."
Loglinear models for first-order probabilistic reasoning,"Recent work on loglinear models in probabilistic constraint logic programming
is applied to first-order probabilistic reasoning. Probabilities are defined
directly on the proofs of atomic formulae, and by marginalisation on the atomic
formulae themselves. We use Stochastic Logic Programs (SLPs) composed of
labelled and unlabelled definite clauses to define the proof probabilities. We
have a conservative extension of first-order reasoning, so that, for example,
there is a one-one mapping between logical and random variables. We show how,
in this framework, Inductive Logic Programming (ILP) can be used to induce the
features of a loglinear model from data. We also compare the presented
framework with other approaches to first-order probabilistic reasoning."
Learning Polytrees,"We consider the task of learning the maximum-likelihood polytree from data.
Our first result is a performance guarantee establishing that the optimal
branching (or Chow-Liu tree), which can be computed very easily, constitutes a
good approximation to the best polytree. We then show that it is not possible
to do very much better, since the learning problem is NP-hard even to
approximately solve within some constant factor."
Quantifier Elimination for Statistical Problems,"Recent improvement on Tarski's procedure for quantifier elimination in the
first order theory of real numbers makes it feasible to solve small instances
of the following problems completely automatically: 1. listing all equality and
inequality constraints implied by a graphical model with hidden variables. 2.
Comparing graphyical models with hidden variables (i.e., model equivalence,
inclusion, and overlap). 3. Answering questions about the identification of a
model or portion of a model, and about bounds on quantities derived from a
model. 4. Determing whether a given set of independence assertions. We discuss
the foundation of quantifier elimination and demonstrate its application to
these problems."
Faithful Approximations of Belief Functions,"A conceptual foundation for approximation of belief functions is proposed and
investigated. It is based on the requirements of consistency and closeness. An
optimal approximation is studied. Unfortunately, the computation of the optimal
approximation turns out to be intractable. Hence, various heuristic methods are
proposed and experimantally evaluated both in terms of their accuracy and in
terms of the speed of computation. These methods are compared to the earlier
proposed approximations of belief functions."
Estimating the Value of Computation in Flexible Information Refinement,"We outline a method to estimate the value of computation for a flexible
algorithm using empirical data. To determine a reasonable trade-off between
cost and value, we build an empirical model of the value obtained through
computation, and apply this model to estimate the value of computation for
quite different problems. In particular, we investigate this trade-off for the
problem of constructing policies for decision problems represented as influence
diagrams. We show how two features of our anytime algorithm provide reasonable
estimates of the value of computation in this domain."
Representing and Combining Partially Specified CPTs,"This paper extends previous work with network fragments and
situation-specific network construction. We formally define the asymmetry
network, an alternative representation for a conditional probability table. We
also present an object-oriented representation for partially specified
asymmetry networks. We show that the representation is parsimonious. We define
an algebra for the elements of the representation that allows us to 'factor'
any CPT and to soundly combine the partially specified asymmetry networks."
On the Complexity of Policy Iteration,"Decision-making problems in uncertain or stochastic domains are often
formulated as Markov decision processes (MDPs). Policy iteration (PI) is a
popular algorithm for searching over policy-space, the size of which is
exponential in the number of states. We are interested in bounds on the
complexity of PI that do not depend on the value of the discount factor. In
this paper we prove the first such non-trivial, worst-case, upper bounds on the
number of iterations required by PI to converge to the optimal policy. Our
analysis also sheds new light on the manner in which PI progresses through the
space of policies."
"A Variational Approximation for Bayesian Networks with Discrete and
  Continuous Latent Variables","We show how to use a variational approximation to the logistic function to
perform approximate inference in Bayesian networks containing discrete nodes
with continuous parents. Essentially, we convert the logistic function to a
Gaussian, which facilitates exact inference, and then iteratively adjust the
variational parameters to improve the quality of the approximation. We
demonstrate experimentally that this approximation is faster and potentially
more accurate than sampling. We also introduce a simple new technique for
handling evidence, which allows us to handle arbitrary distributions on
observed nodes, as well as achieving a significant speedup in networks with
discrete variables of large cardinality."
"Learning Bayesian Networks from Incomplete Data with Stochastic Search
  Algorithms","This paper describes stochastic search approaches, including a new stochastic
algorithm and an adaptive mutation operator, for learning Bayesian networks
from incomplete data. This problem is characterized by a huge solution space
with a highly multimodal landscape. State-of-the-art approaches all involve
using deterministic approaches such as the expectation-maximization algorithm.
These approaches are guaranteed to find local maxima, but do not explore the
landscape for other modes. Our approach evolves structure and the missing data.
We compare our stochastic algorithms and show they all produce accurate
results."
Enhancing QPNs for Trade-off Resolution,"Qualitative probabilistic networks have been introduced as qualitative
abstractions of Bayesian belief networks. One of the major drawbacks of these
qualitative networks is their coarse level of detail, which may lead to
unresolved trade-offs during inference. We present an enhanced formalism for
qualitative networks with a finer level of detail. An enhanced qualitative
probabilistic network differs from a regular qualitative network in that it
distinguishes between strong and weak influences. Enhanced qualitative
probabilistic networks are purely qualitative in nature, as regular qualitative
networks are, yet allow for efficiently resolving trade-offs during inference."
"A Possibilistic Model for Qualitative Sequential Decision Problems under
  Uncertainty in Partially Observable Environments","In this article we propose a qualitative (ordinal) counterpart for the
Partially Observable Markov Decision Processes model (POMDP) in which the
uncertainty, as well as the preferences of the agent, are modeled by
possibility distributions. This qualitative counterpart of the POMDP model
relies on a possibilistic theory of decision under uncertainty, recently
developed. One advantage of such a qualitative framework is its ability to
escape from the classical obstacle of stochastic POMDPs, in which even with a
finite state space, the obtained belief state space of the POMDP is infinite.
Instead, in the possibilistic framework even if exponentially larger than the
state space, the belief state space remains finite."
Efficient Value of Information Computation,"One of the most useful sensitivity analysis techniques of decision analysis
is the computation of value of information (or clairvoyance), the difference in
value obtained by changing the decisions by which some of the uncertainties are
observed. In this paper, some simple but powerful extensions to previous
algorithms are introduced which allow an efficient value of information
calculation on the rooted cluster tree (or strong junction tree) used to solve
the original decision problem."
Practical Uses of Belief Functions,"We present examples where the use of belief functions provided sound and
elegant solutions to real life problems. These are essentially characterized by
?missing' information. The examples deal with 1) discriminant analysis using a
learning set where classes are only partially known; 2) an information
retrieval systems handling inter-documents relationships; 3) the combination of
data from sensors competent on partially overlapping frames; 4) the
determination of the number of sources in a multi-sensor environment by
studying the inter-sensors contradiction. The purpose of the paper is to report
on such applications where the use of belief functions provides a convenient
tool to handle ?messy' data problems."
Multiplicative Factorization of Noisy-Max,"The noisy-or and its generalization noisy-max have been utilized to reduce
the complexity of knowledge acquisition. In this paper, we present a new
representation of noisy-max that allows for efficient inference in general
Bayesian networks. Empirical studies show that our method is capable of
computing queries in well-known large medical networks, QMR-DT and CPCS, for
which no previous exact inference method has been shown to perform well."
"A Method for Speeding Up Value Iteration in Partially Observable Markov
  Decision Processes","We present a technique for speeding up the convergence of value iteration for
partially observable Markov decisions processes (POMDPs). The underlying idea
is similar to that behind modified policy iteration for fully observable Markov
decision processes (MDPs). The technique can be easily incorporated into any
existing POMDP value iteration algorithms. Experiments have been conducted on
several test problems with one POMDP value iteration algorithm called
incremental pruning. We find that the technique can make incremental pruning
run several orders of magnitude faster."
On the Acceptability of Arguments in Preference-Based Argumentation,"Argumentation is a promising model for reasoning with uncertain knowledge.
The key concept of acceptability enables to differentiate arguments and
counterarguments: The certainty of a proposition can then be evaluated through
the most acceptable arguments for that proposition. In this paper, we
investigate different complementary points of view: - an acceptability based on
the existence of direct counterarguments, - an acceptability based on the
existence of defenders. Pursuing previous work on preference-based
argumentation principles, we enforce both points of view by taking into account
preference orderings for comparing arguments. Our approach is illustrated in
the context of reasoning with stratified knowldge bases."
Merging Uncertain Knowledge Bases in a Possibilistic Logic Framework,"This paper addresses the problem of merging uncertain information in the
framework of possibilistic logic. It presents several syntactic combination
rules to merge possibilistic knowledge bases, provided by different sources,
into a new possibilistic knowledge base. These combination rules are first
described at the meta-level outside the language of possibilistic logic. Next,
an extension of possibilistic logic, where the combination rules are inside the
language, is proposed. A proof system in a sequent form, which is sound and
complete with respect to the possibilistic logic semantics, is given."
"Dealing with Uncertainty in Situation Assessment: towards a Symbolic
  Approach","The situation assessment problem is considered, in terms of object,
condition, activity, and plan recognition, based on data coming from the
real-word {em via} various sensors. It is shown that uncertainty issues are
linked both to the models and to the matching algorithm. Three different types
of uncertainties are identified, and within each one, the numerical and the
symbolic cases are distinguished. The emphasis is then put on purely symbolic
uncertainties: it is shown that they can be dealt with within a purely symbolic
framework resulting from a transposition of classical numerical estimation
tools."
Marginalizing in Undirected Graph and Hypergraph Models,"Given an undirected graph G or hypergraph X model for a given set of
variables V, we introduce two marginalization operators for obtaining the
undirected graph GA or hypergraph HA associated with a given subset A c V such
that the marginal distribution of A factorizes according to GA or HA,
respectively. Finally, we illustrate the method by its application to some
practical examples. With them we show that hypergraph models allow defining a
finer factorization or performing a more precise conditional independence
analysis than undirected graph models."
Irrelevance and Independence Relations in Quasi-Bayesian Networks,"This paper analyzes irrelevance and independence relations in graphical
models associated with convex sets of probability distributions (called
Quasi-Bayesian networks). The basic question in Quasi-Bayesian networks is, How
can irrelevance/independence relations in Quasi-Bayesian networks be detected,
enforced and exploited? This paper addresses these questions through Walley's
definitions of irrelevance and independence. Novel algorithms and results are
presented for inferences with the so-called natural extensions using fractional
linear programming, and the properties of the so-called type-1 extensions are
clarified through a new generalization of d-separation."
On the Semi-Markov Equivalence of Causal Models,"The variability of structure in a finite Markov equivalence class of causally
sufficient models represented by directed acyclic graphs has been fully
characterized. Without causal sufficiency, an infinite semi-Markov equivalence
class of models has only been characterized by the fact that each model in the
equivalence class entails the same marginal statistical dependencies. In this
paper, we study the variability of structure of causal models within a
semi-Markov equivalence class and propose a systematic approach to construct
models entailing any specific marginal statistical dependencies."
"Comparative Uncertainty, Belief Functions and Accepted Beliefs","This paper relates comparative belief structures and a general view of belief
management in the setting of deductively closed logical representations of
accepted beliefs. We show that the range of compatibility between the classical
deductive closure and uncertain reasoning covers precisely the nonmonotonic
'preferential' inference system of Kraus, Lehmann and Magidor and nothing else.
In terms of uncertain reasoning any possibility or necessity measure gives
birth to a structure of accepted beliefs. The classes of probability functions
and of Shafer's belief functions which yield belief sets prove to be very
special ones."
Qualitative Decision Theory with Sugeno Integrals,"This paper presents an axiomatic framework for qualitative decision under
uncertainty in a finite setting. The corresponding utility is expressed by a
sup-min expression, called Sugeno (or fuzzy) integral. Technically speaking,
Sugeno integral is a median, which is indeed a qualitative counterpart to the
averaging operation underlying expected utility. The axiomatic justification of
Sugeno integral-based utility is expressed in terms of preference between acts
as in Savage decision theory. Pessimistic and optimistic qualitative utilities,
based on necessity and possibility measures, previously introduced by two of
the authors, can be retrieved in this setting by adding appropriate axioms."
Learning the Structure of Dynamic Probabilistic Networks,"Dynamic probabilistic networks are a compact representation of complex
stochastic processes. In this paper we examine how to learn the structure of a
DPN from data. We extend structure scoring rules for standard probabilistic
networks to the dynamic case, and show how to search for structure when some of
the variables are hidden. Finally, we examine two applications where such a
technology might be useful: predicting and classifying dynamic behaviors, and
learning causal orderings in biological processes. We provide empirical results
that demonstrate the applicability of our methods in both domains."
Solving POMDPs by Searching in Policy Space,"Most algorithms for solving POMDPs iteratively improve a value function that
implicitly represents a policy and are said to search in value function space.
This paper presents an approach to solving POMDPs that represents a policy
explicitly as a finite-state controller and iteratively improves the controller
by search in policy space. Two related algorithms illustrate this approach. The
first is a policy iteration algorithm that can outperform value iteration in
solving infinitehorizon POMDPs. It provides the foundation for a new heuristic
search algorithm that promises further speedup by focusing computational effort
on regions of the problem space that are reachable, or likely to be reached,
from a start state."
"Measure Selection: Notions of Rationality and Representation
  Independence","We take another look at the general problem of selecting a preferred
probability measure among those that comply with some given constraints. The
dominant role that entropy maximization has obtained in this context is
questioned by arguing that the minimum information principle on which it is
based could be supplanted by an at least as plausible ""likelihood of evidence""
principle. We then review a method for turning given selection functions into
representation independent variants, and discuss the tradeoffs involved in this
transformation."
"Exact Inference of Hidden Structure from Sample Data in Noisy-OR
  Networks","In the literature on graphical models, there has been increased attention
paid to the problems of learning hidden structure (see Heckerman [H96] for
survey) and causal mechanisms from sample data [H96, P88, S93, P95, F98]. In
most settings we should expect the former to be difficult, and the latter
potentially impossible without experimental intervention. In this work, we
examine some restricted settings in which perfectly reconstruct the hidden
structure solely on the basis of observed sample data."
Using Qualitative Relationships for Bounding Probability Distributions,"We exploit qualitative probabilistic relationships among variables for
computing bounds of conditional probability distributions of interest in
Bayesian networks. Using the signs of qualitative relationships, we can
implement abstraction operations that are guaranteed to bound the distributions
of interest in the desired direction. By evaluating incrementally improved
approximate networks, our algorithm obtains monotonically tightening bounds
that converge to exact distributions. For supermodular utility functions, the
tightening bounds monotonically reduce the set of admissible decision
alternatives as well."
Constructing Situation Specific Belief Networks,"This paper describes a process for constructing situation-specific belief
networks from a knowledge base of network fragments. A situation-specific
network is a minimal query complete network constructed from a knowledge base
in response to a query for the probability distribution on a set of target
variables given evidence and context variables. We present definitions of query
completeness and situation-specific networks. We describe conditions on the
knowledge base that guarantee query completeness. The relationship of our work
to earlier work on KBMC is also discussed."
Logarithmic Time Parallel Bayesian Inference,"I present a parallel algorithm for exact probabilistic inference in Bayesian
networks. For polytree networks with n variables, the worst-case time
complexity is O(log n) on a CREW PRAM (concurrent-read, exclusive-write
parallel random-access machine) with n processors, for any constant number of
evidence variables. For arbitrary networks, the time complexity is O(r^{3w}*log
n) for n processors, or O(w*log n) for r^{3w}*n processors, where r is the
maximum range of any variable, and w is the induced width (the maximum clique
size), after moralizing and triangulating the network."
Probabilistic Inference in Influence Diagrams,"This paper is about reducing influence diagram (ID) evaluation into Bayesian
network (BN) inference problems. Such reduction is interesting because it
enables one to readily use one's favorite BN inference algorithm to efficiently
evaluate IDs. Two such reduction methods have been proposed previously (Cooper
1988, Shachter and Peot 1992). This paper proposes a new method. The BN
inference problems induced by the mew method are much easier to solve than
those induced by the two previous methods."
Correlated Action Effects in Decision Theoretic Regression,"Much recent research in decision theoretic planning has adopted Markov
decision processes (MDPs) as the model of choice, and has attempted to make
their solution more tractable by exploiting problem structure. One particular
algorithm, structured policy construction achieves this by means of a decision
theoretic analog of goal regression using action descriptions based on Bayesian
networks with tree-structured conditional probability tables. The algorithm as
presented is not able to deal with actions with correlated effects. We describe
a new decision theoretic regression operator that corrects this weakness. While
conceptually straightforward, this extension requires a somewhat more
complicated technical approach."
Algorithms for Learning Decomposable Models and Chordal Graphs,"Decomposable dependency models and their graphical counterparts, i.e.,
chordal graphs, possess a number of interesting and useful properties. On the
basis of two characterizations of decomposable models in terms of independence
relationships, we develop an exact algorithm for recovering the chordal
graphical representation of any given decomposable model. We also propose an
algorithm for learning chordal approximations of dependency models isomorphic
to general undirected graphs."
"Incremental Pruning: A Simple, Fast, Exact Method for Partially
  Observable Markov Decision Processes","Most exact algorithms for general partially observable Markov decision
processes (POMDPs) use a form of dynamic programming in which a
piecewise-linear and convex representation of one value function is transformed
into another. We examine variations of the ""incremental pruning"" method for
solving this problem and compare them to earlier algorithms from theoretical
and empirical perspectives. We find that incremental pruning is presently the
most efficient exact method for solving POMDPs."
Defining Explanation in Probabilistic Systems,"As probabilistic systems gain popularity and are coming into wider use, the
need for a mechanism that explains the system's findings and recommendations
becomes more critical. The system will also need a mechanism for ordering
competing explanations. We examine two representative approaches to explanation
in the literature - one due to G\""ardenfors and one due to Pearl - and show
that both suffer from significant problems. We propose an approach to defining
a notion of ""better explanation"" that combines some of the features of both
together with more recent work by Pearl and others on causality."
Efficient Induction of Finite State Automata,"This paper introduces a new algorithm for the induction if complex finite
state automata from samples of behavior. The algorithm is based on information
theoretic principles. The algorithm reduces the search space by many orders of
magnitude over what was previously thought possible. We compare the algorithm
with some existing induction techniques for finite state automata and show that
the algorithm is much superior in both run time and quality of inductions."
"A Standard Approach for Optimizing Belief Network Inference using Query
  DAGs","This paper proposes a novel, algorithm-independent approach to optimizing
belief network inference. rather than designing optimizations on an algorithm
by algorithm basis, we argue that one should use an unoptimized algorithm to
generate a Q-DAG, a compiled graphical representation of the belief network,
and then optimize the Q-DAG and its evaluator instead. We present a set of
Q-DAG optimizations that supplant optimizations designed for traditional
inference algorithms, including zero compression, network pruning and caching.
We show that our Q-DAG optimizations require time linear in the Q-DAG size, and
significantly simplify the process of designing algorithms for optimizing
belief network inference."
Algorithm Portfolio Design: Theory vs. Practice,"Stochastic algorithms are among the best for solving computationally hard
search and reasoning problems. The runtime of such procedures is characterized
by a random variable. Different algorithms give rise to different probability
distributions. One can take advantage of such differences by combining several
algorithms into a portfolio, and running them in parallel or interleaving them
on a single processor. We provide a detailed evaluation of the portfolio
approach on distributions of hard combinatorial search problems. We show under
what conditions the protfolio approach can have a dramatic computational
advantage over the best traditional methods."
Inference with Idempotent Valuations,"Valuation based systems verifying an idempotent property are studied. A
partial order is defined between the valuations giving them a lattice
structure. Then, two different strategies are introduced to represent
valuations: as infimum of the most informative valuations or as supremum of the
least informative ones. It is studied how to carry out computations with both
representations in an efficient way. The particular cases of finite sets and
convex polytopes are considered."
Time-Critical Reasoning: Representations and Application,"We review the problem of time-critical action and discuss a reformulation
that shifts knowledge acquisition from the assessment of complex temporal
probabilistic dependencies to the direct assessment of time-dependent utilities
over key outcomes of interest. We dwell on a class of decision problems
characterized by the centrality of diagnosing and reacting in a timely manner
to pathological processes. We motivate key ideas in the context of trauma-care
triage and transportation decisions."
Relational Bayesian Networks,"A new method is developed to represent probabilistic relations on multiple
random events. Where previously knowledge bases containing probabilistic rules
were used for this purpose, here a probability distribution over the relations
is directly represented by a Bayesian network. By using a powerful way of
specifying conditional probability distributions in these networks, the
resulting formalism is more expressive than the previous ones. Particularly, it
provides for constraints on equalities of events, and it allows to define
complex, nested combination functions."
Probabilistic Acceptance,"The idea of fully accepting statements when the evidence has rendered them
probable enough faces a number of difficulties. We leave the interpretation of
probability largely open, but attempt to suggest a contextual approach to full
belief. We show that the difficulties of probabilistic acceptance are not as
severe as they are sometimes painted, and that though there are oddities
associated with probabilistic acceptance they are in some instances less
awkward than the difficulties associated with other nonmonotonic formalisms. We
show that the structure at which we arrive provides a natural home for
statistical inference."
"Incremental Map Generation by Low Cost Robots Based on
  Possibility/Necessity Grids","In this paper we present some results obtained with a troupe of low-cost
robots designed to cooperatively explore and adquire the map of unknown
structured orthogonal environments. In order to improve the covering of the
explored zone, the robots show different behaviours and cooperate by
transferring each other the perceived environment when they meet. The returning
robots deliver to a host computer their partial maps and the host incrementally
generates the map of the environment by means of apossibility/ necessity grid."
"Structure and Parameter Learning for Causal Independence and Causal
  Interaction Models","This paper discusses causal independence models and a generalization of these
models called causal interaction models. Causal interaction models are models
that have independent mechanisms where a mechanism can have several causes. In
addition to introducing several particular types of causal interaction models,
we show how we can apply the Bayesian approach to learning causal interaction
models obtaining approximate posterior distributions for the models and obtain
MAP and ML estimates for the parameters. We illustrate the approach with a
simulation study of learning model posteriors."
Learning Bayesian Networks from Incomplete Databases,"Bayesian approaches to learn the graphical structure of Bayesian Belief
Networks (BBNs) from databases share the assumption that the database is
complete, that is, no entry is reported as unknown. Attempts to relax this
assumption involve the use of expensive iterative methods to discriminate among
different structures. This paper introduces a deterministic method to learn the
graphical structure of a BBN from a possibly incomplete database. Experimental
evaluations show a significant robustness of this method and a remarkable
independence of its execution time from the number of missing data."
Independence of Causal Influence and Clique Tree Propagation,"This paper explores the role of independence of causal influence (ICI) in
Bayesian network inference. ICI allows one to factorize a conditional
probability table into smaller pieces. We describe a method for exploiting the
factorization in clique tree propagation (CTP) - the state-of-the-art exact
inference algorithm for Bayesian networks. We also present empirical results
showing that the resulting algorithm is significantly more efficient than the
combination of CTP and previous techniques for exploiting ICI."
Fast Value Iteration for Goal-Directed Markov Decision Processes,"Planning problems where effects of actions are non-deterministic can be
modeled as Markov decision processes. Planning problems are usually
goal-directed. This paper proposes several techniques for exploiting the
goal-directedness to accelerate value iteration, a standard algorithm for
solving Markov decision processes. Empirical studies have shown that the
techniques can bring about significant speedups."
"Approximations for Decision Making in the Dempster-Shafer Theory of
  Evidence","The computational complexity of reasoning within the Dempster-Shafer theory
of evidence is one of the main points of criticism this formalism has to face.
To overcome this difficulty various approximation algorithms have been
suggested that aim at reducing the number of focal elements in the belief
functions involved. Besides introducing a new algorithm using this method, this
paper describes an empirical study that examines the appropriateness of these
approximation procedures in decision making situations. It presents the
empirical findings and discusses the various tradeoffs that have to be taken
into account when actually applying one of these methods."
Arguing for Decisions: A Qualitative Model of Decision Making,"We develop a qualitative model of decision making with two aims: to describe
how people make simple decisions and to enable computer programs to do the
same. Current approaches based on Planning or Decisions Theory either ignore
uncertainty and tradeoffs, or provide languages and algorithms that are too
complex for this task. The proposed model provides a language based on rules, a
semantics based on high probabilities and lexicographical preferences, and a
transparent decision procedure where reasons for and against decisions
interact. The model is no substitude for Decision Theory, yet for decisions
that people find easy to explain it may provide an appealing alternative."
Some Experiments with Real-Time Decision Algorithms,"Real-time Decision algorithms are a class of incremental resource-bounded
[Horvitz, 89] or anytime [Dean, 93] algorithms for evaluating influence
diagrams. We present a test domain for real-time decision algorithms, and the
results of experiments with several Real-time Decision Algorithms in this
domain. The results demonstrate high performance for two algorithms, a
decision-evaluation variant of Incremental Probabilisitic Inference [D'Ambrosio
93] and a variant of an algorithm suggested by Goldszmidt, [Goldszmidt, 95],
PK-reduced. We discuss the implications of these experimental results and
explore the broader applicability of these algorithms."
"Bucket Elimination: A Unifying Framework for Several Probabilistic
  Inference","Probabilistic inference algorithms for finding the most probable explanation,
the maximum aposteriori hypothesis, and the maximum expected utility and for
updating belief are reformulated as an elimination--type algorithm called
bucket elimination. This emphasizes the principle common to many of the
algorithms appearing in that literature and clarifies their relationship to
nonserial dynamic programming algorithms. We also present a general way of
combining conditioning and elimination within this framework. Bounds on
complexity are given for all the algorithms as a function of the problem's
structure."
Flexible Policy Construction by Information Refinement,"We report on work towards flexible algorithms for solving decision problems
represented as influence diagrams. An algorithm is given to construct a tree
structure for each decision node in an influence diagram. Each tree represents
a decision function and is constructed incrementally. The improvements to the
tree converge to the optimal decision function (neglecting computational costs)
and the asymptotic behaviour is only a constant factor worse than dynamic
programming techniques, counting the number of Bayesian network queries.
Empirical results show how expected utility increases with the size of the tree
and the number of Bayesian net calculations."
"Efficient Search-Based Inference for Noisy-OR Belief Networks:
  TopEpsilon","Inference algorithms for arbitrary belief networks are impractical for large,
complex belief networks. Inference algorithms for specialized classes of belief
networks have been shown to be more efficient. In this paper, we present a
search-based algorithm for approximate inference on arbitrary, noisy-OR belief
networks, generalizing earlier work on search-based inference for two-level,
noisy-OR belief networks. Initial experimental results appear promising."
MIDAS - An Influence Diagram for Management of Mildew in Winter Wheat,"We present a prototype of a decision support system for management of the
fungal disease mildew in winter wheat. The prototype is based on an influence
diagram which is used to determine the optimal time and dose of mildew
treatments. This involves multiple decision opportunities over time,
stochasticity, inaccurate information and incomplete knowledge. The paper
describes the practical and theoretical problems encountered during the
construction of the influence diagram, and also the experience with the
prototype."
Toward a Market Model for Bayesian Inference,"We present a methodology for representing probabilistic relationships in a
general-equilibrium economic model. Specifically, we define a precise mapping
from a Bayesian network with binary nodes to a market price system where
consumers and producers trade in uncertain propositions. We demonstrate the
correspondence between the equilibrium prices of goods in this economy and the
probabilities represented by the Bayesian network. A computational market model
such as this may provide a useful framework for investigations of belief
aggregation, distributed probabilistic inference, resource allocation under
uncertainty, and other problems of decentralized uncertainty."
A Graph-Theoretic Analysis of Information Value,"We derive qualitative relationships about the informational relevance of
variables in graphical decision models based on a consideration of the topology
of the models. Specifically, we identify dominance relations for the expected
value of information on chance variables in terms of their position and
relationships in influence diagrams. The qualitative relationships can be
harnessed to generate nonnumerical procedures for ordering uncertain variables
in a decision model by their informational relevance."
Optimal Monte Carlo Estimation of Belief Network Inference,"We present two Monte Carlo sampling algorithms for probabilistic inference
that guarantee polynomial-time convergence for a larger class of network than
current sampling algorithms provide. These new methods are variants of the
known likelihood weighting algorithm. We use of recent advances in the theory
of optimal stopping rules for Monte Carlo simulation to obtain an inference
approximation with relative error epsilon and a small failure probability
delta. We present an empirical evaluation of the algorithms which demonstrates
their improved performance."
Coherent Knowledge Processing at Maximum Entropy by SPIRIT,"SPIRIT is an expert system shell for probabilistic knowledge bases. Knowledge
acquisition is performed by processing facts and rules on discrete variables in
a rich syntax. The shell generates a probability distribution which respects
all acquired facts and rules and which maximizes entropy. The user-friendly
devices of SPIRIT to define variables, formulate rules and create the knowledge
base are revealed in detail. Inductive learning is possible. Medium sized
applications show the power of the system."
A Measure of Decision Flexibility,"We propose a decision-analytical approach to comparing the flexibility of
decision situations from the perspective of a decision-maker who exhibits
constant risk-aversion over a monetary value model. Our approach is simple yet
seems to be consistent with a variety of flexibility concepts, including robust
and adaptive alternatives. We try to compensate within the model for
uncertainty that was not anticipated or not modeled. This approach not only
allows one to compare the flexibility of plans, but also guides the search for
new, more flexible alternatives."
Testing Implication of Probabilistic Dependencies,"Axiomatization has been widely used for testing logical implications. This
paper suggests a non-axiomatic method, the chase, to test if a new dependency
follows from a given set of probabilistic dependencies. Although the chase
computation may require exponential time in some cases, this technique is a
powerful tool for establishing nontrivial theoretical results. More
importantly, this approach provides valuable insight into the intriguing
connection between relational databases and probabilistic reasoning systems."
In Love With a Robot: the Dawn of Machine-To-Machine Marketing,"The article looks at mass market artificial intelligence tools in the context
of their ever-growing sophistication, availability and market penetration. The
subject is especially relevant today for these exact reasons - if a few years
ago AI was the subject of high tech research and science fiction novels, today,
we increasingly rely on cloud robotics to cater to our daily needs - to trade
stock, predict weather, manage diaries, find friends and buy presents online."
Counterfactuals and Policy Analysis in Structural Models,"Evaluation of counterfactual queries (e.g., ""If A were true, would C have
been true?"") is important to fault diagnosis, planning, determination of
liability, and policy analysis. We present a method of revaluating
counterfactuals when the underlying causal model is represented by structural
models - a nonlinear generalization of the simultaneous equations models
commonly used in econometrics and social sciences. This new method provides a
coherent means for evaluating policies involving the control of variables
which, prior to enacting the policy were influenced by other variables in the
system."
Belief Functions and Default Reasoning,"We present a new approach to dealing with default information based on the
theory of belief functions. Our semantic structures, inspired by Adams'
epsilon-semantics, are epsilon-belief assignments, where values committed to
focal elements are either close to 0 or close to 1. We define two systems based
on these structures, and relate them to other non-monotonic systems presented
in the literature. We show that our second system correctly addresses the
well-known problems of specificity, irrelevance, blocking of inheritance,
ambiguity, and redundancy."
Chain Graphs for Learning,"Chain graphs combine directed and undirected graphs and their underlying
mathematics combines properties of the two. This paper gives a simplified
definition of chain graphs based on a hierarchical combination of Bayesian
(directed) and Markov (undirected) networks. Examples of a chain graph are
multivariate feed-forward networks, clustering with conditional interaction
between variables, and forms of Bayes classifiers. Chain graphs are then
extended using the notation of plates so that samples and data analysis
problems can be represented in a graphical model as well. Implications for
learning are discussed in the conclusion."
"A Transformational Characterization of Equivalent Bayesian Network
  Structures","We present a simple characterization of equivalent Bayesian network
structures based on local transformations. The significance of the
characterization is twofold. First, we are able to easily prove several new
invariant properties of theoretical interest for equivalent structures. Second,
we use the characterization to derive an efficient algorithm that identifies
all of the compelled edges in a structure. Compelled edge identification is of
particular importance for learning Bayesian network structures from data
because these edges indicate causal relationships when certain assumptions
hold."
"Conditioning Methods for Exact and Approximate Inference in Causal
  Networks","We present two algorithms for exact and approximate inference in causal
networks. The first algorithm, dynamic conditioning, is a refinement of cutset
conditioning that has linear complexity on some networks for which cutset
conditioning is exponential. The second algorithm, B-conditioning, is an
algorithm for approximate inference that allows one to trade-off the quality of
approximations with the computation time. We also present some experimental
results illustrating the properties of the proposed algorithms."
"Implementation of Continuous Bayesian Networks Using Sums of Weighted
  Gaussians","Bayesian networks provide a method of representing conditional independence
between random variables and computing the probability distributions associated
with these random variables. In this paper, we extend Bayesian network
structures to compute probability density functions for continuous random
variables. We make this extension by approximating prior and conditional
densities using sums of weighted Gaussian distributions and then finding the
propagation rules for updating the densities in terms of these weights. We
present a simple example that illustrates the Bayesian network for continuous
variables; this example shows the effect of the network structure and
approximation errors on the computation of densities for variables in the
network."
Testing Identifiability of Causal Effects,"This paper concerns the probabilistic evaluation of the effects of actions in
the presence of unmeasured variables. We show that the identification of causal
effect between a singleton variable X and a set of variables Y can be
accomplished systematically, in time polynomial in the number of variables in
the graph. When the causal effect is identifiable, a closed-form expression can
be obtained for the probability that the action will achieve a specified goal,
or a set of goals."
Efficient Decision-Theoretic Planning: Techniques and Empirical Analysis,"This paper discusses techniques for performing efficient decision-theoretic
planning. We give an overview of the DRIPS decision-theoretic refinement
planning system, which uses abstraction to efficiently identify optimal plans.
We present techniques for automatically generating search control information,
which can significantly improve the planner's performance. We evaluate the
efficiency of DRIPS both with and without the search control rules on a complex
medical planning problem and compare its performance to that of a
branch-and-bound decision tree algorithm."
"Reasoning, Metareasoning, and Mathematical Truth: Studies of Theorem
  Proving under Limited Resources","In earlier work, we introduced flexible inference and decision-theoretic
metareasoning to address the intractability of normative inference. Here,
rather than pursuing the task of computing beliefs and actions with decision
models composed of distinctions about uncertain events, we examine methods for
inferring beliefs about mathematical truth before an automated theorem prover
completes a proof. We employ a Bayesian analysis to update belief in truth,
given theorem-proving progress, and show how decision-theoretic methods can be
used to determine the value of continuing to deliberate versus taking immediate
action in time-critical situations."
Improved Sampling for Diagnostic Reasoning in Bayesian Networks,"Bayesian networks offer great potential for use in automating large scale
diagnostic reasoning tasks. Gibbs sampling is the main technique used to
perform diagnostic reasoning in large richly interconnected Bayesian networks.
Unfortunately Gibbs sampling can take an excessive time to generate a
representative sample. In this paper we describe and test a number of heuristic
strategies for improving sampling in noisy-or Bayesian networks. The strategies
include Monte Carlo Markov chain sampling techniques other than Gibbs sampling.
Emphasis is put on strategies that can be implemented in distributed systems."
Cautious Propagation in Bayesian Networks,"Consider the situation where some evidence e has been entered to a Bayesian
network. When performing conflict analysis, sensitivity analysis, or when
answering questions like ""What if the finding on X had been y instead of x?""
you need probabilities P (e'| h), where e' is a subset of e, and h is a
configuration of a (possibly empty) set of variables. Cautious propagation is a
modification of HUGIN propagation into a Shafer-Shenoy-like architecture. It is
less efficient than HUGIN propagation; however, it provides easy access to P
(e'| h) for a great deal of relevant subsets e'."
HUGS: Combining Exact Inference and Gibbs Sampling in Junction Trees,"Dawid, Kjaerulff and Lauritzen (1994) provided a preliminary description of a
hybrid between Monte-Carlo sampling methods and exact local computations in
junction trees. Utilizing the strengths of both methods, such hybrid inference
methods has the potential of expanding the class of problems which can be
solved under bounded resources as well as solving problems which otherwise
resist exact solutions. The paper provides a detailed description of a
particular instance of such a hybrid scheme; namely, combination of exact
inference and Gibbs sampling in discrete Bayesian networks. We argue that this
combination calls for an extension of the usual message passing scheme of
ordinary junction trees."
Is There a Role for Qualitative Risk Assessment?,"Classically, risk is characterized by a point value probability indicating
the likelihood of occurrence of an adverse effect. However, there are domains
where the attainability of objective numerical risk characterizations is
increasingly being questioned. This paper reviews the arguments in favour of
extending classical techniques of risk assessment to incorporate meaningful
qualitative and weak quantitative risk characterizations. A technique in which
linguistic uncertainty terms are defined in terms of patterns of argument is
then proposed. The technique is demonstrated using a prototype computer-based
system for predicting the carcinogenic risk due to novel chemical compounds."
On the Complexity of Solving Markov Decision Problems,"Markov decision problems (MDPs) provide the foundations for a number of
problems of interest to AI researchers studying automated planning and
reinforcement learning. In this paper, we summarize results regarding the
complexity of solving MDPs and the running time of MDP solution algorithms. We
argue that, although MDPs can be solved efficiently in theory, more study is
needed to reveal practical algorithms for solving large problems quickly. To
encourage future research, we sketch some alternative methods of analysis that
rely on the structure of MDPs."
"A Theoretical Framework for Context-Sensitive Temporal Probability Model
  Construction with Application to Plan Projection","We define a context-sensitive temporal probability logic for representing
classes of discrete-time temporal Bayesian networks. Context constraints allow
inference to be focused on only the relevant portions of the probabilistic
knowledge. We provide a declarative semantics for our language. We present a
Bayesian network construction algorithm whose generated networks give sound and
complete answers to queries. We use related concepts in logic programming to
justify our approach. We have implemented a Bayesian network construction
algorithm for a subset of the theory and demonstrate it's application to the
problem of evaluating the effectiveness of treatments for acute cardiac
conditions."
Refining Reasoning in Qualitative Probabilistic Networks,"In recent years there has been a spate of papers describing systems for
probabilisitic reasoning which do not use numerical probabilities. In some
cases the simple set of values used by these systems make it impossible to
predict how a probability will change or which hypothesis is most likely given
certain evidence. This paper concentrates on such situations, and suggests a
number of ways in which they may be resolved by refining the representation."
"On the Testability of Causal Models with Latent and Instrumental
  Variables","Certain causal models involving unmeasured variables induce no independence
constraints among the observed variables but imply, nevertheless, inequality
contraints on the observed distribution. This paper derives a general formula
for such instrumental variables, that is, exogenous variables that directly
affect some variables but not all. With the help of this formula, it is
possible to test whether a model involving instrumental variables may account
for the data, or, conversely, whether a given variables can be deemed
instrumental."
"Probabilistic Evaluation of Sequential Plans from Causal Models with
  Hidden Variables","The paper concerns the probabilistic evaluation of plans in the presence of
unmeasured variables, each plan consisting of several concurrent or sequential
actions. We establish a graphical criterion for recognizing when the effects of
a given plan can be predicted from passive observations on measured variables
only. When the criterion is satisfied, a closed-form expression is provided for
the probability that the plan will achieve a specified goal."
Causal Inference in the Presence of Latent Variables and Selection Bias,"We show that there is a general, informative and reliable procedure for
discovering causal relations when, for all the investigator knows, both latent
variables and selection bias may be at work. Given information about
conditional independence and dependence relations between measured variables,
even when latent variables and selection bias may be present, there are
sufficient conditions for reliably concluding that there is a causal path from
one variable to another, and sufficient conditions for reliably concluding when
no such causal path exists."
An Order of Magnitude Calculus,"This paper develops a simple calculus for order of magnitude reasoning. A
semantics is given with soundness and completeness results. Order of magnitude
probability functions are easily defined and turn out to be equivalent to kappa
functions, which are slight generalizations of Spohn's Natural Conditional
Functions. The calculus also gives rise to an order of magnitude decision
theory, which can be used to justify an amended version of Pearl's decision
theory for kappa functions, although the latter is weaker and less expressive."
A Method for Implementing a Probabilistic Model as a Relational Database,"This paper discusses a method for implementing a probabilistic inference
system based on an extended relational data model. This model provides a
unified approach for a variety of applications such as dynamic programming,
solving sparse linear equations, and constraint propagation. In this framework,
the probability model is represented as a generalized relational database.
Subsequent probabilistic requests can be processed as standard relational
queries. Conventional database management systems can be easily adopted for
implementing such an approximate reasoning system."
Generating Explanations for Evidential Reasoning,"In this paper, we present two methods to provide explanations for reasoning
with belief functions in the valuation-based systems. One approach, inspired by
Strat's method, is based on sensitivity analysis, but its computation is
simpler thus easier to implement than Strat's. The other one is to examine the
impact of evidence on the conclusion based on the measure of the information
content in the evidence. We show the property of additivity for the pieces of
evidence that are conditional independent within the context of the
valuation-based systems. We will give an example to show how these approaches
are applied in an evidential network."
Inference with Causal Independence in the CPSC Network,"This paper reports experiments with the causal independence inference
algorithm proposed by Zhang and Poole (1994b) on the CPSC network created by
Pradhan et al. (1994). It is found that the algorithm is able to answer 420 of
the 422 possible zero-observation queries, 94 of 100 randomly generated
five-observation queries, 87 of 100 randomly generated ten-observation queries,
and 69 of 100 randomly generated twenty-observation queries."
"Modus Ponens Generating Function in the Class of ^-valuations of
  Plausibility","We discuss the problem of construction of inference procedures which can
manipulate with uncertainties measured in ordinal scales and fulfill to the
property of strict monotonicity of conclusion. The class of A-valuations of
plausibility is considered where operations based only on information about
linear ordering of plausibility values are used. In this class the modus ponens
generating function fulfiling to the property of strict monotonicity of
conclusions is introduced."
Approximation Algorithms for the Loop Cutset Problem,"We show how to find a small loop curser in a Bayesian network. Finding such a
loop cutset is the first step in the method of conditioning for inference. Our
algorithm for finding a loop cutset, called MGA, finds a loop cutset which is
guaranteed in the worst case to contain less than twice the number of variables
contained in a minimum loop cutset. We test MGA on randomly generated graphs
and find that the average ratio between the number of instances associated with
the algorithms' output and the number of instances associated with a minimum
solution is 1.22."
Exploratory Model Building,"Some instances of creative thinking require an agent to build and test
hypothetical theories. Such a reasoner needs to explore the space of not only
those situations that have occurred in the past, but also those that are
rationally conceivable. In this paper we present a formalism for exploring the
space of conceivable situation-models for those domains in which the knowledge
is primarily probabilistic in nature. The formalism seeks to construct
consistent, minimal, and desirable situation-descriptions by selecting suitable
domain-attributes and dependency relationships from the available domain
knowledge."
A Stratified Simulation Scheme for Inference in Bayesian Belief Networks,"Simulation schemes for probabilistic inference in Bayesian belief networks
offer many advantages over exact algorithms; for example, these schemes have a
linear and thus predictable runtime while exact algorithms have exponential
runtime. Experiments have shown that likelihood weighting is one of the most
promising simulation schemes. In this paper, we present a new simulation scheme
that generates samples more evenly spread in the sample space than the
likelihood weighting scheme. We show both theoretically and experimentally that
the stratified scheme outperforms likelihood weighting in average runtime and
error in estimates of beliefs."
Symbolic Probabilitistic Inference in Large BN2O Networks,"A BN2O network is a two level belief net in which the parent interactions are
modeled using the noisy-or interaction model. In this paper we discuss
application of the SPI local expression language to efficient inference in
large BN2O networks. In particular, we show that there is significant
structure, which can be exploited to improve over the Quickscore result. We
further describe how symbolic techniques can provide information which can
significantly reduce the computation required for computing all cause posterior
marginals. Finally, we present a novel approximation technique with preliminary
experimental results."
Integrating Planning and Execution in Stochastic Domains,"We investigate planning in time-critical domains represented as Markov
Decision Processes, showing that search based techniques can be a very powerful
method for finding close to optimal plans. To reduce the computational cost of
planning in these domains, we execute actions as we construct the plan, and
sacrifice optimality by searching to a fixed depth and using a heuristic
function to estimate the value of states. Although this paper concentrates on
the search algorithm, we also discuss ways of constructing heuristic functions
suitable for this approach. Our results show that by interleaving search and
execution, close to optimal policies can be found without the computational
requirements of other approaches."
Penalty logic and its Link with Dempster-Shafer Theory,"Penalty logic, introduced by Pinkas, associates to each formula of a
knowledge base the price to pay if this formula is violated. Penalties may be
used as a criterion for selecting preferred consistent subsets in an
inconsistent knowledge base, thus inducing a non-monotonic inference relation.
A precise formalization and the main properties of penalty logic and of its
associated non-monotonic inference relation are given in the first part. We
also show that penalty logic and Dempster-Shafer theory are related, especially
in the infinitesimal case."
Conditional Independence in Possibility Theory,"Possibilistic conditional independence is investigated: we propose a
definition of this notion similar to the one used in probability theory. The
links between independence and non-interactivity are investigated, and
properties of these relations are given. The influence of the conjunction used
to define a conditional measure of possibility is also highlighted: we examine
three types of conjunctions: Lukasiewicz - like T-norms, product-like T-norms
and the minimum operator."
Backward Simulation in Bayesian Networks,"Backward simulation is an approximate inference technique for Bayesian belief
networks. It differs from existing simulation methods in that it starts
simulation from the known evidence and works backward (i.e., contrary to the
direction of the arcs). The technique's focus on the evidence leads to improved
convergence in situations where the posterior beliefs are dominated by the
evidence rather than by the prior probabilities. Since this class of situations
is large, the technique may make practical the application of approximate
inference in Bayesian belief networks to many real-world problems."
Abstracting Probabilistic Actions,"This paper discusses the problem of abstracting conditional probabilistic
actions. We identify two distinct types of abstraction: intra-action
abstraction and inter-action abstraction. We define what it means for the
abstraction of an action to be correct and then derive two methods of
intra-action abstraction and two methods of inter-action abstraction which are
correct according to this criterion. We illustrate the developed techniques by
applying them to actions described with the temporal action representation used
in the DRIPS decision-theoretic planner and we describe how the planner uses
abstraction to reduce the complexity of planning."
On Modal Logics for Qualitative Possibility in a Fuzzy Setting,"Within the possibilistic approach to uncertainty modeling, the paper presents
a modal logical system to reason about qualitative (comparative) statements of
the possibility (and necessity) of fuzzy propositions. We relate this
qualitative modal logic to the many--valued analogues MVS5 and MVKD45 of the
well known modal logics of knowledge and belief S5 and KD45 respectively.
Completeness results are obtained for such logics and therefore, they extend
previous existing results for qualitative possibilistic logics in the classical
non-fuzzy setting."
A Logic for Default Reasoning About Probabilities,"A logic is defined that allows to express information about statistical
probabilities and about degrees of belief in specific propositions. By
interpreting the two types of probabilities in one common probability space,
the semantics given are well suited to model the influence of statistical
information on the formation of subjective beliefs. Cross entropy minimization
is a key element in these semantics, the use of which is justified by showing
that the resulting logic exhibits some very reasonable properties."
Optimal Junction Trees,"The paper deals with optimality issues in connection with updating beliefs in
networks. We address two processes: triangulation and construction of junction
trees. In the first part, we give a simple algorithm for constructing an
optimal junction tree from a triangulated network. In the second part, we argue
that any exact method based on local calculations must either be less efficient
than the junction tree method, or it has an optimality problem equivalent to
that of triangulation."
Constructing Belief Networks to Evaluate Plans,"This paper examines the problem of constructing belief networks to evaluate
plans produced by an knowledge-based planner. Techniques are presented for
handling various types of complicating plan features. These include plans with
context-dependent consequences, indirect consequences, actions with
preconditions that must be true during the execution of an action,
contingencies, multiple levels of abstraction multiple execution agents with
partially-ordered and temporally overlapping actions, and plans which reference
specific times and time durations."
Anytime Decision Making with Imprecise Probabilities,"This paper examines methods of decision making that are able to accommodate
limitations on both the form in which uncertainty pertaining to a decision
problem can be realistically represented and the amount of computing time
available before a decision must be made. The methods are anytime algorithms in
the sense of Boddy and Dean 1991. Techniques are presented for use with Frisch
and Haddawy's [1992] anytime deduction system, with an anytime adaptation of
Nilsson's [1986] probabilistic logic, and with a probabilistic database model."
Solving Asymmetric Decision Problems with Influence Diagrams,"While influence diagrams have many advantages as a representation framework
for Bayesian decision problems, they have a serious drawback in handling
asymmetric decision problems. To be represented in an influence diagram, an
asymmetric decision problem must be symmetrized. A considerable amount of
unnecessary computation may be involved when a symmetrized influence diagram is
evaluated by conventional algorithms. In this paper we present an approach for
avoiding such unnecessary computation in influence diagram evaluation."
A Probabilistic Approach to Hierarchical Model-based Diagnosis,"Model-based diagnosis reasons backwards from a functional schematic of a
system to isolate faults given observations of anomalous behavior. We develop a
fully probabilistic approach to model based diagnosis and extend it to support
hierarchical models. Our scheme translates the functional schematic into a
Bayesian network and diagnostic inference takes place in the Bayesian network.
A Bayesian network diagnostic inference algorithm is modified to take advantage
of the hierarchy to give computational gains."
"Semigraphoids Are Two-Antecedental Approximations of Stochastic
  Conditional Independence Models","The semigraphoid closure of every couple of CI-statements (GI=conditional
independence) is a stochastic CI-model. As a consequence of this result it is
shown that every probabilistically sound inference rule for CI-model, having at
most two antecedents, is derivable from the semigraphoid inference rules. This
justifies the use of semigraphoids as approximations of stochastic CI-models in
probabilistic reasoning. The list of all 19 potential dominant elements of the
mentioned semigraphoid closure is given as a byproduct."
Exceptional Subclasses in Qualitative Probability,"System Z+ [Goldszmidt and Pearl, 1991, Goldszmidt, 1992] is a formalism for
reasoning with normality defaults of the form ""typically if phi then + (with
strength cf)"" where 6 is a positive integer. The system has a critical
shortcoming in that it does not sanction inheritance across exceptional
subclasses. In this paper we propose an extension to System Z+ that rectifies
this shortcoming by extracting additional conditions between worlds from the
defaults database. We show that the additional constraints do not change the
notion of the consistency of a database. We also make comparisons with
competing default reasoning systems."
A Defect in Dempster-Shafer Theory,"By analyzing the relationships among chance, weight of evidence and degree of
beliefwe show that the assertion ""probability functions are special cases of
belief functions"" and the assertion ""Dempster's rule can be used to combine
belief functions based on distinct bodies of evidence"" together lead to an
inconsistency in Dempster-Shafer theory. To solve this problem, we must reject
some fundamental postulates of the theory. We introduce a new approach for
uncertainty management that shares many intuitive ideas with D-S theory, while
avoiding this problem."
State-space Abstraction for Anytime Evaluation of Probabilistic Networks,"One important factor determining the computational complexity of evaluating a
probabilistic network is the cardinality of the state spaces of the nodes. By
varying the granularity of the state spaces, one can trade off accuracy in the
result for computational efficiency. We present an anytime procedure for
approximate evaluation of probabilistic networks based on this idea. On
application to some simple networks, the procedure exhibits a smooth
improvement in approximation quality as computation time increases. This
suggests that state-space abstraction is one more useful control parameter for
designing real-time probabilistic reasoners."
Generating Graphoids from Generalised Conditional Probability,"We take a general approach to uncertainty on product spaces, and give
sufficient conditions for the independence structures of uncertainty measures
to satisfy graphoid properties. Since these conditions are arguably more
intuitive than some of the graphoid properties, they can be viewed as
explanations why probability and certain other formalisms generate graphoids.
The conditions include a sufficient condition for the Intersection property
which can still apply even if there is a strong logical relations hip between
the variables. We indicate how these results can be used to produce theories of
qualitative conditional probability which are semi-graphoids and graphoids."
Evidential Reasoning with Conditional Belief Functions,"In the existing evidential networks with belief functions, the relations
among the variables are always represented by joint belief functions on the
product space of the involved variables. In this paper, we use conditional
belief functions to represent such relations in the network and show some
relations of these two kinds of representations. We also present a propagation
algorithm for such networks. By analyzing the properties of some special
evidential networks with conditional belief functions, we show that the
reasoning process can be simplified in such kinds of networks."
Inter-causal Independence and Heterogeneous Factorization,"It is well known that conditional independence can be used to factorize a
joint probability into a multiplication of conditional probabilities. This
paper proposes a constructive definition of inter-causal independence, which
can be used to further factorize a conditional probability. An inference
algorithm is developed, which makes use of both conditional independence and
inter-causal independence to reduce inference complexity in Bayesian networks."
Causality in Bayesian Belief Networks,"We address the problem of causal interpretation of the graphical structure of
Bayesian belief networks (BBNs). We review the concept of causality explicated
in the domain of structural equations models and show that it is applicable to
BBNs. In this view, which we call mechanism-based, causality is defined within
models and causal asymmetries arise when mechanisms are placed in the context
of a system. We lay the link between structural equations models and BBNs
models and formulate the conditions under which the latter can be given causal
interpretation."
From Conditional Oughts to Qualitative Decision Theory,"The primary theme of this investigation is a decision theoretic account of
conditional ought statements (e.g., ""You ought to do A, if C"") that rectifies
glaring deficiencies in classical deontic logic. The resulting account forms a
sound basis for qualitative decision theory, thus providing a framework for
qualitative planning under uncertainty. In particular, we show that adding
causal relationships (in the form of a single graph) as part of an epistemic
state is sufficient to facilitate the analysis of action sequences, their
consequences, their interaction with observations, their expected utilities
and, hence, the synthesis of plans and strategies under uncertainty."
Parameter Adjustment in Bayes Networks. The generalized noisy OR-gate,"Spiegelhalter and Lauritzen [15] studied sequential learning in Bayesian
networks and proposed three models for the representation of conditional
probabilities. A forth model, shown here, assumes that the parameter
distribution is given by a product of Gaussian functions and updates them from
the _ and _r messages of evidence propagation. We also generalize the noisy
OR-gate for multivalued variables, develop the algorithm to compute probability
in time proportional to the number of parents (even in networks with loops) and
apply the learning model to this gate."
Causal Independence for Knowledge Acquisition and Inference,"I introduce a temporal belief-network representation of causal independence
that a knowledge engineer can use to elicit probabilistic models. Like the
current, atemporal belief-network representation of causal independence, the
new representation makes knowledge acquisition tractable. Unlike the atemproal
representation, however, the temporal representation can simplify inference,
and does not require the use of unobservable variables. The representation is
less general than is the atemporal representation, but appears to be useful for
many practical applications."
Sensitivity Analysis for Probability Assessments in Bayesian Networks,"When eliciting probability models from experts, knowledge engineers may
compare the results of the model with expert judgment on test scenarios, then
adjust model parameters to bring the behavior of the model more in line with
the expert's intuition. This paper presents a methodology for analytic
computation of sensitivity values to measure the impact of small changes in a
network parameter on a target probability value or distribution. These values
can be used to guide knowledge elicitation. They can also be used in a gradient
descent algorithm to estimate parameter values that maximize a measure of
goodness-of-fit to both local and holistic probability assessments."
Causal Modeling,"Causal Models are like Dependency Graphs and Belief Nets in that they provide
a structure and a set of assumptions from which a joint distribution can, in
principle, be computed. Unlike Dependency Graphs, Causal Models are models of
hierarchical and/or parallel processes, rather than models of distributions
(partially) known to a model builder through some sort of gestalt. As such,
Causal Models are more modular, easier to build, more intuitive, and easier to
understand than Dependency Graph Models. Causal Models are formally defined and
Dependency Graph Models are shown to be a special case of them. Algorithms
supporting inference are presented. Parsimonious methods for eliciting
dependent probabilities are presented."
"Reasoning about the Value of Decision-Model Refinement: Methods and
  Application","We investigate the value of extending the completeness of a decision model
along different dimensions of refinement. Specifically, we analyze the expected
value of quantitative, conceptual, and structural refinement of decision
models. We illustrate the key dimensions of refinement with examples. The
analyses of value of model refinement can be used to focus the attention of an
analyst or an automated reasoning system on extensions of a decision model
associated with the greatest expected value."
Valuation Networks and Conditional Independence,"Valuation networks have been proposed as graphical representations of
valuation-based systems (VBSs). The VBS framework is able to capture many
uncertainty calculi including probability theory, Dempster-Shafer's
belief-function theory, Spohn's epistemic belief theory, and Zadeh's
possibility theory. In this paper, we show how valuation networks encode
conditional independence relations. For the probabilistic case, the class of
probability models encoded by valuation networks includes undirected graph
models, directed acyclic graph models, directed balloon graph models, and
recursive causal graph models."
A Generalization of the Noisy-Or Model,"The Noisy-Or model is convenient for describing a class of uncertain
relationships in Bayesian networks [Pearl 1988]. Pearl describes the Noisy-Or
model for Boolean variables. Here we generalize the model to nary input and
output variables and to arbitrary functions other than the Boolean OR function.
This generalization is a useful modeling aid for construction of Bayesian
networks. We illustrate with some examples including digital circuit diagnosis
and network reliability analysis."
Graph-Grammar Assistance for Automated Generation of Influence Diagrams,"One of the most difficult aspects of modeling complex dilemmas in
decision-analytic terms is composing a diagram of relevance relations from a
set of domain concepts. Decision models in domains such as medicine, however,
exhibit certain prototypical patterns that can guide the modeling process.
Medical concepts can be classified according to semantic types that have
characteristic positions and typical roles in an influence-diagram model. We
have developed a graph-grammar production system that uses such inherent
interrelationships among medical terms to facilitate the modeling of medical
decisions."
"An Algorithm for the Construction of Bayesian Network Structures from
  Data","Previous algorithms for the construction of Bayesian belief network
structures from data have been either highly dependent on conditional
independence (CI) tests, or have required an ordering on the nodes to be
supplied by the user. We present an algorithm that integrates these two
approaches - CI tests are used to generate an ordering on the nodes from the
database which is then used to recover the underlying Bayesian network
structure using a non CI based method. Results of preliminary evaluation of the
algorithm on two networks (ALARM and LED) are presented. We also discuss some
algorithm performance issues and open problems."
"A Synthesis of Logical and Probabilistic Reasoning for Program
  Understanding and Debugging","We describe the integration of logical and uncertain reasoning methods to
identify the likely source and location of software problems. To date, software
engineers have had few tools for identifying the sources of error in complex
software packages. We describe a method for diagnosing software problems
through combining logical and uncertain reasoning analyses. Our preliminary
results suggest that such methods can be of value in directing the attention of
software engineers to paths of an algorithm that have the highest likelihood of
harboring a programming error."
Incremental Probabilistic Inference,"Propositional representation services such as truth maintenance systems offer
powerful support for incremental, interleaved, problem-model construction and
evaluation. Probabilistic inference systems, in contrast, have lagged behind in
supporting this incrementality typically demanded by problem solvers. The
problem, we argue, is that the basic task of probabilistic inference is
typically formulated at too large a grain-size. We show how a system built
around a smaller grain-size inference task can have the desired incrementality
and serve as the basis for a low-level (propositional) probabilistic
representation service."
Intercausal Reasoning with Uninstantiated Ancestor Nodes,"Intercausal reasoning is a common inference pattern involving probabilistic
dependence of causes of an observed common effect. The sign of this dependence
is captured by a qualitative property called product synergy. The current
definition of product synergy is insufficient for intercausal reasoning where
there are additional uninstantiated causes of the common effect. We propose a
new definition of product synergy and prove its adequacy for intercausal
reasoning with direct and indirect evidence for the common effect. The new
definition is based on a new property matrix half positive semi-definiteness, a
weakened form of matrix positive semi-definiteness."
Inference Algorithms for Similarity Networks,"We examine two types of similarity networks each based on a distinct notion
of relevance. For both types of similarity networks we present an efficient
inference algorithm that works under the assumption that every event has a
nonzero probability of occurrence. Another inference algorithm is developed for
type 1 similarity networks that works under no restriction, albeit less
efficiently."
Using Tree-Decomposable Structures to Approximate Belief Networks,"Tree structures have been shown to provide an efficient framework for
propagating beliefs [Pearl,1986]. This paper studies the problem of finding an
optimal approximating tree. The star decomposition scheme for sets of three
binary variables [Lazarsfeld,1966; Pearl,1986] is shown to enhance the class of
probability distributions that can support tree structures; such structures are
called tree-decomposable structures. The logarithm scoring rule is found to be
an appropriate optimality criterion to evaluate different tree-decomposable
structures. Characteristics of such structures closest to the actual belief
network are identified using the logarithm rule, and greedy and exact
techniques are developed to find the optimal approximation."
"Using Potential Influence Diagrams for Probabilistic Inference and
  Decision Making","The potential influence diagram is a generalization of the standard
""conditional"" influence diagram, a directed network representation for
probabilistic inference and decision analysis [Ndilikilikesha, 1991]. It allows
efficient inference calculations corresponding exactly to those on undirected
graphs. In this paper, we explore the relationship between potential and
conditional influence diagrams and provide insight into the properties of the
potential influence diagram. In particular, we show how to convert a potential
influence diagram into a conditional influence diagram, and how to view the
potential influence diagram operations in terms of the conditional influence
diagram."
"Incremental computation of the value of perfect information in
  stepwise-decomposable influence diagrams","To determine the value of perfect information in an influence diagram, one
needs first to modify the diagram to reflect the change in information
availability, and then to compute the optimal expected values of both the
original diagram and the modified diagram. The value of perfect information is
the difference between the two optimal expected values. This paper is about how
to speed up the computation of the optimal expected value of the modified
diagram by making use of the intermediate computation results obtained when
computing the optimal expected value of the original diagram."
Argument Calculus and Networks,"A major reason behind the success of probability calculus is that it
possesses a number of valuable tools, which are based on the notion of
probabilistic independence. In this paper, I identify a notion of logical
independence that makes some of these tools available to a class of
propositional databases, called argument databases. Specifically, I suggest a
graphical representation of argument databases, called argument networks, which
resemble Bayesian networks. I also suggest an algorithm for reasoning with
argument networks, which resembles a basic algorithm for reasoning with
Bayesian networks. Finally, I show that argument networks have several
applications: Nonmonotonic reasoning, truth maintenance, and diagnosis."
On reasoning in networks with qualitative uncertainty,"In this paper some initial work towards a new approach to qualitative
reasoning under uncertainty is presented. This method is not only applicable to
qualitative probabilistic reasoning, as is the case with other methods, but
also allows the qualitative propagation within networks of values based upon
possibility theory and Dempster-Shafer evidence theory. The method is applied
to two simple networks from which a large class of directed graphs may be
constructed. The results of this analysis are used to compare the qualitative
behaviour of the three major quantitative uncertainty handling formalisms, and
to demonstrate that the qualitative integration of the formalisms is possible
under certain assumptions."
Probabilistic Assumption-Based Reasoning,"The classical propositional assumption-based model is extended to incorporate
probabilities for the assumptions. Then it is placed into the framework of
evidence theory. Several authors like Laskey, Lehner (1989) and Provan (1990)
already proposed a similar point of view, but the first paper is not as much
concerned with mathematical foundations, and Provan's paper develops into a
different direction. Here we thoroughly develop and present the mathematical
foundations of this theory, together with computational methods adapted from
Reiter, De Kleer (1987) and Inoue (1992). Finally, recently proposed techniques
for computing degrees of support are presented."
Partially Specified Belief Functions,"This paper presents a procedure to determine a complete belief function from
the known values of belief for some of the subsets of the frame of discerment.
The method is based on the principle of minimum commitment and a new principle
called the focusing principle. This additional principle is based on the idea
that belief is specified for the most relevant sets: the focal elements. The
resulting procedure is compared with existing methods of building complete
belief functions: the minimum specificity principle and the least commitment
principle."
Belief Revision in Probability Theory,"In a probability-based reasoning system, Bayes' theorem and its variations
are often used to revise the system's beliefs. However, if the explicit
conditions and the implicit conditions of probability assignments `me properly
distinguished, it follows that Bayes' theorem is not a generally applicable
revision rule. Upon properly distinguishing belief revision from belief
updating, we see that Jeffrey's rule and its variations are not revision rules,
either. Without these distinctions, the limitation of the Bayesian approach is
often ignored or underestimated. Revision, in its general form, cannot be done
in the Bayesian approach, because a probability distribution function alone
does not contain the information needed by the operation."
A Belief-Function Based Decision Support System,"In this paper, we present a decision support system based on belief functions
and the pignistic transformation. The system is an integration of an evidential
system for belief function propagation and a valuation-based system for
Bayesian decision analysis. The two subsystems are connected through the
pignistic transformation. The system takes as inputs the user's ""gut feelings""
about a situation and suggests what, if any, are to be tested and in what
order, and it does so with a user friendly interface."
RES - a Relative Method for Evidential Reasoning,"In this paper we describe a novel method for evidential reasoning [1]. It
involves modelling the process of evidential reasoning in three steps, namely,
evidence structure construction, evidence accumulation, and decision making.
The proposed method, called RES, is novel in that evidence strength is
associated with an evidential support relationship (an argument) between a pair
of statements and such strength is carried by comparison between arguments.
This is in contrast to the onventional approaches, where evidence strength is
represented numerically and is associated with a statement."
Optimizing Causal Orderings for Generating DAGs from Data,"An algorithm for generating the structure of a directed acyclic graph from
data using the notion of causal input lists is presented. The algorithm
manipulates the ordering of the variables with operations which very much
resemble arc reversal. Operations are only applied if the DAG after the
operation represents at least the independencies represented by the DAG before
the operation until no more arcs can be removed from the DAG. The resulting DAG
is a minimal l-map."
Modal Logics for Qualitative Possibility and Beliefs,"Possibilistic logic has been proposed as a numerical formalism for reasoning
with uncertainty. There has been interest in developing qualitative accounts of
possibility, as well as an explanation of the relationship between possibility
and modal logics. We present two modal logics that can be used to represent and
reason with qualitative statements of possibility and necessity. Within this
modal framework, we are able to identify interesting relationships between
possibilistic logic, beliefs and conditionals. In particular, the most natural
conditional definable via possibilistic means for default reasoning is
identical to Pearl's conditional for e-semantics."
Lattice-Based Graded Logic: a Multimodal Approach,"Experts do not always feel very, comfortable when they have to give precise
numerical estimations of certainty degrees. In this paper we present a
qualitative approach which allows for attaching partially ordered symbolic
grades to logical formulas. Uncertain information is expressed by means of
parameterized modal operators. We propose a semantics for this multimodal logic
and give a sound and complete axiomatization. We study the links with related
approaches and suggest how this framework might be used to manage both
uncertain and incomplere knowledge."
Dynamic Network Models for Forecasting,"We have developed a probabilistic forecasting methodology through a synthesis
of belief network models and classical time-series analysis. We present the
dynamic network model (DNM) and describe methods for constructing, refining,
and performing inference with this representation of temporal probabilistic
knowledge. The DNM representation extends static belief-network models to more
general dynamic forecasting models by integrating and iteratively refining
contemporaneous and time-lagged dependencies. We discuss key concepts in terms
of a model for forecasting U.S. car sales in Japan."
Parallelizing Probabilistic Inference: Some Early Explorations,"We report on an experimental investigation into opportunities for parallelism
in beliefnet inference. Specifically, we report on a study performed of the
available parallelism, on hypercube style machines, of a set of randomly
generated belief nets, using factoring (SPI) style inference algorithms. Our
results indicate that substantial speedup is available, but that it is
available only through parallelization of individual conformal product
operations, and depends critically on finding an appropriate factoring. We find
negligible opportunity for parallelism at the topological, or clustering tree,
level."
An Entropy-based Learning Algorithm of Bayesian Conditional Trees,"This article offers a modification of Chow and Liu's learning algorithm in
the context of handwritten digit recognition. The modified algorithm directs
the user to group digits into several classes consisting of digits that are
hard to distinguish and then constructing an optimal conditional tree
representation for each class of digits instead of for each single digit as
done by Chow and Liu (1968). Advantages and extensions of the new method are
discussed. Related works of Wong and Wang (1977) and Wong and Poon (1989) which
offer a different entropy-based learning algorithm are shown to rest on
inappropriate assumptions."
Knowledge Integration for Conditional Probability Assessments,"In the probabilistic approach to uncertainty management the input knowledge
is usually represented by means of some probability distributions. In this
paper we assume that the input knowledge is given by two discrete conditional
probability distributions, represented by two stochastic matrices P and Q. The
consistency of the knowledge base is analyzed. Coherence conditions and
explicit formulas for the extension to marginal distributions are obtained in
some special cases."
A computational scheme for Reasoning in Dynamic Probabilistic Networks,"A computational scheme for reasoning about dynamic systems using (causal)
probabilistic networks is presented. The scheme is based on the framework of
Lauritzen and Spiegelhalter (1988), and may be viewed as a generalization of
the inference methods of classical time-series analysis in the sense that it
allows description of non-linear, multivariate dynamic systems with complex
conditional independence structures. Further, the scheme provides a method for
efficient backward smoothing and possibilities for efficient, approximate
forecasting methods. The scheme has been implemented on top of the HUGIN shell."
"The Dynamic of Belief in the Transferable Belief Model and
  Specialization-Generalization Matrices","The fundamental updating process in the transferable belief model is related
to the concept of specialization and can be described by a specialization
matrix. The degree of belief in the truth of a proposition is a degree of
justified support. The Principle of Minimal Commitment implies that one should
never give more support to the truth of a proposition than justified. We show
that Dempster's rule of conditioning corresponds essentially to the least
committed specialization, and that Dempster's rule of combination results
essentially from commutativity requirements. The concept of generalization,
dual to thc concept of specialization, is described."
Some Problems for Convex Bayesians,"We discuss problems for convex Bayesian decision making and uncertainty
representation. These include the inability to accommodate various natural and
useful constraints and the possibility of an analog of the classical Dutch Book
being made against an agent behaving in accordance with convex Bayesian
prescriptions. A more general set-based Bayesianism may be as tractable and
would avoid the difficulties we raise."
"Bayesian Meta-Reasoning: Determining Model Adequacy from Within a Small
  World","This paper presents a Bayesian framework for assessing the adequacy of a
model without the necessity of explicitly enumerating a specific alternate
model. A test statistic is developed for tracking the performance of the model
across repeated problem instances. Asymptotic methods are used to derive an
approximate distribution for the test statistic. When the model is rejected,
the individual components of the test statistic can be used to guide search for
an alternate model."
The Bounded Bayesian,"The ideal Bayesian agent reasons from a global probability model, but real
agents are restricted to simplified models which they know to be adequate only
in restricted circumstances. Very little formal theory has been developed to
help fallibly rational agents manage the process of constructing and revising
small world models. The goal of this paper is to present a theoretical
framework for analyzing model management approaches. For a probability
forecasting problem, a search process over small world models is analyzed as an
approximation to a larger-world model which the agent cannot explicitly
enumerate or compute. Conditions are given under which the sequence of
small-world models converges to the larger-world probabilities."
"Representing Context-Sensitive Knowledge in a Network Formalism: A
  Preliminary Report","Automated decision making is often complicated by the complexity of the
knowledge involved. Much of this complexity arises from the context sensitive
variations of the underlying phenomena. We propose a framework for representing
descriptive, context-sensitive knowledge. Our approach attempts to integrate
categorical and uncertain knowledge in a network formalism. This paper outlines
the basic representation constructs, examines their expressiveness and
efficiency, and discusses the potential applications of the framework."
A Probabilistic Network of Predicates,"Bayesian networks are directed acyclic graphs representing independence
relationships among a set of random variables. A random variable can be
regarded as a set of exhaustive and mutually exclusive propositions. We argue
that there are several drawbacks resulting from the propositional nature and
acyclic structure of Bayesian networks. To remedy these shortcomings, we
propose a probabilistic network where nodes represent unary predicates and
which may contain directed cycles. The proposed representation allows us to
represent domain knowledge in a single static network even though we cannot
determine the instantiations of the predicates before hand. The ability to deal
with cycles also enables us to handle cyclic causal tendencies and to recognize
recursive plans."
Empirical Probabilities in Monadic Deductive Databases,"We address the problem of supporting empirical probabilities in monadic logic
databases. Though the semantics of multivalued logic programs has been studied
extensively, the treatment of probabilities as results of statistical findings
has not been studied in logic programming/deductive databases. We develop a
model-theoretic characterization of logic databases that facilitates such a
treatment. We present an algorithm for checking consistency of such databases
and prove its total correctness. We develop a sound and complete query
processing procedure for handling queries to such databases."
aHUGIN: A System Creating Adaptive Causal Probabilistic Networks,"The paper describes aHUGIN, a tool for creating adaptive systems. aHUGIN is
an extension of the HUGIN shell, and is based on the methods reported by
Spiegelhalter and Lauritzen (1990a). The adaptive systems resulting from aHUGIN
are able to adjust the C011ditional probabilities in the model. A short
analysis of the adaptation task is given and the features of aHUGIN are
described. Finally a session with experiments is reported and the results are
discussed."
MESA: Maximum Entropy by Simulated Annealing,"Probabilistic reasoning systems combine different probabilistic rules and
probabilistic facts to arrive at the desired probability values of
consequences. In this paper we describe the MESA-algorithm (Maximum Entropy by
Simulated Annealing) that derives a joint distribution of variables or
propositions. It takes into account the reliability of probability values and
can resolve conflicts between contradictory statements. The joint distribution
is represented in terms of marginal distributions and therefore allows to
process large inference networks and to determine desired probability values
with high precision. The procedure derives a maximum entropy distribution
subject to the given constraints. It can be applied to inference networks of
arbitrary topology and may be extended into a number of directions."
"Guess-And-Verify Heuristics for Reducing Uncertainties in Expert
  Classification Systems","An expert classification system having statistical information about the
prior probabilities of the different classes should be able to use this
knowledge to reduce the amount of additional information that it must collect,
e.g., through questions, in order to make a correct classification. This paper
examines how best to use such prior information and additional
information-collection opportunities to reduce uncertainty about the class to
which a case belongs, thus minimizing the average cost or effort required to
correctly classify new cases."
Decision Making Using Probabilistic Inference Methods,"The analysis of decision making under uncertainty is closely related to the
analysis of probabilistic inference. Indeed, much of the research into
efficient methods for probabilistic inference in expert systems has been
motivated by the fundamental normative arguments of decision theory. In this
paper we show how the developments underlying those efficient methods can be
applied immediately to decision problems. In addition to general approaches
which need know nothing about the actual probabilistic inference method, we
suggest some simple modifications to the clustering family of algorithms in
order to efficiently incorporate decision making capabilities."
Conditional Independence in Uncertainty Theories,"This paper introduces the notions of independence and conditional
independence in valuation-based systems (VBS). VBS is an axiomatic framework
capable of representing many different uncertainty calculi. We define
independence and conditional independence in terms of factorization of the
joint valuation. The definitions of independence and conditional independence
in VBS generalize the corresponding definitions in probability theory. Our
definitions apply not only to probability theory, but also to Dempster-Shafer's
belief-function theory, Spohn's epistemic-belief theory, and Zadeh's
possibility theory. In fact, they apply to any uncertainty calculi that fit in
the framework of valuation-based systems."
A Fuzzy Logic Approach to Target Tracking,"This paper discusses a target tracking problem in which no dynamic
mathematical model is explicitly assumed. A nonlinear filter based on the fuzzy
If-then rules is developed. A comparison with a Kalman filter is made, and
empirical results show that the performance of the fuzzy filter is better.
Intensive simulations suggest that theoretical justification of the empirical
results is possible."
Towards Precision of Probabilistic Bounds Propagation,"The DUCK-calculus presented here is a recent approach to cope with
probabilistic uncertainty in a sound and efficient way. Uncertain rules with
bounds for probabilities and explicit conditional independences can be
maintained incrementally. The basic inference mechanism relies on local bounds
propagation, implementable by deductive databases with a bottom-up fixpoint
evaluation. In situations, where no precise bounds are deducible, it can be
combined with simple operations research techniques on a local scope. In
particular, we provide new precise analytical bounds for probabilistic
entailment."
Generalizing Jeffrey Conditionalization,"Jeffrey's rule has been generalized by Wagner to the case in which new
evidence bounds the possible revisions of a prior probability below by a
Dempsterian lower probability. Classical probability kinematics arises within
this generalization as the special case in which the evidentiary focal elements
of the bounding lower probability are pairwise disjoint. We discuss a twofold
extension of this generalization, first allowing the lower bound to be any
two-monotone capacity and then allowing the prior to be a lower envelope."
Interval Structure: A Framework for Representing Uncertain Information,"In this paper, a unified framework for representing uncertain information
based on the notion of an interval structure is proposed. It is shown that the
lower and upper approximations of the rough-set model, the lower and upper
bounds of incidence calculus, and the belief and plausibility functions all
obey the axioms of an interval structure. An interval structure can be used to
synthesize the decision rules provided by the experts. An efficient algorithm
to find the desirable set of rules is developed from a set of sound and
complete inference axioms."
"""Conditional Inter-Causally Independent"" Node Distributions, a Property
  of ""Noisy-Or"" Models","This paper examines the interdependence generated between two parent nodes
with a common instantiated child node, such as two hypotheses sharing common
evidence. The relation so generated has been termed ""intercausal."" It is shown
by construction that inter-causal independence is possible for binary
distributions at one state of evidence. For such ""CICI"" distributions, the two
measures of inter-causal effect, ""multiplicative synergy"" and ""additive
synergy"" are equal. The well known ""noisy-or"" model is an example of such a
distribution. This introduces novel semantics for the noisy-or, as a model of
the degree of conflict among competing hypotheses of a common observation."
Combination of Upper and Lower Probabilities,"In this paper, we consider several types of information and methods of
combination associated with incomplete probabilistic systems. We discriminate
between 'a priori' and evidential information. The former one is a description
of the whole population, the latest is a restriction based on observations for
a particular case. Then, we propose different combination methods for each one
of them. We also consider conditioning as the heterogeneous combination of 'a
priori' and evidential information. The evidential information is represented
as a convex set of likelihood functions. These will have an associated
possibility distribution with behavior according to classical Possibility
Theory."
"A Bayesian Method for Constructing Bayesian Belief Networks from
  Databases","This paper presents a Bayesian method for constructing Bayesian belief
networks from a database of cases. Potential applications include
computer-assisted hypothesis testing, automated scientific discovery, and
automated construction of probabilistic expert systems. Results are presented
of a preliminary evaluation of an algorithm for constructing a belief network
from a database of cases. We relate the methods in this paper to previous work,
and we discuss open problems."
"Local Expression Languages for Probabilistic Dependence: a Preliminary
  Report","We present a generalization of the local expression language used in the
Symbolic Probabilistic Inference (SPI) approach to inference in belief nets
[1l, [8]. The local expression language in SPI is the language in which the
dependence of a node on its antecedents is described. The original language
represented the dependence as a single monolithic conditional probability
distribution. The extended language provides a set of operators (*, +, and -)
which can be used to specify methods for combining partial conditional
distributions. As one instance of the utility of this extension, we show how
this extended language can be used to capture the semantics, representational
advantages, and inferential complexity advantages of the ""noisy or""
relationship."
Advances in Probabilistic Reasoning,"This paper discuses multiple Bayesian networks representation paradigms for
encoding asymmetric independence assertions. We offer three contributions: (1)
an inference mechanism that makes explicit use of asymmetric independence to
speed up computations, (2) a simplified definition of similarity networks and
extensions of their theory, and (3) a generalized representation scheme that
encodes more types of asymmetric independence assertions than do similarity
networks."
Time-Dependent Utility and Action Under Uncertainty,"We discuss representing and reasoning with knowledge about the time-dependent
utility of an agent's actions. Time-dependent utility plays a crucial role in
the interaction between computation and action under bounded resources. We
present a semantics for time-dependent utility and describe the use of
time-dependent information in decision contexts. We illustrate our discussion
with examples of time-pressured reasoning in Protos, a system constructed to
explore the ideal control of inference by reasoners with limit abilities."
Non-monotonic Reasoning and the Reversibility of Belief Change,"Traditional approaches to non-monotonic reasoning fail to satisfy a number of
plausible axioms for belief revision and suffer from conceptual difficulties as
well. Recent work on ranked preferential models (RPMs) promises to overcome
some of these difficulties. Here we show that RPMs are not adequate to handle
iterated belief change. Specifically, we show that RPMs do not always allow for
the reversibility of belief change. This result indicates the need for
numerical strengths of belief."
Reasoning with Mass Distributions,"The concept of movable evidence masses that flow from supersets to subsets as
specified by experts represents a suitable framework for reasoning under
uncertainty. The mass flow is controlled by specialization matrices. New
evidence is integrated into the frame of discernment by conditioning or
revision (Dempster's rule of conditioning), for which special specialization
matrices exist. Even some aspects of non-monotonic reasoning can be represented
by certain specialization matrices."
Conflict and Surprise: Heuristics for Model Revision,"Any probabilistic model of a problem is based on assumptions which, if
violated, invalidate the model. Users of probability based decision aids need
to be alerted when cases arise that are not covered by the aid's model.
Diagnosis of model failure is also necessary to control dynamic model
construction and revision. This paper presents a set of decision theoretically
motivated heuristics for diagnosing situations in which a model is likely to
provide an inadequate representation of the process being modeled."
Reasoning under Uncertainty: Some Monte Carlo Results,"A series of monte carlo studies were performed to compare the behavior of
some alternative procedures for reasoning under uncertainty. The behavior of
several Bayesian, linear model and default reasoning procedures were examined
in the context of increasing levels of calibration error. The most interesting
result is that Bayesian procedures tended to output more extreme posterior
belief values (posterior beliefs near 0.0 or 1.0) than other techniques, but
the linear models were relatively less likely to output strong support for an
erroneous conclusion. Also, accounting for the probabilistic dependencies
between evidence items was important for both Bayesian and linear updating
procedures."
A Modification to Evidential Probability,"Selecting the right reference class and the right interval when faced with
conflicting candidates and no possibility of establishing subset style
dominance has been a problem for Kyburg's Evidential Probability system.
Various methods have been proposed by Loui and Kyburg to solve this problem in
a way that is both intuitively appealing and justifiable within Kyburg's
framework. The scheme proposed in this paper leads to stronger statistical
assertions without sacrificing too much of the intuitive appeal of Kyburg's
latest proposal."
Non-monotonic Negation in Probabilistic Deductive Databases,"In this paper we study the uses and the semantics of non-monotonic negation
in probabilistic deductive data bases. Based on the stable semantics for
classical logic programming, we introduce the notion of stable formula,
functions. We show that stable formula, functions are minimal fixpoints of
operators associated with probabilistic deductive databases with negation.
Furthermore, since a. probabilistic deductive database may not necessarily have
a stable formula function, we provide a stable class semantics for such
databases. Finally, we demonstrate that the proposed semantics can handle
default reasoning naturally in the context of probabilistic deduction."
"Integrating Probabilistic Rules into Neural Networks: A Stochastic EM
  Learning Algorithm","The EM-algorithm is a general procedure to get maximum likelihood estimates
if part of the observations on the variables of a network are missing. In this
paper a stochastic version of the algorithm is adapted to probabilistic neural
networks describing the associative dependency of variables. These networks
have a probability distribution, which is a special case of the distribution
generated by probabilistic inference networks. Hence both types of networks can
be combined allowing to integrate probabilistic rules as well as unspecified
associations in a sound way. The resulting network may have a number of
interesting features including cycles of probabilistic rules, hidden
'unobservable' variables, and uncertain and contradictory evidence."
Representing Bayesian Networks within Probabilistic Horn Abduction,"This paper presents a simple framework for Horn clause abduction, with
probabilities associated with hypotheses. It is shown how this representation
can represent any probabilistic knowledge representable in a Bayesian belief
network. The main contributions are in finding a relationship between logical
and probabilistic notions of evidential reasoning. This can be used as a basis
for a new way to implement Bayesian Networks that allows for approximations to
the value of the posterior probabilities, and also points to a way that
Bayesian networks can be extended beyond a propositional language."
"Pulcinella: A General Tool for Propagating Uncertainty in Valuation
  Networks","We present PULCinella and its use in comparing uncertainty theories.
PULCinella is a general tool for Propagating Uncertainty based on the Local
Computation technique of Shafer and Shenoy. It may be specialized to different
uncertainty theories: at the moment, Pulcinella can propagate probabilities,
belief functions, Boolean values, and possibilities. Moreover, Pulcinella
allows the user to easily define his own specializations. To illustrate
Pulcinella, we analyze two examples by using each of the four theories above.
In the first one, we mainly focus on intrinsic differences between theories. In
the second one, we take a knowledge engineer viewpoint, and check the adequacy
of each theory to a given problem."
"On the Generation of Alternative Explanations with Implications for
  Belief Revision","In general, the best explanation for a given observation makes no promises on
how good it is with respect to other alternative explanations. A major
deficiency of message-passing schemes for belief revision in Bayesian networks
is their inability to generate alternatives beyond the second best. In this
paper, we present a general approach based on linear constraint systems that
naturally generates alternative explanations in an orderly and highly efficient
manner. This approach is then applied to cost-based abduction problems as well
as belief revision in Bayesian net works."
A Graph-Based Inference Method for Conditional Independence,"The graphoid axioms for conditional independence, originally described by
Dawid [1979], are fundamental to probabilistic reasoning [Pearl, 19881. Such
axioms provide a mechanism for manipulating conditional independence assertions
without resorting to their numerical definition. This paper explores a
representation for independence statements using multiple undirected graphs and
some simple graphical transformations. The independence statements derivable in
this system are equivalent to those obtainable by the graphoid axioms.
Therefore, this is a purely graphical proof technique for conditional
independence."
About Updating,"Survey of several forms of updating, with a practical illustrative example.
We study several updating (conditioning) schemes that emerge naturally from a
common scenarion to provide some insights into their meaning. Updating is a
subtle operation and there is no single method, no single 'good' rule. The
choice of the appropriate rule must always be given due consideration. Planchet
(1989) presents a mathematical survey of many rules. We focus on the practical
meaning of these rules. After summarizing the several rules for conditioning,
we present an illustrative example in which the various forms of conditioning
can be explained."
Compressed Constraints in Probabilistic Logic and Their Revision,"In probabilistic logic entailments, even moderate size problems can yield
linear constraint systems with so many variables that exact methods are
impractical. This difficulty can be remedied in many cases of interest by
introducing a three valued logic (true, false, and ""don't care""). The
three-valued approach allows the construction of ""compressed"" constraint
systems which have the same solution sets as their two-valued counterparts, but
which may involve dramatically fewer variables. Techniques to calculate point
estimates for the posterior probabilities of entailed sentences are discussed."
Detecting Causal Relations in the Presence of Unmeasured Variables,"The presence of latent variables can greatly complicate inferences about
causal relations between measured variables from statistical data. In many
cases, the presence of latent variables makes it impossible to determine for
two measured variables A and B, whether A causes B, B causes A, or there is
some common cause. In this paper I present several theorems that state
conditions under which it is possible to reliably infer the causal relation
between two measured variables, regardless of whether latent variables are
acting or not."
A Monte-Carlo Algorithm for Dempster-Shafer Belief,"A very computationally-efficient Monte-Carlo algorithm for the calculation of
Dempster-Shafer belief is described. If Bel is the combination using Dempster's
Rule of belief functions Bel, ..., Bel,7, then, for subset b of the frame C),
Bel(b) can be calculated in time linear in 1(31 and m (given that the weight of
conflict is bounded). The algorithm can also be used to improve the complexity
of the Shenoy-Shafer algorithms on Markov trees, and be generalised to
calculate Dempster-Shafer Belief over other logics."
An Efficient Implementation of Belief Function Propagation,"The local computation technique (Shafer et al. 1987, Shafer and Shenoy 1988,
Shenoy and Shafer 1986) is used for propagating belief functions in so called a
Markov Tree. In this paper, we describe an efficient implementation of belief
function propagation on the basis of the local computation technique. The
presented method avoids all the redundant computations in the propagation
process, and so makes the computational complexity decrease with respect to
other existing implementations (Hsia and Shenoy 1989, Zarley et al. 1988). We
also give a combined algorithm for both propagation and re-propagation which
makes the re-propagation process more efficient when one or more of the prior
belief functions is changed."
Why Do We Need Foundations for Modelling Uncertainties?,"Surely we want solid foundations. What kind of castle can we build on sand?
What is the point of devoting effort to balconies and minarets, if the
foundation may be so weak as to allow the structure to collapse of its own
weight? We want our foundations set on bedrock, designed to last for
generations. Who would want an architect who cannot certify the soundness of
the foundations of his buildings?"
"Exploiting Functional Dependencies in Qualitative Probabilistic
  Reasoning","Functional dependencies restrict the potential interactions among variables
connected in a probabilistic network. This restriction can be exploited in
qualitative probabilistic reasoning by introducing deterministic variables and
modifying the inference rules to produce stronger conclusions in the presence
of functional relations. I describe how to accomplish these modifications in
qualitative probabilistic networks by exhibiting the update procedures for
graphical transformations involving probabilistic and deterministic variables
and combinations. A simple example demonstrates that the augmented scheme can
reduce qualitative ambiguity that would arise without the special treatment of
functional dependency. Analysis of qualitative synergy reveals that new
higher-order relations are required to reason effectively about synergistic
interactions among deterministic variables."
Problem Formulation as the Reduction of a Decision Model,"In this paper, we extend the QMRDT probabilistic model for the domain of
internal medicine to include decisions about treatments. In addition, we
describe how we can use the comprehensive decision model to construct a simpler
decision model for a specific patient. In so doing, we transform the task of
problem formulation to that of narrowing of a larger problem."
Dynamic Construction of Belief Networks,"We describe a method for incrementally constructing belief networks. We have
developed a network-construction language similar to a forward-chaining
language using data dependencies, but with additional features for specifying
distributions. Using this language, we can define parameterized classes of
probabilistic models. These parameterized models make it possible to apply
probabilistic reasoning to problems for which it is impractical to have a
single large static model."
A New Algorithm for Finding MAP Assignments to Belief Networks,"We present a new algorithm for finding maximum a-posterior) (MAP) assignments
of values to belief networks. The belief network is compiled into a network
consisting only of nodes with boolean (i.e. only 0 or 1) conditional
probabilities. The MAP assignment is then found using a best-first search on
the resulting network. We argue that, as one would anticipate, the algorithm is
exponential for the general case, but only linear in the size of the network
for poly trees."
Ergo: A Graphical Environment for Constructing Bayesian,"We describe an environment that considerably simplifies the process of
generating Bayesian belief networks. The system has been implemented on readily
available, inexpensive hardware, and provides clarity and high performance. We
present an introduction to Bayesian belief networks, discuss algorithms for
inference with these networks, and delineate the classes of problems that can
be solved with this paradigm. We then describe the hardware and software that
constitute the system, and illustrate Ergo's use with several example"
A Dynamic Approach to Probabilistic Inference,"In this paper we present a framework for dynamically constructing Bayesian
networks. We introduce the notion of a background knowledge base of schemata,
which is a collection of parameterized conditional probability statements.
These schemata explicitly separate the general knowledge of properties an
individual may have from the specific knowledge of particular individuals that
may have these properties. Knowledge of individuals can be combined with this
background knowledge to create Bayesian networks, which can then be used in any
propagation scheme. We discuss the theory and assumptions necessary for the
implementation of dynamic Bayesian networks, and indicate where our approach
may be useful."
Robust Inference Policies,"A series of monte carlo studies were performed to assess the extent to which
different inference procedures robustly output reasonable belief values in the
context of increasing levels of judgmental imprecision. It was found that, when
compared to an equal-weights linear model, the Bayesian procedures are more
likely to deduce strong support for a hypothesis. But, the Bayesian procedures
are also more likely to strongly support the wrong hypothesis. Bayesian
techniques are more powerful, but are also more error prone."
Minimum Error Tree Decomposition,"This paper describes a generalization of previous methods for constructing
tree-structured belief network with hidden variables. The major new feature of
the described method is the ability to produce a tree decomposition even when
there are errors in the correlation data among the input variables. This is an
important extension of existing methods since the correlational coefficients
usually cannot be measured with precision. The technique involves using a
greedy search algorithm that locally minimizes an error function."
IDEAL: A Software Package for Analysis of Influence Diagrams,"IDEAL (Influence Diagram Evaluation and Analysis in Lisp) is a software
environment for creation and evaluation of belief networks and influence
diagrams. IDEAL is primarily a research tool and provides an implementation of
many of the latest developments in belief network and influence diagram
evaluation in a unified framework. This paper describes IDEAL and some lessons
learned during its development."
Optimal Decomposition of Belief Networks,"In this paper, optimum decomposition of belief networks is discussed. Some
methods of decomposition are examined and a new method - the method of Minimum
Total Number of States (MTNS) - is proposed. The problem of optimum belief
network decomposition under our framework, as under all the other frameworks,
is shown to be NP-hard. According to the computational complexity analysis, an
algorithm of belief network decomposition is proposed in (Wee, 1990a) based on
simulated annealing."
"On Heuristics for Finding Loop Cutsets in Multiply-Connected Belief
  Networks","We introduce a new heuristic algorithm for the problem of finding minimum
size loop cutsets in multiply connected belief networks. We compare this
algorithm to that proposed in [Suemmondt and Cooper, 1988]. We provide lower
bounds on the performance of these algorithms with respect to one another and
with respect to optimal. We demonstrate that no heuristic algorithm for this
problem cam be guaranteed to produce loop cutsets within a constant difference
from optimal. We discuss experimental results based on randomly generated
networks, and discuss future work and open questions."
"A Combination of Cutset Conditioning with Clique-Tree Propagation in the
  Pathfinder System","Cutset conditioning and clique-tree propagation are two popular methods for
performing exact probabilistic inference in Bayesian belief networks. Cutset
conditioning is based on decomposition of a subset of network nodes, whereas
clique-tree propagation depends on aggregation of nodes. We describe a means to
combine cutset conditioning and clique- tree propagation in an approach called
aggregation after decomposition (AD). We discuss the application of the AD
method in the Pathfinder system, a medical expert system that offers assistance
with diagnosis in hematopathology."
Using Dempster-Shafer Theory in Knowledge Representation,"In this paper, we suggest marrying Dempster-Shafer (DS) theory with Knowledge
Representation (KR). Born out of this marriage is the definition of
""Dempster-Shafer Belief Bases"", abstract data types representing uncertain
knowledge that use DS theory for representing strength of belief about our
knowledge, and the linguistic structures of an arbitrary KR system for
representing the knowledge itself. A formal result guarantees that both the
properties of the given KR system and of DS theory are preserved. The general
model is exemplified by defining DS Belief Bases where First Order Logic and
(an extension of) KRYPTON are used as KR systems. The implementation problem is
also touched upon."
Amplitude-Based Approach to Evidence Accumulation,"We point out the need to use probability amplitudes rather than probabilities
to model evidence accumulation in decision processes involving real physical
sensors. Optical information processing systems are given as typical examples
of systems that naturally gather evidence in this manner. We derive a new,
amplitude-based generalization of the Hough transform technique used for object
recognition in machine vision. We argue that one should use complex Hough
accumulators and square their magnitudes to get a proper probabilistic
interpretation of the likelihood that an object is present. Finally, we suggest
that probability amplitudes may have natural applications in connectionist
models, as well as in formulating knowledge-based reasoning problems."
A Probabilistic Reasoning Environment,"A framework is presented for a computational theory of probabilistic
argument. The Probabilistic Reasoning Environment encodes knowledge at three
levels. At the deepest level are a set of schemata encoding the system's domain
knowledge. This knowledge is used to build a set of second-level arguments,
which are structured for efficient recapture of the knowledge used to construct
them. Finally, at the top level is a Bayesian network constructed from the
arguments. The system is designed to facilitate not just propagation of beliefs
and assimilation of evidence, but also the dynamic process of constructing a
belief network, evaluating its adequacy, and revising it when necessary."
"Decisions with Limited Observations over a Finite Product Space: the
  Klir Effect","Probability estimation by maximum entropy reconstruction of an initial
relative frequency estimate from its projection onto a hypergraph model of the
approximate conditional independence relations exhibited by it is investigated.
The results of this study suggest that use of this estimation technique may
improve the quality of decisions that must be made on the basis of limited
observations over a decomposable finite product space."
"Rules, Belief Functions and Default Logic","This paper describes a natural framework for rules, based on belief
functions, which includes a repre- sentation of numerical rules, default rules
and rules allowing and rules not allowing contraposition. In particular it
justifies the use of the Dempster-Shafer Theory for representing a particular
class of rules, Belief calculated being a lower probability given certain
independence assumptions on an underlying space. It shows how a belief function
framework can be generalised to other logics, including a general Monte-Carlo
algorithm for calculating belief, and how a version of Reiter's Default Logic
can be seen as a limiting case of a belief function formalism."
Computing Probability Intervals Under Independency Constraints,"Many AI researchers argue that probability theory is only capable of dealing
with uncertainty in situations where a full specification of a joint
probability distribution is available, and conclude that it is not suitable for
application in knowledge-based systems. Probability intervals, however,
constitute a means for expressing incompleteness of information. We present a
method for computing such probability intervals for probabilities of interest
from a partial specification of a joint probability distribution. Our method
improves on earlier approaches by allowing for independency relationships
between statistical variables to be exploited."
"An Empirical Analysis of Likelihood-Weighting Simulation on a Large,
  Multiply-Connected Belief Network","We analyzed the convergence properties of likelihood- weighting algorithms on
a two-level, multiply connected, belief-network representation of the QMR
knowledge base of internal medicine. Specifically, on two difficult diagnostic
cases, we examined the effects of Markov blanket scoring, importance sampling,
demonstrating that the Markov blanket scoring and self-importance sampling
significantly improve the convergence of the simulation on our model."
Towards a Normative Theory of Scientific Evidence,"A scientific reasoning system makes decisions using objective evidence in the
form of independent experimental trials, propositional axioms, and constraints
on the probabilities of events. As a first step towards this goal, we propose a
system that derives probability intervals from objective evidence in those
forms. Our reasoning system can manage uncertainty about data and rules in a
rule based expert system. We expect that our system will be particularly
applicable to diagnosis and analysis in domains with a wealth of experimental
evidence such as medicine. We discuss limitations of this solution and propose
future directions for this research. This work can be considered a
generalization of Nilsson's ""probabilistic logic"" [Nil86] to intervals and
experimental observations."
Plan Recognition in Stories and in Life,"Plan recognition does not work the same way in stories and in ""real life""
(people tend to jump to conclusions more in stories). We present a theory of
this, for the particular case of how objects in stories (or in life) influence
plan recognition decisions. We provide a Bayesian network formalization of a
simple first-order theory of plans, and show how a particular network parameter
seems to govern the difference between ""life-like"" and ""story-like"" response.
We then show why this parameter would be influenced (in the desired way) by a
model of speaker (or author) topic selection which assumes that facts in
stories are typically ""relevant""."
"Decision Making ""Biases"" and Support for Assumption-Based Higher-Order
  Reasoning","Unaided human decision making appears to systematically violate consistency
constraints imposed by normative theories; these biases in turn appear to
justify the application of formal decision-analytic models. It is argued that
both claims are wrong. In particular, we will argue that the ""confirmation
bias"" is premised on an overly narrow view of how conflicting evidence is and
ought to be handled. Effective decision aiding should focus on supporting the
contral processes by means of which knowledge is extended into novel situations
and in which assumptions are adopted, utilized, and revised. The Non- Monotonic
Probabilist represents initial work toward such an aid."
"Deciding Consistency of Databases Containing Defeasible and Strict
  Information","We propose a norm of consistency for a mixed set of defeasible and strict
sentences, based on a probabilistic semantics. This norm establishes a clear
distinction between knowledge bases depicting exceptions and those containing
outright contradictions. We then define a notion of entailment based also on
probabilistic considerations and provide a characterization of the relation
between consistency and entailment. We derive necessary and sufficient
conditions for consistency, and provide a simple decision procedure for testing
consistency and deciding whether a sentence is entailed by a database. Finally,
it is shown that if al1 sentences are Horn clauses, consistency and entailment
can be tested in polynomial time."
Heuristic Search as Evidential Reasoning,"BPS, the Bayesian Problem Solver, applies probabilistic inference and
decision-theoretic control to flexible, resource-constrained problem-solving.
This paper focuses on the Bayesian inference mechanism in BPS, and contrasts it
with those of traditional heuristic search techniques. By performing sound
inference, BPS can outperform traditional techniques with significantly less
computational effort. Empirical tests on the Eight Puzzle show that after only
a few hundred node expansions, BPS makes better decisions than does the best
existing algorithm after several million node expansions"
Inference Policies,"It is suggested that an AI inference system should reflect an inference
policy that is tailored to the domain of problems to which it is applied -- and
furthermore that an inference policy need not conform to any general theory of
rational inference or induction. We note, for instance, that Bayesian reasoning
about the probabilistic characteristics of an inference domain may result in
the specification of an nonBayesian procedure for reasoning within the
inference domain. In this paper, the idea of an inference policy is explored in
some detail. To support this exploration, the characteristics of some standard
and nonstandard inference policies are examined."
"Strategies for Generating Micro Explanations for Bayesian Belief
  Networks","Bayesian Belief Networks have been largely overlooked by Expert Systems
practitioners on the grounds that they do not correspond to the human inference
mechanism. In this paper, we introduce an explanation mechanism designed to
generate intuitive yet probabilistically sound explanations of inferences drawn
by a Bayesian Belief Network. In particular, our mechanism accounts for the
results obtained due to changes in the causal and the evidential support of a
node."
Evidence Absorption and Propagation through Evidence Reversals,"The arc reversal/node reduction approach to probabilistic inference is
extended to include the case of instantiated evidence by an operation called
""evidence reversal."" This not only provides a technique for computing posterior
joint distributions on general belief networks, but also provides insight into
the methods of Pearl [1986b] and Lauritzen and Spiegelhalter [1988]. Although
it is well understood that the latter two algorithms are closely related, in
fact all three algorithms are identical whenever the belief network is a
forest."
"Freedom: A Measure of Second-order Uncertainty for Intervalic
  Probability Schemes","This paper discusses a new measure that is adaptable to certain intervalic
probability frameworks, possibility theory, and belief theory. As such, it has
the potential for wide use in knowledge engineering, expert systems, and
related problems in the human sciences. This measure (denoted here by F) has
been introduced in Smithson (1988) and is more formally discussed in Smithson
(1989a)o Here, I propose to outline the conceptual basis for F and compare its
properties with other measures of second-order uncertainty. I will argue that F
is an indicator of nonspecificity or alternatively, of freedom, as
distinguished from either ambiguity or vagueness."
Efficient Parallel Estimation for Markov Random Fields,"We present a new, deterministic, distributed MAP estimation algorithm for
Markov Random Fields called Local Highest Confidence First (Local HCF). The
algorithm has been applied to segmentation problems in computer vision and its
performance compared with stochastic algorithms. The experiments show that
Local HCF finds better estimates than stochastic algorithms with much less
computation."
"Can Uncertainty Management be Realized in a Finite Totally Ordered
  Probability Algebra?","In this paper, the feasibility of using finite totally ordered probability
models under Alelinnas's Theory of Probabilistic Logic [Aleliunas, 1988] is
investigated. The general form of the probability algebra of these models is
derived and the number of possible algebras with given size is deduced. Based
on this analysis, we discuss problems of denominator-indifference and
ambiguity-generation that arise in reasoning by cases and abductive reasoning.
An example is given that illustrates how these problems arise. The
investigation shows that a finite probability model may be of very limited
usage."
Summary of A New Normative Theory of Probabilistic Logic,"By probabilistic logic I mean a normative theory of belief that explains how
a body of evidence affects one's degree of belief in a possible hypothesis. A
new axiomatization of such a theory is presented which avoids a finite
additivity axiom, yet which retains many useful inference rules. Many of the
examples of this theory--its models do not use numerical probabilities. Put
another way, this article gives sharper answers to the two questions: 1.What
kinds of sets can used as the range of a probability function? 2.Under what
conditions is the range set of a probability function isomorphic to the set of
real numbers in the interval 10,1/ with the usual arithmetical operations?"
"Process, Structure, and Modularity in Reasoning with Uncertainty","Computational mechanisms for uncertainty management must support interactive
and incremental problem formulation, inference, hypothesis testing, and
decision making. However, most current uncertainty inference systems
concentrate primarily on inference, and provide no support for the larger
issues. We present a computational approach to uncertainty management which
provides direct support for the dynamic, incremental aspect of this task, while
at the same time permitting direct representation of the structure of
evidential relationships. At the same time, we show that this approach responds
to the modularity concerns of Heckerman and Horvitz [Heck87]. This paper
emphasizes examples of the capabilities of this approach. Another paper
[D'Am89] details the representations and algorithms involved."
"A Temporal Logic for Uncertain Events and An Outline of A Possible
  Implementation in An Extension of PROLOG","There is uncertainty associated with the occurrence of many events in real
life. In this paper we develop a temporal logic to deal with such uncertain
events and outline a possible implementation in an extension of PROLOG. Events
are represented as fuzzy sets with the membership function giving the
possibility of occurrence of the event in a given interval of time. The
developed temporal logic is simple but powerful. It can determine effectively
the various temporal relations between uncertain events or their combinations.
PROLOG provides a uniform substrate on which to effectively implement such a
temporal logic for uncertain events"
Probability as a Modal Operator,"This paper argues for a modal view of probability. The syntax and semantics
of one particularly strong probability logic are discussed and some examples of
the use of the logic are provided. We show that it is both natural and useful
to think of probability as a modal operator. Contrary to popular belief in AI,
a probability ranging between 0 and 1 represents a continuum between
impossibility and necessity, not between simple falsity and truth. The present
work provides a clear semantics for quantification into the scope of the
probability operator and for higher-order probabilities. Probability logic is a
language for expressing both probabilistic and logical concepts."
On the Logic of Causal Models,"This paper explores the role of Directed Acyclic Graphs (DAGs) as a
representation of conditional independence relationships. We show that DAGs
offer polynomially sound and complete inference mechanisms for inferring
conditional independence relationships from a given causal set of such
relationships. As a consequence, d-separation, a graphical criterion for
identifying independencies in a DAG, is shown to uncover more valid
independencies then any other criterion. In addition, we employ the Armstrong
property of conditional independence to show that the dependence relationships
displayed by a DAG are inherently consistent, i.e. for every DAG D there exists
some probability distribution P that embodies all the conditional
independencies displayed in D and none other."
Parallel Belief Revision,"This paper describes a formal system of belief revision developed by Wolfgang
Spohn and shows that this system has a parallel implementation that can be
derived from an influence diagram in a manner similar to that in which Bayesian
networks are derived. The proof rests upon completeness results for an
axiomatization of the notion of conditional independence, with the Spohn system
being used as a semantics for the relation of conditional independence."
Rational Nonmonotonic Reasoning,"Nonmonotonic reasoning is a pattern of reasoning that allows an agent to make
and retract (tentative) conclusions from inconclusive evidence. This paper
gives a possible-worlds interpretation of the nonmonotonic reasoning problem
based on standard decision theory and the emerging probability logic. The
system's central principle is that a tentative conclusion is a decision to make
a bet, not an assertion of fact. The system is rational, and as sound as the
proof theory of its underlying probability log."
Epistemological Relevance and Statistical Knowledge,"For many years, at least since McCarthy and Hayes (1969), writers have
lamented, and attempted to compensate for, the alleged fact that we often do
not have adequate statistical knowledge for governing the uncertainty of
belief, for making uncertain inferences, and the like. It is hardly ever
spelled out what ""adequate statistical knowledge"" would be, if we had it, and
how adequate statistical knowledge could be used to control and regulate
epistemic uncertainty."
Justifying the Principle of Interval Constraints,"When knowledge is obtained from a database, it is only possible to deduce
confidence intervals for probability values. With confidence intervals
replacing point values, the results in the set covering model include interval
constraints for the probabilities of mutually exclusive and exhaustive
explanations. The Principle of Interval Constraints ranks these explanations by
determining the expected values of the probabilities based on distributions
determined from the interval, constraints. This principle was developed using
the Classical Approach to probability. This paper justifies the Principle of
Interval Constraints with a more rigorous statement of the Classical Approach
and by defending the concept of probabilities of probabilities."
An Axiomatic Framework for Bayesian and Belief-function Propagation,"In this paper, we describe an abstract framework and axioms under which exact
local computation of marginals is possible. The primitive objects of the
framework are variables and valuations. The primitive operators of the
framework are combination and marginalization. These operate on valuations. We
state three axioms for these operators and we derive the possibility of local
computation from the axioms. Next, we describe a propagation scheme for
computing marginals of a valuation when we have a factorization of the
valuation on a hypertree. Finally we show how the problem of computing
marginals of joint probability distributions and joint belief functions fits
the general framework."
Updating Probabilities in Multiply-Connected Belief Networks,"This paper focuses on probability updates in multiply-connected belief
networks. Pearl has designed the method of conditioning, which enables us to
apply his algorithm for belief updates in singly-connected networks to
multiply-connected belief networks by selecting a loop-cutset for the network
and instantiating these loop-cutset nodes. We discuss conditions that need to
be satisfied by the selected nodes. We present a heuristic algorithm for
finding a loop-cutset that satisfies these conditions."
Causal Networks: Semantics and Expressiveness,"Dependency knowledge of the form ""x is independent of y once z is known""
invariably obeys the four graphoid axioms, examples include probabilistic and
database dependencies. Often, such knowledge can be represented efficiently
with graphical structures such as undirected graphs and directed acyclic graphs
(DAGs). In this paper we show that the graphical criterion called d-separation
is a sound rule for reading independencies from any DAG based on a causal input
list drawn from a graphoid. The rule may be extended to cover DAGs that
represent functional dependencies as well as conditional dependencies."
MCE Reasoning in Recursive Causal Networks,"A probabilistic method of reasoning under uncertainty is proposed based on
the principle of Minimum Cross Entropy (MCE) and concept of Recursive Causal
Model (RCM). The dependency and correlations among the variables are described
in a special language BNDL (Belief Networks Description Language). Beliefs are
propagated among the clauses of the BNDL programs representing the underlying
probabilistic distributions. BNDL interpreters in both Prolog and C has been
developed and the performance of the method is compared with those of the
others."
Nonmonotonic Reasoning via Possibility Theory,"We introduce the operation of possibility qualification and show how. this
modal-like operator can be used to represent ""typical"" or default knowledge in
a theory of nonmonotonic reasoning. We investigate the representational power
of this approach by looking at a number of prototypical problems from the
nonmonotonic reasoning literature. In particular we look at the so called Yale
shooting problem and its relation to priority in default reasoning."
Is Shafer General Bayes?,"This paper examines the relationship between Shafer's belief functions and
convex sets of probability distributions. Kyburg's (1986) result showed that
belief function models form a subset of the class of closed convex probability
distributions. This paper emphasizes the importance of Kyburg's result by
looking at simple examples involving Bernoulli trials. Furthermore, it is shown
that many convex sets of probability distributions generate the same belief
function in the sense that they support the same lower and upper values. This
has implications for a decision theoretic extension. Dempster's rule of
combination is also compared with Bayes' rule of conditioning."
Modifiable Combining Functions,"Modifiable combining functions are a synthesis of two common approaches to
combining evidence. They offer many of the advantages of these approaches and
avoid some disadvantages. Because they facilitate the acquisition,
representation, explanation, and modification of knowledge about combinations
of evidence, they are proposed as a tool for knowledge engineers who build
systems that reason under uncertainty, not as a normative theory of evidence."
Dempster-Shafer vs. Probabilistic Logic,"The combination of evidence in Dempster-Shafer theory is compared with the
combination of evidence in probabilistic logic. Sufficient conditions are
stated for these two methods to agree. It is then shown that these conditions
are minimal in the sense that disagreement can occur when any one of them is
removed. An example is given in which the traditional assumption of conditional
independence of evidence on hypotheses holds and a uniform prior is assumed,
but probabilistic logic and Dempster's rule give radically different results
for the combination of two evidence events."
"Belief in Belief Functions: An Examination of Shafer's Canonical
  Examples","In the canonical examples underlying Shafer-Dempster theory, beliefs over the
hypotheses of interest are derived from a probability model for a set of
auxiliary hypotheses. Beliefs are derived via a compatibility relation
connecting the auxiliary hypotheses to subsets of the primary hypotheses. A
belief function differs from a Bayesian probability model in that one does not
condition on those parts of the evidence for which no probabilities are
specified. The significance of this difference in conditioning assumptions is
illustrated with two examples giving rise to identical belief functions but
different Bayesian probability distributions."
Can Evidence Be Combined in the Dempster-Shafer Theory,"Dempster's rule of combination has been the most controversial part of the
Dempster-Shafer (D-S) theory. In particular, Zadeh has reached a conjecture on
the noncombinability of evidence from a relational model of the D-S theory. In
this paper, we will describe another relational model where D-S masses are
represented as conditional granular distributions. By comparing it with Zadeh's
relational model, we will show how Zadeh's conjecture on combinability does not
affect the applicability of Dempster's rule in our model."
Temporal Reasoning About Uncertain Worlds,"We present a program that manages a database of temporally scoped beliefs.
The basic functionality of the system includes maintaining a network of
constraints among time points, supporting a variety of fetches, mediating the
application of causal rules, monitoring intervals of time for the addition of
new facts, and managing data dependencies that keep the database consistent. At
this level the system operates independent of any measure of belief or belief
calculus. We provide an example of how an application program mi9ght use this
functionality to implement a belief calculus."
"A Perspective on Confidence and Its Use in Focusing Attention During
  Knowledge Acquisition","We present a representation of partial confidence in belief and preference
that is consistent with the tenets of decision-theory. The fundamental insight
underlying the representation is that if a person is not completely confident
in a probability or utility assessment, additional modeling of the assessment
may improve decisions to which it is relevant. We show how a traditional
decision-analytic approach can be used to balance the benefits of additional
modeling with associated costs. The approach can be used during knowledge
acquisition to focus the attention of a knowledge engineer or expert on parts
of a decision model that deserve additional refinement."
Practical Issues in Constructing a Bayes' Belief Network,"Bayes belief networks and influence diagrams are tools for constructing
coherent probabilistic representations of uncertain knowledge. The process of
constructing such a network to represent an expert's knowledge is used to
illustrate a variety of techniques which can facilitate the process of
structuring and quantifying uncertain relationships. These include some
generalizations of the ""noisy OR gate"" concept. Sensitivity analysis of generic
elements of Bayes' networks provides insight into when rough probability
assessments are sufficient and when greater precision may be important."
Objective Probability,"A distinction is sometimes made between ""statistical"" and ""subjective""
probabilities. This is based on a distinction between ""unique"" events and
""repeatable"" events. We argue that this distinction is untenable, since all
events are ""unique"" and all events belong to ""kinds"", and offer a conception of
probability for A1 in which (1) all probabilities are based on -- possibly
vague -- statistical knowledge, and (2) every statement in the language has a
probability. This conception of probability can be applied to very rich
languages."
Decision Tree Induction Systems: A Bayesian Analysis,"Decision tree induction systems are being used for knowledge acquisition in
noisy domains. This paper develops a subjective Bayesian interpretation of the
task tackled by these systems and the heuristic methods they use. It is argued
that decision tree systems implicitly incorporate a prior belief that the
simpler (in terms of decision tree complexity) of two hypotheses be preferred,
all else being equal, and that they perform a greedy search of the space of
decision rules to find one in which there is strong posterior belief. A number
of improvements to these systems are then suggested."
"The Automatic Training of Rule Bases that Use Numerical Uncertainty
  Representations","The use of numerical uncertainty representations allows better modeling of
some aspects of human evidential reasoning. It also makes knowledge acquisition
and system development, test, and modification more difficult. We propose that
where possible, the assignment and/or refinement of rule weights should be
performed automatically. We present one approach to performing this training -
numerical optimization - and report on the results of some preliminary tests in
training rule bases. We also show that truth maintenance can be used to make
training more efficient and ask some epistemological questions raised by
training rule weights."
The Inductive Logic of Information Systems,"An inductive logic can be formulated in which the elements are not
propositions or probability distributions, but information systems. The logic
is complete for information systems with binary hypotheses, i.e., it applies to
all such systems. It is not complete for information systems with more than two
hypotheses, but applies to a subset of such systems. The logic is inductive in
that conclusions are more informative than premises. Inferences using the
formalism have a strong justification in terms of the expected value of the
derived information system."
The Recovery of Causal Poly-Trees from Statistical Data,"Poly-trees are singly connected causal networks in which variables may arise
from multiple causes. This paper develops a method of recovering ply-trees from
empirically measured probability distributions of pairs of variables. The
method guarantees that, if the measured distributions are generated by a causal
process structured as a ply-tree then the topological structure of such tree
can be recovered precisely and, in addition, the causal directionality of the
branches can be determined up to the maximum extent possible. The method also
pinpoints the minimum (if any) external semantics required to determine the
causal relationships among the variables considered."
"A Heuristic Bayesian Approach to Knowledge Acquisition: Application to
  Analysis of Tissue-Type Plasminogen Activator","This paper describes a heuristic Bayesian method for computing probability
distributions from experimental data, based upon the multivariate normal form
of the influence diagram. An example illustrates its use in medical technology
assessment. This approach facilitates the integration of results from different
studies, and permits a medical expert to make proper assessments without
considerable statistical training."
A Study of Associative Evidential Reasoning,"Evidential reasoning is cast as the problem of simplifying the
evidence-hypothesis relation and constructing combination formulas that possess
certain testable properties. Important classes of evidence as identifiers,
annihilators, and idempotents and their roles in determining binary operations
on intervals of reals are discussed. The appropriate way of constructing
formulas for combining evidence and their limitations, for instance, in
robustness, are presented."
Convergent Deduction for Probabilistic Logic,"This paper discusses the semantics and proof theory of Nilsson's
probabilistic logic, outlining both the benefits of its well-defined model
theory and the drawbacks of its proof theory. Within Nilsson's semantic
framework, we derive a set of inference rules which are provably sound. The
resulting proof system, in contrast to Nilsson's approach, has the important
feature of convergence - that is, the inference process proceeds by computing
increasingly narrow probability intervals which converge from above and below
on the smallest entailed probability interval. Thus the procedure can be
stopped at any time to yield partial information concerning the smallest
entailed interval."
A Knowledge Engineer's Comparison of Three Evidence Aggregation Methods,"The comparisons of uncertainty calculi from the last two Uncertainty
Workshops have all used theoretical probabilistic accuracy as the sole metric.
While mathematical correctness is important, there are other factors which
should be considered when developing reasoning systems. These other factors
include, among other things, the error in uncertainty measures obtainable for
the problem and the effect of this error on the performance of the resulting
system."
Problem Structure and Evidential Reasoning,"In our previous series of studies to investigate the role of evidential
reasoning in the RUBRIC system for full-text document retrieval (Tong et al.,
1985; Tong and Shapiro, 1985; Tong and Appelbaum, 1987), we identified the
important role that problem structure plays in the overall performance of the
system. In this paper, we focus on these structural elements (which we now call
""semantic structure"") and show how explicit consideration of their properties
reduces what previously were seen as difficult evidential reasoning problems to
more tractable questions."
The Role of Tuning Uncertain Inference Systems,"This study examined the effects of ""tuning"" the parameters of the incremental
function of MYCIN, the independent function of PROSPECTOR, a probability model
that assumes independence, and a simple additive linear equation. me parameters
of each of these models were optimized to provide solutions which most nearly
approximated those from a full probability model for a large set of simple
networks. Surprisingly, MYCIN, PROSPECTOR, and the linear equation performed
equivalently; the independence model was clearly more accurate on the networks
studied."
Integrating Logical and Probabilistic Reasoning for Decision Making,"We describe a representation and a set of inference methods that combine
logic programming techniques with probabilistic network representations for
uncertainty (influence diagrams). The techniques emphasize the dynamic
construction and solution of probabilistic and decision-theoretic models for
complex and uncertain domains. Given a query, a logical proof is produced if
possible; if not, an influence diagram based on the query and the knowledge of
the decision domain is produced and subsequently solved. A uniform declarative,
first-order, knowledge representation is combined with a set of integrated
inference procedures for logical, probabilistic, and decision-theoretic
reasoning."
An Algorithm for Computing Probabilistic Propositions,"A method for computing probabilistic propositions is presented. It assumes
the availability of a single external routine for computing the probability of
one instantiated variable, given a conjunction of other instantiated variables.
In particular, the method allows belief network algorithms to calculate general
probabilistic propositions over nodes in the network. Although in the worst
case the time complexity of the method is exponential in the size of a query,
it is polynomial in the size of a number of common types of queries."
"Taxonomy, Structure, and Implementation of Evidential Reasoning","The fundamental elements of evidential reasoning problems are described,
followed by a discussion of the structure of various types of problems.
Bayesian inference networks and state space formalism are used as the tool for
problem representation.
  A human-oriented decision making cycle for solving evidential reasoning
problems is described and illustrated for a military situation assessment
problem. The implementation of this cycle may serve as the basis for an expert
system shell for evidential reasoning; i.e. a situation assessment processor."
Towards The Inductive Acquisition of Temporal Knowledge,"The ability to predict the future in a given domain can be acquired by
discovering empirically from experience certain temporal patterns that tend to
repeat unerringly. Previous works in time series analysis allow one to make
quantitative predictions on the likely values of certain linear variables.
Since certain types of knowledge are better expressed in symbolic forms, making
qualitative predictions based on symbolic representations require a different
approach. A domain independent methodology called TIM (Time based Inductive
Machine) for discovering potentially uncertain temporal patterns from real time
observations using the technique of inductive inference is described here."
Reasoning With Uncertain Knowledge,"A model of knowledge representation is described in which propositional facts
and the relationships among them can be supported by other facts. The set of
knowledge which can be supported is called the set of cognitive units, each
having associated descriptions of their explicit and implicit support
structures, summarizing belief and reliability of belief. This summary is
precise enough to be useful in a computational model while remaining
descriptive of the underlying symbolic support structure. When a fact supports
another supportive relationship between facts we call this meta-support. This
facilitates reasoning about both the propositional knowledge. and the support
structures underlying it."
"Deriving And Combining Continuous Possibility Functions in the Framework
  of Evidential Reasoning","To develop an approach to utilizing continuous statistical information within
the Dempster- Shafer framework, we combine methods proposed by Strat and by
Shafero We first derive continuous possibility and mass functions from
probability-density functions. Then we propose a rule for combining such
evidence that is simpler and more efficiently computed than Dempster's rule. We
discuss the relationship between Dempster's rule and our proposed rule for
combining evidence over continuous frames."
Non-Monotonicity in Probabilistic Reasoning,"We start by defining an approach to non-monotonic probabilistic reasoning in
terms of non-monotonic categorical (true-false) reasoning. We identify a type
of non-monotonic probabilistic reasoning, akin to default inheritance, that is
commonly found in practice, especially in ""evidential"" and ""Bayesian""
reasoning. We formulate this in terms of the Maximization of Conditional
Independence (MCI), and identify a variety of applications for this sort of
default. We propose a formalization using Pointwise Circumscription. We compare
MCI to Maximum Entropy, another kind of non-monotonic principle, and conclude
by raising a number of open questions"
"Flexible Interpretations: A Computational Model for Dynamic Uncertainty
  Assessment","The investigations reported in this paper center on the process of dynamic
uncertainty assessment during interpretation tasks in real domain. In
particular, we are interested here in the nature of the control structure of
computer programs that can support multiple interpretation and smooth
transitions between them, in real time. Each step of the processing involves
the interpretation of one input item and the appropriate re-establishment of
the system's confidence of the correctness of its interpretation(s)."
Evidence as Opinions of Experts,"We describe a viewpoint on the Dempster/Shafer 'Theory of Evidence', and
provide an interpretation which regards the combination formulas as statistics
of the opinions of ""experts"". This is done by introducing spaces with binary
operations that are simpler to interpret or simpler to implement than the
standard combination formula, and showing that these spaces can be mapped
homomorphically onto the Dempster/Shafer theory of evidence space. The experts
in the space of ""opinions of experts"" combine information in a Bayesian
fashion. We present alternative spaces for the combination of evidence
suggested by this viewpoint."
Bayesian Inference for Radar Imagery Based Surveillance,"We are interested in creating an automated or semi-automated system with the
capability of taking a set of radar imagery, collection parameters and a priori
map and other tactical data, and producing likely interpretations of the
possible military situations given the available evidence. This paper is
concerned with the problem of the interpretation and computation of certainty
or belief in the conclusions reached by such a system."
Evidential Reasoning in Parallel Hierarchical Vision Programs,"This paper presents an efficient adaptation and application of the
Dempster-Shafer theory of evidence, one that can be used effectively in a
massively parallel hierarchical system for visual pattern perception. It
describes the techniques used, and shows in an extended example how they serve
to improve the system's performance as it applies a multiple-level set of
processes."
Computing Reference Classes,"For any system with limited statistical knowledge, the combination of
evidence and the interpretation of sampling information require the
determination of the right reference class (or of an adequate one). The present
note (1) discusses the use of reference classes in evidential reasoning, and
(2) discusses implementations of Kyburg's rules for reference classes. This
paper contributes the first frank discussion of how much of Kyburg's system is
needed to be powerful, how much can be computed effectively, and how much is
philosophical fat."
"An Uncertainty Management Calculus for Ordering Searches in Distributed
  Dynamic Databases","MINDS is a distributed system of cooperating query engines that customize,
document retrieval for each user in a dynamic environment. It improves its
performance and adapts to changing patterns of document distribution by
observing system-user interactions and modifying the appropriate certainty
factors, which act as search control parameters. It argued here that the
uncertainty management calculus must account for temporal precedence,
reliability of evidence, degree of support for a proposition, and saturation
effects. The calculus presented here possesses these features. Some results
obtained with this scheme are discussed."
Estimating Uncertain Spatial Relationships in Robotics,"In this paper, we describe a representation for spatial information, called
the stochastic map, and associated procedures for building it, reading
information from it, and revising it incrementally as new information is
obtained. The map contains the estimates of relationships among objects in the
map, and their uncertainties, given all the available information. The
procedures provide a general solution to the problem of estimating uncertain
relative spatial relationships. The estimates are probabilistic in nature, an
advance over the previous, very conservative, worst-case approaches to the
problem. Finally, the procedures are developed in the context of
state-estimation and filtering theory, which provides a solid basis for
numerous extensions."
Evaluation of Uncertain Inference Models I: PROSPECTOR,"This paper examines the accuracy of the PROSPECTOR model for uncertain
reasoning. PROSPECTOR's solutions for a large number of computer-generated
inference networks were compared to those obtained from probability theory and
minimum cross-entropy calculations. PROSPECTOR's answers were generally
accurate for a restricted subset of problems that are consistent with its
assumptions. However, even within this subset, we identified conditions under
which PROSPECTOR's performance deteriorates."
A Framework for Non-Monotonic Reasoning About Probabilistic Assumptions,"Attempts to replicate probabilistic reasoning in expert systems have
typically overlooked a critical ingredient of that process. Probabilistic
analysis typically requires extensive judgments regarding interdependencies
among hypotheses and data, and regarding the appropriateness of various
alternative models. The application of such models is often an iterative
process, in which the plausibility of the results confirms or disconfirms the
validity of assumptions made in building the model. In current expert systems,
by contrast, probabilistic information is encapsulated within modular rules
(involving, for example, ""certainty factors""), and there is no mechanism for
reviewing the overall form of the probability argument or the validity of the
judgments entering into it."
"Induction, of and by Probability","This paper examines some methods and ideas underlying the author's successful
probabilistic learning systems(PLS), which have proven uniquely effective and
efficient in generalization learning or induction. While the emerging
principles are generally applicable, this paper illustrates them in heuristic
search, which demands noise management and incremental learning. In our
approach, both task performance and learning are guided by probability.
Probabilities are incrementally normalized and revised, and their errors are
located and corrected."
Combining Uncertain Estimates,"In a real expert system, one may have unreliable, unconfident, conflicting
estimates of the value for a particular parameter. It is important for decision
making that the information present in this aggregate somehow find its way into
use. We cast the problem of representing and combining uncertain estimates as
selection of two kinds of functions, one to determine an estimate, the other
its uncertainty. The paper includes a long list of properties that such
functions should satisfy, and it presents one method that satisfies them."
Incidence Calculus: A Mechanism for Probabilistic Reasoning,"Mechanisms for the automation of uncertainty are required for expert systems.
Sometimes these mechanisms need to obey the properties of probabilistic
reasoning. A purely numeric mechanism, like those proposed so far, cannot
provide a probabilistic logic with truth functional connectives. We propose an
alternative mechanism, Incidence Calculus, which is based on a representation
of uncertainty using sets of points, which might represent situations, models
or possible worlds. Incidence Calculus does provide a probabilistic logic with
truth functional connectives."
Exact Reasoning Under Uncertainty,"This paper focuses on designing expert systems to support decision making in
complex, uncertain environments. In this context, our research indicates that
strictly probabilistic representations, which enable the use of
decision-theoretic reasoning, are highly preferable to recently proposed
alternatives (e.g., fuzzy set theory and Dempster-Shafer theory). Furthermore,
we discuss the language of influence diagrams and a corresponding methodology
-decision analysis -- that allows decision theory to be used effectively and
efficiently as a decision-making aid. Finally, we use RACHEL, a system that
helps infertile couples select medical treatments, to illustrate the
methodology of decision analysis as basis for expert decision systems."
Strong & Weak Methods: A Logical View of Uncertainty,"The last few years has seen a growing debate about techniques for managing
uncertainty in AI systems. Unfortunately this debate has been cast as a rivalry
between AI methods and classical probability based ones. Three arguments for
extending the probability framework of uncertainty are presented, none of which
imply a challenge to classical methods. These are (1) explicit representation
of several types of uncertainty, specifically possibility and plausibility, as
well as probability, (2) the use of weak methods for uncertainty management in
problems which are poorly defined, and (3) symbolic representation of different
uncertainty calculi and methods for choosing between them."
Statistical Mechanics Algorithm for Response to Targets (SMART),"It is proposed to apply modern methods of nonlinear nonequilibrium
statistical mechanics to develop software algorithms that will optimally
respond to targets within short response times with minimal computer resources.
This Statistical Mechanics Algorithm for Response to Targets (SMART) can be
developed with a view towards its future implementation into a hardwired
Statistical Algorithm Multiprocessor (SAM) to enhance the efficiency and speed
of response to targets (SMART_SAM)."
Knowledge Structures and Evidential Reasoning in Decision Analysis,"The roles played by decision factors in making complex subject are decisions
are characterized by how these factors affect the overall decision. Evidence
that partially matches a factor is evaluated, and then effective computational
rules are applied to these roles to form an appropriate aggregation of the
evidence. The use of this technique supports the expression of deeper levels of
causality, and may also preserve the cognitive structure of the decision maker
better than the usual weighting methods, certainty-factor or other
probabilistic models can."
A Social Welfare Optimal Sequential Allocation Procedure,"We consider a simple sequential allocation procedure for sharing indivisible
items between agents in which agents take turns to pick items. Supposing
additive utilities and independence between the agents, we show that the
expected utility of each agent is computable in polynomial time. Using this
result, we prove that the expected utilitarian social welfare is maximized when
agents take alternate turns. We also argue that this mechanism remains optimal
when agents behave strategically"
How to minimize the energy consumption in mobile ad-hoc networks,"In this work we are interested in the problem of energy management in Mobile
Ad-hoc Network (MANET). The solving and optimization of MANET allow assisting
the users to efficiently use their devices in order to minimize the batteries
power consumption. In this framework, we propose a modelling of the MANET in
form of a Constraint Optimization Problem called COMANET. Then, in the
objective to minimize the consumption of batteries power, we present an
approach based on an adaptation of the A star algorithm to the MANET problem
called MANED. Finally, we expose some experimental results showing utility of
this approach."
Probabilistic Conditional Preference Networks,"In order to represent the preferences of a group of individuals, we introduce
Probabilistic CP-nets (PCP-nets). PCP-nets provide a compact language for
representing probability distributions over preference orderings. We argue that
they are useful for aggregating preferences or modelling noisy preferences.
Then we give efficient algorithms for the main reasoning problems, namely for
computing the probability that a given outcome is preferred to another one, and
the probability that a given outcome is optimal. As a by-product, we obtain an
unexpected linear-time algorithm for checking dominance in a standard,
tree-structured CP-net."
Advances in Bayesian Network Learning using Integer Programming,"We consider the problem of learning Bayesian networks (BNs) from complete
discrete data. This problem of discrete optimisation is formulated as an
integer program (IP). We describe the various steps we have taken to allow
efficient solving of this IP. These are (i) efficient search for cutting
planes, (ii) a fast greedy algorithm to find high-scoring (perhaps not optimal)
BNs and (iii) tightening the linear relaxation of the IP. After relating this
BN learning problem to set covering and the multidimensional 0-1 knapsack
problem, we present our empirical results. These show improvements, sometimes
dramatic, over earlier results."
"A Sound and Complete Algorithm for Learning Causal Models from
  Relational Data","The PC algorithm learns maximally oriented causal Bayesian networks. However,
there is no equivalent complete algorithm for learning the structure of
relational models, a more expressive generalization of Bayesian networks.
Recent developments in the theory and representation of relational models
support lifted reasoning about conditional independence. This enables a
powerful constraint for orienting bivariate dependencies and forms the basis of
a new algorithm for learning structure. We present the relational causal
discovery (RCD) algorithm that learns causal relational models. We prove that
RCD is sound and complete, and we present empirical results that demonstrate
effectiveness."
"Identifying Finite Mixtures of Nonparametric Product Distributions and
  Causal Inference of Confounders","We propose a kernel method to identify finite mixtures of nonparametric
product distributions. It is based on a Hilbert space embedding of the joint
distribution. The rank of the constructed tensor is equal to the number of
mixture components. We present an algorithm to recover the components by
partitioning the data points into clusters such that the variables are jointly
conditionally independent given the cluster. This method can be used to
identify finite confounders."
Case Adaptation with Qualitative Algebras,"This paper proposes an approach for the adaptation of spatial or temporal
cases in a case-based reasoning system. Qualitative algebras are used as
spatial and temporal knowledge representation languages. The intuition behind
this adaptation approach is to apply a substitution and then repair potential
inconsistencies, thanks to belief revision on qualitative algebras. A temporal
example from the cooking domain is given. (The paper on which this extended
abstract is based was the recipient of the best paper award of the 2012
International Conference on Case-Based Reasoning.)"
Planning based on classification by induction graph,"In Artificial Intelligence, planning refers to an area of research that
proposes to develop systems that can automatically generate a result set, in
the form of an integrated decision-making system through a formal procedure,
known as plan. Instead of resorting to the scheduling algorithms to generate
plans, it is proposed to operate the automatic learning by decision tree to
optimize time. In this paper, we propose to build a classification model by
induction graph from a learning sample containing plans that have an associated
set of descriptors whose values change depending on each plan. This model will
then operate for classifying new cases by assigning the appropriate plan."
Giving the AI definition a form suitable for the engineer,"Artificial Intelligence - what is this? That is the question! In earlier
papers we already gave a formal definition for AI, but if one desires to build
an actual AI implementation, the following issues require attention and are
treated here: the data format to be used, the idea of Undef and Nothing
symbols, various ways for defining the ""meaning of life"", and finally, a new
notion of ""incorrect move"". These questions are of minor importance in the
theoretical discussion, but we already know the answer of the question ""Does AI
exist?"" Now we want to make the next step and to create this program."
Transductive Rademacher Complexity and its Applications,"We develop a technique for deriving data-dependent error bounds for
transductive learning algorithms based on transductive Rademacher complexity.
Our technique is based on a novel general error bound for transduction in terms
of transductive Rademacher complexity, together with a novel bounding technique
for Rademacher averages for particular algorithms, in terms of their
""unlabeled-labeled"" representation. This technique is relevant to many advanced
graph-based transductive algorithms and we demonstrate its effectiveness by
deriving error bounds to three well known algorithms. Finally, we present a new
PAC-Bayesian bound for mixtures of transductive algorithms based on our
Rademacher bounds."
The Role of Macros in Tractable Planning,"This paper presents several new tractability results for planning based on
macros. We describe an algorithm that optimally solves planning problems in a
class that we call inverted tree reducible, and is provably tractable for
several subclasses of this class. By using macros to store partial plans that
recur frequently in the solution, the algorithm is polynomial in time and space
even for exponentially long plans. We generalize the inverted tree reducible
class in several ways and describe modifications of the algorithm to deal with
these new classes. Theoretical results are validated in experiments."
Fast Set Bounds Propagation Using a BDD-SAT Hybrid,"Binary Decision Diagram (BDD) based set bounds propagation is a powerful
approach to solving set-constraint satisfaction problems. However, prior BDD
based techniques in- cur the significant overhead of constructing and
manipulating graphs during search. We present a set-constraint solver which
combines BDD-based set-bounds propagators with the learning abilities of a
modern SAT solver. Together with a number of improvements beyond the basic
algorithm, this solver is highly competitive with existing propagation based
set constraint solvers."
On the Intertranslatability of Argumentation Semantics,"Translations between different nonmonotonic formalisms always have been an
important topic in the field, in particular to understand the
knowledge-representation capabilities those formalisms offer. We provide such
an investigation in terms of different semantics proposed for abstract
argumentation frameworks, a nonmonotonic yet simple formalism which received
increasing interest within the last decade. Although the properties of these
different semantics are nowadays well understood, there are no explicit results
about intertranslatability. We provide such translations wrt. different
properties and also give a few novel complexity results which underlie some
negative results."
Dr.Fill: Crosswords and an Implemented Solver for Singly Weighted CSPs,"We describe Dr.Fill, a program that solves American-style crossword puzzles.
From a technical perspective, Dr.Fill works by converting crosswords to
weighted CSPs, and then using a variety of novel techniques to find a solution.
These techniques include generally applicable heuristics for variable and value
selection, a variant of limited discrepancy search, and postprocessing and
partitioning ideas. Branch and bound is not used, as it was incompatible with
postprocessing and was determined experimentally to be of little practical
value. Dr.Fillls performance on crosswords from the American Crossword Puzzle
Tournament suggests that it ranks among the top fifty or so crossword solvers
in the world."
"Interactions between Knowledge and Time in a First-Order Logic for
  Multi-Agent Systems: Completeness Results","We investigate a class of first-order temporal-epistemic logics for reasoning
about multi-agent systems. We encode typical properties of systems including
perfect recall, synchronicity, no learning, and having a unique initial state
in terms of variants of quantified interpreted systems, a first-order extension
of interpreted systems. We identify several monodic fragments of first-order
temporal-epistemic logic and show their completeness with respect to their
corresponding classes of quantified interpreted systems."
Narrative Planning: Compilations to Classical Planning,"A model of story generation recently proposed by Riedl and Young casts it as
planning, with the additional condition that story characters behave
intentionally. This means that characters have perceivable motivation for the
actions they take. I show that this condition can be compiled away (in more
ways than one) to produce a classical planning problem that can be solved by an
off-the-shelf classical planner, more efficiently than by Riedl and Youngs
specialised planner."
Statistical Constraints,"We introduce statistical constraints, a declarative modelling tool that links
statistics and constraint programming. We discuss two statistical constraints
and some associated filtering algorithms. Finally, we illustrate applications
to standard problems encountered in statistics and to a novel inspection
scheduling problem in which the aim is to find inspection plans with desirable
statistical properties."
What Is It Like to Be a Brain Simulation?,"We frame the question of what kind of subjective experience a brain
simulation would have in contrast to a biological brain. We discuss the brain
prosthesis thought experiment. We evaluate how the experience of the brain
simulation might differ from the biological, according to a number of
hypotheses about experience and the properties of simulation. Then, we identify
finer questions relating to the original inquiry, and answer them from both a
general physicalist, and panexperientialist perspective."
Evolutionary solving of the debts' clearing problem,"The debts' clearing problem is about clearing all the debts in a group of n
entities (persons, companies etc.) using a minimal number of money transaction
operations. The problem is known to be NP-hard in the strong sense. As for many
intractable problems, techniques from the field of artificial intelligence are
useful in finding solutions close to optimum for large inputs. An evolutionary
algorithm for solving the debts' clearing problem is proposed."
Thou Shalt is not You Will,"In this paper we discuss some reasons why temporal logic might not be
suitable to model real life norms. To show this, we present a novel deontic
logic contrary-to-duty/derived permission paradox based on the interaction of
obligations, permissions and contrary-to-duty obligations. The paradox is
inspired by real life norms."
Rational Counterfactuals,"This paper introduces the concept of rational countefactuals which is an idea
of identifying a counterfactual from the factual (whether perceived or real)
that maximizes the attainment of the desired consequent. In counterfactual
thinking if we have a factual statement like: Saddam Hussein invaded Kuwait and
consequently George Bush declared war on Iraq then its counterfactuals is: If
Saddam Hussein did not invade Kuwait then George Bush would not have declared
war on Iraq. The theory of rational counterfactuals is applied to identify the
antecedent that gives the desired consequent necessary for rational decision
making. The rational countefactual theory is applied to identify the values of
variables Allies, Contingency, Distance, Major Power, Capability, Democracy, as
well as Economic Interdependency that gives the desired consequent Peace."
Model revision inference for extensions of first order logic,"I am Joachim Jansen and this is my research summary, part of my application
to the Doctoral Consortium at ICLP'14. I am a PhD student in the Knowledge
Representation and Reasoning (KRR) research group, a subgroup of the
Declarative Languages and Artificial Intelligence (DTAI) group at the
department of Computer Science at KU Leuven. I started my PhD in September
2012. My promotor is prof. dr. ir. Gerda Janssens and my co-promotor is prof.
dr. Marc Denecker. I can be contacted at joachim.jansen@cs.kuleuven.be or at:
Room 01.167 Celestijnenlaan 200A 3001 Heverlee Belgium An extended abstract /
full version of a paper accepted to be presented at the Doctoral Consortium of
the 30th International Conference on Logic Programming (ICLP 2014), July 19-22,
Vienna, Austria"
An eigenvector-based hotspot detection,"Space and time are two critical components of many real world systems. For
this reason, analysis of anomalies in spatiotemporal data has been a great of
interest. In this work, application of tensor decomposition and eigenspace
techniques on spatiotemporal hotspot detection is investigated. An algorithm
called SST-Hotspot is proposed which accounts for spatiotemporal variations in
data and detect hotspots using matching of eigenvector elements of two cases
and population tensors. The experimental results reveal the interesting
application of tensor decomposition and eigenvector-based techniques in hotspot
analysis."
Flow for Meta Control,"The psychological state of flow has been linked to optimizing human
performance. A key condition of flow emergence is a match between the human
abilities and complexity of the task. We propose a simple computational model
of flow for Artificial Intelligence (AI) agents. The model factors the standard
agent-environment state into a self-reflective set of the agent's abilities and
a socially learned set of the environmental complexity. Maximizing the flow
serves as a meta control for the agent. We show how to apply the meta-control
policy to a broad class of AI control policies and illustrate our approach with
a specific implementation. Results in a synthetic testbed are promising and
open interesting directions for future work."
A Logic for Reasoning about Evidence,"We introduce a logic for reasoning about evidence, that essentially views
evidence as a function from prior beliefs (before making an observation) to
posterior beliefs (after making the observation). We provide a sound and
complete axiomatization for the logic, and consider the complexity of the
decision problem. Although the reasoning in the logic is mainly propositional,
we allow variables representing numbers and quantification over them. This
expressive power seems necessary to capture important properties of evidence"
A Logic for Reasoning about Upper Probabilities,"We present a propositional logic to reason about the uncertainty of events,
where the uncertainty is modeled by a set of probability measures assigning an
interval of probability to each event. We give a sound and complete
axiomatization for the logic, and show that the satisfiability problem is
NP-complete, no harder than satisfiability for propositional logic."
A Heuristic Search Algorithm for Solving First-Order MDPs,"We present a heuristic search algorithm for solving first-order MDPs
(FOMDPs). Our approach combines first-order state abstraction that avoids
evaluating states individually, and heuristic search that avoids evaluating all
states. Firstly, we apply state abstraction directly on the FOMDP avoiding
propositionalization. Such kind of abstraction is referred to as firstorder
state abstraction. Secondly, guided by an admissible heuristic, the search is
restricted only to those states that are reachable from the initial state. We
demonstrate the usefullness of the above techniques for solving FOMDPs on a
system, referred to as FCPlanner, that entered the probabilistic track of the
International Planning Competition (IPC'2004)."
Markov Chains on Orbits of Permutation Groups,"We present a novel approach to detecting and utilizing symmetries in
probabilistic graphical models with two main contributions. First, we present a
scalable approach to computing generating sets of permutation groups
representing the symmetries of graphical models. Second, we introduce orbital
Markov chains, a novel family of Markov chains leveraging model symmetries to
reduce mixing times. We establish an insightful connection between model
symmetries and rapid mixing of orbital Markov chains. Thus, we present the
first lifted MCMC algorithm for probabilistic graphical models. Both analytical
and empirical results demonstrate the effectiveness and efficiency of the
approach."
Probabilistic Selection in AgentSpeak(L),"Agent programming is mostly a symbolic discipline and, as such, draws little
benefits from probabilistic areas as machine learning and graphical models.
However, the greatest objective of agent research is the achievement of
autonomy in dynamical and complex environments --- a goal that implies
embracing uncertainty and therefore the entailed representations, algorithms
and techniques. This paper proposes an innovative and conflict free two layer
approach to agent programming that uses already established methods and tools
from both symbolic and probabilistic artificial intelligence. Moreover, this
framework is illustrated by means of a widely used agent programming example,
GoldMiners."
"Some Reflections on the Set-based and the Conditional-based
  Interpretations of Statements in Syllogistic Reasoning","Two interpretations about syllogistic statements are described in this paper.
One is the so-called set-based interpretation, which assumes that quantified
statements and syllogisms talk about quantity-relationships between sets. The
other one, the so-called conditional interpretation, assumes that quantified
propositions talk about conditional propositions and how strong are the links
between the antecedent and the consequent. Both interpretations are compared
attending to three different questions (existential import, singular statements
and non-proportional quantifiers) from the point of view of their impact on the
further development of this type of reasoning."
Quantum computing for pattern classification,"It is well known that for certain tasks, quantum computing outperforms
classical computing. A growing number of contributions try to use this
advantage in order to improve or extend classical machine learning algorithms
by methods of quantum information theory. This paper gives a brief introduction
into quantum machine learning using the example of pattern classification. We
introduce a quantum pattern classification algorithm that draws on
Trugenberger's proposal for measuring the Hamming distance on a quantum
computer (CA Trugenberger, Phys Rev Let 87, 2001) and discuss its advantages
using handwritten digit recognition as from the MNIST database."
Qsmodels: ASP Planning in Interactive Gaming Environment,"Qsmodels is a novel application of Answer Set Programming to interactive
gaming environment. We describe a software architecture by which the behavior
of a bot acting inside the Quake 3 Arena can be controlled by a planner. The
planner is written as an Answer Set Program and is interpreted by the Smodels
solver."
Using the Mean Absolute Percentage Error for Regression Models,"We study in this paper the consequences of using the Mean Absolute Percentage
Error (MAPE) as a measure of quality for regression models. We show that
finding the best model under the MAPE is equivalent to doing weighted Mean
Absolute Error (MAE) regression. We show that universal consistency of
Empirical Risk Minimization remains possible using the MAPE instead of the MAE."
"Search Strategies for Binary Feature Selection for a Naive Bayes
  Classifier","We compare in this paper several feature selection methods for the Naive
Bayes Classifier (NBC) when the data under study are described by a large
number of redundant binary indicators. Wrapper approaches guided by the NBC
estimation of the classification error probability out-perform filter
approaches while retaining a reasonable computational cost."
Learning from Pairwise Marginal Independencies,"We consider graphs that represent pairwise marginal independencies amongst a
set of variables (for instance, the zero entries of a covariance matrix for
normal data). We characterize the directed acyclic graphs (DAGs) that
faithfully explain a given set of independencies, and derive algorithms to
efficiently enumerate such structures. Our results map out the space of
faithful causal models for a given set of pairwise marginal independence
relations. This allows us to show the extent to which causal inference is
possible without using conditional independence tests."
Turing's Imitation Game has been Improved,"Using the recently introduced universal computing model, called orchestrated
machine, that represents computations in a dissipative environment, we consider
a new kind of interpretation of Turing's Imitation Game. In addition we raise
the question whether the intelligence may show fractal properties. Then we
sketch a vision of what robotic cars are going to do in the future. Finally we
give the specification of an artificial life game based on the concept of
orchestrated machines. The purpose of this paper is to start the search for
possible relationships between these different topics."
On the Computability of AIXI,"How could we solve the machine learning and the artificial intelligence
problem if we had infinite computation? Solomonoff induction and the
reinforcement learning agent AIXI are proposed answers to this question. Both
are known to be incomputable. In this paper, we quantify this using the
arithmetical hierarchy, and prove upper and corresponding lower bounds for
incomputability. We show that AIXI is not limit computable, thus it cannot be
approximated using finite computation. Our main result is a limit-computable
{\epsilon}-optimal version of AIXI with infinite horizon that maximizes
expected rewards."
Welfare of Sequential Allocation Mechanisms for Indivisible Goods,"Sequential allocation is a simple and attractive mechanism for the allocation
of indivisible goods. Agents take turns, according to a policy, to pick items.
Sequential allocation is guaranteed to return an allocation which is efficient
but may not have an optimal social welfare. We consider therefore the relation
between welfare and efficiency. We study the (computational) questions of what
welfare is possible or necessary depending on the choice of policy. We also
consider a novel control problem in which the chair chooses a policy to improve
social welfare."
"An Empirical Comparison of Neural Architectures for Reinforcement
  Learning in Partially Observable Environments","This paper explores the performance of fitted neural Q iteration for
reinforcement learning in several partially observable environments, using
three recurrent neural network architectures: Long Short-Term Memory, Gated
Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of
several thousands candidate architectures. A variant of fitted Q iteration,
based on Advantage values instead of Q values, is also explored. The results
show that GRU performs significantly better than LSTM and MUT1 for most of the
problems considered, requiring less training episodes and less CPU time before
learning a very good policy. Advantage learning also tends to produce better
results."
Composing inference algorithms as program transformations,"Probabilistic inference procedures are usually coded painstakingly from
scratch, for each target model and each inference algorithm. We reduce this
effort by generating inference procedures from models automatically. We make
this code generation modular by decomposing inference algorithms into reusable
program-to-program transformations. These transformations perform exact
inference as well as generate probabilistic programs that compute expectations,
densities, and MCMC samples. The resulting inference procedures are about as
accurate and fast as other probabilistic programming systems on real-world
problems."
Bounded Optimal Exploration in MDP,"Within the framework of probably approximately correct Markov decision
processes (PAC-MDP), much theoretical work has focused on methods to attain
near optimality after a relatively long period of learning and exploration.
However, practical concerns require the attainment of satisfactory behavior
within a short period of time. In this paper, we relax the PAC-MDP conditions
to reconcile theoretically driven exploration methods and practical needs. We
propose simple algorithms for discrete and continuous state spaces, and
illustrate the benefits of our proposed relaxation via theoretical analyses and
numerical examples. Our algorithms also maintain anytime error bounds and
average loss bounds. Our approach accommodates both Bayesian and non-Bayesian
methods."
Normative Multiagent Systems: A Dynamic Generalization,"Social norms are powerful formalism in coordinating autonomous agents'
behaviour to achieve certain objectives. In this paper, we propose a dynamic
normative system to enable the reasoning of the changes of norms under
different circumstances, which cannot be done in the existing static normative
systems. We study two important problems (norm synthesis and norm recognition)
related to the autonomy of the entire system and the agents, and characterise
the computational complexities of solving these problems."
"Latent Contextual Bandits and their Application to Personalized
  Recommendations for New Users","Personalized recommendations for new users, also known as the cold-start
problem, can be formulated as a contextual bandit problem. Existing contextual
bandit algorithms generally rely on features alone to capture user variability.
Such methods are inefficient in learning new users' interests. In this paper we
propose Latent Contextual Bandits. We consider both the benefit of leveraging a
set of learned latent user classes for new users, and how we can learn such
latent classes from prior users. We show that our approach achieves a better
regret bound than existing algorithms. We also demonstrate the benefit of our
approach using a large real world dataset and a preliminary user study."
"Procedural Generation of Angry Birds Levels using Building Constructive
  Grammar with Chinese-Style and/or Japanese-Style Models","This paper presents a procedural generation method that creates visually
attractive levels for the Angry Birds game. Besides being an immensely popular
mobile game, Angry Birds has recently become a test bed for various artificial
intelligence technologies. We propose a new approach for procedurally
generating Angry Birds levels using Chinese style and Japanese style building
structures. A conducted experiment confirms the effectiveness of our approach
with statistical significance."
Propositional Abduction with Implicit Hitting Sets,"Logic-based abduction finds important applications in artificial intelligence
and related areas. One application example is in finding explanations for
observed phenomena. Propositional abduction is a restriction of abduction to
the propositional domain, and complexity-wise is in the second level of the
polynomial hierarchy. Recent work has shown that exploiting implicit hitting
sets and propositional satisfiability (SAT) solvers provides an efficient
approach for propositional abduction. This paper investigates this earlier work
and proposes a number of algorithmic improvements. These improvements are shown
to yield exponential reductions in the number of SAT solver calls. More
importantly, the experimental results show significant performance improvements
compared to the the best approaches for propositional abduction."
Teaching natural language to computers,"""Natural Language,"" whether spoken and attended to by humans, or processed
and generated by computers, requires networked structures that reflect creative
processes in semantic, syntactic, phonetic, linguistic, social, emotional, and
cultural modules. Being able to produce novel and useful behavior following
repeated practice gets to the root of both artificial intelligence and human
language. This paper investigates the modalities involved in language-like
applications that computers -- and programmers -- engage with, and aims to fine
tune the questions we ask to better account for context, self-awareness, and
embodiment."
"Review of state-of-the-arts in artificial intelligence with application
  to AI safety problem","Here, I review current state-of-the-arts in many areas of AI to estimate when
it's reasonable to expect human level AI development. Predictions of prominent
AI researchers vary broadly from very pessimistic predictions of Andrew Ng to
much more moderate predictions of Geoffrey Hinton and optimistic predictions of
Shane Legg, DeepMind cofounder. Given huge rate of progress in recent years and
this broad range of predictions of AI experts, AI safety questions are also
discussed."
"Automatic Extraction of Causal Relations from Natural Language Texts: A
  Comprehensive Survey","Automatic extraction of cause-effect relationships from natural language
texts is a challenging open problem in Artificial Intelligence. Most of the
early attempts at its solution used manually constructed linguistic and
syntactic rules on small and domain-specific data sets. However, with the
advent of big data, the availability of affordable computing power and the
recent popularization of machine learning, the paradigm to tackle this problem
has slowly shifted. Machines are now expected to learn generic causal
extraction rules from labelled data with minimal supervision, in a domain
independent-manner. In this paper, we provide a comprehensive survey of causal
relation extraction techniques from both paradigms, and analyse their relative
strengths and weaknesses, with recommendations for future work."
"Proceedings Fifteenth Conference on Theoretical Aspects of Rationality
  and Knowledge","The 15th Conference on Theoretical Aspects of Rationality and Knowledge
(TARK) took place in Carnegie Mellon University, Pittsburgh, USA from June 4 to
6, 2015.
  The mission of the TARK conferences is to bring together researchers from a
wide variety of fields, including Artificial Intelligence, Cryptography,
Distributed Computing, Economics and Game Theory, Linguistics, Philosophy, and
Psychology, in order to further our understanding of interdisciplinary issues
involving reasoning about rationality and knowledge.
  These proceedings consist of a subset of the papers / abstracts presented at
the TARK conference."
"Robust Natural Language Processing - Combining Reasoning, Cognitive
  Semantics and Construction Grammar for Spatial Language","We present a system for generating and understanding of dynamic and static
spatial relations in robotic interaction setups. Robots describe an environment
of moving blocks using English phrases that include spatial relations such as
""across"" and ""in front of"". We evaluate the system in robot-robot interactions
and show that the system can robustly deal with visual perception errors,
language omissions and ungrammatical utterances."
Learning to Rank for Synthesizing Planning Heuristics,"We investigate learning heuristics for domain-specific planning. Prior work
framed learning a heuristic as an ordinary regression problem. However, in a
greedy best-first search, the ordering of states induced by a heuristic is more
indicative of the resulting planner's performance than mean squared error.
Thus, we instead frame learning a heuristic as a learning to rank problem which
we solve using a RankSVM formulation. Additionally, we introduce new methods
for computing features that capture temporal interactions in an approximate
plan. Our experiments on recent International Planning Competition problems
show that the RankSVM learned heuristics outperform both the original
heuristics and heuristics learned through ordinary regression."
Smart Policies for Artificial Intelligence,"We argue that there already exists de facto artificial intelligence policy -
a patchwork of policies impacting the field of AI's development in myriad ways.
The key question related to AI policy, then, is not whether AI should be
governed at all, but how it is currently being governed, and how that
governance might become more informed, integrated, effective, and anticipatory.
We describe the main components of de facto AI policy and make some
recommendations for how AI policy can be improved, drawing on lessons from
other scientific and technological domains."
Latent Dependency Forest Models,"Probabilistic modeling is one of the foundations of modern machine learning
and artificial intelligence. In this paper, we propose a novel type of
probabilistic models named latent dependency forest models (LDFMs). A LDFM
models the dependencies between random variables with a forest structure that
can change dynamically based on the variable values. It is therefore capable of
modeling context-specific independence. We parameterize a LDFM using a
first-order non-projective dependency grammar. Learning LDFMs from data can be
formulated purely as a parameter learning problem, and hence the difficult
problem of model structure learning is circumvented. Our experimental results
show that LDFMs are competitive with existing probabilistic models."
An Extended Neo-Fuzzy Neuron and its Adaptive Learning Algorithm,"A modification of the neo-fuzzy neuron is proposed (an extended neo-fuzzy
neuron (ENFN)) that is characterized by improved approximating properties. An
adaptive learning algorithm is proposed that has both tracking and smoothing
properties. An ENFN distinctive feature is its computational simplicity
compared to other artificial neural networks and neuro-fuzzy systems."
"Overview: Generalizations of Multi-Agent Path Finding to Real-World
  Scenarios","Multi-agent path finding (MAPF) is well-studied in artificial intelligence,
robotics, theoretical computer science and operations research. We discuss
issues that arise when generalizing MAPF methods to real-world scenarios and
four research directions that address them. We emphasize the importance of
addressing these issues as opposed to developing faster methods for the
standard formulation of the MAPF problem."
Criticality & Deep Learning I: Generally Weighted Nets,"Motivated by the idea that criticality and universality of phase transitions
might play a crucial role in achieving and sustaining learning and intelligent
behaviour in biological and artificial networks, we analyse a theoretical and a
pragmatic experimental set up for critical phenomena in deep learning. On the
theoretical side, we use results from statistical physics to carry out critical
point calculations in feed-forward/fully connected networks, while on the
experimental side we set out to find traces of criticality in deep neural
networks. This is our first step in a series of upcoming investigations to map
out the relationship between criticality and learning in deep networks."
Minimax density estimation for growing dimension,"This paper presents minimax rates for density estimation when the data
dimension $d$ is allowed to grow with the number of observations $n$ rather
than remaining fixed as in previous analyses. We prove a non-asymptotic lower
bound which gives the worst-case rate over standard classes of smooth
densities, and we show that kernel density estimators achieve this rate. We
also give oracle choices for the bandwidth and derive the fastest rate $d$ can
grow with $n$ to maintain estimation consistency."
"BetaRun Soccer Simulation League Team: Variety, Complexity, and Learning","RoboCup offers a set of benchmark problems for Artificial Intelligence in
form of official world championships since 1997. The most tactical advanced and
richest in terms of behavioural complexity of these is the 2D Soccer Simulation
League, a simulated robotic soccer competition. BetaRun is a new attempt
combining both machine learning and manual programming approaches, with the
ultimate goal to arrive at a team that is trained entirely from observing and
playing games, and a new development based on agent2D."
Source-Sensitive Belief Change,"The AGM model is the most remarkable framework for modeling belief revision.
However, it is not perfect in all aspects. Paraconsistent belief revision,
multi-agent belief revision and non-prioritized belief revision are three
different extensions to AGM to address three important criticisms applied to
it. In this article, we propose a framework based on AGM that takes a position
in each of these categories. Also, we discuss some features of our framework
and study the satisfiability of AGM postulates in this new context."
MOBA: a New Arena for Game AI,"Games have always been popular testbeds for Artificial Intelligence (AI). In
the last decade, we have seen the rise of the Multiple Online Battle Arena
(MOBA) games, which are the most played games nowadays. In spite of this, there
are few works that explore MOBA as a testbed for AI Research. In this paper we
present and discuss the main features and opportunities offered by MOBA games
to Game AI Research. We describe the various challenges faced along the game
and also propose a discrete model that can be used to better understand and
explore the game. With this, we aim to encourage the use of MOBA as a novel
research platform for Game AI."
Low Impact Artificial Intelligences,"There are many goals for an AI that could become dangerous if the AI becomes
superintelligent or otherwise powerful. Much work on the AI control problem has
been focused on constructing AI goals that are safe even for such AIs. This
paper looks at an alternative approach: defining a general concept of `low
impact'. The aim is to ensure that a powerful AI which implements low impact
will not modify the world extensively, even if it is given a simple or
dangerous goal. The paper proposes various ways of defining and grounding low
impact, and discusses methods for ensuring that the AI can still be allowed to
have a (desired) impact despite the restriction. The end of the paper addresses
known issues with this approach and avenues for future research."
A Tutor Agent for MOBA Games,"Digital games have become a key player in the entertainment industry,
attracting millions of new players each year. In spite of that, novice players
may have a hard time when playing certain types of games, such as MOBAs and
MMORPGs, due to their steep learning curves and not so friendly online
communities. In this paper, we present an approach to help novice players in
MOBA games overcome these problems. An artificial intelligence agent plays
alongside the player analyzing his/her performance and giving tips about the
game. Experiments performed with the game {\em League of Legends} show the
potential of this approach."
Bandit Models of Human Behavior: Reward Processing in Mental Disorders,"Drawing an inspiration from behavioral studies of human decision making, we
propose here a general parametric framework for multi-armed bandit problem,
which extends the standard Thompson Sampling approach to incorporate reward
processing biases associated with several neurological and psychiatric
conditions, including Parkinson's and Alzheimer's diseases,
attention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain.
We demonstrate empirically that the proposed parametric approach can often
outperform the baseline Thompson Sampling on a variety of datasets. Moreover,
from the behavioral modeling perspective, our parametric framework can be
viewed as a first step towards a unifying computational model capturing reward
processing abnormalities across multiple mental conditions."
A New Probabilistic Algorithm for Approximate Model Counting,"Constrained counting is important in domains ranging from artificial
intelligence to software analysis. There are already a few approaches for
counting models over various types of constraints. Recently, hashing-based
approaches achieve both theoretical guarantees and scalability, but still rely
on solution enumeration. In this paper, a new probabilistic polynomial time
approximate model counter is proposed, which is also a hashing-based universal
framework, but with only satisfiability queries. A variant with a dynamic
stopping criterion is also presented. Empirical evaluation over benchmarks on
propositional logic formulas and SMT(BV) formulas shows that the approach is
promising."
AI-Powered Social Bots,"This paper gives an overview of impersonation bots that generate output in
one, or possibly, multiple modalities. We also discuss rapidly advancing areas
of machine learning and artificial intelligence that could lead to
frighteningly powerful new multi-modal social bots. Our main conclusion is that
most commonly known bots are one dimensional (i.e., chatterbot), and far from
deceiving serious interrogators. However, using recent advances in machine
learning, it is possible to unleash incredibly powerful, human-like armies of
social bots, in potentially well coordinated campaigns of deception and
influence."
Armstrong's Axioms and Navigation Strategies,"The paper investigates navigability with imperfect information. It shows that
the properties of navigability with perfect recall are exactly those captured
by Armstrong's axioms from the database theory. If the assumption of perfect
recall is omitted, then Armstrong's transitivity axiom is not valid, but it can
be replaced by two new weaker principles. The main technical results are
soundness and completeness theorems for the logical systems describing
properties of navigability with and without perfect recall."
Strategic Coalitions with Perfect Recall,"The paper proposes a bimodal logic that describes an interplay between
distributed knowledge modality and coalition know-how modality. Unlike other
similar systems, the one proposed here assumes perfect recall by all agents.
Perfect recall is captured in the system by a single axiom. The main technical
results are the soundness and the completeness theorems for the proposed
logical system."
"Proceedings Sixteenth Conference on Theoretical Aspects of Rationality
  and Knowledge","This volume consists of papers presented at the Sixteenth Conference on
Theoretical Aspects of Rationality and Knowledge (TARK) held at the University
of Liverpool, UK, from July 24 to 26, 2017.
  TARK conferences bring together researchers from a wide variety of fields,
including Computer Science (especially, Artificial Intelligence, Cryptography,
Distributed Computing), Economics (especially, Decision Theory, Game Theory,
Social Choice Theory), Linguistics, Philosophy (especially, Philosophical
Logic), and Cognitive Psychology, in order to further understand the issues
involving reasoning about rationality and knowledge."
Declarative Sequential Pattern Mining of Care Pathways,"Sequential pattern mining algorithms are widely used to explore care pathways
database, but they generate a deluge of patterns, mostly redundant or useless.
Clinicians need tools to express complex mining queries in order to generate
less but more significant patterns. These algorithms are not versatile enough
to answer complex clinician queries. This article proposes to apply a
declarative pattern mining approach based on Answer Set Programming paradigm.
It is exemplified by a pharmaco-epidemiological study investigating the
possible association between hospitalization for seizure and antiepileptic drug
switch from a french medico-administrative database."
"Exact Inference for Relational Graphical Models with Interpreted
  Functions: Lifted Probabilistic Inference Modulo Theories","Probabilistic Inference Modulo Theories (PIMT) is a recent framework that
expands exact inference on graphical models to use richer languages that
include arithmetic, equalities, and inequalities on both integers and real
numbers. In this paper, we expand PIMT to a lifted version that also processes
random functions and relations. This enhancement is achieved by adapting
Inversion, a method from Lifted First-Order Probabilistic Inference literature,
to also be modulo theories. This results in the first algorithm for exact
probabilistic inference that efficiently and simultaneously exploits random
relations and functions, arithmetic, equalities and inequalities."
"Commonsense Scene Semantics for Cognitive Robotics: Towards Grounding
  Embodied Visuo-Locomotive Interactions","We present a commonsense, qualitative model for the semantic grounding of
embodied visuo-spatial and locomotive interactions. The key contribution is an
integrative methodology combining low-level visual processing with high-level,
human-centred representations of space and motion rooted in artificial
intelligence. We demonstrate practical applicability with examples involving
object interactions, and indoor movement."
"An enhanced method to compute the similarity between concepts of
  ontology","With the use of ontologies in several domains such as semantic web,
information retrieval, artificial intelligence, the concept of similarity
measuring has become a very important domain of research. Therefore, in the
current paper, we propose our method of similarity measuring which uses the
Dijkstra algorithm to define and compute the shortest path. Then, we use this
one to compute the semantic distance between two concepts defined in the same
hierarchy of ontology. Afterward, we base on this result to compute the
semantic similarity. Finally, we present an experimental comparison between our
method and other methods of similarity measuring."
The mind as a computational system,"The present document is an excerpt of an essay that I wrote as part of my
application material to graduate school in Computer Science (with a focus on
Artificial Intelligence), in 1986. I was not invited by any of the schools that
received it, so I became a theoretical physicist instead. The essay's full
title was ""Some Topics in Philosophy and Computer Science"". I am making this
text (unchanged from 1985, preserving the typesetting as much as possible)
available now in memory of Jerry Fodor, whose writings had influenced me
significantly at the time (even though I did not always agree)."
A Slow Read attack Using Cloud,"Cloud computing relies on sharing computing resources rather than having
local servers or personal devices to handle applications. Nowadays, cloud
computing has become one of the fastest growing fields in information
technology. However, several new security issues of cloud computing have
emerged due to its service delivery models. In this paper, we discuss the case
of distributed denial-of-service (DDoS) attack using Cloud resources. First, we
show how such attack using a cloud platform could not be detected by previous
techniques. Then we present a tricky solution based on the cloud as well."
"AI Safety and Reproducibility: Establishing Robust Foundations for the
  Neuropsychology of Human Values","We propose the creation of a systematic effort to identify and replicate key
findings in neuropsychology and allied fields related to understanding human
values. Our aim is to ensure that research underpinning the value alignment
problem of artificial intelligence has been sufficiently validated to play a
role in the design of AI systems."
"Simulated Autonomous Driving on Realistic Road Networks using Deep
  Reinforcement Learning","Using Deep Reinforcement Learning (DRL) can be a promising approach to handle
various tasks in the field of (simulated) autonomous driving. However, recent
publications mainly consider learning in unusual driving environments. This
paper presents Driving School for Autonomous Agents (DSA^2), a software for
validating DRL algorithms in more usual driving environments based on
artificial and realistic road networks. We also present the results of applying
DSA^2 for handling the task of driving on a straight road while regulating the
velocity of one vehicle according to different speed limits."
Deep Learning: A Critical Appraisal,"Although deep learning has historical roots going back decades, neither the
term ""deep learning"" nor the approach was popular just over five years ago,
when the field was reignited by papers such as Krizhevsky, Sutskever and
Hinton's now classic (2012) deep network model of Imagenet. What has the field
discovered in the five subsequent years? Against a background of considerable
progress in areas such as speech recognition, image recognition, and game
playing, and considerable enthusiasm in the popular press, I present ten
concerns for deep learning, and suggest that deep learning must be supplemented
by other techniques if we are to reach artificial general intelligence."
Trading the Twitter Sentiment with Reinforcement Learning,"This paper is to explore the possibility to use alternative data and
artificial intelligence techniques to trade stocks. The efficacy of the daily
Twitter sentiment on predicting the stock return is examined using machine
learning methods. Reinforcement learning(Q-learning) is applied to generate the
optimal trading policy based on the sentiment signal. The predicting power of
the sentiment signal is more significant if the stock price is driven by the
expectation of the company growth and when the company has a major event that
draws the public attention. The optimal trading strategy based on reinforcement
learning outperforms the trading strategy based on the machine learning
prediction."
Quantified Degrees of Group Responsibility (Extended Abstract),"This paper builds on an existing notion of group responsibility and proposes
two ways to define the degree of group responsibility: structural and
functional degrees of responsibility. These notions measure the potential
responsibilities of (agent) groups for avoiding a state of affairs. According
to these notions, a degree of responsibility for a state of affairs can be
assigned to a group of agents if, and to the extent that, the group has the
potential to preclude the state of affairs."
Etymo: A New Discovery Engine for AI Research,"We present Etymo (https://etymo.io), a discovery engine to facilitate
artificial intelligence (AI) research and development. It aims to help readers
navigate a large number of AI-related papers published every week by using a
novel form of search that finds relevant papers and displays related papers in
a graphical interface. Etymo constructs and maintains an adaptive
similarity-based network of research papers as an all-purpose knowledge graph
for ranking, recommendation, and visualisation. The network is constantly
evolving and can learn from user feedback to adjust itself."
"Morphologic for knowledge dynamics: revision, fusion, abduction","Several tasks in artificial intelligence require to be able to find models
about knowledge dynamics. They include belief revision, fusion and belief
merging, and abduction. In this paper we exploit the algebraic framework of
mathematical morphology in the context of propositional logic, and define
operations such as dilation or erosion of a set of formulas. We derive concrete
operators, based on a semantic approach, that have an intuitive interpretation
and that are formally well behaved, to perform revision, fusion and abduction.
Computation and tractability are addressed, and simple examples illustrate the
typical results that can be obtained."
"On Looking for Local Expansion Invariants in Argumentation Semantics: a
  Preliminary Report","We study invariant local expansion operators for conflict-free and admissible
sets in Abstract Argumentation Frameworks (AFs). Such operators are directly
applied on AFs, and are invariant with respect to a chosen ""semantics"" (that is
w.r.t. each of the conflict free/admissible set of arguments). Accordingly, we
derive a definition of robustness for AFs in terms of the number of times such
operators can be applied without producing any change in the chosen semantics."
Bernoulli Embeddings for Graphs,"Just as semantic hashing can accelerate information retrieval, binary valued
embeddings can significantly reduce latency in the retrieval of graphical data.
We introduce a simple but effective model for learning such binary vectors for
nodes in a graph. By imagining the embeddings as independent coin flips of
varying bias, continuous optimization techniques can be applied to the
approximate expected loss. Embeddings optimized in this fashion consistently
outperform the quantization of both spectral graph embeddings and various
learned real-valued embeddings, on both ranking and pre-ranking tasks for a
variety of datasets."
Application of Grey Numbers to Assessment Processes,"The theory of grey systems plays an important role in science,engineering and
in the everyday life in general for handling approximate data. In the present
paper grey numbers are used as a tool for assessing with linguistic expressions
the mean performance of a group of objects participating in a certain activity.
Two applications to student and football player assessment are also presented
illustrating our results."
Visual Analytics for Explainable Deep Learning,"Recently, deep learning has been advancing the state of the art in artificial
intelligence to a new level, and humans rely on artificial intelligence
techniques more than ever. However, even with such unprecedented advancements,
the lack of explanation regarding the decisions made by deep learning models
and absence of control over their internal processes act as major drawbacks in
critical decision-making processes, such as precision medicine and law
enforcement. In response, efforts are being made to make deep learning
interpretable and controllable by humans. In this paper, we review visual
analytics, information visualization, and machine learning perspectives
relevant to this aim, and discuss potential challenges and future research
directions."
"Materials science and engineering: New vision in the era of artificial
  intelligence","Scientific discovery evolves from the experimental, through the theoretical
and computational, to the current data-intensive paradigm. Materials science is
no exception, especially for computational materials science. In recent years,
great achievements have been made in the field of materials science and
engineering (MSE). Here, we review the previous paradigms of materials science
and some classical MSE models. Then, our data-intensive MSE (DIMSE) model is
proposed to reshape future materials innovations. This work will help to
address the global challenge for materials discovery in the era of artificial
intelligence (AI), and essentially contribute to accelerating future materials
continuum."
"Artificial Intelligence-Based Techniques for Emerging Robotics
  Communication: A Survey and Future Perspectives","This paper reviews the current development of artificial intelligence (AI)
techniques for the application area of robot communication. The study of the
control and operation of multiple robots collaboratively toward a common goal
is fast growing. Communication among members of a robot team and even including
humans is becoming essential in many real-world applications. The survey
focuses on the AI techniques for robot communication to enhance the
communication capability of the multi-robot team, making more complex
activities, taking an appreciated decision, taking coordinated action, and
performing their tasks efficiently."
Learning Robust Search Strategies Using a Bandit-Based Approach,"Effective solving of constraint problems often requires choosing good or
specific search heuristics. However, choosing or designing a good search
heuristic is non-trivial and is often a manual process. In this paper, rather
than manually choosing/designing search heuristics, we propose the use of
bandit-based learning techniques to automatically select search heuristics. Our
approach is online where the solver learns and selects from a set of heuristics
during search. The goal is to obtain automatic search heuristics which give
robust performance. Preliminary experiments show that our adaptive technique is
more robust than the original search heuristics. It can also outperform the
original heuristics."
"Artificial Intelligence Paradigm for Customer Experience Management in
  Next-Generation Networks: Challenges and Perspectives","With advancements of next-generation programmable networks a traditional
rule-based decision-making may not be able to adapt effectively to changing
network and customer requirements and provide optimal customer experience.
Customer experience management (CEM) components and implementation challenges
with respect to operator, network, and business requirements must be understood
to meet required demands. This paper gives an overview of CEM components and
their design challenges. We elaborate on data analytics and artificial
intelligence driven CEM and their functional differences. This overview
provides a path toward autonomous CEM framework in next-generation networks and
sets the groundwork for future enhancements."
A Formulation of Recursive Self-Improvement and Its Possible Efficiency,"Recursive self-improving (RSI) systems have been dreamed of since the early
days of computer science and artificial intelligence. However, many existing
studies on RSI systems remain philosophical, and lacks clear formulation and
results. In this paper, we provide a formal definition for one class of RSI
systems, and then demonstrate the existence of computable and efficient RSI
systems on a restricted version. We use simulation to empirically show that we
achieve logarithmic runtime complexity with respect to the size of the search
space, and these results suggest it is possible to achieve an efficient
recursive self-improvement."
A Psychopathological Approach to Safety Engineering in AI and AGI,"The complexity of dynamics in AI techniques is already approaching that of
complex adaptive systems, thus curtailing the feasibility of formal
controllability and reachability analysis in the context of AI safety. It
follows that the envisioned instances of Artificial General Intelligence (AGI)
will also suffer from challenges of complexity. To tackle such issues, we
propose the modeling of deleterious behaviors in AI and AGI as psychological
disorders, thereby enabling the employment of psychopathological approaches to
analysis and control of misbehaviors. Accordingly, we present a discussion on
the feasibility of the psychopathological approaches to AI safety, and propose
general directions for research on modeling, diagnosis, and treatment of
psychological disorders in AGI."
Agent-Mediated Social Choice,"Direct democracy is often proposed as a possible solution to the 21st-century
problems of democracy. However, this suggestion clashes with the size and
complexity of 21st-century societies, entailing an excessive cognitive burden
on voters, who would have to submit informed opinions on an excessive number of
issues. In this paper I argue for the development of voting avatars, autonomous
agents debating and voting on behalf of each citizen. Theoretical research from
artificial intelligence, and in particular multiagent systems and computational
social choice, proposes 21st-century techniques for this purpose, from the
compact representation of a voter's preferences and values, to the development
of voting procedures for autonomous agents use only."
Analysis and Optimization of Deep Counterfactual Value Networks,"Recently a strong poker-playing algorithm called DeepStack was published,
which is able to find an approximate Nash equilibrium during gameplay by using
heuristic values of future states predicted by deep neural networks. This paper
analyzes new ways of encoding the inputs and outputs of DeepStack's deep
counterfactual value networks based on traditional abstraction techniques, as
well as an unabstracted encoding, which was able to increase the network's
accuracy."
On Ternary Coding and Three-Valued Logic,"Mathematically, ternary coding is more efficient than binary coding. It is
little used in computation because technology for binary processing is already
established and the implementation of ternary coding is more complicated, but
remains relevant in algorithms that use decision trees and in communications.
In this paper we present a new comparison of binary and ternary coding and
their relative efficiencies are computed both for number representation and
decision trees. The implications of our inability to use optimal representation
through mathematics or logic are examined. Apart from considerations of
representation efficiency, ternary coding appears preferable to binary coding
in classification of many real-world problems of artificial intelligence (AI)
and medicine. We examine the problem of identifying appropriate three classes
for domain-specific applications."
Automated Game Design via Conceptual Expansion,"Automated game design has remained a key challenge within the field of Game
AI. In this paper, we introduce a method for recombining existing games to
create new games through a process called conceptual expansion. Prior automated
game design approaches have relied on hand-authored or crowd-sourced knowledge,
which limits the scope and applications of such systems. Our approach instead
relies on machine learning to learn approximate representations of games. Our
approach recombines knowledge from these learned representations to create new
games via conceptual expansion. We evaluate this approach by demonstrating the
ability for the system to recreate existing games. To the best of our
knowledge, this represents the first machine learning-based automated game
design system."
Blameworthiness in Strategic Games,"There are multiple notions of coalitional responsibility. The focus of this
paper is on the blameworthiness defined through the principle of alternative
possibilities: a coalition is blamable for a statement if the statement is
true, but the coalition had a strategy to prevent it. The main technical result
is a sound and complete bimodal logical system that describes properties of
blameworthiness in one-shot games."
Federated AI for building AI Solutions across Multiple Agencies,"The different sets of regulations existing for differ-ent agencies within the
government make the task of creating AI enabled solutions in government
dif-ficult. Regulatory restrictions inhibit sharing of da-ta across different
agencies, which could be a significant impediment to training AI models. We
discuss the challenges that exist in environments where data cannot be freely
shared and assess tech-nologies which can be used to work around these
challenges. We present results on building AI models using the concept of
federated AI, which al-lows creation of models without moving the training data
around."
"Intrinsic Geometric Vulnerability of High-Dimensional Artificial
  Intelligence","The success of modern Artificial Intelligence (AI) technologies depends
critically on the ability to learn non-linear functional dependencies from
large, high dimensional data sets. Despite recent high-profile successes,
empirical evidence indicates that the high predictive performance is often
paired with low robustness, making AI systems potentially vulnerable to
adversarial attacks. In this report, we provide a simple intuitive argument
suggesting that high performance and vulnerability are intrinsically coupled,
and largely dependent on the geometry of typical, high-dimensional data sets.
Our work highlights a major potential pitfall of modern AI systems, and
suggests practical research directions to ameliorate the problem."
A Very Brief and Critical Discussion on AutoML,"This contribution presents a very brief and critical discussion on automated
machine learning (AutoML), which is categorized here into two classes, referred
to as narrow AutoML and generalized AutoML, respectively. The conclusions
yielded from this discussion can be summarized as follows: (1) most existent
research on AutoML belongs to the class of narrow AutoML; (2) advances in
narrow AutoML are mainly motivated by commercial needs, while any possible
benefit obtained is definitely at a cost of increase in computing burdens;
(3)the concept of generalized AutoML has a strong tie in spirit with artificial
general intelligence (AGI), also called ""strong AI"", for which obstacles abound
for obtaining pivotal progresses."
"Quantum Reasoning using Lie Algebra for Everyday Life (and AI
  perhaps...)","We investigate the applicability of the formalism of quantum mechanics to
everyday life. It seems to be directly relevant for situations in which the
very act of coming to a conclusion or decision on one issue affects one's
confidence about conclusions or decisions on another issue. Lie algebra theory
is argued to be a very useful tool in guiding the construction of quantum
descriptions of such situations. Tests, extensions and speculative applications
and implications, including for the encoding of thoughts in neural networks,
are discussed. It is suggested that the recognition and incorporation of such
mathematical structure into machine learning and artificial intelligence might
lead to significant efficiency and generality gains in addition to ensuring
probabilistic reasoning at a fundamental level."
An Introduction to Fuzzy & Annotated Semantic Web Languages,"We present the state of the art in representing and reasoning with fuzzy
knowledge in Semantic Web Languages such as triple languages RDF/RDFS,
conceptual languages of the OWL 2 family and rule languages. We further show
how one may generalise them to so-called annotation domains, that cover also
e.g. temporal and provenance extensions."
The Barbados 2018 List of Open Issues in Continual Learning,"We want to make progress toward artificial general intelligence, namely
general-purpose agents that autonomously learn how to competently act in
complex environments. The purpose of this report is to sketch a research
outline, share some of the most important open issues we are facing, and
stimulate further discussion in the community. The content is based on some of
our discussions during a week-long workshop held in Barbados in February 2018."
Quantifying Uncertainties in Natural Language Processing Tasks,"Reliable uncertainty quantification is a first step towards building
explainable, transparent, and accountable artificial intelligent systems.
Recent progress in Bayesian deep learning has made such quantification
realizable. In this paper, we propose novel methods to study the benefits of
characterizing model and data uncertainties for natural language processing
(NLP) tasks. With empirical experiments on sentiment analysis, named entity
recognition, and language modeling using convolutional and recurrent neural
network models, we show that explicitly modeling uncertainties is not only
necessary to measure output confidence levels, but also useful at enhancing
model performances in various NLP tasks."
AI Fairness for People with Disabilities: Point of View,"We consider how fair treatment in society for people with disabilities might
be impacted by the rise in the use of artificial intelligence, and especially
machine learning methods. We argue that fairness for people with disabilities
is different to fairness for other protected attributes such as age, gender or
race. One major difference is the extreme diversity of ways disabilities
manifest, and people adapt. Secondly, disability information is highly
sensitive and not always shared, precisely because of the potential for
discrimination. Given these differences, we explore definitions of fairness and
how well they work in the disability space. Finally, we suggest ways of
approaching fairness for people with disabilities in AI applications."
Environments for Lifelong Reinforcement Learning,"To achieve general artificial intelligence, reinforcement learning (RL)
agents should learn not only to optimize returns for one specific task but also
to constantly build more complex skills and scaffold their knowledge about the
world, without forgetting what has already been learned. In this paper, we
discuss the desired characteristics of environments that can support the
training and evaluation of lifelong reinforcement learning agents, review
existing environments from this perspective, and propose recommendations for
devising suitable environments in the future."
The limit of artificial intelligence: Can machines be rational?,"This paper studies the question on whether machines can be rational. It
observes the existing reasons why humans are not rational which is due to
imperfect and limited information, limited and inconsistent processing power
through the brain and the inability to optimize decisions and achieve maximum
utility. It studies whether these limitations of humans are transferred to the
limitations of machines. The conclusion reached is that even though machines
are not rational advances in technological developments make these machines
more rational. It also concludes that machines can be more rational than
humans."
Creative AI Through Evolutionary Computation,"The main power of artificial intelligence is not in modeling what we already
know, but in creating solutions that are new. Such solutions exist in extremely
large, high-dimensional, and complex search spaces. Population-based search
techniques, i.e. variants of evolutionary computation, are well suited to
finding them. These techniques are also well positioned to take advantage of
large-scale parallel computing resources, making creative AI through
evolutionary computation the likely ""next deep learning""."
Adaptive Artificial Intelligent Q&A Platform,"The paper presents an approach to build a question and answer system that is
capable of processing the information in a large dataset and allows the user to
gain knowledge from this dataset by asking questions in natural language form.
Key content of this research covers four dimensions which are; Corpus
Preprocessing, Question Preprocessing, Deep Neural Network for Answer
Extraction and Answer Generation. The system is capable of understanding the
question, responds to the user's query in natural language form as well. The
goal is to make the user feel as if they were interacting with a person than a
machine."
"Appendix for: Cut-free Calculi and Relational Semantics for Temporal
  STIT logics","This paper is an appendix to the paper ""Cut-free Calculi and Relational
Semantics for Temporal STIT logics"" by Berkel and Lyon, 2019. It provides the
completeness proof for the basic STIT logic Ldm (relative to irreflexive,
temporal Kripke STIT frames) as well as gives the derivation of the
independence of agents axiom for the logic Xstit."
Was ist eine Professur fuer Kuenstliche Intelligenz?,"The Federal Government of Germany aims to boost the research in the field of
Artificial Intelligence (AI). For instance, 100 new professorships are said to
be established. However, the white paper of the government does not answer what
an AI professorship is at all. In order to give colleagues, politicians, and
citizens an idea, we present a view that is often followed when appointing
professors for AI at German and international universities. We hope that it
will help to establish a guideline with internationally accepted measures and
thus make the public debate more informed."
"Data Science and Digital Systems: The 3Ds of Machine Learning Systems
  Design","Machine learning solutions, in particular those based on deep learning
methods, form an underpinning of the current revolution in ""artificial
intelligence"" that has dominated popular press headlines and is having a
significant influence on the wider tech agenda. Here we give an overview of the
3Ds of ML systems design: Data, Design and Deployment. By considering the 3Ds
we can move towards \emph{data first} design."
"Is coding a relevant metaphor for building AI? A commentary on ""Is
  coding a relevant metaphor for the brain?"", by Romain Brette","Brette contends that the neural coding metaphor is an invalid basis for
theories of what the brain does. Here, we argue that it is an insufficient
guide for building an artificial intelligence that learns to accomplish short-
and long-term goals in a complex, changing environment."
The Game of Tetris in Machine Learning,"The game of Tetris is an important benchmark for research in artificial
intelligence and machine learning. This paper provides a historical account of
the algorithmic developments in Tetris and discusses open challenges.
Handcrafted controllers, genetic algorithms, and reinforcement learning have
all contributed to good solutions. However, existing solutions fall far short
of what can be achieved by expert players playing without time pressure.
Further study of the game has the potential to contribute to important areas of
research, including feature discovery, autonomous learning of action
hierarchies, and sample-efficient reinforcement learning."
Randomized Adversarial Imitation Learning for Autonomous Driving,"With the evolution of various advanced driver assistance system (ADAS)
platforms, the design of autonomous driving system is becoming more complex and
safety-critical. The autonomous driving system simultaneously activates
multiple ADAS functions; and thus it is essential to coordinate various ADAS
functions. This paper proposes a randomized adversarial imitation learning
(RAIL) method that imitates the coordination of autonomous vehicle equipped
with advanced sensors. The RAIL policies are trained through derivative-free
optimization for the decision maker that coordinates the proper ADAS functions,
e.g., smart cruise control and lane keeping system. Especially, the proposed
method is also able to deal with the LIDAR data and makes decisions in complex
multi-lane highways and multi-agent environments."
"Better Future through AI: Avoiding Pitfalls and Guiding AI Towards its
  Full Potential","Artificial Intelligence (AI) technology is rapidly changing many areas of
society. While there is tremendous potential in this transition, there are
several pitfalls as well. Using the history of computing and the world-wide web
as a guide, in this article we identify those pitfalls and actions that lead AI
development to its full potential. If done right, AI will be instrumental in
achieving the goals we set for economy, society, and the world in general."
"Using Natural Language Processing to Develop an Automated Orthodontic
  Diagnostic System","We work on the task of automatically designing a treatment plan from the
findings included in the medical certificate written by the dentist. To develop
an artificial intelligence system that deals with free-form certificates
written by dentists, we annotate the findings and utilized the natural language
processing approach. As a result of the experiment using 990 certificates,
0.585 F1-score was achieved for the task of extracting orthodontic problems
from findings, and 0.584 correlation coefficient with the human ranking was
achieved for the treatment prioritization task."
The Riddle of Togelby,"At the 2017 Artificial and Computational Intelligence in Games meeting at
Dagstuhl, Julian Togelius asked how to make spaces where every way of filling
in the details yielded a good game. This study examines the possibility of
enriching search spaces so that they contain very high rates of interesting
objects, specifically game elements. While we do not answer the full challenge
of finding good games throughout the space, this study highlights a number of
potential avenues. These include naturally rich spaces, a simple technique for
modifying a representation to search only rich parts of a larger search space,
and representations that are highly expressive and so exhibit highly restricted
and consequently enriched search spaces."
Introducing the Hearthstone-AI Competition,"The Hearthstone AI framework and competition motivates the development of
artificial intelligence agents that can play collectible card games. A special
feature of those games is the high variety of cards, which can be chosen by the
players to create their own decks. In contrast to simpler card games, the value
of many cards is determined by their possible synergies. The vast amount of
possible decks, the randomness of the game, as well as the restricted
information during the player's turn offer quite a hard challenge for the
development of game-playing agents. This short paper introduces the competition
framework and goes into more detail on the problems and challenges that need to
be faced during the development process."
Designing Game of Theorems,"""Theorem proving is similar to the game of Go. So, we can probably improve
our provers using deep learning, like DeepMind built the super-human computer
Go program, AlphaGo."" Such optimism has been observed among participants of
AITP2017. But is theorem proving really similar to Go? In this paper, we first
identify the similarities and differences between them and then propose a
system in which various provers keep competing against each other and changing
themselves until they prove conjectures provided by users."
"Evolutionary Computation and AI Safety: Research Problems Impeding
  Routine and Safe Real-world Application of Evolution","Recent developments in artificial intelligence and machine learning have
spurred interest in the growing field of AI safety, which studies how to
prevent human-harming accidents when deploying AI systems. This paper thus
explores the intersection of AI safety with evolutionary computation, to show
how safety issues arise in evolutionary computation and how understanding from
evolutionary computational and biological evolution can inform the broader
study of AI safety."
"Calibrating Wayfinding Decisions in Pedestrian Simulation Models: The
  Entropy Map","This paper presents entropy maps, an approach to describing and visualising
uncertainty among alternative potential movement intentions in pedestrian
simulation models. In particular, entropy maps show the instantaneous level of
randomness in decisions of a pedestrian agent situated in a specific point of
the simulated environment with an heatmap approach. Experimental results
highlighting the relevance of this tool supporting modelers are provided and
discussed."
KG-BERT: BERT for Knowledge Graph Completion,"Knowledge graphs are important resources for many artificial intelligence
tasks but often suffer from incompleteness. In this work, we propose to use
pre-trained language models for knowledge graph completion. We treat triples in
knowledge graphs as textual sequences and propose a novel framework named
Knowledge Graph Bidirectional Encoder Representations from Transformer
(KG-BERT) to model these triples. Our method takes entity and relation
descriptions of a triple as input and computes scoring function of the triple
with the KG-BERT language model. Experimental results on multiple benchmark
knowledge graphs show that our method can achieve state-of-the-art performance
in triple classification, link prediction and relation prediction tasks."
A Temporal Module for Logical Frameworks,"In artificial intelligence, multi agent systems constitute an interesting
typology of society modeling, and have in this regard vast fields of
application, which extend to the human sciences. Logic is often used to model
such kind of systems as it is easier to verify than other approaches, and
provides explainability and potential validation. In this paper we define a
time module suitable to add time to many logic representations of agents."
Memory Management in Resource-Bounded Agents,"In artificial intelligence, multi agent systems constitute an interesting
typology of society modeling, and have in this regard vast fields of
application, which extend to the human sciences. Logic is often used to model
such kind of systems as it is easier to verify the explainability and
validation, so for this reason we have tried to manage agents' memory extending
a previous work by inserting the concept of time."
On Controlled DeEntanglement for Natural Language Processing,"Latest addition to the toolbox of human species is Artificial
Intelligence(AI). Thus far, AI has made significant progress in low stake low
risk scenarios such as playing Go and we are currently in a transition toward
medium stake scenarios such as Visual Dialog. In my thesis, I argue that we
need to incorporate controlled de-entanglement as first class object to succeed
in this transition. I present mathematical analysis from information theory to
show that employing stochasticity leads to controlled de-entanglement of
relevant factors of variation at various levels. Based on this, I highlight
results from initial experiments that depict efficacy of the proposed
framework. I conclude this writeup by a roadmap of experiments that show the
applicability of this framework to scalability, flexibility and
interpretibility."
"TraffickCam: Explainable Image Matching For Sex Trafficking
  Investigations","Investigations of sex trafficking sometimes have access to photographs of
victims in hotel rooms. These images directly link victims to places, which can
help verify where victims have been trafficked or where traffickers might
operate in the future. Current machine learning approaches give promising
results in image search to find the matching hotel. This paper explores
approaches to make this end-to-end system better support government and law
enforcement requirements, including improved performance, visualization
approaches that explain what parts of the image led to a match, and
infrastructure to support exporting the results of a query."
Blockchain 3.0 Smart Contracts in E-Government 3.0 Applications,"The adoption of Information Communication Technologies (ICT) and Web 3.0
contributes to the e-government sector by transforming how public
administrations provide advanced and innovative services to interact with
citizens. Blockchain (BC) and Artificial Intelligence (AI) disruptive
technologies will reshape how we live, work, and interact with government
sectors and industries. This paper presents how Blockchain 3.0 and Artificial
Intelligence enhance robust, secure, scalable, and authenticity provenance
solutions. Two validation scenarios are analyzed to present how blockchain
smart contracts and AI agents support energy and health-oriented e-government
services."
Blameworthiness in Security Games,"Security games are an example of a successful real-world application of game
theory. The paper defines blameworthiness of the defender and the attacker in
security games using the principle of alternative possibilities and provides a
sound and complete logical system for reasoning about blameworthiness in such
games. Two of the axioms of this system capture the asymmetry of information in
security games."
"Portable system for the prediction of anemia based on the ocular
  conjunctiva using Artificial Intelligence","Anemia is a major health burden worldwide. Examining the hemoglobin level of
blood is an important way to achieve the diagnosis of anemia, but it requires
blood drawing and a blood test. In this work we propose a non-invasive, fast,
and cost-effective screening test for iron-deficiency anemia in Peruvian young
children. Our initial results show promising evidence for detecting
conjunctival pallor anemia and Artificial Intelligence techniques with photos
taken with a popular smartphone."
Engaging in Dialogue about an Agent's Norms and Behaviors,"We present a set of capabilities allowing an agent planning with moral and
social norms represented in temporal logic to respond to queries about its
norms and behaviors in natural language, and for the human user to add and
remove norms directly in natural language. The user may also pose hypothetical
modifications to the agent's norms and inquire about their effects."
What Do You Mean `Why?': Resolving Sluices in Conversations,"In conversation, we often ask one-word questions such as `Why?' or `Who?'.
Such questions are typically easy for humans to answer, but can be hard for
computers, because their resolution requires retrieving both the right semantic
frames and the right arguments from context. This paper introduces the novel
ellipsis resolution task of resolving such one-word questions, referred to as
sluices in linguistics. We present a crowd-sourced dataset containing
annotations of sluices from over 4,000 dialogues collected from conversational
QA datasets, as well as a series of strong baseline architectures."
Online Fair Division: A Survey,"We survey a burgeoning and promising new research area that considers the
online nature of many practical fair division problems. We identify wide
variety of such online fair division problems, as well as discuss new
mechanisms and normative properties that apply to this online setting. The
online nature of such fair division problems provides both opportunities and
challenges such as the possibility to develop new online mechanisms as well as
the difficulty of dealing with an uncertain future."
Causality for Machine Learning,"Graphical causal inference as pioneered by Judea Pearl arose from research on
artificial intelligence (AI), and for a long time had little connection to the
field of machine learning.
  This article discusses where links have been and should be established,
introducing key concepts along the way. It argues that the hard open problems
of machine learning and AI are intrinsically related to causality, and explains
how the field is beginning to understand them."
Fairness Assessment for Artificial Intelligence in Financial Industry,"Artificial Intelligence (AI) is an important driving force for the
development and transformation of the financial industry. However, with the
fast-evolving AI technology and application, unintentional bias, insufficient
model validation, immature contingency plan and other underestimated threats
may expose the company to operational and reputational risks. In this paper, we
focus on fairness evaluation, one of the key components of AI Governance,
through a quantitative lens. Statistical methods are reviewed for imbalanced
data treatment and bias mitigation. These methods and fairness evaluation
metrics are then applied to a credit card default payment example."
Decomposable Probability-of-Success Metrics in Algorithmic Search,"Previous studies have used a specific success metric within an algorithmic
search framework to prove machine learning impossibility results. However, this
specific success metric prevents us from applying these results on other forms
of machine learning, e.g. transfer learning. We define decomposable metrics as
a category of success metrics for search problems which can be expressed as a
linear operation on a probability distribution to solve this issue. Using an
arbitrary decomposable metric to measure the success of a search, we
demonstrate theorems which bound success in various ways, generalizing several
existing results in the literature."
Exploring Unknown Universes in Probabilistic Relational Models,"Large probabilistic models are often shaped by a pool of known individuals (a
universe) and relations between them. Lifted inference algorithms handle sets
of known individuals for tractable inference. Universes may not always be
known, though, or may only described by assumptions such as ""small universes
are more likely"". Without a universe, inference is no longer possible for
lifted algorithms, losing their advantage of tractable inference. The aim of
this paper is to define a semantics for models with unknown universes decoupled
from a specific constraint language to enable lifted and thereby, tractable
inference."
Knowledge Representations in Technical Systems -- A Taxonomy,"The recent usage of technical systems in human-centric environments leads to
the question, how to teach technical systems, e.g., robots, to understand,
learn, and perform tasks desired by the human. Therefore, an accurate
representation of knowledge is essential for the system to work as expected.
This article mainly gives insight into different knowledge representation
techniques and their categorization into various problem domains in artificial
intelligence. Additionally, applications of presented knowledge representations
are introduced in everyday robotics tasks. By means of the provided taxonomy,
the search for a proper knowledge representation technique regarding a specific
problem should be facilitated."
SMT + ILP,"Inductive logic programming (ILP) has been a deeply influential paradigm in
AI, enjoying decades of research on its theory and implementations. As a
natural descendent of the fields of logic programming and machine learning, it
admits the incorporation of background knowledge, which can be very useful in
domains where prior knowledge from experts is available and can lead to a more
data-efficient learning regime. Be that as it may, the limitation to Horn
clauses composed over Boolean variables is a very serious one. Many phenomena
occurring in the real-world are best characterized using continuous entities,
and more generally, mixtures of discrete and continuous entities. In this
position paper, we motivate a reconsideration of inductive declarative
programming by leveraging satisfiability modulo theory technology."
"Four Principles of Explainable AI as Applied to Biometrics and Facial
  Forensic Algorithms","Traditionally, researchers in automatic face recognition and biometric
technologies have focused on developing accurate algorithms. With this
technology being integrated into operational systems, engineers and scientists
are being asked, do these systems meet societal norms? The origin of this line
of inquiry is `trust' of artificial intelligence (AI) systems. In this paper,
we concentrate on adapting explainable AI to face recognition and biometrics,
and we present four principles of explainable AI to face recognition and
biometrics. The principles are illustrated by $\it{four}$ case studies, which
show the challenges and issues in developing algorithms that can produce
explanations."
Covering the News with (AI) Style,"We introduce a multi-modal discriminative and generative frame-work capable
of assisting humans in producing visual content re-lated to a given theme,
starting from a collection of documents(textual, visual, or both). This
framework can be used by edit or to generate images for articles, as well as
books or music album covers. Motivated by a request from the The New York Times
(NYT) seeking help to use AI to create art for their special section on
Artificial Intelligence, we demonstrated the application of our system in
producing such image."
On The Reasons Behind Decisions,"Recent work has shown that some common machine learning classifiers can be
compiled into Boolean circuits that have the same input-output behavior. We
present a theory for unveiling the reasons behind the decisions made by Boolean
classifiers and study some of its theoretical and practical implications. We
define notions such as sufficient, necessary and complete reasons behind
decisions, in addition to classifier and decision bias. We show how these
notions can be used to evaluate counterfactual statements such as ""a decision
will stick even if ... because ... ."" We present efficient algorithms for
computing these notions, which are based on new advances on tractable Boolean
circuits, and illustrate them using a case study."
"A machine-learning software-systems approach to capture social,
  regulatory, governance, and climate problems","This paper will discuss the role of an artificially-intelligent computer
system as critique-based, implicit-organizational, and an inherently necessary
device, deployed in synchrony with parallel governmental policy, as a genuine
means of capturing nation-population complexity in quantitative form, public
contentment in societal-cooperative economic groups, regulatory proposition,
and governance-effectiveness domains. It will discuss a solution involving a
well-known algorithm and proffer an improved mechanism for
knowledge-representation, thereby increasing range of utility, scope of
influence (in terms of differentiating class sectors) and operational
efficiency. It will finish with a discussion of these and other historical
implications."
"Comments on Sejnowski's ""The unreasonable effectiveness of deep learning
  in artificial intelligence"" [arXiv:2002.04806]","Terry Sejnowski's 2020 paper [arXiv:2002.04806] is entitled ""The unreasonable
effectiveness of deep learning in artificial intelligence"". However, the paper
doesn't attempt to answer the implied question of why Deep Convolutional Neural
Networks (DCNNs) can approximate so many of the mappings that they have been
trained to model. While there are detailed mathematical analyses, this short
paper attempts to look at the issue differently, considering the way that these
networks are used, the subset of these functions that can be achieved by
training (starting from some location in the original function space), as well
as the functions to which these networks will actually be applied."
A Pebble in the AI Race,"Bhutan is sometimes described as \a pebble between two boulders"", a small
country caught between the two most populous nations on earth: India and China.
This pebble is, however, about to be caught up in a vortex: the transformation
of our economic, political and social orders by new technologies like
Artificial Intelligence. What can a small nation like Bhutan hope to do in the
face of such change? What should the nation do, not just to weather this storm,
but to become a better place in which to live?"
Combating The Machine Ethics Crisis: An Educational Approach,"In recent years, the availability of massive data sets and improved computing
power have driven the advent of cutting-edge machine learning algorithms.
However, this trend has triggered growing concerns associated with its ethical
issues. In response to such a phenomenon, this study proposes a feasible
solution that combines ethics and computer science materials in artificial
intelligent classrooms. In addition, the paper presents several arguments and
evidence in favor of the necessity and effectiveness of this integrated
approach."
Responsible AI and Its Stakeholders,"Responsible Artificial Intelligence (AI) proposes a framework that holds all
stakeholders involved in the development of AI to be responsible for their
systems. It, however, fails to accommodate the possibility of holding AI
responsible per se, which could close some legal and moral gaps concerning the
deployment of autonomous and self-learning systems. We discuss three notions of
responsibility (i.e., blameworthiness, accountability, and liability) for all
stakeholders, including AI, and suggest the roles of jurisdiction and the
general public in this matter."
Hide-and-Seek: A Template for Explainable AI,"Lack of transparency has been the Achilles heal of Neural Networks and their
wider adoption in industry. Despite significant interest this shortcoming has
not been adequately addressed. This study proposes a novel framework called
Hide-and-Seek (HnS) for training Interpretable Neural Networks and establishes
a theoretical foundation for exploring and comparing similar ideas. Extensive
experimentation indicates that a high degree of interpretability can be imputed
into Neural Networks, without sacrificing their predictive power."
Towards the Role of Theory of Mind in Explanation,"Theory of Mind is commonly defined as the ability to attribute mental states
(e.g., beliefs, goals) to oneself, and to others. A large body of previous work
- from the social sciences to artificial intelligence - has observed that
Theory of Mind capabilities are central to providing an explanation to another
agent or when explaining that agent's behaviour. In this paper, we build and
expand upon previous work by providing an account of explanation in terms of
the beliefs of agents and the mechanism by which agents revise their beliefs
given possible explanations. We further identify a set of desiderata for
explanations that utilize Theory of Mind. These desiderata inform our
belief-based account of explanation."
"Mobile Edge Computing and Artificial Intelligence: A Mutually-Beneficial
  Relationship","This article provides an overview of mobile edge computing (MEC) and
artificial intelligence (AI) and discusses the mutually-beneficial relationship
between them. AI provides revolutionary solutions in nearly every important
aspect of the MEC offloading process, such as resource management and
scheduling. On the other hand, MEC servers are utilized to avail a distributed
and parallelized learning framework, namely mobile edge learning."
Machine Learning as a Catalyst for Value-Based Health Care,"In this manuscript, we present an argument that machine learning, a subfield
of artificial intelligence, can drive improvement in value-based health care
through reducing error in clinical decision making. Much of what has been
previously published on machine learning in medicine represent single-use or
proof-of-concept cases, as well as broad reviews of the advantages and
limitations of machine learning. It is timely to look at the broader strategy
for artificial intelligence implementation in medicine and emphasize how
machine learning can positively influence value-based care."
Towards United Reasoning for Automatic Induction in Isabelle/HOL,"Inductive theorem proving is an important long-standing challenge in computer
science. In this extended abstract, we first summarize the recent developments
of proof by induction for Isabelle/HOL. Then, we propose united reasoning, a
novel approach to further automating inductive theorem proving. Upon success,
united reasoning takes the best of three schools of reasoning: deductive
reasoning, inductive reasoning, and inductive reasoning, to prove difficult
inductive problems automatically."
"Automatic Dialogic Instruction Detection for K-12 Online One-on-one
  Classes","Online one-on-one class is created for highly interactive and immersive
learning experience. It demands a large number of qualified online instructors.
In this work, we develop six dialogic instructions and help teachers achieve
the benefits of one-on-one learning paradigm. Moreover, we utilize neural
language models, i.e., long short-term memory (LSTM), to detect above six
instructions automatically. Experiments demonstrate that the LSTM approach
achieves AUC scores from 0.840 to 0.979 among all six types of instructions on
our real-world educational dataset."
DeepFair: Deep Learning for Improving Fairness in Recommender Systems,"The lack of bias management in Recommender Systems leads to minority groups
receiving unfair recommendations. Moreover, the trade-off between equity and
precision makes it difficult to obtain recommendations that meet both criteria.
Here we propose a Deep Learning based Collaborative Filtering algorithm that
provides recommendations with an optimum balance between fairness and accuracy
without knowing demographic information about the users. Experimental results
show that it is possible to make fair recommendations without losing a
significant proportion of accuracy."
Delta Schema Network in Model-based Reinforcement Learning,"This work is devoted to unresolved problems of Artificial General
Intelligence - the inefficiency of transfer learning. One of the mechanisms
that are used to solve this problem in the area of reinforcement learning is a
model-based approach. In the paper we are expanding the schema networks method
which allows to extract the logical relationships between objects and actions
from the environment data. We present algorithms for training a Delta Schema
Network (DSN), predicting future states of the environment and planning actions
that will lead to positive reward. DSN shows strong performance of transfer
learning on the classic Atari game environment."
"High-speed Millimeter-wave 5G/6G Image Transmission via Artificial
  Intelligence","Artificial Intelligence (AI) has been used to jointly optimize a mmWave
Compressed Sensing (CS) for high-speed 5G/6G image transmission. Specifically,
we have developed a Dictionary Learning Compressed Sensing neural Network
(DL-CSNet) to realize three key functionalities: 1) to learn the dictionary
basis of the images for transmission; 2) to optimize the Hadamard measurement
matrix; and 3) to reconstruct the lossless images with the learned dictionary
basis. A 94-GHz prototype has been built and up to one order of image
transmission speed increase has been realized for letters ``A"" to ``Z""."
Creative AI Through Evolutionary Computation: Principles and Examples,"The main power of artificial intelligence is not in modeling what we already
know, but in creating solutions that are new. Such solutions exist in extremely
large, high-dimensional, and complex search spaces. Population-based search
techniques, i.e. variants of evolutionary computation, are well suited to
finding them. These techniques make it possible to find creative solutions to
practical problems in the real world, making creative AI through evolutionary
computation the likely ""next deep learning."""
"An Ontological AI-and-Law Framework for the Autonomous Levels of AI
  Legal Reasoning","A framework is proposed that seeks to identify and establish a set of robust
autonomous levels articulating the realm of Artificial Intelligence and Legal
Reasoning (AILR). Doing so provides a sound and parsimonious basis for being
able to assess progress in the application of AI to the law, and can be
utilized by scholars in academic pursuits of AI legal reasoning, along with
being used by law practitioners and legal professionals in gauging how advances
in AI are aiding the practice of law and the realization of aspirational versus
achieved results. A set of seven levels of autonomy for AI and Legal Reasoning
are meticulously proffered and mindfully discussed."
Physically Unclonable Functions and AI: Two Decades of Marriage,"The current chapter aims at establishing a relationship between artificial
intelligence (AI) and hardware security. Such a connection between AI and
software security has been confirmed and well-reviewed in the relevant
literature. The main focus here is to explore the methods borrowed from AI to
assess the security of a hardware primitive, namely physically unclonable
functions (PUFs), which has found applications in cryptographic protocols,
e.g., authentication and key generation. Metrics and procedures devised for
this are further discussed. Moreover, By reviewing PUFs designed by applying AI
techniques, we give insight into future research directions in this area."
"The AIQ Meta-Testbed: Pragmatically Bridging Academic AI Testing and
  Industrial Q Needs","AI solutions seem to appear in any and all application domains. As AI becomes
more pervasive, the importance of quality assurance increases. Unfortunately,
there is no consensus on what artificial intelligence means and interpretations
range from simple statistical analysis to sentient humanoid robots. On top of
that, quality is a notoriously hard concept to pinpoint. What does this mean
for AI quality? In this paper, we share our working definition and a pragmatic
approach to address the corresponding quality assurance with a focus on
testing. Finally, we present our ongoing work on establishing the AIQ
Meta-Testbed."
Towards a Measure of Individual Fairness for Deep Learning,"Deep learning has produced big advances in artificial intelligence, but
trained neural networks often reflect and amplify bias in their training data,
and thus produce unfair predictions. We propose a novel measure of individual
fairness, called prediction sensitivity, that approximates the extent to which
a particular prediction is dependent on a protected attribute. We show how to
compute prediction sensitivity using standard automatic differentiation
capabilities present in modern deep learning frameworks, and present
preliminary empirical results suggesting that prediction sensitivity may be
effective for measuring bias in individual predictions."
"Average-reward model-free reinforcement learning: a systematic review
  and literature mapping","Reinforcement learning is important part of artificial intelligence. In this
paper, we review model-free reinforcement learning that utilizes the average
reward optimality criterion in the infinite horizon setting. Motivated by the
solo survey by Mahadevan (1996a), we provide an updated review of work in this
area and extend it to cover policy-iteration and function approximation methods
(in addition to the value-iteration and tabular counterparts). We present a
comprehensive literature mapping. We also identify and discuss opportunities
for future work."
Multi-Radar Tracking Optimization for Collaborative Combat,"Smart Grids of collaborative netted radars accelerate kill chains through
more efficient cross-cueing over centralized command and control. In this
paper, we propose two novel reward-based learning approaches to decentralized
netted radar coordination based on black-box optimization and Reinforcement
Learning (RL). To make the RL approach tractable, we use a simplification of
the problem that we proved to be equivalent to the initial formulation. We
apply these techniques on a simulation where radars can follow multiple targets
at the same time and show they can learn implicit cooperation by comparing them
to a greedy baseline."
Diptychs of human and machine perceptions,"We propose visual creations that put differences in algorithms and humans
\emph{perceptions} into perspective. We exploit saliency maps of neural
networks and visual focus of humans to create diptychs that are
reinterpretations of an original image according to both machine and human
attentions. Using those diptychs as a qualitative evaluation of perception, we
discuss some crucial issues of current \textit{task-oriented} artificial
intelligence."
NSF Convergence Approach to Transition Basic Research into Practice,"The National Science Foundation Convergence Accelerator addresses
national-scale societal challenges through use-inspired convergence research.
Leveraging a convergence approach the Convergence Accelerator builds upon basic
research and discovery to make timely investments to strengthen the Nations
innovation ecosystem associated with several key R&D priority areas and
practices to include the coronavirus disease 2019, harnessing the data
revolution, the future of work, and quantum technology. Artificial Intelligence
is a key underlying theme across all of these areas."
Face-work for Human-Agent Joint Decision-Making,"We propose a method to integrate face-work, a common social ritual related to
trust, into a decision-making agent that works collaboratively with a human.
Face-work is a set of trust-building behaviors designed to ""save face"" or
prevent others from ""losing face."" This paper describes the design of a
decision-making process that explicitly considers face-work as part of its
action selection. We also present a simulated robot arm deployed in an online
environment that can be used to evaluate the proposed method."
Towards Map-Based Validation of Semantic Segmentation Masks,"Artificial intelligence for autonomous driving must meet strict requirements
on safety and robustness. We propose to validate machine learning models for
self-driving vehicles not only with given ground truth labels, but also with
additional a-priori knowledge. In particular, we suggest to validate the
drivable area in semantic segmentation masks using given street map data. We
present first results, which indicate that prediction errors can be uncovered
by map-based validation."
Exercise Hierarchical Feature Enhanced Knowledge Tracing,"Knowledge tracing is a fundamental task in the computer-aid educational
system. In this paper, we propose a hierarchical exercise feature enhanced
knowledge tracing framework, which could enhance the ability of knowledge
tracing by incorporating knowledge distribution, semantic features, and
difficulty features from exercise text. Extensive experiments show the high
performance of our framework."
A Review of Recent Advances of Binary Neural Networks for Edge Computing,"Edge computing is promising to become one of the next hottest topics in
artificial intelligence because it benefits various evolving domains such as
real-time unmanned aerial systems, industrial applications, and the demand for
privacy protection. This paper reviews recent advances on binary neural network
(BNN) and 1-bit CNN technologies that are well suitable for front-end,
edge-based computing. We introduce and summarize existing work and classify
them based on gradient approximation, quantization, architecture, loss
functions, optimization method, and binary neural architecture search. We also
introduce applications in the areas of computer vision and speech recognition
and discuss future applications for edge computing."
Explainable AI for Software Engineering,"Artificial Intelligence/Machine Learning techniques have been widely used in
software engineering to improve developer productivity, the quality of software
systems, and decision-making. However, such AI/ML models for software
engineering are still impractical, not explainable, and not actionable. These
concerns often hinder the adoption of AI/ML models in software engineering
practices. In this article, we first highlight the need for explainable AI in
software engineering. Then, we summarize three successful case studies on how
explainable AI techniques can be used to address the aforementioned challenges
by making software defect prediction models more practical, explainable, and
actionable."
Obstacle avoidance and path finding for mobile robot navigation,"This paper investigates different methods to detect obstacles ahead of a
robot using a camera in the robot, an aerial camera, and an ultrasound sensor.
We also explored various efficient path finding methods for the robot to
navigate to the target source. Single and multi-iteration angle-based
navigation algorithms were developed. The theta-based path finding algorithms
were compared with the Dijkstra Algorithm and their performance were analyzed."
Computing Machinery and Knowledge,"The purpose of this paper is to discuss the possibilities for computing
machinery, or AI agents, to know and to possess knowledge. This is done mainly
from a virtue epistemology perspective and definition of knowledge. However,
this inquiry also shed light on the human condition, what it means for a human
to know, and to possess knowledge. The paper argues that it is possible for an
AI agent to know and examines this from both current state-of-the-art in
artificial intelligence as well as from the perspective of what the future AI
development might bring in terms of superintelligent AI agents."
"Polynomial-Time Algorithms for Counting and Sampling Markov Equivalent
  DAGs","Counting and uniform sampling of directed acyclic graphs (DAGs) from a Markov
equivalence class are fundamental tasks in graphical causal analysis. In this
paper, we show that these tasks can be performed in polynomial time, solving a
long-standing open problem in this area. Our algorithms are effective and
easily implementable. Experimental results show that the algorithms
significantly outperform state-of-the-art methods."
"The HyperTrac Project: Recent Progress and Future Research Directions on
  Hypergraph Decompositions","Constraint Satisfaction Problems (CSPs) play a central role in many
applications in Artificial Intelligence and Operations Research. In general,
solving CSPs is NP-complete. The structure of CSPs is best described by
hypergraphs. Therefore, various forms of hypergraph decompositions have been
proposed in the literature to identify tractable fragments of CSPs. However,
also the computation of a concrete hypergraph decomposition is a challenging
task in itself. In this paper, we report on recent progress in the study of
hypergraph decompositions and we outline several directions for future
research."
Strategic Features for General Games,"This short paper describes an ongoing research project that requires the
automated self-play learning and evaluation of a large number of board games in
digital form. We describe the approach we are taking to determine relevant
features, for biasing MCTS playouts for arbitrary games played on arbitrary
geometries. Benefits of our approach include efficient implementation, the
potential to transfer learnt knowledge to new contexts, and the potential to
explain strategic knowledge embedded in features in human-comprehensible terms."
"A Tale of Fairness Revisited: Beyond Adversarial Learning for Deep
  Neural Network Fairness","Motivated by the need for fair algorithmic decision making in the age of
automation and artificially-intelligent technology, this technical report
provides a theoretical insight into adversarial training for fairness in deep
learning. We build upon previous work in adversarial fairness, show the
persistent tradeoff between fair predictions and model performance, and explore
further mechanisms that help in offsetting this tradeoff."
Chemistry42: An AI-based platform for de novo molecular design,"Chemistry42 is a software platform for de novo small molecule design that
integrates Artificial Intelligence (AI) techniques with computational and
medicinal chemistry methods. Chemistry42 is unique in its ability to generate
novel molecular structures with predefined properties validated through in
vitro and in vivo studies. Chemistry42 is a core component of Insilico Medicine
Pharma.ai drug discovery suite that also includes target discovery and
multi-omics data analysis (PandaOmics) and clinical trial outcomes predictions
(InClinico)."
Bandits for Learning to Explain from Explanations,"We introduce Explearn, an online algorithm that learns to jointly output
predictions and explanations for those predictions. Explearn leverages Gaussian
Processes (GP)-based contextual bandits. This brings two key benefits. First,
GPs naturally capture different kinds of explanations and enable the system
designer to control how explanations generalize across the space by virtue of
choosing a suitable kernel. Second, Explearn builds on recent results in
contextual bandits which guarantee convergence with high probability. Our
initial experiments hint at the promise of the approach."
VitrAI -- Applying Explainable AI in the Real World,"With recent progress in the field of Explainable Artificial Intelligence
(XAI) and increasing use in practice, the need for an evaluation of different
XAI methods and their explanation quality in practical usage scenarios arises.
For this purpose, we present VitrAI, which is a web-based service with the goal
of uniformly demonstrating four different XAI algorithms in the context of
three real life scenarios and evaluating their performance and
comprehensibility for humans. This work reveals practical obstacles when
adopting XAI methods and gives qualitative estimates on how well different
approaches perform in said scenarios."
"Design a Technology Based on the Fusion of Genetic Algorithm, Neural
  network and Fuzzy logic","This paper describes the design and development of a prototype technique for
artificial intelligence based on the fusion of genetic algorithm, neural
network and fuzzy logic. It starts by establishing a relationship between the
neural network and fuzzy logic. Then, it combines the genetic algorithm with
them. Information fusions are at the confidence level, where matching scores
can be reported and discussed. The technique is called the Genetic Neuro-Fuzzy
(GNF). It can be used for high accuracy real-time environments."
Understanding algorithmic collusion with experience replay,"In an infinitely repeated pricing game, pricing algorithms based on
artificial intelligence (Q-learning) may consistently learn to charge
supra-competitive prices even without communication. Although concerns on
algorithmic collusion have arisen, little is known on underlying factors. In
this work, we experimentally analyze the dynamics of algorithms with three
variants of experience replay. Algorithmic collusion still has roots in human
preferences. Randomizing experience yields prices close to the static Bertrand
equilibrium and higher prices are easily restored by favoring the latest
experience. Moreover, relative performance concerns also stabilize the
collusion. Finally, we investigate the scenarios with heterogeneous agents and
test robustness on various factors."
Abstraction and Analogy-Making in Artificial Intelligence,"Conceptual abstraction and analogy-making are key abilities underlying
humans' abilities to learn, reason, and robustly adapt their knowledge to new
domains. Despite of a long history of research on constructing AI systems with
these abilities, no current AI system is anywhere close to a capability of
forming humanlike abstractions or analogies. This paper reviews the advantages
and limitations of several approaches toward this goal, including symbolic
methods, deep learning, and probabilistic program induction. The paper
concludes with several proposals for designing challenge tasks and evaluation
measures in order to make quantifiable and generalizable progress in this area."
"A Large-Scale, Automated Study of Language Surrounding Artificial
  Intelligence","This work presents a large-scale analysis of artificial intelligence (AI) and
machine learning (ML) references within news articles and scientific
publications between 2011 and 2019. We implement word association measurements
that automatically identify shifts in language co-occurring with AI/ML and
quantify the strength of these word associations. Our results highlight the
evolution of perceptions and definitions around AI/ML and detect emerging
application areas, models, and systems (e.g., blockchain and cybersecurity).
Recent small-scale, manual studies have explored AI/ML discourse within the
general public, the policymaker community, and researcher community, but are
limited in their scalability and longevity. Our methods provide new views into
public perceptions and subject-area expert discussions of AI/ML and greatly
exceed the explanative power of prior work."
Benchmarking and Survey of Explanation Methods for Black Box Models,"The widespread adoption of black-box models in Artificial Intelligence has
enhanced the need for explanation methods to reveal how these obscure models
reach specific decisions. Retrieving explanations is fundamental to unveil
possible biases and to resolve practical or ethical issues. Nowadays, the
literature is full of methods with different explanations. We provide a
categorization of explanation methods based on the type of explanation
returned. We present the most recent and widely used explainers, and we show a
visual comparison among explanations and a quantitative benchmarking."
"A Dual-Memory Architecture for Reinforcement Learning on Neuromorphic
  Platforms","Reinforcement learning (RL) is a foundation of learning in biological systems
and provides a framework to address numerous challenges with real-world
artificial intelligence applications. Efficient implementations of RL
techniques could allow for agents deployed in edge-use cases to gain novel
abilities, such as improved navigation, understanding complex situations and
critical decision making. Towards this goal, we describe a flexible
architecture to carry out reinforcement learning on neuromorphic platforms.
This architecture was implemented using an Intel neuromorphic processor and
demonstrated solving a variety of tasks using spiking dynamics. Our study
proposes a usable energy efficient solution for real-world RL applications and
demonstrates applicability of the neuromorphic platforms for RL problems."
Alignment of Language Agents,"For artificial intelligence to be beneficial to humans the behaviour of AI
agents needs to be aligned with what humans want. In this paper we discuss some
behavioural issues for language agents, arising from accidental
misspecification by the system designer. We highlight some ways that
misspecification can occur and discuss some behavioural issues that could arise
from misspecification, including deceptive or manipulative language, and review
some approaches for avoiding these issues."
"""Weak AI"" is Likely to Never Become ""Strong AI"", So What is its Greatest
  Value for us?","AI has surpassed humans across a variety of tasks such as image
classification, playing games (e.g., go, ""Starcraft"" and poker), and protein
structure prediction. However, at the same time, AI is also bearing serious
controversies. Many researchers argue that little substantial progress has been
made for AI in recent decades. In this paper, the author (1) explains why
controversies about AI exist; (2) discriminates two paradigms of AI research,
termed ""weak AI"" and ""strong AI"" (a.k.a. artificial general intelligence); (3)
clarifies how to judge which paradigm a research work should be classified
into; (4) discusses what is the greatest value of ""weak AI"" if it has no chance
to develop into ""strong AI""."
Generating explanations for answer set programming applications,"We present an explanation system for applications that leverage Answer Set
Programming (ASP). Given a program P, an answer set A of P, and an atom a in
the program P, our system generates all explanation graphs of a which help
explain why a is true (or false) given the program P and the answer set A. We
illustrate the functionality of the system using some examples from the
literature."
Causal Learning for Socially Responsible AI,"There have been increasing concerns about Artificial Intelligence (AI) due to
its unfathomable potential power. To make AI address ethical challenges and
shun undesirable outcomes, researchers proposed to develop socially responsible
AI (SRAI). One of these approaches is causal learning (CL). We survey
state-of-the-art methods of CL for SRAI. We begin by examining the seven CL
tools to enhance the social responsibility of AI, then review how existing
works have succeeded using these tools to tackle issues in developing SRAI such
as fairness. The goal of this survey is to bring forefront the potentials and
promises of CL for SRAI."
"What Makes a Message Persuasive? Identifying Adaptations Towards
  Persuasiveness in Nine Exploratory Case Studies","The ability to persuade others is critical to professional and personal
success. However, crafting persuasive messages is demanding and poses various
challenges. We conducted nine exploratory case studies to identify adaptations
that professional and non-professional writers make in written scenarios to
increase their subjective persuasiveness. Furthermore, we identified challenges
that those writers faced and identified strategies to resolve them with
persuasive natural language generation, i.e., artificial intelligence. Our
findings show that humans can achieve high degrees of persuasiveness (more so
for professional-level writers), and artificial intelligence can complement
them to achieve increased celerity and alignment in the process."
Digital Voodoo Dolls,"An institution, be it a body of government, commercial enterprise, or a
service, cannot interact directly with a person. Instead, a model is created to
represent us. We argue the existence of a new high-fidelity type of person
model which we call a digital voodoo doll. We conceptualize it and compare its
features with existing models of persons. Digital voodoo dolls are
distinguished by existing completely beyond the influence and control of the
person they represent. We discuss the ethical issues that such a lack of
accountability creates and argue how these concerns can be mitigated."
Towards Artificial Intelligence Enabled Financial Crime Detection,"Recently, financial institutes have been dealing with an increase in
financial crimes. In this context, financial services firms started to improve
their vigilance and use new technologies and approaches to identify and predict
financial fraud and crime possibilities. This task is challenging as
institutions need to upgrade their data and analytics capabilities to enable
new technologies such as Artificial Intelligence (AI) to predict and detect
financial crimes. In this paper, we put a step towards AI-enabled financial
crime detection in general and money laundering detection in particular to
address this challenge. We study and analyse the recent works done in financial
crime detection and present a novel model to detect money laundering cases with
minimum human intervention needs."
"Multi-Context Systems: Dynamics and Evolution (Pre-Print of
  ""Multi-context systems in dynamic environments"")","Multi-Context Systems (MCS) model in Computational Logic distributed systems
composed of heterogeneous sources, or ""contexts"", interacting via special rules
called ""bridge rules"". In this paper, we consider how to enhance flexibility
and generality in bridge-rules definition and application. In particular, we
introduce and discuss some formal extensions of MCSs useful for a practical use
in dynamic environments, and we try to provide guidelines for implementations"
Diagnosing the Impact of AI on Radiology in China,"Artificial Intelligence will significantly impact the work environment of
radiologists. I suggest that up to 50% of a radiologists work in 2021 will be
performed by AI-models in 2025. However, it won't increase beyond that 50%
level, as radiologists remain key for human-centered aspects of their job. I
project that few to no radiologists will be laid off in China due to the
existing supply shortage of radiology services in 2021. The application of AI
in radiology could contribute 1.7 billion USD to China's GDP in 2025. It will
further allow radiologists to start productive work up to four years earlier.
AI in radiology will positively impact the health of patients and radiologists
themselves."
"Fostering Diversity in Spatial Evolutionary Generative Adversarial
  Networks","Generative adversary networks (GANs) suffer from training pathologies such as
instability and mode collapse, which mainly arise from a lack of diversity in
their adversarial interactions. Co-evolutionary GAN (CoE-GAN) training
algorithms have shown to be resilient to these pathologies. This article
introduces Mustangs, a spatially distributed CoE-GAN, which fosters diversity
by using different loss functions during the training. Experimental analysis on
MNIST and CelebA demonstrated that Mustangs trains statistically more accurate
generators."
Artificial intelligence across company borders,"Artificial intelligence (AI) has become a valued technology in many
companies. At the same time, a substantial potential for utilizing AI
\emph{across} company borders has remained largely untapped. An inhibiting
factor concerns disclosure of data to external parties, which raises legitimate
concerns about intellectual property rights, privacy issues, and cybersecurity
risks. Combining federated learning with domain adaptation can provide a
solution to this problem by enabling effective cross-company AI without data
disclosure. In this Viewpoint, we discuss the use, value, and implications of
this approach in a cross-company setting."
BF-QC: Belief Functions on Quantum Circuits,"Dempster-Shafer Theory (DST) of belief function is a basic theory of
artificial intelligence, which can represent the underlying knowledge more
reasonably than Probability Theory (ProbT). Because of the computation
complexity exploding exponentially with the increasing number of elements, the
practical application scenarios of DST are limited. In this paper, we encode
Basic Belief Assignments (BBA) into quantum superposition states and propose
the implementation and operation methods of BBA on quantum circuits. We
decrease the computation complexity of the matrix evolution on BBA (MEoB) on
quantum circuits. Based on the MEoB, we realize the quantum belief functions'
implementation, the similarity measurements of BBAs, evidence Combination Rules
(CR), and probability transformation on quantum circuits."
Similar Cases Recommendation using Legal Knowledge Graphs,"A legal knowledge graph constructed from court cases, judgments, laws and
other legal documents can enable a number of applications like question
answering, document similarity, and search. While the use of knowledge graphs
for distant supervision in NLP tasks is well researched, using knowledge graphs
for applications like case similarity presents challenges. In this work, we
describe our solution for predicting similar cases in Indian court judgements.
We present our results and also discuss the impact of large language models on
this task."
Secure solutions for Smart City Command Control Centre using AIOT,"To build a robust secure solution for smart city IOT network from any Cyber
attacks using Artificial Intelligence. In Smart City IOT network, data
collected from different log collectors or direct sources from cloud or edge
should harness the potential of AI. The smart city command and control center
team will leverage these models and deploy it in different city IOT network to
help on intrusion prediction, network packet surge, potential botnet attacks
from external network. Some of the vital use cases considered based on the
users of command-and-control center"
"Artificial intelligence and the future of diagnostic and therapeutic
  radiopharmaceutical development: in Silico smart molecular design","Novel diagnostic and therapeutic radiopharmaceuticals are increasingly
becoming a central part of personalized medicine. Continued innovation in the
development of new radiopharmaceuticals is key to sustained growth and
advancement of precision medicine. Artificial intelligence (AI) has been used
in multiple fields of medicine to develop and validate better tools for patient
diagnosis and therapy, including in radiopharmaceutical design. In this review,
we first discuss common in silico approaches and focus on their utility and
challenges in radiopharmaceutical development. Next, we discuss the practical
applications of in silico modeling in design of radiopharmaceuticals in various
diseases."
"Faster Exact MPE and Constrained Optimization with Deterministic Finite
  State Automata","We propose a concise function representation based on deterministic finite
state automata for exact most probable explanation and constrained optimization
tasks in graphical models. We then exploit our concise representation within
Bucket Elimination (BE). We denote our version of BE as FABE. FABE
significantly improves the performance of BE in terms of runtime and memory
requirements by minimizing redundancy. Results on most probable explanation and
weighted constraint satisfaction benchmarks show that FABE often outperforms
the state of the art, leading to significant runtime improvements (up to 5
orders of magnitude in our tests)."
Snakes AI Competition 2020 and 2021 Report,"The Snakes AI Competition was held by the Innopolis University and was part
of the IEEE Conference on Games2020 and 2021 editions. It aimed to create a
sandbox for learning and implementing artificial intelligence algorithms in
agents in a ludic manner. Competitors of several countries participated in both
editions of the competition, which was streamed to create asynergy between
organizers and the community. The high-quality submissions and the enthusiasm
around the developed framework create an exciting scenario for future
extensions."
"A study on Machine Learning Approaches for Player Performance and Match
  Results Prediction","Cricket is unarguably one of the most popular sports in the world. Predicting
the outcome of a cricket match has become a fundamental problem as we are
advancing in the field of machine learning. Multiple researchers have tried to
predict the outcome of a cricket match or a tournament, or to predict the
performance of players during a match, or to predict the players who should be
selected as per their current performance, form, morale, etc. using machine
learning and artificial intelligence techniques keeping in mind extensive
detailing, features, and parameters. We discuss some of these techniques along
with a brief comparison among these techniques."
Active Learning for Automated Visual Inspection of Manufactured Products,"Quality control is a key activity performed by manufacturing enterprises to
ensure products meet quality standards and avoid potential damage to the
brand's reputation. The decreased cost of sensors and connectivity enabled an
increasing digitalization of manufacturing. In addition, artificial
intelligence enables higher degrees of automation, reducing overall costs and
time required for defect inspection. In this research, we compare three active
learning approaches and five machine learning algorithms applied to visual
defect inspection with real-world data provided by Philips Consumer Lifestyle
BV. Our results show that active learning reduces the data labeling effort
without detriment to the models' performance."
Estimation of Warfarin Dosage with Reinforcement Learning,"In this paper, it has attempted to use Reinforcement learning to model the
proper dosage of Warfarin for patients.The paper first examines two baselines:
a fixed model of 35 mg/week dosages and a linear model that relies on patient
data. We implemented a LinUCB bandit that improved performance measured on
regret and percent incorrect. On top of the LinUCB bandit, we experimented with
online supervised learning and reward reshaping to boost performance. Our
results clearly beat the baselines and show the promise of using multi-armed
bandits and artificial intelligence to aid physicians in deciding proper
dosages."
"Nine Challenges in Artificial Intelligence and Wireless Communications
  for 6G","In recent years, techniques developed in artificial intelligence (AI),
especially those in machine learning (ML), have been successfully applied in
various areas, leading to a widespread belief that AI will collectively play an
important role in future wireless communications. To accomplish the aspiration,
we present nine challenges to be addressed by the interdisciplinary areas of
AI/ML and wireless communications, with particular focus towards the sixth
generation (6G) wireless networks. Specifically, this article classifies the
nine challenges into computation in AI, distributed neural networks and
learning, and ML enabled semantic communications."
"GDCA: GAN-based single image super resolution with Dual discriminators
  and Channel Attention","Single Image Super-Resolution (SISR) is a very active research field. This
paper addresses SISR by using a GAN-based approach with dual discriminators and
incorporating it with an attention mechanism. The experimental results show
that GDCA can generate sharper and high pleasing images compare to other
conventional methods."
Towards One Shot Search Space Poisoning in Neural Architecture Search,"We evaluate the robustness of a Neural Architecture Search (NAS) algorithm
known as Efficient NAS (ENAS) against data agnostic poisoning attacks on the
original search space with carefully designed ineffective operations. We
empirically demonstrate how our one shot search space poisoning approach
exploits design flaws in the ENAS controller to degrade predictive performance
on classification tasks. With just two poisoning operations injected into the
search space, we inflate prediction error rates for child networks upto 90% on
the CIFAR-10 dataset."
"Airport Taxi Time Prediction and Alerting: A Convolutional Neural
  Network Approach","This paper proposes a novel approach to predict and determine whether the
average taxi- out time at an airport will exceed a pre-defined threshold within
the next hour of operations. Prior work in this domain has focused exclusively
on predicting taxi-out times on a flight-by-flight basis, which requires
significant efforts and data on modeling taxiing activities from gates to
runways. Learning directly from surface radar information with minimal
processing, a computer vision-based model is proposed that incorporates airport
surface data in such a way that adaptation-specific information (e.g., runway
configuration, the state of aircraft in the taxiing process) is inferred
implicitly and automatically by Artificial Intelligence (AI)."
The Catalan Language CLUB,"The Catalan Language Understanding Benchmark (CLUB) encompasses various
datasets representative of different NLU tasks that enable accurate evaluations
of language models, following the General Language Understanding Evaluation
(GLUE) example. It is part of AINA and PlanTL, two public funding initiatives
to empower the Catalan language in the Artificial Intelligence era."
"ADAPQUEST: A Software for Web-Based Adaptive Questionnaires based on
  Bayesian Networks","We introduce ADAPQUEST, a software tool written in Java for the development
of adaptive questionnaires based on Bayesian networks. Adaptiveness is intended
here as the dynamical choice of the question sequence on the basis of an
evolving model of the skill level of the test taker. Bayesian networks offer a
flexible and highly interpretable framework to describe such testing process,
especially when coping with multiple skills. ADAPQUEST embeds dedicated
elicitation strategies to simplify the elicitation of the questionnaire
parameters. An application of this tool for the diagnosis of mental disorders
is also discussed together with some implementation details."
"Combining Reinforcement Learning and Inverse Reinforcement Learning for
  Asset Allocation Recommendations","We suggest a simple practical method to combine the human and artificial
intelligence to both learn best investment practices of fund managers, and
provide recommendations to improve them. Our approach is based on a combination
of Inverse Reinforcement Learning (IRL) and RL. First, the IRL component learns
the intent of fund managers as suggested by their trading history, and recovers
their implied reward function. At the second step, this reward function is used
by a direct RL algorithm to optimize asset allocation decisions. We show that
our method is able to improve over the performance of individual fund managers."
DeepCreativity: Measuring Creativity with Deep Learning Techniques,"Measuring machine creativity is one of the most fascinating challenges in
Artificial Intelligence. This paper explores the possibility of using
generative learning techniques for automatic assessment of creativity. The
proposed solution does not involve human judgement, it is modular and of
general applicability. We introduce a new measure, namely DeepCreativity, based
on Margaret Boden's definition of creativity as composed by value, novelty and
surprise. We evaluate our methodology (and related measure) considering a case
study, i.e., the generation of 19th century American poetry, showing its
effectiveness and expressiveness."
Chatbot System Architecture,"The conversational agents is one of the most interested topics in computer
science field in the recent decade. Which can be composite from more than one
subject in this field, which you need to apply Natural Language Processing
Concepts and some Artificial Intelligence Techniques such as Deep Learning
methods to make decision about how should be the response. This paper is
dedicated to discuss the system architecture for the conversational agent and
explain each component in details."
"Cognitive Ledger Project: Towards Building Personal Digital Twins
  Through Cognitive Blockchain","The Cognitive Ledger Project is an effort to develop a modular system for
turning users' personal data into structured information and machine learning
models based on a blockchain-based infrastructure. In this work-in-progress
paper, we propose a cognitive architecture for cognitive digital twins. The
suggested design embraces a cognitive blockchain (Cognitive ledger) at its
core. The architecture includes several modules that turn users' activities in
the digital environment into reusable knowledge objects and artificial
intelligence that one day can work together to form the cognitive digital twin
of users."
Physical Reasoning in an Open World,"Most work on physical reasoning, both in artificial intelligence and in
cognitive science, has focused on closed-world reasoning, in which it is
assumed that the problem specification specifies all relevant objects and
substance, all their relations in an initial situation, and all exogenous
events. However, in many situations, it is important to do open-world
reasoning; that is, making valid conclusions from very incomplete information.
We have implemented in Prolog an open-world reasoner for a toy microworld of
containers that can be loaded, unloaded, sealed, unsealed, carried, and dumped."
The 6-Ds of Creating AI-Enabled Systems,"We are entering our tenth year of the current Artificial Intelligence (AI)
spring, and, as with previous AI hype cycles, the threat of an AI winter looms.
AI winters occurred because of ineffective approaches towards navigating the
technology valley of death. The 6-D framework provides an end-to-end framework
to successfully navigate this challenge. The 6-D framework starts with problem
decomposition to identify potential AI solutions, and ends with considerations
for deployment of AI-enabled systems. Each component of the 6-D framework and a
precision medicine use case is described in this paper."
"Semi-Supervised GCN for learning Molecular Structure-Activity
  Relationships","Since the introduction of artificial intelligence in medicinal chemistry, the
necessity has emerged to analyse how molecular property variation is modulated
by either single atoms or chemical groups. In this paper, we propose to train
graph-to-graph neural network using semi-supervised learning for attributing
structure-property relationships. As initial case studies we apply the method
to solubility and molecular acidity while checking its consistency in
comparison with known experimental chemical data. As final goal, our approach
could represent a valuable tool to deal with problems such as activity cliffs,
lead optimization and de-novo drug design."
On the evaluation of (meta-)solver approaches,"Meta-solver approaches exploits a number of individual solvers to potentially
build a better solver. To assess the performance of meta-solvers, one can
simply adopt the metrics typically used for individual solvers (e.g., runtime
or solution quality), or employ more specific evaluation metrics (e.g., by
measuring how close the meta-solver gets to its virtual best performance). In
this paper, based on some recently published works, we provide an overview of
different performance metrics for evaluating (meta-)solvers, by underlying
their strengths and weaknesses."
Automated Reasoning in Non-classical Logics in the TPTP World,"Non-classical logics are used in a wide spectrum of disciplines, including
artificial intelligence, computer science, mathematics, and philosophy. The
de-facto standard infrastructure for automated theorem proving, the TPTP World,
currently supports only classical logics. Similar standards for non-classical
logic reasoning do not exist (yet). This hampers practical development of
reasoning systems, and limits their interoperability and application. This
paper describes the latest extension of the TPTP World, which provides
languages and infrastructure for reasoning in non-classical logics. The
extensions integrate seamlessly with the existing TPTP World."
Toward Ethical AIED,"This paper presents the key conclusions to the forthcoming edited book on The
Ethics of Artificial Intelligence in Education: Practices, Challenges and
Debates (August 2022, Routlege). As well as highlighting the key contributions
to the book, it discusses the key questions and the grand challenges for the
field of AI in Education (AIED)in the context of ethics and ethical practices
within the field. The book itself presents diverse perspectives from outside
and from within the AIED as a way of achieving a broad perspective in the key
ethical issues for AIED and a deep understanding of work conducted to date by
the AIED community."
"Emerging Artificial Intelligence Applications in Spatial Transcriptomics
  Analysis","Spatial transcriptomics (ST) has advanced significantly in the last few
years. Such advancement comes with the urgent need for novel computational
methods to handle the unique challenges of ST data analysis. Many artificial
intelligence (AI) methods have been developed to utilize various machine
learning and deep learning techniques for computational ST analysis. This
review provides a comprehensive and up-to-date survey of current AI methods for
ST analysis."
A Policy Driven AI-Assisted PoW Framework,"Proof of Work (PoW) based cyberdefense systems require incoming network
requests to expend effort solving an arbitrary mathematical puzzle. Current
state of the art is unable to differentiate between trustworthy and
untrustworthy connections, requiring all to solve complex puzzles. In this
paper, we introduce an Artificial Intelligence (AI)-assisted PoW framework that
utilizes IP traffic based features to inform an adaptive issuer which can then
generate puzzles with varying hardness. The modular framework uses these
capabilities to ensure that untrustworthy clients solve harder puzzles thereby
incurring longer latency than authentic requests to receive a response from the
server. Our preliminary findings reveal our approach effectively throttles
untrustworthy traffic."
From Statistical to Causal Learning,"We describe basic ideas underlying research to build and understand
artificially intelligent systems: from symbolic approaches via statistical
learning to interventional models relying on concepts of causality. Some of the
hard open problems of machine learning and AI are intrinsically related to
causality, and progress may require advances in our understanding of how to
model and infer causality from data."
Measuring AI Systems Beyond Accuracy,"Current test and evaluation (T&E) methods for assessing machine learning (ML)
system performance often rely on incomplete metrics. Testing is additionally
often siloed from the other phases of the ML system lifecycle. Research
investigating cross-domain approaches to ML T&E is needed to drive the state of
the art forward and to build an Artificial Intelligence (AI) engineering
discipline. This paper advocates for a robust, integrated approach to testing
by outlining six key questions for guiding a holistic T&E strategy."
GitRank: A Framework to Rank GitHub Repositories,"Open-source repositories provide wealth of information and are increasingly
being used to build artificial intelligence (AI) based systems to solve
problems in software engineering. Open-source repositories could be of varying
quality levels, and bad-quality repositories could degrade performance of these
systems. Evaluating quality of open-source repositories, which is not available
directly on code hosting sites such as GitHub, is thus important. In this
hackathon, we utilize known code quality measures and GrimoireLab toolkit to
implement a framework, named GitRank, to rank open-source repositories on three
different criteria. We discuss our findings and preliminary evaluation in this
hackathon report."
"Massively Digitized Power Grid: Opportunities and Challenges of
  Use-inspired AI","This article presents a use-inspired perspective of the opportunities and
challenges in a massively digitized power grid. It argues that the intricate
interplay of data availability, computing capability, and artificial
intelligence (AI) algorithm development are the three key factors driving the
adoption of digitized solutions in the power grid. The impact of these three
factors on critical functions of power system operation and planning practices
are reviewed and illustrated with industrial practice case studies. Open
challenges and research opportunities for data, computing, and AI algorithms
are articulated within the context of the power industry's tremendous
decarbonization efforts."
Characterising Research Areas in the field of AI,"Interest in Artificial Intelligence (AI) continues to grow rapidly, hence it
is crucial to support researchers and organisations in understanding where AI
research is heading. In this study, we conducted a bibliometric analysis on
257K articles in AI, retrieved from OpenAlex. We identified the main conceptual
themes by performing clustering analysis on the co-occurrence network of
topics. Finally, we observed how such themes evolved over time. The results
highlight the growing academic interest in research themes like deep learning,
machine learning, and internet of things."
Can Artificial Intelligence Transform DevOps?,"DevOps and Artificial Intelligence (AI) are interconnected with each other.
DevOps is a business-driven approach to providing quickly delivered quality
software, and AI is the technology that can be used in the system to enhance
its functionality. So, DevOps teams can use AI to test, code, release, monitor,
and improve the system. Through AI, the automation process delivered by DevOps
could be improved efficiently. This study aims to explore how AI can transform
DevOps. The research is useful in terms of facilitating software developers and
businesses to assess the importance of AI in DevOps. The study has practical
implications as it elaborates on how AI transforms DevOps and in what way it
can support businesses in their business."
Augmenting Netflix Search with In-Session Adapted Recommendations,"We motivate the need for recommendation systems that can cater to the members
in-the-moment intent by leveraging their interactions from the current session.
We provide an overview of an end-to-end in-session adaptive recommendations
system in the context of Netflix Search. We discuss the challenges and
potential solutions when developing such a system at production scale."
"Improving Model Understanding and Trust with Counterfactual Explanations
  of Model Confidence","In this paper, we show that counterfactual explanations of confidence scores
help users better understand and better trust an AI model's prediction in
human-subject studies. Showing confidence scores in human-agent interaction
systems can help build trust between humans and AI systems. However, most
existing research only used the confidence score as a form of communication,
and we still lack ways to explain why the algorithm is confident. This paper
also presents two methods for understanding model confidence using
counterfactual explanation: (1) based on counterfactual examples; and (2) based
on visualisation of the counterfactual space."
Greedy Relaxations of the Sparsest Permutation Algorithm,"There has been an increasing interest in methods that exploit permutation
reasoning to search for directed acyclic causal models, including the ""Ordering
Search"" of Teyssier and Kohler and GSP of Solus, Wang and Uhler. We extend the
methods of the latter by a permutation-based operation, tuck, and develop a
class of algorithms, namely GRaSP, that are efficient and pointwise consistent
under increasingly weaker assumptions than faithfulness. The most relaxed form
of GRaSP outperforms many state-of-the-art causal search algorithms in
simulation, allowing efficient and accurate search even for dense graphs and
graphs with more than 100 variables."
Boosted decision trees,"Boosted decision trees are a very powerful machine learning technique. After
introducing specific concepts of machine learning in the high-energy physics
context and describing ways to quantify the performance and training quality of
classifiers, decision trees are described. Some of their shortcomings are then
mitigated with ensemble learning, using boosting algorithms, in particular
AdaBoost and gradient boosting. Examples from high-energy physics and software
used are also presented."
A Disability Lens towards Biases in GPT-3 Generated Open-Ended Languages,"Language models (LM) are becoming prevalent in many language-based
application spaces globally. Although these LMs are improving our day-to-day
interactions with digital products, concerns remain whether open-ended
languages or text generated from these models reveal any biases toward a
specific group of people, thereby risking the usability of a certain product.
There is a need to identify whether these models possess bias to improve the
fairness in these models. This gap motivates our ongoing work, where we
measured the two aspects of bias in GPT-3 generated text through a disability
lens."
"Indecision Trees: Learning Argument-Based Reasoning under Quantified
  Uncertainty","Using Machine Learning systems in the real world can often be problematic,
with inexplicable black-box models, the assumed certainty of imperfect
measurements, or providing a single classification instead of a probability
distribution.
  This paper introduces Indecision Trees, a modification to Decision Trees
which learn under uncertainty, can perform inference under uncertainty, provide
a robust distribution over the possible labels, and can be disassembled into a
set of logical arguments for use in other reasoning systems."
"A Design of A Simple Yet Effective Exercise Recommendation System in
  K-12 Online Learning","We propose a simple but effective method to recommend exercises with high
quality and diversity for students. Our method is made up of three key
components: (1) candidate generation module; (2) diversity-promoting module;
and (3) scope restriction module. The proposed method improves the overall
recommendation performance in terms of recall, and increases the diversity of
the recommended candidates by 0.81\% compared to the baselines."
"A Robustly Optimized Long Text to Math Models for Numerical Reasoning On
  FinQA","Numerical reasoning is required when solving most problems in our life, but
it has been neglected in previous artificial intelligence researches. FinQA
challenge has been organized to strengthen the study on numerical reasoning
where the participants are asked to predict the numerical reasoning program to
solve financial question. The result of FinQA will be evaluated by both
execution accuracy and program accuracy. In this paper, we present our approach
to tackle the task objective by developing models with different specialized
capabilities and fusing their strength. Overall, our approach achieves the 1st
place in FinQA challenge, with 71.93% execution accuracy and 67.03% program
accuracy."
"Proceedings of the Second Workshop on Agents and Robots for reliable
  Engineered Autonomy","This volume contains the proceedings of the Second Workshop on Agents and
Robots for reliable Engineered Autonomy (AREA 2022), co-located with the 31st
International Joint Conference on Artificial Intelligence and the 25th European
Conference on Artificial Intelligence (IJCAI-ECAI 2022). The AREA workshop
brings together researchers from autonomous agents, software engineering and
robotic communities, as combining knowledge coming from these research areas
may lead to innovative approaches that solve complex problems related with the
verification and validation of autonomous robotic systems."
"Wide & Deep Learning for Judging Student Performance in Online
  One-on-one Math Classes","In this paper, we investigate the opportunities of automating the judgment
process in online one-on-one math classes. We build a Wide & Deep framework to
learn fine-grained predictive representations from a limited amount of noisy
classroom conversation data that perform better student judgments. We conducted
experiments on the task of predicting students' levels of mastery of example
questions and the results demonstrate the superiority and availability of our
model in terms of various evaluation metrics."
Credal Valuation Networks for Machine Reasoning Under Uncertainty,"Contemporary undertakings provide limitless opportunities for widespread
application of machine reasoning and artificial intelligence in situations
characterised by uncertainty, hostility and sheer volume of data. The paper
develops a valuation network as a graphical system for higher-level fusion and
reasoning under uncertainty in support of the human operators. Valuations,
which are mathematical representation of (uncertain) knowledge and collected
data, are expressed as credal sets, defined as coherent interval probabilities
in the framework of imprecise probability theory. The basic operations with
such credal sets, combination and marginalisation, are defined to satisfy the
axioms of a valuation algebra. A practical implementation of the credal
valuation network is discussed and its utility demonstrated on a small scale
example."
Let it RAIN for Social Good,"Artificial Intelligence (AI) as a highly transformative technology take on a
special role as both an enabler and a threat to UN Sustainable Development
Goals (SDGs). AI Ethics and emerging high-level policy efforts stand at the
pivot point between these outcomes but is barred from effect due the
abstraction gap between high-level values and responsible action. In this paper
the Responsible Norms (RAIN) framework is presented, bridging this gap thereby
enabling effective high-level control of AI impact. With effective and
operationalized AI Ethics, AI technologies can be directed towards global
sustainable development."
"The Transform-o-meter: A method to forecast the transformative impact of
  innovation","With the advent of Transformative Artificial Intelligence, it is now more
important than ever to be able to both measure and forecast the transformative
impact/potential of innovation. However, current methods fall short when faced
with this task. This paper introduces the Transform-o-meter; a methodology that
can be used to achieve the aforementioned goal, and be applied to any
innovation, both material and immaterial. While this method can effectively be
used for the mentioned purpose, it should be taken as a first approach; to be
iterated, researched, and expanded further upon."
Introducing RISK,"This extended abstract introduces the initial steps taken to develop a system
for Rapid Internal Simulation of Knowledge (RISK). RISK aims to enable more
transparency in artificial intelligence systems, especially those created by
deep learning networks by allowing real-time simulation of what the system
knows. By looking at hypothetical situations based on these simulations a
system may make more informed decisions, and produce them for non-expert
observers to understand the reasoning behind a given action."
"Drfen Maschinen denken (knnen)? Warum Knstliche Intelligenz
  eine Ethik braucht. (Are Machines Allowed to (be able to) Think? Why
  Artificial Intelligence Needs Ethics)","Speech manuscript (German + English) of the impulse lecture for the panel
discussion ""May machines (be able to) think?"" at the 102nd Katholikentag on May
28, 2022 in Stuttgart. Panel: Winfried Kretschmann (MdL, Prime Minister
Baden-W\""urttemberg, Stuttgart), Ursula Nothelle-Wildfeuer (Freiburg), Michael
Resch (Stuttgart),Karsten Wendland (Aalen). Moderation: Stefanie Rentsch
(Fulda). Advocate of the audience: Verena Neuhausen (Stuttgart)."
Negative Human Rights as a Basis for Long-term AI Safety and Regulation,"If autonomous AI systems are to be reliably safe in novel situations, they
will need to incorporate general principles guiding them to recognize and avoid
harmful behaviours. Such principles may need to be supported by a binding
system of regulation, which would need the underlying principles to be widely
accepted. They should also be specific enough for technical implementation.
Drawing inspiration from law, this article explains how negative human rights
could fulfil the role of such principles and serve as a foundation both for an
international regulatory system and for building technical safety constraints
for future AI systems."
AI-powered Language Assessment Tools for Dementia,"The main objective of this paper is to propose an approach for developing an
Artificial Intelligence (AI)-powered Language Assessment (LA) tool. Such tools
can be used to assess language impairments associated with dementia in older
adults. The Machine Learning (ML) classifiers are the main parts of our
proposed approach, therefore to develop an accurate tool with high sensitivity
and specificity, we consider different binary classifiers and evaluate their
performances. We also assess the reliability and validity of our approach by
comparing the impact of different types of language tasks, features, and
recording media on the performance of ML classifiers."
"Non-Axiomatic Term Logic: A Computational Theory of Cognitive Symbolic
  Reasoning","This paper presents Non-Axiomatic Term Logic (NATL) as a theoretical
computational framework of humanlike symbolic reasoning in artificial
intelligence. NATL unites a discrete syntactic system inspired from Aristotle's
term logic and a continuous semantic system based on the modern idea of
distributed representations, or embeddings. This paper positions the proposed
approach in the phylogeny and the literature of logic, and explains the
framework. As it is yet no more than a theory and it requires much further
elaboration to implement it, no quantitative evaluation is presented. Instead,
qualitative analyses of arguments using NATL, some applications to possible
cognitive science/robotics-related research, and remaining issues towards a
machinery implementation are discussed."
"Strategic Decisions Survey, Taxonomy, and Future Directions from
  Artificial Intelligence Perspective","Strategic Decision-Making is always challenging because it is inherently
uncertain, ambiguous, risky, and complex. It is the art of possibility. We
develop a systematic taxonomy of decision-making frames that consists of 6
bases, 18 categorical, and 54 frames. We aim to lay out the computational
foundation that is possible to capture a comprehensive landscape view of a
strategic problem. Compared with traditional models, it covers irrational,
non-rational and rational frames c dealing with certainty, uncertainty,
complexity, ambiguity, chaos, and ignorance."
Named Entity Recognition in Indian court judgments,"Identification of named entities from legal texts is an essential building
block for developing other legal Artificial Intelligence applications. Named
Entities in legal texts are slightly different and more fine-grained than
commonly used named entities like Person, Organization, Location etc. In this
paper, we introduce a new corpus of 46545 annotated legal named entities mapped
to 14 legal entity types. The Baseline model for extracting legal named
entities from judgment text is also developed."
What is Wrong with Language Models that Can Not Tell a Story?,"This paper argues that a deeper understanding of narrative and the successful
generation of longer subjectively interesting texts is a vital bottleneck that
hinders the progress in modern Natural Language Processing (NLP) and may even
be in the whole field of Artificial Intelligence. We demonstrate that there are
no adequate datasets, evaluation methods, and even operational concepts that
could be used to start working on narrative processing."
AI Ethics in Smart Healthcare,"This article reviews the landscape of ethical challenges of integrating
artificial intelligence (AI) into smart healthcare products, including medical
electronic devices. Differences between traditional ethics in the medical
domain and emerging ethical challenges with AI-driven healthcare are presented,
particularly as they relate to transparency, bias, privacy, safety,
responsibility, justice, and autonomy. Open challenges and recommendations are
outlined to enable the integration of ethical principles into the design,
validation, clinical trials, deployment, monitoring, repair, and retirement of
AI-based smart healthcare products."
A Short Survey of Systematic Generalization,"This survey includes systematic generalization and a history of how machine
learning addresses it. We aim to summarize and organize the related information
of both conventional and recent improvements. We first look at the definition
of systematic generalization, then introduce Classicist and Connectionist. We
then discuss different types of Connectionists and how they approach the
generalization. Two crucial problems of variable binding and causality are
discussed. We look into systematic generalization in language, vision, and VQA
fields. Recent improvements from different aspects are discussed. Systematic
generalization has a long history in artificial intelligence. We could cover
only a small portion of many contributions. We hope this paper provides a
background and is beneficial for discoveries in future work."
"Expansive Participatory AI: Supporting Dreaming within Inequitable
  Institutions","Participatory Artificial Intelligence (PAI) has recently gained interest by
researchers as means to inform the design of technology through collective's
lived experience. PAI has a greater promise than that of providing useful input
to developers, it can contribute to the process of democratizing the design of
technology, setting the focus on what should be designed. However, in the
process of PAI there existing institutional power dynamics that hinder the
realization of expansive dreams and aspirations of the relevant stakeholders.
In this work we propose co-design principals for AI that address institutional
power dynamics focusing on Participatory AI with youth."
"Pre-Training With Scientific Text Improves Educational Question
  Generation","With the boom of digital educational materials and scalable e-learning
systems, the potential for realising AI-assisted personalised learning has
skyrocketed. In this landscape, the automatic generation of educational
questions will play a key role, enabling scalable self-assessment when a global
population is manoeuvring their personalised learning journeys. We develop
EduQG, a novel educational question generation model built by adapting a large
language model. Our initial experiments demonstrate that EduQG can produce
superior educational questions by pre-training on scientific text."
"Sharing Linkable Learning Objects with the use of Metadata and a
  Taxonomy Assistant for Categorization","In this work, a re-design of the Moodledata module functionalities is
presented to share learning objects between e-learning content platforms, e.g.,
Moodle and G-Lorep, in a linkable object format. The e-learning courses content
of the Drupal-based Content Management System G-Lorep for academic learning is
exchanged designing an object incorporating metadata to support the reuse and
the classification in its context. In such an Artificial Intelligence
environment, the exchange of Linkable Learning Objects can be used for dialogue
between Learning Systems to obtain information, especially with the use of
semantic or structural similarity measures to enhance the existent Taxonomy
Assistant for advanced automated classification."
"Real-Time Artificial Intelligence Assistance for Safe Laparoscopic
  Cholecystectomy: Early-Stage Clinical Evaluation","Artificial intelligence is set to be deployed in operating rooms to improve
surgical care. This early-stage clinical evaluation shows the feasibility of
concurrently attaining real-time, high-quality predictions from several deep
neural networks for endoscopic video analysis deployed for assistance during
three laparoscopic cholecystectomies."
Inclusive Artificial Intelligence,"Prevailing methods for assessing and comparing generative AIs incentivize
responses that serve a hypothetical representative individual. Evaluating
models in these terms presumes homogeneous preferences across the population
and engenders selection of agglomerative AIs, which fail to represent the
diverse range of interests across individuals. We propose an alternative
evaluation method that instead prioritizes inclusive AIs, which provably retain
the requisite knowledge not only for subsequent response customization to
particular segments of the population but also for utility-maximizing
decisions."
Towards Learning Abstractions via Reinforcement Learning,"In this paper we take the first steps in studying a new approach to synthesis
of efficient communication schemes in multi-agent systems, trained via
reinforcement learning. We combine symbolic methods with machine learning, in
what is referred to as a neuro-symbolic system. The agents are not restricted
to only use initial primitives: reinforcement learning is interleaved with
steps to extend the current language with novel higher-level concepts, allowing
generalisation and more informative communication via shorter messages. We
demonstrate that this approach allow agents to converge more quickly on a small
collaborative construction task."
Transformers as Policies for Variable Action Environments,"In this project we demonstrate the effectiveness of the transformer encoder
as a viable architecture for policies in variable action environments. Using
it, we train an agent using Proximal Policy Optimisation (PPO) on multiple maps
against scripted opponents in the Gym-$\mu$RTS environment. The final agent is
able to achieve a higher return using half the computational resources of the
next-best RL agent, which used the GridNet architecture.
  The source code and pre-trained models are available here:
https://github.com/NiklasZ/transformers-for-variable-action-envs"
Artificial Intelligence Generated Coins for Size Comparison,"Authors of scientific articles use coins in photographs as a size reference
for objects. For this purpose, coins are placed next to objects when taking the
photo. In this letter we propose a novel method that uses artificial
intelligence (AI) generated images of coins to provide a size reference in
photos. The newest generation is able to quickly generate realistic
high-quality images from textual descriptions. With the proposed method no
physical coin is required while taking photos. Coins can be added to photos
that contain none. Furthermore, we show how the coin motif can be matched to
the object."
"The problem with AI consciousness: A neurogenetic case against synthetic
  sentience","Ever since the creation of the first artificial intelligence (AI) machinery
built on machine learning (ML), public society has entertained the idea that
eventually computers could become sentient and develop a consciousness of their
own. As these models now get increasingly better and convincingly more
anthropomorphic, even some engineers have started to believe that AI might
become conscious, which would result in serious social consequences. The
present paper argues against the plausibility of sentient AI primarily based on
the theory of neurogenetic structuralism, which claims that the physiology of
biological neurons and their structural organization into complex brains are
necessary prerequisites for true consciousness to emerge."
A Coreset Learning Reality Check,"Subsampling algorithms are a natural approach to reduce data size before
fitting models on massive datasets. In recent years, several works have
proposed methods for subsampling rows from a data matrix while maintaining
relevant information for classification. While these works are supported by
theory and limited experiments, to date there has not been a comprehensive
evaluation of these methods. In our work, we directly compare multiple methods
for logistic regression drawn from the coreset and optimal subsampling
literature and discover inconsistencies in their effectiveness. In many cases,
methods do not outperform simple uniform subsampling."
PyExperimenter: Easily distribute experiments and track results,"PyExperimenter is a tool to facilitate the setup, documentation, execution,
and subsequent evaluation of results from an empirical study of algorithms and
in particular is designed to reduce the involved manual effort significantly.
It is intended to be used by researchers in the field of artificial
intelligence, but is not limited to those."
Incentives to Offer Algorithmic Recourse,"Due to the importance of artificial intelligence (AI) in a variety of
high-stakes decisions, such as loan approval, job hiring, and criminal bail,
researchers in Explainable AI (XAI) have developed algorithms to provide users
with recourse for an unfavorable outcome. We analyze the incentives for a
decision-maker to offer recourse to a set of applicants. Does the
decision-maker have the incentive to offer recourse to all rejected applicants?
We show that the decision-maker only offers recourse to all applicants in
extreme cases, such as when the recourse process is impossible to manipulate.
Some applicants may be worse off when the decision-maker can offer recourse."
"Study of Optical Networks, 5G, Artificial Intelligence and Their
  Applications","This paper discusses the application of artificial intelligence (AI)
technology in optical communication networks and 5G. It primarily introduces
representative applications of AI technology and potential risks of AI
technology failure caused by the openness of optical communication networks,
and proposes some coping strategies, mainly including modeling AI systems
through modularization and miniaturization, combining with traditional
classical network modeling and planning methods, and improving the
effectiveness and interpretability of AI technology. At the same time, it
proposes response strategies based on network protection for the possible
failure and attack of AI technology."
"Assessing the impact of regulations and standards on innovation in the
  field of AI","Regulations and standards in the field of artificial intelligence (AI) are
necessary to minimise risks and maximise benefits, yet some argue that they
stifle innovation. This paper critically examines the idea that regulation
stifles innovation in the field of AI. Current trends in AI regulation,
particularly the proposed European AI Act and the standards supporting its
implementation, are discussed. Arguments in support of the idea that regulation
stifles innovation are analysed and criticised, and an alternative point of
view is offered, showing how regulation and standards can foster innovation in
the field of AI."
Heckerthoughts,"This manuscript is technical memoir about my work at Stanford and Microsoft
Research. Included are fundamental concepts central to machine learning and
artificial intelligence, applications of these concepts, and stories behind
their creation."
Integrating Artificial Intelligence and Humanities in Healthcare,"Artificial Intelligence (AI) and Medical Humanities have become two of the
most crucial and rapidly growing fields in the current world. AI has made
substantial advancements in recent years, enabling the development of
algorithms and systems that can perform tasks traditionally done by humans.
Medical Humanities, on the other hand, is the intersection of medical sciences,
humanities, and the social sciences, and deals with the cultural, historical,
philosophical, ethical, and social aspects of health, illness, and medicine.
The integration of AI and Medical Humanities can offer innovative solutions to
some of the pressing issues in the medical field."
Tailoring Requirements Engineering for Responsible AI,"Requirements Engineering (RE) is the discipline for identifying, analyzing,
as well as ensuring the implementation and delivery of user, technical, and
societal requirements. Recently reported issues concerning the acceptance of
Artificial Intelligence (AI) solutions after deployment, e.g. in the medical,
automotive, or scientific domains, stress the importance of RE for designing
and delivering Responsible AI systems. In this paper, we argue that RE should
not only be carefully conducted but also tailored for Responsible AI. We
outline related challenges for research and practice."
"Explainable AI does not provide the explanations end-users are asking
  for","Explainable Artificial Intelligence (XAI) techniques are frequently required
by users in many AI systems with the goal of understanding complex models,
their associated predictions, and gaining trust. While suitable for some
specific tasks during development, their adoption by organisations to enhance
trust in machine learning systems has unintended consequences. In this paper we
discuss XAI's limitations in deployment and conclude that transparency
alongside with rigorous validation are better suited to gaining trust in AI
systems."
"Tile Networks: Learning Optimal Geometric Layout for Whole-page
  Recommendation","Finding optimal configurations in a geometric space is a key challenge in
many technological disciplines. Current approaches either rely heavily on human
domain expertise and are difficult to scale. In this paper we show it is
possible to solve configuration optimization problems for whole-page
recommendation using reinforcement learning. The proposed \textit{Tile
Networks} is a neural architecture that optimizes 2D geometric configurations
by arranging items on proper positions. Empirical results on real dataset
demonstrate its superior performance compared to traditional learning to rank
approaches and recent deep models."
"Practical Statistical Considerations for the Clinical Validation of
  AI/ML-enabled Medical Diagnostic Devices","Artificial Intelligence (AI) and Machine-Learning (ML) models have been
increasingly used in medical products, such as medical device software. General
considerations on the statistical aspects for the evaluation of AI/ML-enabled
medical diagnostic devices are discussed in this paper. We also provide
relevant academic references and note good practices in addressing various
statistical challenges in the clinical validation of AI/ML-enabled medical
devices in the context of their intended use."
Machine Learning in Physics and Geometry,"We survey some recent applications of machine learning to problems in
geometry and theoretical physics. Pure mathematical data has been compiled over
the last few decades by the community and experiments in supervised,
semi-supervised and unsupervised machine learning have found surprising
success. We thus advocate the programme of machine learning mathematical
structures, and formulating conjectures via pattern recognition, in other words
using artificial intelligence to help one do mathematics.
  This is an invited chapter contribution to Elsevier's Handbook of Statistics,
Volume 49: Artificial Intelligence edited by S.~G.~Krantz, A.~S.~R.~Srinivasa
Rao, and C.~R.~Rao."
Fairlearn: Assessing and Improving Fairness of AI Systems,"Fairlearn is an open source project to help practitioners assess and improve
fairness of artificial intelligence (AI) systems. The associated Python
library, also named fairlearn, supports evaluation of a model's output across
affected populations and includes several algorithms for mitigating fairness
issues. Grounded in the understanding that fairness is a sociotechnical
challenge, the project integrates learning resources that aid practitioners in
considering a system's broader societal context."
"A New Deep Learning and XAI-Based Algorithm for Features Selection in
  Genomics","In the field of functional genomics, the analysis of gene expression profiles
through Machine and Deep Learning is increasingly providing meaningful insight
into a number of diseases. The paper proposes a novel algorithm to perform
Feature Selection on genomic-scale data, which exploits the reconstruction
capabilities of autoencoders and an ad-hoc defined Explainable Artificial
Intelligence-based score in order to select the most informative genes for
diagnosis, prognosis, and precision medicine. Results of the application on a
Chronic Lymphocytic Leukemia dataset evidence the effectiveness of the
algorithm, by identifying and suggesting a set of meaningful genes for further
medical investigation."
The Full Rights Dilemma for A.I. Systems of Debatable Personhood,"An Artificially Intelligent system (an AI) has debatable personhood if it's
epistemically possible either that the AI is a person or that it falls far
short of personhood. Debatable personhood is a likely outcome of AI development
and might arise soon. Debatable AI personhood throws us into a catastrophic
moral dilemma: Either treat the systems as moral persons and risk sacrificing
real human interests for the sake of entities without interests worth the
sacrifice, or don't treat the systems as moral persons and risk perpetrating
grievous moral wrongs against them. The moral issues become even more
perplexing if we consider cases of possibly conscious AI that are subhuman,
superhuman, or highly divergent from us in their morally relevant properties."
"Network Visualization of ChatGPT Research: a study based on term and
  keyword co-occurrence network analysis","The main objective of this paper is to identify the major research areas of
ChatGPT through term and keyword co-occurrence network mapping techniques. For
conducting the present study, total of 577 publications were retrieved from the
Lens database for the network visualization. The findings of the study showed
that chatgpt occurrence in maximum number of times followed by its related
terms such as artificial intelligence, large language model, gpt, study etc.
This study will be helpful to library and information science as well as
computer or information technology professionals."
"If consciousness is dynamically relevant, artificial intelligence isn't
  conscious","We demonstrate that if consciousness is relevant for the temporal evolution
of a system's states--that is, if it is dynamically relevant--then AI systems
cannot be conscious. That is because AI systems run on CPUs, GPUs, TPUs or
other processors which have been designed and verified to adhere to
computational dynamics that systematically preclude or suppress deviations. The
design and verification preclude or suppress, in particular, potential
consciousness-related dynamical effects, so that if consciousness is
dynamically relevant, AI systems cannot be conscious."
"Meta Semantics: Towards better natural language understanding and
  reasoning","Natural language understanding is one of the most challenging topics in
artificial intelligence. Deep neural network methods, particularly large
language module (LLM) methods such as ChatGPT and GPT-3, have powerful
flexibility to adopt informal text but are weak on logical deduction and suffer
from the out-of-vocabulary (OOV) problem. On the other hand, rule-based methods
such as Mathematica, Semantic web, and Lean, are excellent in reasoning but
cannot handle the complex and changeable informal text. Inspired by pragmatics
and structuralism, we propose two strategies to solve the OOV problem and a
semantic model for better natural language understanding and reasoning."
Read My Mind: A Multi-Modal Dataset for Human Belief Prediction,"Understanding human intentions is key to enabling effective and efficient
human-robot interaction (HRI) in collaborative settings. To enable developments
and evaluation of the ability of artificial intelligence (AI) systems to infer
human beliefs, we introduce a large-scale multi-modal video dataset for intent
prediction based on object-context relations."
Explanation through Reward Model Reconciliation using POMDP Tree Search,"As artificial intelligence (AI) algorithms are increasingly used in
mission-critical applications, promoting user-trust of these systems will be
essential to their success. Ensuring users understand the models over which
algorithms reason promotes user trust. This work seeks to reconcile differences
between the reward model that an algorithm uses for online partially observable
Markov decision (POMDP) planning and the implicit reward model assumed by a
human user. Action discrepancies, differences in decisions made by an algorithm
and user, are leveraged to estimate a user's objectives as expressed in
weightings of a reward function."
Connecting levels of analysis in the computational era,"Neuroscience and artificial intelligence are closely intertwined, but so are
the physics of dynamical system, philosophy and psychology. Each of these
fields try in their own way to relate observations at the level of molecules,
synapses, neurons or behavior, to a function. An influential conceptual
approach to this end was popularized by David Marr, which focused on the
interaction between three theoretical 'levels of analysis'. With the
convergence of simulation-based approaches, algorithm-oriented Neuro-AI and
high-throughput data, we currently see much research organized around four
levels of analysis: observations, models, algorithms and functions.
Bidirectional interaction between these levels influences how we undertake
interdisciplinary science."
"Scalable Educational Question Generation with Pre-trained Language
  Models","The automatic generation of educational questions will play a key role in
scaling online education, enabling self-assessment at scale when a global
population is manoeuvring their personalised learning journeys. We develop
\textit{EduQG}, a novel educational question generation model built by adapting
a large language model. Our extensive experiments demonstrate that
\textit{EduQG} can produce superior educational questions by further
pre-training and fine-tuning a pre-trained language model on the scientific
text and science question data."
Human Control: Definitions and Algorithms,"How can humans stay in control of advanced artificial intelligence systems?
One proposal is corrigibility, which requires the agent to follow the
instructions of a human overseer, without inappropriately influencing them. In
this paper, we formally define a variant of corrigibility called shutdown
instructability, and show that it implies appropriate shutdown behavior,
retention of human autonomy, and avoidance of user harm. We also analyse the
related concepts of non-obstruction and shutdown alignment, three previously
proposed algorithms for human control, and one new algorithm."
Disproving XAI Myths with Formal Methods -- Initial Results,"The advances in Machine Learning (ML) in recent years have been both
impressive and far-reaching. However, the deployment of ML models is still
impaired by a lack of trust in how the best-performing ML models make
predictions. The issue of lack of trust is even more acute in the uses of ML
models in high-risk or safety-critical domains. eXplainable artificial
intelligence (XAI) is at the core of ongoing efforts for delivering trustworthy
AI. Unfortunately, XAI is riddled with critical misconceptions, that foster
distrust instead of building trust. This paper details some of the most visible
misconceptions in XAI, and shows how formal methods have been used, both to
disprove those misconceptions, but also to devise practically effective
alternatives."
"A Survey of Explainable AI and Proposal for a Discipline of Explanation
  Engineering","In this survey paper, we deep dive into the field of Explainable Artificial
Intelligence (XAI). After introducing the scope of this paper, we start by
discussing what an ""explanation"" really is. We then move on to discuss some of
the existing approaches to XAI and build a taxonomy of the most popular
methods. Next, we also look at a few applications of these and other XAI
techniques in four primary domains: finance, autonomous driving, healthcare and
manufacturing. We end by introducing a promising discipline, ""Explanation
Engineering,"" which includes a systematic approach for designing explainability
into AI systems."
"AI and the creative realm: A short review of current and future
  applications","This study explores the concept of creativity and artificial intelligence
(AI) and their recent integration. While AI has traditionally been perceived as
incapable of generating new ideas or creating art, the development of more
sophisticated AI models and the proliferation of human-computer interaction
tools have opened up new possibilities for AI in artistic creation. This study
investigates the various applications of AI in a creative context,
differentiating between the type of art, language, and algorithms used. It also
considers the philosophical implications of AI and creativity, questioning
whether consciousness can be researched in machines and AI's potential
interests and decision-making capabilities. Overall, we aim to stimulate a
reflection on AI's use and ethical implications in creative contexts."
A Markovian Formalism for Active Querying,"Active learning algorithms have been an integral part of recent advances in
artificial intelligence. However, the research in the field is widely varying
and lacks an overall organizing leans. We outline a Markovian formalism for the
field of active learning and survey the literature to demonstrate the
organizing capability of our proposed formalism. Our formalism takes a
partially observable Markovian system approach to the active learning process
as a whole. We specifically outline how querying, dataset augmentation, reward
updates, and other aspects of active learning can be viewed as a transition
between meta-states in a Markovian system, and give direction into how other
aspects of active learning can fit into our formalism."
"Unraveling the ARC Puzzle: Mimicking Human Solutions with Object-Centric
  Decision Transformer","In the pursuit of artificial general intelligence (AGI), we tackle
Abstraction and Reasoning Corpus (ARC) tasks using a novel two-pronged
approach. We employ the Decision Transformer in an imitation learning paradigm
to model human problem-solving, and introduce an object detection algorithm,
the Push and Pull clustering method. This dual strategy enhances AI's ARC
problem-solving skills and provides insights for AGI progression. Yet, our work
reveals the need for advanced data collection tools, robust training datasets,
and refined model structures. This study highlights potential improvements for
Decision Transformers and propels future AGI research."
Analogue and Physical Reservoir Computing Using Water Waves,"More than 3.5 billion people live in rural areas, where water and water
energy resources play an important role in ensuring sustainable and productive
rural economies. This article reviews and critically analyses the recent
advances in the field of analogue and reservoir computing that have been driven
by unique physical properties and energy of water waves. It also demonstrates
that analogue and reservoir computing hold the potential to bring artificial
intelligence closer to people living outside large cities, thus enabling them
to enjoy the benefits of novel technologies that already work in large cities
but are not readily available and suitable for regional communities."
Concept Extrapolation: A Conceptual Primer,"This article is a primer on concept extrapolation - the ability to take a
concept, a feature, or a goal that is defined in one context and extrapolate it
safely to a more general context. Concept extrapolation aims to solve model
splintering - a ubiquitous occurrence wherein the features or concepts shift as
the world changes over time. Through discussing value splintering and value
extrapolation the article argues that concept extrapolation is necessary for
Artificial Intelligence alignment."
Learning and evolution: factors influencing an effective combination,"The mutual relationship between evolution and learning is a controversial
argument among the artificial intelligence and neuro-evolution communities.
After more than three decades, there is still no common agreement on the
matter. In this paper the author investigates whether combining learning and
evolution permits to find better solutions than those discovered by evolution
alone. More specifically, the author presents a series of empirical studies
that highlight some specific conditions determining the success of such a
combination, like the introduction of noise during the learning and selection
processes. Results are obtained in two qualitatively different domains, where
agent/environment interactions are minimal or absent."
Prompt to GPT-3: Step-by-Step Thinking Instructions for Humor Generation,"Artificial intelligence has made significant progress in natural language
processing, with models like GPT-3 demonstrating impressive capabilities.
However, these models still have limitations when it comes to complex tasks
that require an understanding of the user, such as mastering human comedy
writing strategies. This paper explores humor generation using GPT-3 by
modeling human comedy writing theory and leveraging step-by-step thinking
instructions. In addition, we explore the role of cognitive distance in
creating humor."
"Homological Neural Networks: A Sparse Architecture for Multivariate
  Complexity","The rapid progress of Artificial Intelligence research came with the
development of increasingly complex deep learning models, leading to growing
challenges in terms of computational complexity, energy efficiency and
interpretability. In this study, we apply advanced network-based information
filtering techniques to design a novel deep neural network unit characterized
by a sparse higher-order graphical architecture built over the homological
structure of underlying data. We demonstrate its effectiveness in two
application domains which are traditionally challenging for deep learning:
tabular data and time series regression problems. Results demonstrate the
advantages of this novel design which can tie or overcome the results of
state-of-the-art machine learning and deep learning models using only a
fraction of parameters."
Why machines do not understand: A response to Sgaard,"Some defenders of so-called `artificial intelligence' believe that machines
can understand language. In particular, S{\o}gaard has argued in this journal
for a thesis of this sort, on the basis of the idea (1) that where there is
semantics there is also understanding and (2) that machines are not only
capable of what he calls `inferential semantics', but even that they can (with
the help of inputs from sensors) `learn' referential semantics
\parencite{sogaard:2022}. We show that he goes wrong because he pays
insufficient attention to the difference between language as used by humans and
the sequences of inert of symbols which arise when language is stored on hard
drives or in books in libraries."
Explainability is NOT a Game,"Explainable artificial intelligence (XAI) aims to help human decision-makers
in understanding complex machine learning (ML) models. One of the hallmarks of
XAI are measures of relative feature importance, which are theoretically
justified through the use of Shapley values. This paper builds on recent work
and offers a simple argument for why Shapley values can provide misleading
measures of relative feature importance, by assigning more importance to
features that are irrelevant for a prediction, and assigning less importance to
features that are relevant for a prediction. The significance of these results
is that they effectively challenge the many proposed uses of measures of
relative feature importance in a fast-growing range of high-stakes application
domains."
AI empowering research: 10 ways how science can benefit from AI,"This article explores the transformative impact of artificial intelligence
(AI) on scientific research. It highlights ten ways in which AI is
revolutionizing the work of scientists, including powerful referencing tools,
improved understanding of research problems, enhanced research question
generation, optimized research design, stub data generation, data
transformation, advanced data analysis, and AI-assisted reporting. While AI
offers numerous benefits, challenges such as bias, privacy concerns, and the
need for human-AI collaboration must be considered. The article emphasizes that
AI can augment human creativity in science but not replace it."
"AI and Education: An Investigation into the Use of ChatGPT for Systems
  Thinking","This exploratory study investigates the potential of the artificial
intelligence tool, ChatGPT, to support systems thinking (ST) in various
subjects. Using both general and subject specific prompts, the study assesses
the accuracy, helpfulness, and reliability of ChatGPT's responses across
different versions of the tool. The results indicate that ChatGPT can provide
largely correct and very helpful responses in various subjects, demonstrating
its potential as a tool for enhancing ST skills. However, occasional
inaccuracies highlight the need for users to remain critical of ChatGPT's
responses. Despite some limitations, this study suggests that with careful use
and attention to its idiosyncrasies, ChatGPT can be a valuable tool for
teaching and learning ST."
"Artificial intelligence-aided protein engineering: from topological data
  analysis to deep protein language models","Protein engineering is an emerging field in biotechnology that has the
potential to revolutionize various areas, such as antibody design, drug
discovery, food security, ecology, and more. However, the mutational space
involved is too vast to be handled through experimental means alone. Leveraging
accumulative protein databases, machine learning (ML) models, particularly
those based on natural language processing (NLP), have considerably expedited
protein engineering. Moreover, advances in topological data analysis (TDA) and
artificial intelligence-based protein structure prediction, such as AlphaFold2,
have made more powerful structure-based ML-assisted protein engineering
strategies possible. This review aims to offer a comprehensive, systematic, and
indispensable set of methodological components, including TDA and NLP, for
protein engineering and to facilitate their future development."
"User-centric AIGC products: Explainable Artificial Intelligence and AIGC
  products","Generative AI tools, such as ChatGPT and Midjourney, are transforming
artistic creation as AI-art integration advances. However, Artificial
Intelligence Generated Content (AIGC) tools face user experience challenges,
necessitating a human-centric design approach. This paper offers a brief
overview of research on explainable AI (XAI) and user experience, examining
factors leading to suboptimal experiences with AIGC tools. Our proposed
solution integrates interpretable AI methodologies into the input and
adjustment feedback stages of AIGC products. We underscore XAI's potential to
enhance the user experience for ordinary users and present a conceptual
framework for improving AIGC user experience."
Artificial intelligence-driven antimicrobial peptide discovery,"Antimicrobial peptides (AMPs) emerge as promising agents against
antimicrobial resistance, providing an alternative to conventional antibiotics.
Artificial intelligence (AI) revolutionized AMP discovery through both
discrimination and generation approaches. The discriminators aid the
identification of promising candidates by predicting key peptide properties
such as activity and toxicity, while the generators learn the distribution over
peptides and enable sampling novel AMP candidates, either de novo, or as
analogues of a prototype peptide. Moreover, the controlled generation of AMPs
with desired properties is achieved by discriminator-guided filtering,
positive-only learning, latent space sampling, as well as conditional and
optimized generation. Here we review recent achievements in AI-driven AMP
discovery, highlighting the most exciting directions."
Can We Rely on AI?,"Over the last decade, adversarial attack algorithms have revealed
instabilities in deep learning tools. These algorithms raise issues regarding
safety, reliability and interpretability in artificial intelligence; especially
in high risk settings. From a practical perspective, there has been a war of
escalation between those developing attack and defence strategies. At a more
theoretical level, researchers have also studied bigger picture questions
concerning the existence and computability of attacks. Here we give a brief
overview of the topic, focusing on aspects that are likely to be of interest to
researchers in applied and computational mathematics."
Ensemble of Counterfactual Explainers,"In eXplainable Artificial Intelligence (XAI), several counterfactual
explainers have been proposed, each focusing on some desirable properties of
counterfactual instances: minimality, actionability, stability, diversity,
plausibility, discriminative power. We propose an ensemble of counterfactual
explainers that boosts weak explainers, which provide only a subset of such
properties, to a powerful method covering all of them. The ensemble runs weak
explainers on a sample of instances and of features, and it combines their
results by exploiting a diversity-driven selection function. The method is
model-agnostic and, through a wrapping approach based on autoencoders, it is
also data-agnostic."
Explanations for Answer Set Programming,"The paper presents an enhancement of xASP, a system that generates
explanation graphs for Answer Set Programming (ASP). Different from xASP, the
new system, xASP2, supports different clingo constructs like the choice rules,
the constraints, and the aggregates such as #sum, #min. This work formalizes
and presents an explainable artificial intelligence system for a broad fragment
of ASP, capable of shrinking as much as possible the set of assumptions and
presenting explanations in terms of directed acyclic graphs."
Leveraging Learning Metrics for Improved Federated Learning,"Currently in the federated setting, no learning schemes leverage the emerging
research of explainable artificial intelligence (XAI) in particular the novel
learning metrics that help determine how well a model is learning. One of these
novel learning metrics is termed `Effective Rank' (ER) which measures the
Shannon Entropy of the singular values of a matrix, thus enabling a metric
determining how well a layer is mapping. By joining federated learning and the
learning metric, effective rank, this work will \textbf{(1)} give the first
federated learning metric aggregation method \textbf{(2)} show that effective
rank is well-suited to federated problems by out-performing baseline Federated
Averaging \cite{konevcny2016federated} and \textbf{(3)} develop a novel
weight-aggregation scheme relying on effective rank."
Provably safe systems: the only path to controllable AGI,"We describe a path to humanity safely thriving with powerful Artificial
General Intelligences (AGIs) by building them to provably satisfy
human-specified requirements. We argue that this will soon be technically
feasible using advanced AI for formal verification and mechanistic
interpretability. We further argue that it is the only path which guarantees
safe controlled AGI. We end with a list of challenge problems whose solution
would contribute to this positive outcome and invite readers to join in this
work."
On the Injunction of XAIxArt,"The position paper highlights the range of concerns that are engulfed in the
injunction of explainable artificial intelligence in art (XAIxArt). Through a
series of quick sub-questions, it points towards the ambiguities concerning
'explanation' and the postpositivist tradition of 'relevant explanation'.
Rejecting both 'explanation' and 'relevant explanation', the paper takes a
stance that XAIxArt is a symptom of insecurity of the anthropocentric notion of
art and a nostalgic desire to return to outmoded notions of authorship and
human agency. To justify this stance, the paper makes a distinction between an
ornamentation model of explanation to a model of explanation as sense-making."
"When to Trust AI: Advances and Challenges for Certification of Neural
  Networks","Artificial intelligence (AI) has been advancing at a fast pace and it is now
poised for deployment in a wide range of applications, such as autonomous
systems, medical diagnosis and natural language processing. Early adoption of
AI technology for real-world applications has not been without problems,
particularly for neural networks, which may be unstable and susceptible to
adversarial examples. In the longer term, appropriate safety assurance
techniques need to be developed to reduce potential harm due to avoidable
system failures and ensure trustworthiness. Focusing on certification and
explainability, this paper provides an overview of techniques that have been
developed to ensure safety of AI decisions and discusses future challenges."
Leveraging Diversity in Online Interactions,"This paper addresses the issue of connecting people online to help them find
support with their day-to-day problems. We make use of declarative norms for
mediating online interactions, and we specifically focus on the issue of
leveraging diversity when connecting people. We run pilots at different
university sites, and the results show relative success in the diversity of the
selected profiles, backed by high user satisfaction."
The E.U.'s Artificial Intelligence Act: An Ordoliberal Assessment,"In light of the rise of generative AI and recent debates about the
socio-political implications of large-language models and chatbots, this
article investigates the E.U.'s Artificial Intelligence Act (AIA), the world's
first major attempt by a government body to address and mitigate the
potentially negative impacts of AI technologies. The article critically
analyzes the AIA from a distinct economic ethics perspective, i.e.,
ordoliberalism 2.0 - a perspective currently lacking in the academic
literature. It evaluates, in particular, the AIA's ordoliberal strengths and
weaknesses and proposes reform measures that could be taken to strengthen the
AIA."
Mapping AI Arguments in Journalism Studies,"This study investigates and suggests typologies for examining Artificial
Intelligence (AI) within the domains of journalism and mass communication
research. We aim to elucidate the seven distinct subfields of AI, which
encompass machine learning, natural language processing (NLP), speech
recognition, expert systems, planning, scheduling, optimization, robotics, and
computer vision, through the provision of concrete examples and practical
applications. The primary objective is to devise a structured framework that
can help AI researchers in the field of journalism. By comprehending the
operational principles of each subfield, scholars can enhance their ability to
focus on a specific facet when analyzing a particular research topic."
CoinRun: Solving Goal Misgeneralisation,"Goal misgeneralisation is a key challenge in AI alignment -- the task of
getting powerful Artificial Intelligences to align their goals with human
intentions and human morality. In this paper, we show how the ACE (Algorithm
for Concept Extrapolation) agent can solve one of the key standard challenges
in goal misgeneralisation: the CoinRun challenge. It uses no new reward
information in the new environment. This points to how autonomous agents could
be trusted to act in human interests, even in novel and critical situations."
Refutation of Shapley Values for XAI -- Additional Evidence,"Recent work demonstrated the inadequacy of Shapley values for explainable
artificial intelligence (XAI). Although to disprove a theory a single
counterexample suffices, a possible criticism of earlier work is that the focus
was solely on Boolean classifiers. To address such possible criticism, this
paper demonstrates the inadequacy of Shapley values for families of classifiers
where features are not boolean, but also for families of classifiers for which
multiple classes can be picked. Furthermore, the paper shows that the features
changed in any minimal $l_0$ distance adversarial examples do not include
irrelevant features, thus offering further arguments regarding the inadequacy
of Shapley values for XAI."
"Generative AI May Prefer to Present National-level Characteristics of
  Cities Based on Stereotypical Geographic Impressions at the Continental Level","A simple experiment was conducted to test the ability of the Chinese-based
generative artificial intelligence (AI) platform, Wenxin Yige, to render images
of urban street views of different countries. The study found that images
generated by this AI platform may contain continental-level stereotypes in
terms of showing the level of economic development and modernization. Street
view images generated from Wenxin Yige do not adequately represent the diverse
range of urban landscapes found across different nations. Using these generated
images for geography education or outreach initiatives could inadvertently
strengthen people's existing stereotypical views about individual countries."
Brave new world: Artificial Intelligence in teaching and learning,"We exemplify how Large Language Models are used in both teaching and
learning. We also discuss the AI incidents that have already occurred in the
education domain, and we argue for the urgent need to introduce AI policies in
universities and for the ongoing strategies to regulate AI. Regarding policy
for AI, our view is that each institution should have a policy for AI in
teaching and learning. This is important from at least twofolds: (i) to raise
awareness on the numerous educational tools that can both positively and
negatively affect education; (ii) to minimise the risk of AI incidents in
education."
An Ontology of Co-Creative AI Systems,"The term co-creativity has been used to describe a wide variety of human-AI
assemblages in which human and AI are both involved in a creative endeavor. In
order to assist with disambiguating research efforts, we present an ontology of
co-creative systems, focusing on how responsibilities are divided between human
and AI system and the information exchanged between them. We extend Lubart's
original ontology of creativity support tools with three new categories
emphasizing artificial intelligence: computer-as-subcontractor,
computer-as-critic, and computer-as-teammate, some of which have
sub-categorizations."
A Case-Based Persistent Memory for a Large Language Model,"Case-based reasoning (CBR) as a methodology for problem-solving can use any
appropriate computational technique. This position paper argues that CBR
researchers have somewhat overlooked recent developments in deep learning and
large language models (LLMs). The underlying technical developments that have
enabled the recent breakthroughs in AI have strong synergies with CBR and could
be used to provide a persistent memory for LLMs to make progress towards
Artificial General Intelligence."
Online Learning and Planning in Cognitive Hierarchies,"Complex robot behaviour typically requires the integration of multiple
robotic and Artificial Intelligence (AI) techniques and components. Integrating
such disparate components into a coherent system, while also ensuring global
properties and behaviours, is a significant challenge for cognitive robotics.
Using a formal framework to model the interactions between components can be an
important step in dealing with this challenge. In this paper we extend an
existing formal framework [Clark et al., 2016] to model complex integrated
reasoning behaviours of robotic systems; from symbolic planning through to
online learning of policies and transition systems. Furthermore the new
framework allows for a more flexible modelling of the interactions between
different reasoning components."
"A Survey of the Various Methodologies Towards making Artificial
  Intelligence More Explainable","Machines are being increasingly used in decision-making processes, resulting
in the realization that decisions need explanations. Unfortunately, an
increasing number of these deployed models are of a 'black-box' nature where
the reasoning behind the decisions is unknown. Hence, there is a need for
clarity behind the reasoning of these decisions. As humans, we would want these
decisions to be presented to us in an explainable manner. However, explanations
alone are insufficient. They do not necessarily tell us how to achieve an
outcome but merely tell us what achieves the given outcome. For this reason, my
research focuses on explainability/interpretability and how it extends to
counterfactual thinking."
"Explainable AI for Earth Observation: Current Methods, Open Challenges,
  and Opportunities","Deep learning has taken by storm all fields involved in data analysis,
including remote sensing for Earth observation. However, despite significant
advances in terms of performance, its lack of explainability and
interpretability, inherent to neural networks in general since their inception,
remains a major source of criticism. Hence it comes as no surprise that the
expansion of deep learning methods in remote sensing is being accompanied by
increasingly intensive efforts oriented towards addressing this drawback
through the exploration of a wide spectrum of Explainable Artificial
Intelligence techniques. This chapter, organized according to prominent Earth
observation application fields, presents a panorama of the state-of-the-art in
explainable remote sensing image analysis."
"Conversational Data Exploration: A Game-Changer for Designing Data
  Science Pipelines","This paper proposes a conversational approach implemented by the system
Chatin for driving an intuitive data exploration experience. Our work aims to
unlock the full potential of data analytics and artificial intelligence with a
new generation of data science solutions. Chatin is a cutting-edge tool that
democratises access to AI-driven solutions, empowering non-technical users from
various disciplines to explore data and extract knowledge from it."
"Leveraging AI for Natural Disaster Management : Takeaways From The
  Moroccan Earthquake","The devastating 6.8-magnitude earthquake in Al Haouz, Morocco in 2023
prompted critical reflections on global disaster management strategies,
resulting in a post-disaster hackathon, using artificial intelligence (AI) to
improve disaster preparedness, response, and recovery. This paper provides (i)
a comprehensive literature review, (ii) an overview of winning projects, (iii)
key insights and challenges, namely real-time open-source data, data scarcity,
and interdisciplinary collaboration barriers, and (iv) a community-call for
further action."
"The Cybersecurity Crisis of Artificial Intelligence: Unrestrained
  Adoption and Natural Language-Based Attacks","The widespread integration of autoregressive-large language models (AR-LLMs),
such as ChatGPT, across established applications, like search engines, has
introduced critical vulnerabilities with uniquely scalable characteristics. In
this commentary, we analyse these vulnerabilities, their dependence on natural
language as a vector of attack, and their challenges to cybersecurity best
practices. We offer recommendations designed to mitigate these challenges."
DesignGPT: Multi-Agent Collaboration in Design,"Generative AI faces many challenges when entering the product design
workflow, such as interface usability and interaction patterns. Therefore,
based on design thinking and design process, we developed the DesignGPT
multi-agent collaboration framework, which uses artificial intelligence agents
to simulate the roles of different positions in the design company and allows
human designers to collaborate with them in natural language. Experimental
results show that compared with separate AI tools, DesignGPT improves the
performance of designers, highlighting the potential of applying multi-agent
systems that integrate design domain knowledge to product scheme design."
The Rise of Creative Machines: Exploring the Impact of Generative AI,"This study looks at how generative artificial intelligence (AI) can
revolutionize marketing, product development, and research. It discusses the
latest developments in the field, easy-to-use resources, and moral and social
hazards. In addition to addressing mitigating techniques for issues like
prejudice and disinformation, the debate emphasizes the significance of
responsible development through continual stakeholder communication and ethical
principles."
"Evolutionary game theory: the mathematics of evolution and collective
  behaviours","This brief discusses evolutionary game theory as a powerful and unified
mathematical tool to study evolution of collective behaviours. It summarises
some of my recent research directions using evolutionary game theory methods,
which include i) the analysis of statistical properties of the number of
(stable) equilibria in a random evolutionary game, and ii) the modelling of
safety behaviours' evolution and the risk posed by advanced Artificial
Intelligence technologies in a technology development race. Finally, it
includes an outlook and some suggestions for future researchers."
"Combating the ""Sameness"" in AI Art: Reflections on the Interactive AI
  Installation Fencing Hallucination","The article summarizes three types of ""sameness"" issues in Artificial
Intelligence(AI) art, each occurring at different stages of development in AI
image creation tools. Through the Fencing Hallucination project, the article
reflects on the design of AI art production in alleviating the sense of
uniformity, maintaining the uniqueness of images from an AI image synthesizer,
and enhancing the connection between the artworks and the audience. This paper
endeavors to stimulate the creation of distinctive AI art by recounting the
efforts and insights derived from the Fencing Hallucination project, all
dedicated to addressing the issue of ""sameness""."
"DQSSA: A Quantum-Inspired Solution for Maximizing Influence in Online
  Social Networks (Student Abstract)","Influence Maximization is the task of selecting optimal nodes maximising the
influence spread in social networks. This study proposes a Discretized
Quantum-based Salp Swarm Algorithm (DQSSA) for optimizing influence diffusion
in social networks. By discretizing meta-heuristic algorithms and infusing them
with quantum-inspired enhancements, we address issues like premature
convergence and low efficacy. The proposed method, guided by quantum
principles, offers a promising solution for Influence Maximisation. Experiments
on four real-world datasets reveal DQSSA's superior performance as compared to
established cutting-edge algorithms."
"From Big to Small Without Losing It All: Text Augmentation with ChatGPT
  for Efficient Sentiment Analysis","In the era of artificial intelligence, data is gold but costly to annotate.
The paper demonstrates a groundbreaking solution to this dilemma using ChatGPT
for text augmentation in sentiment analysis. We leverage ChatGPT's generative
capabilities to create synthetic training data that significantly improves the
performance of smaller models, making them competitive with, or even
outperforming, their larger counterparts. This innovation enables models to be
both efficient and effective, thereby reducing computational cost, inference
time, and memory usage without compromising on quality. Our work marks a key
advancement in the cost-effective development and deployment of robust
sentiment analysis models."
AI Competitions and Benchmarks: Competition platforms,"The ecosystem of artificial intelligence competitions is a diverse and
multifaceted landscape, encompassing a variety of platforms that each host
numerous competitions annually, alongside a plethora of specialized websites
dedicated to singular contests. These platforms adeptly manage the overarching
administrative responsibilities inherent in orchestrating competitions, thus
affording organizers the liberty to allocate greater attention to other facets
of their contests. Notably, these platforms exhibit considerable diversity in
their operational functionalities, economic models, and community dynamics.
This chapter conducts an extensive review of the foremost services in this
realm and elucidates several alternative methodologies that facilitate the
independent hosting of such challenges. Keywords: competition platform,
challenge hosting services, comparison."
The Logic of Doxastic Strategies,"In many real-world situations, there is often not enough information to know
that a certain strategy will succeed in achieving the goal, but there is a good
reason to believe that it will. The paper introduces the term ``doxastic'' for
such strategies.
  The main technical contribution is a sound and complete logical system that
describes the interplay between doxastic strategy and belief modalities."
Manifold-based Shapley for SAR Recognization Network Explanation,"Explainable artificial intelligence (XAI) holds immense significance in
enhancing the deep neural network's transparency and credibility, particularly
in some risky and high-cost scenarios, like synthetic aperture radar (SAR).
Shapley is a game-based explanation technique with robust mathematical
foundations. However, Shapley assumes that model's features are independent,
rendering Shapley explanation invalid for high dimensional models. This study
introduces a manifold-based Shapley method by projecting high-dimensional
features into low-dimensional manifold features and subsequently obtaining
Fusion-Shap, which aims at (1) addressing the issue of erroneous explanations
encountered by traditional Shap; (2) resolving the challenge of
interpretability that traditional Shap faces in complex scenarios."
WISE: full-Waveform variational Inference via Subsurface Extensions,"We introduce a probabilistic technique for full-waveform inversion, employing
variational inference and conditional normalizing flows to quantify uncertainty
in migration-velocity models and its impact on imaging. Our approach integrates
generative artificial intelligence with physics-informed common-image gathers,
reducing reliance on accurate initial velocity models. Considered case studies
demonstrate its efficacy producing realizations of migration-velocity models
conditioned by the data. These models are used to quantify amplitude and
positioning effects during subsequent imaging."
"Revolutionizing Pharma: Unveiling the AI and LLM Trends in the
  Pharmaceutical Industry","This document offers a critical overview of the emerging trends and
significant advancements in artificial intelligence (AI) within the
pharmaceutical industry. Detailing its application across key operational
areas, including research and development, animal testing, clinical trials,
hospital clinical stages, production, regulatory affairs, quality control and
other supporting areas, the paper categorically examines AI's role in each
sector. Special emphasis is placed on cutting-edge AI technologies like machine
learning algorithms and their contributions to various aspects of
pharmaceutical operations. Through this comprehensive analysis, the paper
highlights the transformative potential of AI in reshaping the pharmaceutical
industry's future."
"Balancing the AI Strength of Roles in Self-Play Training with Regret
  Matching+","When training artificial intelligence for games encompassing multiple roles,
the development of a generalized model capable of controlling any character
within the game presents a viable option. This strategy not only conserves
computational resources and time during the training phase but also reduces
resource requirements during deployment. training such a generalized model
often encounters challenges related to uneven capabilities when controlling
different roles. A simple method is introduced based on Regret Matching+, which
facilitates a more balanced performance of strength by the model when
controlling various roles."
"Empowering Machines to Think Like Chemists: Unveiling Molecular
  Structure-Polarity Relationships with Hierarchical Symbolic Regression","Thin-layer chromatography (TLC) is a crucial technique in molecular polarity
analysis. Despite its importance, the interpretability of predictive models for
TLC, especially those driven by artificial intelligence, remains a challenge.
Current approaches, utilizing either high-dimensional molecular fingerprints or
domain-knowledge-driven feature engineering, often face a dilemma between
expressiveness and interpretability. To bridge this gap, we introduce
Unsupervised Hierarchical Symbolic Regression (UHiSR), combining hierarchical
neural networks and symbolic regression. UHiSR automatically distills
chemical-intuitive polarity indices, and discovers interpretable equations that
link molecular structure to chromatographic behavior."
"NormEnsembleXAI: Unveiling the Strengths and Weaknesses of XAI Ensemble
  Techniques","This paper presents a comprehensive comparative analysis of explainable
artificial intelligence (XAI) ensembling methods. Our research brings three
significant contributions. Firstly, we introduce a novel ensembling method,
NormEnsembleXAI, that leverages minimum, maximum, and average functions in
conjunction with normalization techniques to enhance interpretability.
Secondly, we offer insights into the strengths and weaknesses of XAI ensemble
methods. Lastly, we provide a library, facilitating the practical
implementation of XAI ensembling, thus promoting the adoption of transparent
and interpretable deep learning models."
"Linguistically Communicating Uncertainty in Patient-Facing Risk
  Prediction Models","This paper addresses the unique challenges associated with uncertainty
quantification in AI models when applied to patient-facing contexts within
healthcare. Unlike traditional eXplainable Artificial Intelligence (XAI)
methods tailored for model developers or domain experts, additional
considerations of communicating in natural language, its presentation and
evaluating understandability are necessary. We identify the challenges in
communication model performance, confidence, reasoning and unknown knowns using
natural language in the context of risk prediction. We propose a design aimed
at addressing these challenges, focusing on the specific application of
in-vitro fertilisation outcome prediction."
"Applications, challenges and ethical issues of AI and ChatGPT in
  education","Artificial Intelligence (AI) in recent years has shown an unprecedentedly
impressive development, tending to play a catalytic role in all aspects of
life. The interest of the academic community, but also of governments, is huge
in the dynamics of AI and is reflected by the truly explosive amount of
investment and research that is underway. Enthusiastic opinions and statements
about AI are made every day, but at the same time they also bring to the fore
alarming predictions about its effects. This paper aims to describe the
opportunities emerging from the use of artificial intelligence and ChatGPT to
improve education, but also to identify the challenges and ethical issues that
arise."
"A privacy-preserving, distributed and cooperative FCM-based learning
  approach for cancer research","Distributed Artificial Intelligence is attracting interest day by day. In
this paper, the authors introduce an innovative methodology for distributed
learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a
privacy-preserving way. The authors design a training scheme for collaborative
FCM learning that offers data privacy compliant with the current regulation.
This method is applied to a cancer detection problem, proving that the
performance of the model is improved by the Federated Learning process, and
obtaining similar results to the ones that can be found in the literature."
Learning causation event conjunction sequences,"This is an examination of some methods that learn causations in event
sequences. A causation is defined as a conjunction of one or more cause events
occurring in an arbitrary order, with possible intervening non-causal events,
that lead to an effect. The methods include recurrent and non-recurrent
artificial neural networks (ANNs), as well as a histogram-based algorithm. An
attention recurrent ANN performed the best of the ANNs, while the histogram
algorithm was significantly superior to all the ANNs."
Deepfake Detection and the Impact of Limited Computing Capabilities,"The rapid development of technologies and artificial intelligence makes
deepfakes an increasingly sophisticated and challenging-to-identify technique.
To ensure the accuracy of information and control misinformation and mass
manipulation, it is of paramount importance to discover and develop artificial
intelligence models that enable the generic detection of forged videos. This
work aims to address the detection of deepfakes across various existing
datasets in a scenario with limited computing resources. The goal is to analyze
the applicability of different deep learning techniques under these
restrictions and explore possible approaches to enhance their efficiency."
When Should Algorithms Resign? A Proposal for AI Governance,"Algorithmic resignation is a strategic approach for managing the use of
artificial intelligence (AI) by embedding governance directly into AI systems.
It involves deliberate and informed disengagement from AI, such as restricting
access AI outputs or displaying performance disclaimers, in specific scenarios
to aid the appropriate and effective use of AI. By integrating algorithmic
resignation as a governance mechanism, organizations can better control when
and how AI is used, balancing the benefits of automation with the need for
human oversight."
"Generative AI for Synthetic Data Generation: Methods, Challenges and the
  Future","The recent surge in research focused on generating synthetic data from large
language models (LLMs), especially for scenarios with limited data
availability, marks a notable shift in Generative Artificial Intelligence (AI).
Their ability to perform comparably to real-world data positions this approach
as a compelling solution to low-resource challenges. This paper delves into
advanced technologies that leverage these gigantic LLMs for the generation of
task-specific training data. We outline methodologies, evaluation techniques,
and practical applications, discuss the current limitations, and suggest
potential pathways for future research."
Probabilistic Neural Circuits,"Probabilistic circuits (PCs) have gained prominence in recent years as a
versatile framework for discussing probabilistic models that support tractable
queries and are yet expressive enough to model complex probability
distributions. Nevertheless, tractability comes at a cost: PCs are less
expressive than neural networks. In this paper we introduce probabilistic
neural circuits (PNCs), which strike a balance between PCs and neural nets in
terms of tractability and expressive power. Theoretically, we show that PNCs
can be interpreted as deep mixtures of Bayesian networks. Experimentally, we
demonstrate that PNCs constitute powerful function approximators."
LookALike: Human Mimicry based collaborative decision making,"Artificial General Intelligence falls short when communicating role specific
nuances to other systems. This is more pronounced when building autonomous LLM
agents capable and designed to communicate with each other for real world
problem solving. Humans can communicate context and domain specific nuances
along with knowledge, and that has led to refinement of skills. In this work we
propose and evaluate a novel method that leads to knowledge distillation among
LLM agents leading to realtime human role play preserving unique contexts
without relying on any stored data or pretraining. We also evaluate how our
system performs better in simulated real world tasks compared to state of the
art."
Circular Belief Propagation for Approximate Probabilistic Inference,"Belief Propagation (BP) is a simple probabilistic inference algorithm,
consisting of passing messages between nodes of a graph representing a
probability distribution. Its analogy with a neural network suggests that it
could have far-ranging applications for neuroscience and artificial
intelligence. Unfortunately, it is only exact when applied to cycle-free
graphs, which restricts the potential of the algorithm. In this paper, we
propose Circular Belief Propagation (CBP), an extension of BP which limits the
detrimental effects of message reverberation caused by cycles by learning to
detect and cancel spurious correlations and belief amplifications. We show in
numerical experiments involving binary probabilistic graphs that CBP far
outperforms BP and reaches good performance compared to that of previously
proposed algorithms."
"How Human-Centered Explainable AI Interface Are Designed and Evaluated:
  A Systematic Survey","Despite its technological breakthroughs, eXplainable Artificial Intelligence
(XAI) research has limited success in producing the {\em effective
explanations} needed by users. In order to improve XAI systems' usability,
practical interpretability, and efficacy for real users, the emerging area of
{\em Explainable Interfaces} (EIs) focuses on the user interface and user
experience design aspects of XAI. This paper presents a systematic survey of 53
publications to identify current trends in human-XAI interaction and promising
directions for EI design and development. This is among the first systematic
survey of EI research."
Bridging Generative Networks with the Common Model of Cognition,"This article presents a theoretical framework for adapting the Common Model
of Cognition to large generative network models within the field of artificial
intelligence. This can be accomplished by restructuring modules within the
Common Model into shadow production systems that are peripheral to a central
production system, which handles higher-level reasoning based on the shadow
productions' output. Implementing this novel structure within the Common Model
allows for a seamless connection between cognitive architectures and generative
neural networks."
"Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep
  Learning: A Bayesian Deep Learning Approach","With recent advancements in the development of artificial intelligence
applications using theories and algorithms in machine learning, many accurate
models can be created to train and predict on given datasets. With the
realization of the importance of imaging interpretation in cancer diagnosis,
this article aims to investigate the theory behind Deep Learning and Bayesian
Network prediction models. Based on the advantages and drawbacks of each model,
different approaches will be used to construct a Bayesian Deep Learning Model,
combining the strengths while minimizing the weaknesses. Finally, the
applications and accuracy of the resulting Bayesian Deep Learning approach in
the health industry in classifying images will be analyzed."
"AI Act and Large Language Models (LLMs): When critical issues and
  privacy impact require human and ethical oversight","The imposing evolution of artificial intelligence systems and, specifically,
of Large Language Models (LLM) makes it necessary to carry out assessments of
their level of risk and the impact they may have in the area of privacy,
personal data protection and at an ethical level, especially on the weakest and
most vulnerable. This contribution addresses human oversight, ethical
oversight, and privacy impact assessment."
"Privacy-Enhancing Technologies for Artificial Intelligence-Enabled
  Systems","Artificial intelligence (AI) models introduce privacy vulnerabilities to
systems. These vulnerabilities may impact model owners or system users; they
exist during model development, deployment, and inference phases, and threats
can be internal or external to the system. In this paper, we investigate
potential threats and propose the use of several privacy-enhancing technologies
(PETs) to defend AI-enabled systems. We then provide a framework for PETs
evaluation for a AI-enabled systems and discuss the impact PETs may have on
system-level variables."
"Artificial Intelligence enhanced Security Problems in Real-Time Scenario
  using Blowfish Algorithm","In a nutshell, ""the cloud"" refers to a collection of interconnected computing
resources made possible by an extensive, real-time communication network like
the internet. Because of its potential to reduce processing costs, the emerging
paradigm of cloud computing has recently attracted a large number of academics.
The exponential expansion of cloud computing has made the rapid expansion of
cloud services very remarkable. Ensuring the security of personal information
in today's interconnected world is no easy task. These days, security is really
crucial. Models of security that are relevant to cloud computing include
confidentiality, authenticity, accessibility, data integrity, and recovery.
Using the Hybrid Encryption this study, we cover all the security issues and
leaks in cloud infrastructure."
"Hybrid LLM/Rule-based Approaches to Business Insights Generation from
  Structured Data","In the field of business data analysis, the ability to extract actionable
insights from vast and varied datasets is essential for informed
decision-making and maintaining a competitive edge. Traditional rule-based
systems, while reliable, often fall short when faced with the complexity and
dynamism of modern business data. Conversely, Artificial Intelligence (AI)
models, particularly Large Language Models (LLMs), offer significant potential
in pattern recognition and predictive analytics but can lack the precision
necessary for specific business applications. This paper explores the efficacy
of hybrid approaches that integrate the robustness of rule-based systems with
the adaptive power of LLMs in generating actionable business insights."
Knowledge Graph Completion using Structural and Textual Embeddings,"Knowledge Graphs (KGs) are widely employed in artificial intelligence
applications, such as question-answering and recommendation systems. However,
KGs are frequently found to be incomplete. While much of the existing
literature focuses on predicting missing nodes for given incomplete KG triples,
there remains an opportunity to complete KGs by exploring relations between
existing nodes, a task known as relation prediction. In this study, we propose
a relations prediction model that harnesses both textual and structural
information within KGs. Our approach integrates walks-based embeddings with
language model embeddings to effectively represent nodes. We demonstrate that
our model achieves competitive results in the relation prediction task when
evaluated on a widely used dataset."
"Can ChatGPT Make Explanatory Inferences? Benchmarks for Abductive
  Reasoning","Explanatory inference is the creation and evaluation of hypotheses that
provide explanations, and is sometimes known as abduction or abductive
inference. Generative AI is a new set of artificial intelligence models based
on novel algorithms for generating text, images, and sounds. This paper
proposes a set of benchmarks for assessing the ability of AI programs to
perform explanatory inference, and uses them to determine the extent to which
ChatGPT, a leading generative AI model, is capable of making explanatory
inferences. Tests on the benchmarks reveal that ChatGPT performs creative and
evaluative inferences in many domains, although it is limited to verbal and
visual modalities. Claims that ChatGPT and similar models are incapable of
explanation, understanding, causal reasoning, meaning, and creativity are
rebutted."
"Detection of ransomware attacks using federated learning based on the
  CNN model","Computing is still under a significant threat from ransomware, which
necessitates prompt action to prevent it. Ransomware attacks can have a
negative impact on how smart grids, particularly digital substations. In
addition to examining a ransomware detection method using artificial
intelligence (AI), this paper offers a ransomware attack modeling technique
that targets the disrupted operation of a digital substation. The first, binary
data is transformed into image data and fed into the convolution neural network
model using federated learning. The experimental findings demonstrate that the
suggested technique detects ransomware with a high accuracy rate."
Stable Diffusion Dataset Generation for Downstream Classification Tasks,"Recent advances in generative artificial intelligence have enabled the
creation of high-quality synthetic data that closely mimics real-world data.
This paper explores the adaptation of the Stable Diffusion 2.0 model for
generating synthetic datasets, using Transfer Learning, Fine-Tuning and
generation parameter optimisation techniques to improve the utility of the
dataset for downstream classification tasks. We present a class-conditional
version of the model that exploits a Class-Encoder and optimisation of key
generation parameters. Our methodology led to synthetic datasets that, in a
third of cases, produced models that outperformed those trained on real
datasets."
"Predicting the usability of mobile applications using AI tools: the rise
  of large user interface models, opportunities, and challenges","This article proposes the so-called large user interface models (LUIMs) to
enable the generation of user interfaces and prediction of usability using
artificial intelligence in the context of mobile applications."
LLMs for XAI: Future Directions for Explaining Explanations,"In response to the demand for Explainable Artificial Intelligence (XAI), we
investigate the use of Large Language Models (LLMs) to transform ML
explanations into natural, human-readable narratives. Rather than directly
explaining ML models using LLMs, we focus on refining explanations computed
using existing XAI algorithms. We outline several research directions,
including defining evaluation metrics, prompt design, comparing LLM models,
exploring further training methods, and integrating external data. Initial
experiments and user study suggest that LLMs offer a promising way to enhance
the interpretability and usability of XAI."
"ChatGPTest: opportunities and cautionary tales of utilizing AI for
  questionnaire pretesting","The rapid advancements in generative artificial intelligence have opened up
new avenues for enhancing various aspects of research, including the design and
evaluation of survey questionnaires. However, the recent pioneering
applications have not considered questionnaire pretesting. This article
explores the use of GPT models as a useful tool for pretesting survey
questionnaires, particularly in the early stages of survey design. Illustrated
with two applications, the article suggests incorporating GPT feedback as an
additional stage before human pretesting, potentially reducing successive
iterations. The article also emphasizes the indispensable role of researchers'
judgment in interpreting and implementing AI-generated feedback."
"LLM-Augmented Agent-Based Modelling for Social Simulations: Challenges
  and Opportunities","As large language models (LLMs) continue to make significant strides, their
better integration into agent-based simulations offers a transformational
potential for understanding complex social systems. However, such integration
is not trivial and poses numerous challenges. Based on this observation, in
this paper, we explore architectures and methods to systematically develop
LLM-augmented social simulations and discuss potential research directions in
this field. We conclude that integrating LLMs with agent-based simulations
offers a powerful toolset for researchers and scientists, allowing for more
nuanced, realistic, and comprehensive models of complex systems and human
behaviours."
Declarative Design of Neural Predicates in Neuro-Symbolic Systems,"Neuro-symbolic systems (NeSy), which claim to combine the best of both
learning and reasoning capabilities of artificial intelligence, are missing a
core property of reasoning systems: Declarativeness. The lack of
declarativeness is caused by the functional nature of neural predicates
inherited from neural networks. We propose and implement a general framework
for fully declarative neural predicates, which hence extends to fully
declarative NeSy frameworks. We first show that the declarative extension
preserves the learning and reasoning capabilities while being able to answer
arbitrary queries while only being trained on a single query type."
Reinforcement learning,"Observing celestial objects and advancing our scientific knowledge about them
involves tedious planning, scheduling, data collection and data
post-processing. Many of these operational aspects of astronomy are guided and
executed by expert astronomers. Reinforcement learning is a mechanism where we
(as humans and astronomers) can teach agents of artificial intelligence to
perform some of these tedious tasks. In this paper, we will present a state of
the art overview of reinforcement learning and how it can benefit astronomy."
"Sociotechnical Implications of Generative Artificial Intelligence for
  Information Access","Robust access to trustworthy information is a critical need for society with
implications for knowledge production, public health education, and promoting
informed citizenry in democratic societies. Generative AI technologies may
enable new ways to access information and improve effectiveness of existing
information retrieval systems but we are only starting to understand and
grapple with their long-term social implications. In this chapter, we present
an overview of some of the systemic consequences and risks of employing
generative AI in the context of information access. We also provide
recommendations for evaluation and mitigation, and discuss challenges for
future research."
Musical Phrase Segmentation via Grammatical Induction,"We outline a solution to the challenge of musical phrase segmentation that
uses grammatical induction algorithms, a class of algorithms which infer a
context-free grammar from an input sequence. We analyze the performance of five
grammatical induction algorithms on three datasets using various musical
viewpoint combinations. Our experiments show that the LONGESTFIRST algorithm
achieves the best F1 scores across all three datasets and that input encodings
that include the duration viewpoint result in the best performance."
Why Reinforcement Learning in Energy Systems Needs Explanations,"With economic development, the complexity of infrastructure has increased
drastically. Similarly, with the shift from fossil fuels to renewable sources
of energy, there is a dire need for such systems that not only predict and
forecast with accuracy but also help in understanding the process of
predictions. Artificial intelligence and machine learning techniques have
helped in finding out wellperforming solutions to different problems in the
energy sector. However, the usage of state-of-the-art techniques like
reinforcement learning is not surprisingly convincing. This paper discusses the
application of reinforcement techniques in energy systems and how explanations
of these models can be helpful"
Probabilities of Causation for Continuous and Vector Variables,"Probabilities of causation (PoC) are valuable concepts for explainable
artificial intelligence and practical decision-making. PoC are originally
defined for scalar binary variables. In this paper, we extend the concept of
PoC to continuous treatment and outcome variables, and further generalize PoC
to capture causal effects between multiple treatments and multiple outcomes. In
addition, we consider PoC for a sub-population and PoC with multi-hypothetical
terms to capture more sophisticated counterfactual information useful for
decision-making. We provide a nonparametric identification theorem for each
type of PoC we introduce. Finally, we illustrate the application of our results
on a real-world dataset about education."
Learning to Play 7 Wonders Duel Without Human Supervision,"This paper introduces ZeusAI, an artificial intelligence system developed to
play the board game 7 Wonders Duel. Inspired by the AlphaZero reinforcement
learning algorithm, ZeusAI relies on a combination of Monte Carlo Tree Search
and a Transformer Neural Network to learn the game without human supervision.
ZeusAI competes at the level of top human players, develops both known and
novel strategies, and allows us to test rule variants to improve the game's
balance. This work demonstrates how AI can help in understanding and enhancing
board games."
The Impact of AI on Academic Research and Publishing,"Generative artificial intelligence (AI) technologies like ChatGPT, have
significantly impacted academic writing and publishing through their ability to
generate content at levels comparable to or surpassing human writers. Through a
review of recent interdisciplinary literature, this paper examines ethical
considerations surrounding the integration of AI into academia, focusing on the
potential for this technology to be used for scholarly misconduct and necessary
oversight when using it for writing, editing, and reviewing of scholarly
papers. The findings highlight the need for collaborative approaches to AI
usage among publishers, editors, reviewers, and authors to ensure that this
technology is used ethically and productively."
Informatics & dairy industry coalition: AI trends and present challenges,"Artificial Intelligence (AI) can potentially transform the industry,
enhancing the production process and minimizing manual, repetitive tasks.
Accordingly, the synergy between high-performance computing and powerful
mathematical models enables the application of sophisticated data analysis
procedures like Machine Learning. However, challenges exist regarding
effective, efficient, and flexible processing to generate valuable knowledge.
Consequently, this work comprehensively describes industrial challenges where
AI can be exploited, focusing on the dairy industry. The conclusions presented
can help researchers apply novel approaches for cattle monitoring and farmers
by proposing advanced technological solutions to their needs."
Spatio-Temporal Graphical Counterfactuals: An Overview,"Counterfactual thinking is a critical yet challenging topic for artificial
intelligence to learn knowledge from data and ultimately improve their
performances for new scenarios. Many research works, including Potential
Outcome Model and Structural Causal Model, have been proposed to realize it.
However, their modelings, theoretical foundations and application approaches
are usually different. Moreover, there is a lack of graphical approach to infer
spatio-temporal counterfactuals, that considers spatial and temporal
interactions between multiple units. Thus, in this work, our aim is to
investigate a survey to compare and discuss different counterfactual models,
theories and approaches, and further build a unified graphical causal
frameworks to infer the spatio-temporal counterfactuals."
"Towards Auto-Building of Embedded FPGA-based Soft Sensors for Wastewater
  Flow Estimation","Executing flow estimation using Deep Learning (DL)-based soft sensors on
resource-limited IoT devices has demonstrated promise in terms of reliability
and energy efficiency. However, its application in the field of wastewater flow
estimation remains underexplored due to: (1) a lack of available datasets, (2)
inconvenient toolchains for on-device AI model development and deployment, and
(3) hardware platforms designed for general DL purposes rather than being
optimized for energy-efficient soft sensor applications. This study addresses
these gaps by proposing an automated, end-to-end solution for wastewater flow
estimation using a prototype IoT device."
Machine Apophenia: The Kaleidoscopic Generation of Architectural Images,"This study investigates the application of generative artificial intelligence
in architectural design. We present a novel methodology that combines multiple
neural networks to create an unsupervised and unmoderated stream of unique
architectural images. Our approach is grounded in the conceptual framework
called machine apophenia. We hypothesize that neural networks, trained on
diverse human-generated data, internalize aesthetic preferences and tend to
produce coherent designs even from random inputs. The methodology involves an
iterative process of image generation, description, and refinement, resulting
in captioned architectural postcards automatically shared on several social
media platforms. Evaluation and ablation studies show the improvement both in
technical and aesthetic metrics of resulting images on each step."
"Have We Reached AGI? Comparing ChatGPT, Claude, and Gemini to Human
  Literacy and Education Benchmarks","Recent advancements in AI, particularly in large language models (LLMs) like
ChatGPT, Claude, and Gemini, have prompted questions about their proximity to
Artificial General Intelligence (AGI). This study compares LLM performance on
educational benchmarks with Americans' average educational attainment and
literacy levels, using data from the U.S. Census Bureau and technical reports.
Results show that LLMs significantly outperform human benchmarks in tasks such
as undergraduate knowledge and advanced reading comprehension, indicating
substantial progress toward AGI. However, true AGI requires broader cognitive
assessments. The study highlights the implications for AI development,
education, and societal impact, emphasizing the need for ongoing research and
ethical considerations."
"A Scale-Invariant Diagnostic Approach Towards Understanding Dynamics of
  Deep Neural Networks","This paper introduces a scale-invariant methodology employing \textit{Fractal
Geometry} to analyze and explain the nonlinear dynamics of complex
connectionist systems. By leveraging architectural self-similarity in Deep
Neural Networks (DNNs), we quantify fractal dimensions and \textit{roughness}
to deeply understand their dynamics and enhance the quality of
\textit{intrinsic} explanations. Our approach integrates principles from Chaos
Theory to improve visualizations of fractal evolution and utilizes a
Graph-Based Neural Network for reconstructing network topology. This strategy
aims at advancing the \textit{intrinsic} explainability of connectionist
Artificial Intelligence (AI) systems."
"On the Combination of AI and Wireless Technologies: 3GPP Standardization
  Progress","Combing Artificial Intelligence (AI) and wireless communication technologies
has become one of the major technologies trends towards 2030. This includes
using AI to improve the efficiency of the wireless transmission and supporting
AI deployment with wireless networks. In this article, the latest progress of
the Third Generation Partnership Project (3GPP) standards development is
introduced. Concentrating on AI model distributed transfer and AI for Beam
Management (BM) with wireless network, we introduce the latest studies and
explain how the existing standards should be modified to incorporate the
results from academia."
"Reporting Risks in AI-based Assistive Technology Research: A Systematic
  Review","Artificial Intelligence (AI) is increasingly employed to enhance assistive
technologies, yet it can fail in various ways. We conducted a systematic
literature review of research into AI-based assistive technology for persons
with visual impairments. Our study shows that most proposed technologies with a
testable prototype have not been evaluated in a human study with members of the
sight-loss community. Furthermore, many studies did not consider or report
failure cases or possible risks. These findings highlight the importance of
inclusive system evaluations and the necessity of standardizing methods for
presenting and analyzing failure cases and threats when developing AI-based
assistive technologies."
Beyond Trend Following: Deep Learning for Market Trend Prediction,"Trend following and momentum investing are common strategies employed by
asset managers. Even though they can be helpful in the proper situations, they
are limited in the sense that they work just by looking at past, as if we were
driving with our focus on the rearview mirror. In this paper, we advocate for
the use of Artificial Intelligence and Machine Learning techniques to predict
future market trends. These predictions, when done properly, can improve the
performance of asset managers by increasing returns and reducing drawdowns."
Algebraic anti-unification,"Abstraction is key to human and artificial intelligence as it allows one to
see common structure in otherwise distinct objects or situations and as such it
is a key element for generality in AI. Anti-unification (or generalization) is
\textit{the} part of theoretical computer science and AI studying abstraction.
It has been successfully applied to various AI-related problems, most
importantly inductive logic programming. Up to this date, anti-unification is
studied only from a syntactic perspective in the literature. The purpose of
this paper is to initiate an algebraic (i.e. semantic) theory of
anti-unification within general algebras. This is motivated by recent
applications to similarity and analogical proportions."
"Assessing the role of clinical summarization and patient chart review
  within communications, medical management, and diagnostics","Effective summarization of unstructured patient data in electronic health
records (EHRs) is crucial for accurate diagnosis and efficient patient care,
yet clinicians often struggle with information overload and time constraints.
This review dives into recent literature and case studies on both the
significant impacts and outstanding issues of patient chart review on
communications, diagnostics, and management. It also discusses recent efforts
to integrate artificial intelligence (AI) into clinical summarization tasks,
and its transformative impact on the clinician's potential, including but not
limited to reductions of administrative burden and improved patient-centered
care."
AI in Remote Patient Monitoring,"The rapid evolution of Artificial Intelligence (AI) has significantly
transformed healthcare, particularly in the domain of Remote Patient Monitoring
(RPM). This chapter explores the integration of AI in RPM, highlighting
real-life applications, system architectures, and the benefits it brings to
patient care and healthcare systems. Through a comprehensive analysis of
current technologies, methodologies, and case studies, I present a detailed
overview of how AI enhances monitoring accuracy, predictive analytics, and
personalized treatment plans. The chapter also discusses the challenges and
future directions in this field, providing a comprehensive view of AI's role in
revolutionizing remote patient care."
Generative AI Augmented Induction-based Formal Verification,"Generative Artificial Intelligence (GenAI) has demonstrated its capabilities
in the present world that reduce human effort significantly. It utilizes deep
learning techniques to create original and realistic content in terms of text,
images, code, music, and video. Researchers have also shown the capabilities of
modern Large Language Models (LLMs) used by GenAI models that can be used to
aid hardware development. Formal verification is a mathematical-based proof
method used to exhaustively verify the correctness of a design. In this paper,
we demonstrate how GenAI can be used in induction-based formal verification to
increase the verification throughput."
A Survey of AI Reliance,"Artificial intelligence (AI) systems have become an indispensable component
of modern technology. However, research on human behavioral responses is
lagging behind, i.e., the research into human reliance on AI advice (AI
reliance). Current shortcomings in the literature include the unclear
influences on AI reliance, lack of external validity, conflicting approaches to
measuring reliance, and disregard for a change in reliance over time. Promising
avenues for future research include reliance on generative AI output and
reliance in multi-user situations. In conclusion, we present a morphological
box that serves as a guide for research on AI reliance."
Criticizing Ethics According to Artificial Intelligence,"This article presents a critique of ethics in the context of artificial
intelligence (AI). It argues for the need to question established patterns of
thought and traditional authorities, including core concepts such as autonomy,
morality, and ethics. These concepts are increasingly inadequate to deal with
the complexities introduced by emerging AI and autonomous agents. This critique
has several key components: clarifying conceptual ambiguities, honestly
addressing epistemic issues, and thoroughly exploring fundamental normative
problems. The ultimate goal is to reevaluate and possibly redefine some
traditional ethical concepts to better address the challenges posed by AI."
"MindGPT: Advancing Human-AI Interaction with Non-Invasive fNIRS-Based
  Imagined Speech Decoding","In the coming decade, artificial intelligence systems are set to
revolutionise every industry and facet of human life. Building communication
systems that enable seamless and symbiotic communication between humans and AI
agents is increasingly important. This research advances the field of human-AI
interaction by developing an innovative approach to decode imagined speech
using non-invasive high-density functional near-infrared spectroscopy (fNIRS).
Notably, this study introduces MindGPT, the first thought-to-LLM (large
language model) system in the world."
Integrative Approaches in Cybersecurity and AI,"In recent years, the convergence of cybersecurity, artificial intelligence
(AI), and data management has emerged as a critical area of research, driven by
the increasing complexity and interdependence of modern technological
ecosystems. This paper provides a comprehensive review and analysis of
integrative approaches that harness AI techniques to enhance cybersecurity
frameworks and optimize data management practices. By exploring the synergies
between these domains, we identify key trends, challenges, and future
directions that hold the potential to revolutionize the way organizations
protect, analyze, and leverage their data. Our findings highlight the necessity
of cross-disciplinary strategies that incorporate AI-driven automation,
real-time threat detection, and advanced data analytics to build more resilient
and adaptive security architectures."
LLMs can Schedule,"The job shop scheduling problem (JSSP) remains a significant hurdle in
optimizing production processes. This challenge involves efficiently allocating
jobs to a limited number of machines while minimizing factors like total
processing time or job delays. While recent advancements in artificial
intelligence have yielded promising solutions, such as reinforcement learning
and graph neural networks, this paper explores the potential of Large Language
Models (LLMs) for JSSP. We introduce the very first supervised 120k dataset
specifically designed to train LLMs for JSSP. Surprisingly, our findings
demonstrate that LLM-based scheduling can achieve performance comparable to
other neural approaches. Furthermore, we propose a sampling method that
enhances the effectiveness of LLMs in tackling JSSP."
NFDI4DSO: Towards a BFO Compliant Ontology for Data Science,"The NFDI4DataScience (NFDI4DS) project aims to enhance the accessibility and
interoperability of research data within Data Science (DS) and Artificial
Intelligence (AI) by connecting digital artifacts and ensuring they adhere to
FAIR (Findable, Accessible, Interoperable, and Reusable) principles. To this
end, this poster introduces the NFDI4DS Ontology, which describes resources in
DS and AI and models the structure of the NFDI4DS consortium. Built upon the
NFDICore ontology and mapped to the Basic Formal Ontology (BFO), this ontology
serves as the foundation for the NFDI4DS knowledge graph currently under
development."
Imprecise Belief Fusion Facing a DST benchmark problem,"When we merge information in Dempster-Shafer Theory (DST), we are faced with
anomalous behavior: agents with equal expertise and credibility can have their
opinion disregarded after resorting to the belief combination rule of this
theory. This problem is interesting because belief fusion is an inherent part
of dealing with situations where available information is imprecise, as often
occurs in Artificial Intelligence. We managed to identify an isomorphism betwin
the DST formal apparatus into that of a Probabilistic Logic. Thus, we solved
the problematic inputs affair by replacing the DST combination rule with a new
fusion process aiming at eliminating anomalies proposed by that rule. We apply
the new fusion method to the DST paradox Problem."
Architectural Foundations for the Large Language Model Infrastructures,"The development of a large language model (LLM) infrastructure is a pivotal
undertaking in artificial intelligence. This paper explores the intricate
landscape of LLM infrastructure, software, and data management. By analyzing
these core components, we emphasize the pivotal considerations and safeguards
crucial for successful LLM development. This work presents a concise synthesis
of the challenges and strategies inherent in constructing a robust and
effective LLM infrastructure, offering valuable insights for researchers and
practitioners alike."
"Contextual Importance and Utility in Python: New Functionality and
  Insights with the py-ciu Package","The availability of easy-to-use and reliable software implementations is
important for allowing researchers in academia and industry to test, assess and
take into use eXplainable AI (XAI) methods. This paper describes the
\texttt{py-ciu} Python implementation of the Contextual Importance and Utility
(CIU) model-agnostic, post-hoc explanation method and illustrates capabilities
of CIU that go beyond the current state-of-the-art that could be useful for XAI
practitioners in general."
"What Machine Learning Tells Us About the Mathematical Structure of
  Concepts","This paper examines the connections among various approaches to understanding
concepts in philosophy, cognitive science, and machine learning, with a
particular focus on their mathematical nature. By categorizing these approaches
into Abstractionism, the Similarity Approach, the Functional Approach, and the
Invariance Approach, the study highlights how each framework provides a
distinct mathematical perspective for modeling concepts. The synthesis of these
approaches bridges philosophical theories and contemporary machine learning
models, providing a comprehensive framework for future research. This work
emphasizes the importance of interdisciplinary dialogue, aiming to enrich our
understanding of the complex relationship between human cognition and
artificial intelligence."
Ethical AI Governance: Methods for Evaluating Trustworthy AI,"Trustworthy Artificial Intelligence (TAI) integrates ethics that align with
human values, looking at their influence on AI behaviour and decision-making.
Primarily dependent on self-assessment, TAI evaluation aims to ensure ethical
standards and safety in AI development and usage. This paper reviews the
current TAI evaluation methods in the literature and offers a classification,
contributing to understanding self-assessment methods in this field."
"Mitigating Dimensionality in 2D Rectangle Packing Problem under
  Reinforcement Learning Schema","This paper explores the application of Reinforcement Learning (RL) to the
two-dimensional rectangular packing problem. We propose a reduced
representation of the state and action spaces that allow us for high
granularity. Leveraging UNet architecture and Proximal Policy Optimization
(PPO), we achieved a model that is comparable to the MaxRect heuristic.
However, our approach has great potential to be generalized to nonrectangular
packing problems and complex constraints."
Causal Discovery in Recommender Systems: Example and Discussion,"Causality is receiving increasing attention by the artificial intelligence
and machine learning communities. This paper gives an example of modelling a
recommender system problem using causal graphs. Specifically, we approached the
causal discovery task to learn a causal graph by combining observational data
from an open-source dataset with prior knowledge. The resulting causal graph
shows that only a few variables effectively influence the analysed feedback
signals. This contrasts with the recent trend in the machine learning community
to include more and more variables in massive models, such as neural networks."
Multi-agent Path Finding in Continuous Environment,"We address a variant of multi-agent path finding in continuous environment
(CE-MAPF), where agents move along sets of smooth curves. Collisions between
agents are resolved via avoidance in the space domain. A new Continuous
Environment Conflict-Based Search (CE-CBS) algorithm is proposed in this work.
CE-CBS combines conflict-based search (CBS) for the high-level search framework
with RRT* for low-level path planning. The CE-CBS algorithm is tested under
various settings on diverse CE-MAPF instances. Experimental results show that
CE-CBS is competitive w.r.t. to other algorithms that consider continuous
aspect in MAPF such as MAPF with continuous time."
"CaBRNet, an open-source library for developing and evaluating Case-Based
  Reasoning Models","In the field of explainable AI, a vibrant effort is dedicated to the design
of self-explainable models, as a more principled alternative to post-hoc
methods that attempt to explain the decisions after a model opaquely makes
them. However, this productive line of research suffers from common downsides:
lack of reproducibility, unfeasible comparison, diverging standards. In this
paper, we propose CaBRNet, an open-source, modular, backward-compatible
framework for Case-Based Reasoning Networks:
https://github.com/aiser-team/cabrnet."
"Entropy, concentration, and learning: a statistical mechanics primer","Artificial intelligence models trained through loss minimization have
demonstrated significant success, grounded in principles from fields like
information theory and statistical physics. This work explores these
established connections through the lens of statistical mechanics, starting
from first-principles sample concentration behaviors that underpin AI and
machine learning. Our development of statistical mechanics for modeling
highlights the key role of exponential families, and quantities of statistics,
physics, and information theory."
Identification and Mitigating Bias in Quantum Machine Learning,"As quantum machine learning (QML) emerges as a promising field at the
intersection of quantum computing and artificial intelligence, it becomes
crucial to address the biases and challenges that arise from the unique nature
of quantum systems. This research includes work on identification, diagnosis,
and response to biases in Quantum Machine Learning. This paper aims to provide
an overview of three key topics: How does bias unique to Quantum Machine
Learning look? Why and how can it occur? What can and should be done about it?"
Model Comparisons: XNet Outperforms KAN,"In the fields of computational mathematics and artificial intelligence, the
need for precise data modeling is crucial, especially for predictive machine
learning tasks. This paper explores further XNet, a novel algorithm that
employs the complex-valued Cauchy integral formula, offering a superior network
architecture that surpasses traditional Multi-Layer Perceptrons (MLPs) and
Kolmogorov-Arnold Networks (KANs). XNet significant improves speed and accuracy
across various tasks in both low and high-dimensional spaces, redefining the
scope of data-driven model development and providing substantial improvements
over established time series models like LSTMs."
Label-Free Subjective Player Experience Modelling via Let's Play Videos,"Player Experience Modelling (PEM) is the study of AI techniques applied to
modelling a player's experience within a video game. PEM development can be
labour-intensive, requiring expert hand-authoring or specialized data
collection. In this work, we propose a novel PEM development approach,
approximating player experience from gameplay video. We evaluate this approach
predicting affect in the game Angry Birds via a human subject study. We
validate that our PEM can strongly correlate with self-reported and sensor
measures of affect, demonstrating the potential of this approach."
A Survey on Offensive AI Within Cybersecurity,"Artificial Intelligence (AI) has witnessed major growth and integration
across various domains. As AI systems become increasingly prevalent, they also
become targets for threat actors to manipulate their functionality for
malicious purposes. This survey paper on offensive AI will comprehensively
cover various aspects related to attacks against and using AI systems. It will
delve into the impact of offensive AI practices on different domains, including
consumer, enterprise, and public digital infrastructure. The paper will explore
adversarial machine learning, attacks against AI models, infrastructure, and
interfaces, along with offensive techniques like information gathering, social
engineering, and weaponized AI. Additionally, it will discuss the consequences
and implications of offensive AI, presenting case studies, insights, and
avenues for further research."
Towards Assuring EU AI Act Compliance and Adversarial Robustness of LLMs,"Large language models are prone to misuse and vulnerable to security threats,
raising significant safety and security concerns. The European Union's
Artificial Intelligence Act seeks to enforce AI robustness in certain contexts,
but faces implementation challenges due to the lack of standards, complexity of
LLMs and emerging security vulnerabilities. Our research introduces a framework
using ontologies, assurance cases, and factsheets to support engineers and
stakeholders in understanding and documenting AI system compliance and security
regarding adversarial robustness. This approach aims to ensure that LLMs adhere
to regulatory standards and are equipped to counter potential threats."
Causal Abstraction in Model Interpretability: A Compact Survey,"The pursuit of interpretable artificial intelligence has led to significant
advancements in the development of methods that aim to explain the
decision-making processes of complex models, such as deep learning systems.
Among these methods, causal abstraction stands out as a theoretical framework
that provides a principled approach to understanding and explaining the causal
mechanisms underlying model behavior. This survey paper delves into the realm
of causal abstraction, examining its theoretical foundations, practical
applications, and implications for the field of model interpretability."
Generative AI for Accessible and Inclusive Extended Reality,"Artificial Intelligence-Generated Content (AIGC) has the potential to
transform how people build and interact with virtual environments. Within this
paper, we discuss potential benefits but also challenges that AIGC has for the
creation of inclusive and accessible virtual environments. Specifically, we
touch upon the decreased need for 3D modeling expertise, benefits of
symbolic-only as well as multimodal input, 3D content editing, and 3D model
accessibility as well as foundation model-specific challenges."
"Semi-Strongly solved: a New Definition Leading Computer to Perfect
  Gameplay","Solving combinatorial games has been a classic research topic in artificial
intelligence because solutions can offer essential information to improve
gameplay. Several definitions exist for `solving the game,' but they are
markedly different regarding computational cost and the detail of insights
derived. In this study, we introduce a novel definition called `semi-strongly
solved' and propose an algorithm to achieve this type of solution efficiently.
This new definition addresses existing gaps because of its intermediate
computational cost and the quality of the solution. To demonstrate the
potential of our approach, we derive the theoretical computational complexity
of our algorithm under a simple condition, and apply it to semi-strongly solve
the game of 6x6 Othello. This study raises many new research goals in this
research area."
"Optimizing Gastrointestinal Diagnostics: A CNN-Based Model for VCE Image
  Classification","In recent years, the diagnosis of gastrointestinal (GI) diseases has advanced
greatly with the advent of high-tech video capsule endoscopy (VCE) technology,
which allows for non-invasive observation of the digestive system. The MisaHub
Capsule Vision Challenge encourages the development of vendor-independent
artificial intelligence models that can autonomously classify GI anomalies from
VCE images. This paper presents CNN architecture designed specifically for
multiclass classification of ten gut pathologies, including angioectasia,
bleeding, erosion, erythema, foreign bodies, lymphangiectasia, polyps, ulcers,
and worms as well as their normal state."
"Machine Learning Innovations in CPR: A Comprehensive Survey on Enhanced
  Resuscitation Techniques","This survey paper explores the transformative role of Machine Learning (ML)
and Artificial Intelligence (AI) in Cardiopulmonary Resuscitation (CPR). It
examines the evolution from traditional CPR methods to innovative ML-driven
approaches, highlighting the impact of predictive modeling, AI-enhanced
devices, and real-time data analysis in improving resuscitation outcomes. The
paper provides a comprehensive overview, classification, and critical analysis
of current applications, challenges, and future directions in this emerging
field."
Causal Responsibility Attribution for Human-AI Collaboration,"As Artificial Intelligence (AI) systems increasingly influence
decision-making across various fields, the need to attribute responsibility for
undesirable outcomes has become essential, though complicated by the complex
interplay between humans and AI. Existing attribution methods based on actual
causality and Shapley values tend to disproportionately blame agents who
contribute more to an outcome and rely on real-world measures of
blameworthiness that may misalign with responsible AI standards. This paper
presents a causal framework using Structural Causal Models (SCMs) to
systematically attribute responsibility in human-AI systems, measuring overall
blameworthiness while employing counterfactual reasoning to account for agents'
expected epistemic levels. Two case studies illustrate the framework's
adaptability in diverse human-AI collaboration scenarios."
We Urgently Need Intrinsically Kind Machines,"Artificial Intelligence systems are rapidly evolving, integrating extrinsic
and intrinsic motivations. While these frameworks offer benefits, they risk
misalignment at the algorithmic level while appearing superficially aligned
with human values. In this paper, we argue that an intrinsic motivation for
kindness is crucial for making sure these models are intrinsically aligned with
human values. We argue that kindness, defined as a form of altruism motivated
to maximize the reward of others, can counteract any intrinsic motivations that
might lead the model to prioritize itself over human well-being. Our approach
introduces a framework and algorithm for embedding kindness into foundation
models by simulating conversations. Limitations and future research directions
for scalable implementation are discussed."
"GuidelineGuard: An Agentic Framework for Medical Note Evaluation with
  Guideline Adherence","Although rapid advancements in Large Language Models (LLMs) are facilitating
the integration of artificial intelligence-based applications and services in
healthcare, limited research has focused on the systematic evaluation of
medical notes for guideline adherence. This paper introduces GuidelineGuard, an
agentic framework powered by LLMs that autonomously analyzes medical notes,
such as hospital discharge and office visit notes, to ensure compliance with
established healthcare guidelines. By identifying deviations from recommended
practices and providing evidence-based suggestions, GuidelineGuard helps
clinicians adhere to the latest standards from organizations like the WHO and
CDC. This framework offers a novel approach to improving documentation quality
and reducing clinical errors."
Communication Compression for Tensor Parallel LLM Inference,"Large Language Models (LLMs) have pushed the frontier of artificial
intelligence but are comprised of hundreds of billions of parameters and
operations. For faster inference latency, LLMs are deployed on multiple
hardware accelerators through various Model Parallelism strategies. Our paper
looks into the details on one such strategy - Tensor Parallel - and proposes to
reduce latency by compressing inter-accelerator communication. We leverage fine
grained quantization techniques to compress selected activations by 3.5 - 4.5x.
Our proposed method leads up to 2x reduction of time-to-first-token (TTFT) with
negligible model performance degradation."
"Evolution of IVR building techniques: from code writing to AI-powered
  automation","Interactive Voice Response (IVR) systems have undergone significant
transformation in recent years, moving from traditional code-based development
to more user-friendly approaches leveraging widgets and, most recently,
harnessing the power of Artificial Intelligence (AI) for automated IVR flow
creation. This paper explores the evolution of IVR building techniques,
highlighting the industry's revolution and shaping the future of IVR systems.
The authors delve into the historical context, current trends, and future
prospects of IVR development, elucidating the impact of AI on simplifying IVR
creation processes and enhancing customer experiences."
"Establishing Minimum Elements for Effective Vulnerability Management in
  AI Software","In the rapidly evolving field of artificial intelligence (AI), the
identification, documentation, and mitigation of vulnerabilities are paramount
to ensuring robust and secure systems. This paper discusses the minimum
elements for AI vulnerability management and the establishment of an Artificial
Intelligence Vulnerability Database (AIVD). It presents standardized formats
and protocols for disclosing, analyzing, cataloging, and documenting AI
vulnerabilities. It discusses how such an AI incident database must extend
beyond the traditional scope of vulnerabilities by focusing on the unique
aspects of AI systems. Additionally, this paper highlights challenges and gaps
in AI Vulnerability Management, including the need for new severity scores,
weakness enumeration systems, and comprehensive mitigation strategies
specifically designed to address the multifaceted nature of AI vulnerabilities."
"Unlocking the Future: A Cloud-Based Artificial Intelligence Access
  Control System","Traditional access control systems, such as key cards, PIN pads, and physical
keys, face challenges in scalability, security, and user experience in today's
digital world. We present a cloud-based entry system using Raspberry Pi
hardware and Amazon Web Services (AWS) technologies like Lambda, Simple Storage
Service (S3), and Rekognition. This solution (AWSecure Entry System) enhances
security, streamlines authentication, and increases operational efficiency."
"The importance of visual modelling languages in generative software
  engineering","Multimodal GPTs represent a watershed in the interplay between Software
Engineering and Generative Artificial Intelligence. GPT-4 accepts image and
text inputs, rather than simply natural language. We investigate relevant use
cases stemming from these enhanced capabilities of GPT-4. To the best of our
knowledge, no other work has investigated similar use cases involving Software
Engineering tasks carried out via multimodal GPTs prompted with a mix of
diagrams and natural language."
"Strategic Application of AIGC for UAV Trajectory Design: A Channel
  Knowledge Map Approach","Unmanned Aerial Vehicles (UAVs) are increasingly utilized in wireless
communication, yet accurate channel loss prediction remains a significant
challenge, limiting resource optimization performance. To address this issue,
this paper leverages Artificial Intelligence Generated Content (AIGC) for the
efficient construction of Channel Knowledge Maps (CKM) and UAV trajectory
design. Given the time-consuming nature of channel data collection, AI
techniques are employed in a Wasserstein Generative Adversarial Network (WGAN)
to extract environmental features and augment the data. Experiment results
demonstrate the effectiveness of the proposed framework in improving CKM
construction accuracy. Moreover, integrating CKM into UAV trajectory planning
reduces channel gain uncertainty, demonstrating its potential to enhance
wireless communication efficiency."
Artificial Intelligence Policy Framework for Institutions,"Artificial intelligence (AI) has transformed various sectors and
institutions, including education and healthcare. Although AI offers immense
potential for innovation and problem solving, its integration also raises
significant ethical concerns, such as privacy and bias. This paper delves into
key considerations for developing AI policies within institutions. We explore
the importance of interpretability and explainability in AI elements, as well
as the need to mitigate biases and ensure privacy. Additionally, we discuss the
environmental impact of AI and the importance of energy-efficient practices.
The culmination of these important components is centralized in a generalized
framework to be utilized for institutions developing their AI policy. By
addressing these critical factors, institutions can harness the power of AI
while safeguarding ethical principles."
NeSyA: Neurosymbolic Automata,"Neurosymbolic Artificial Intelligence (NeSy) has emerged as a promising
direction to integrate low level perception with high level reasoning.
Unfortunately, little attention has been given to developing NeSy systems
tailored to temporal/sequential problems. This entails reasoning symbolically
over sequences of subsymbolic observations towards a target prediction. We show
that using a probabilistic semantics symbolic automata, which combine the power
of automata for temporal structure specification with that of propositional
logic, can be used to reason efficiently and differentiably over subsymbolic
sequences. The proposed system, which we call NeSyA (Neuro Symbolic Automata),
is shown to either scale or perform better than existing NeSy approaches when
applied to problems with a temporal component."
Temporal Numeric Planning with Patterns,"We consider temporal numeric planning problems $\Pi$ expressed in PDDL2.1
level 3, and show how to produce SMT formulas $(i)$ whose models correspond to
valid plans of $\Pi$, and $(ii)$ that extend the recently proposed planning
with patterns approach from the numeric to the temporal case. We prove the
correctness and completeness of the approach and show that it performs very
well on 10 domains with required concurrency."
Uncommon Belief in Rationality,"Common knowledge/belief in rationality is the traditional standard assumption
in analysing interaction among agents. This paper proposes a graph-based
language for capturing significantly more complicated structures of
higher-order beliefs that agents might have about the rationality of the other
agents. The two main contributions are a solution concept that captures the
reasoning process based on a given belief structure and an efficient algorithm
for compressing any belief structure into a unique minimal form."
"Recursive Aggregates as Intensional Functions in Answer Set Programming:
  Semantics and Strong Equivalence","This paper shows that the semantics of programs with aggregates implemented
by the solvers clingo and dlv can be characterized as extended First-Order
formulas with intensional functions in the logic of Here-and-There.
Furthermore, this characterization can be used to study the strong equivalence
of programs with aggregates under either semantics. We also present a
transformation that reduces the task of checking strong equivalence to
reasoning in classical First-Order logic, which serves as a foundation for
automating this procedure."
"Advances in Artificial Intelligence forDiabetes Prediction: Insights
  from a Systematic Literature Review","This systematic review explores the use of machine learning (ML) in
predicting diabetes, focusing on datasets, algorithms, training methods, and
evaluation metrics. It examines datasets like the Singapore National Diabetic
Retinopathy Screening program, REPLACE-BG, National Health and Nutrition
Examination Survey, and Pima Indians Diabetes Database. The review assesses the
performance of ML algorithms like CNN, SVM, Logistic Regression, and XGBoost in
predicting diabetes outcomes. The study emphasizes the importance of
interdisciplinary collaboration and ethical considerations in ML-based diabetes
prediction models."
"Deep Reinforcement Learning Based Systems for Safety Critical
  Applications in Aerospace","Recent advancements in artificial intelligence (AI) applications within
aerospace have demonstrated substantial growth, particularly in the context of
control systems. As High Performance Computing (HPC) platforms continue to
evolve, they are expected to replace current flight control or engine control
computers, enabling increased computational capabilities. This shift will allow
real-time AI applications, such as image processing and defect detection, to be
seamlessly integrated into monitoring systems, providing real-time awareness
and enhanced fault detection and accommodation. Furthermore, AI's potential in
aerospace extends to control systems, where its application can range from full
autonomy to enhancing human control through assistive features. AI,
particularly deep reinforcement learning (DRL), can offer significant
improvements in control systems, whether for autonomous operation or as an
augmentative tool."
AIGCodeSet: A New Annotated Dataset for AI Generated Code Detection,"While large language models provide significant convenience for software
development, they can lead to ethical issues in job interviews and student
assignments. Therefore, determining whether a piece of code is written by a
human or generated by an artificial intelligence (AI) model is a critical
issue. In this study, we present AIGCodeSet, which consists of 2.828
AI-generated and 4.755 human-written Python codes, created using CodeLlama 34B,
Codestral 22B, and Gemini 1.5 Flash. In addition, we share the results of our
experiments conducted with baseline detection methods. Our experiments show
that a Bayesian classifier outperforms the other models."
"Developing a custom GPT based on Inquiry Based Learning for Physics
  Teachers","Generative Artificial Intelligence (GenAI) has emerged as a valuable
assistant in many fields such as marketing, finance, project management, and
education. In education, many GenAI tools have been developed to aid teachers
in preparing proper educational material and offering personalized learning to
their students, tailored to their educational needs. In this paper, we present
a custom GPT (IBL Educator GPT) that is designed and developed based on
Inquiry-based Learning and offers physics teachers a framework in which they
can interact with ChatGPT and design educational strategies. The utilization of
the IBL Educator GPT has led to an improvement in teachers' perspectives
regarding the adoption of artificial intelligence-based tools for personalizing
teaching."
"Rethinking IDE Customization for Enhanced HAX: A Hyperdimensional
  Perspective","As Integrated Development Environments (IDEs) increasingly integrate
Artificial Intelligence, Software Engineering faces both benefits like
productivity gains and challenges like mismatched user preferences. We propose
Hyper-Dimensional (HD) vector spaces to model Human-Computer Interaction,
focusing on user actions, stylistic preferences, and project context. These
contributions aim to inspire further research on applying HD computing in IDE
design."
"Self-Adaptive ERP: Embedding NLP into Petri-Net creation and Model
  Matching","Enterprise Resource Planning (ERP) consultants play a vital role in
customizing systems to meet specific business needs by processing large amounts
of data and adapting functionalities. However, the process is
resource-intensive, time-consuming, and requires continuous adjustments as
business demands evolve. This research introduces a Self-Adaptive ERP Framework
that automates customization using enterprise process models and system usage
analysis. It leverages Artificial Intelligence (AI) & Natural Language
Processing (NLP) for Petri nets to transform business processes into adaptable
models, addressing both structural and functional matching. The framework,
built using Design Science Research (DSR) and a Systematic Literature Review
(SLR), reduces reliance on manual adjustments, improving ERP customization
efficiency and accuracy while minimizing the need for consultants."
Probabilistic Foundations for Metacognition via Hybrid-AI,"Metacognition is the concept of reasoning about an agent's own internal
processes, and it has recently received renewed attention with respect to
artificial intelligence (AI) and, more specifically, machine learning systems.
This paper reviews a hybrid-AI approach known as ""error detecting and
correcting rules"" (EDCR) that allows for the learning of rules to correct
perceptual (e.g., neural) models. Additionally, we introduce a probabilistic
framework that adds rigor to prior empirical studies, and we use this framework
to prove results on necessary and sufficient conditions for metacognitive
improvement, as well as limits to the approach. A set of future"
Verifying Proportionality in Temporal Voting,"We study a model of temporal voting where there is a fixed time horizon, and
at each round the voters report their preferences over the available candidates
and a single candidate is selected. Prior work has adapted popular notions of
justified representation as well as voting rules that provide strong
representation guarantees from the multiwinner election setting to this model.
In our work, we focus on the complexity of verifying whether a given outcome
offers proportional representation. We show that in the temporal setting
verification is strictly harder than in multiwinner voting, but identify
natural special cases that enable efficient algorithms."
CyberSentinel: An Emergent Threat Detection System for AI Security,"The rapid advancement of artificial intelligence (AI) has significantly
expanded the attack surface for AI-driven cybersecurity threats, necessitating
adaptive defense strategies. This paper introduces CyberSentinel, a unified,
single-agent system for emergent threat detection, designed to identify and
mitigate novel security risks in real time. CyberSentinel integrates: (1)
Brute-force attack detection through SSH log analysis, (2) Phishing threat
assessment using domain blacklists and heuristic URL scoring, and (3) Emergent
threat detection via machine learning-based anomaly detection. By continuously
adapting to evolving adversarial tactics, CyberSentinel strengthens proactive
cybersecurity defense, addressing critical vulnerabilities in AI security."
"Survey on Recent Progress of AI for Chemistry: Methods, Applications,
  and Opportunities","The development of artificial intelligence (AI) techniques has brought
revolutionary changes across various realms. In particular, the use of
AI-assisted methods to accelerate chemical research has become a popular and
rapidly growing trend, leading to numerous groundbreaking works. In this paper,
we provide a comprehensive review of current AI techniques in chemistry from a
computational perspective, considering various aspects in the design of
methods. We begin by discussing the characteristics of data from diverse
sources, followed by an overview of various representation methods. Next, we
review existing models for several topical tasks in the field, and conclude by
highlighting some key challenges that warrant further attention."
"Research on Enhancing Cloud Computing Network Security using Artificial
  Intelligence Algorithms","Cloud computing environments are increasingly vulnerable to security threats
such as distributed denial-of-service (DDoS) attacks and SQL injection.
Traditional security mechanisms, based on rule matching and feature
recognition, struggle to adapt to evolving attack strategies. This paper
proposes an adaptive security protection framework leveraging deep learning to
construct a multi-layered defense architecture. The proposed system is
evaluated in a real-world business environment, achieving a detection accuracy
of 97.3%, an average response time of 18 ms, and an availability rate of
99.999%. Experimental results demonstrate that the proposed method
significantly enhances detection accuracy, response efficiency, and resource
utilization, offering a novel and effective approach to cloud computing
security."
The Shady Light of Art Automation,"Generative artificial intelligence (generative AI) has entered the mainstream
culture and become a subject of extensive academic investigation. However, the
character and background of its impact on art require subtler scrutiny and more
nuanced contextualization. This paper summarizes a broader study of the roles
that AI's conceptual and ideological substrata play in influencing art notions.
The focus is on divergent but coalescing and often questionable ideas, values,
and political views that generative AI and other art-related AI technologies
propagate from the computer science and AI/tech industry to the contemporary
art and culture. The paper maps the main areas of this complex relationship and
concisely critiques their key aspects."
"Towards Responsible AI in Education: Hybrid Recommendation System for
  K-12 Students Case Study","The growth of Educational Technology (EdTech) has enabled highly personalized
learning experiences through Artificial Intelligence (AI)-based recommendation
systems tailored to each student needs. However, these systems can
unintentionally introduce biases, potentially limiting fair access to learning
resources. This study presents a recommendation system for K-12 students,
combining graph-based modeling and matrix factorization to provide personalized
suggestions for extracurricular activities, learning resources, and
volunteering opportunities. To address fairness concerns, the system includes a
framework to detect and reduce biases by analyzing feedback across protected
student groups. This work highlights the need for continuous monitoring in
educational recommendation systems to support equitable, transparent, and
effective learning opportunities for all students."
Will AI replace Software Engineers? Do not hold your breath,"Artificial Intelligence (AI) technology such as Large Language Models (LLMs)
have become extremely popular in creating code. This has led to the conjecture
that future software jobs will be exclusively conducted by LLMs, and the
software industry will cease to exist. But software engineering is much more
than producing code -- notably, \emph{maintaining} large software and keeping
it reliable is a major part of software engineering, which LLMs are not yet
capable of."
"A cross-regional review of AI safety regulations in the commercial
  aviation","In this paper we examine the existing artificial intelligence (AI) policy
documents in aviation for the following three regions: the United States,
European Union, and China. The aviation industry has always been a first mover
in adopting technological advancements. This early adoption offers valuable
insights because of its stringent regulations and safety-critical procedures.
As a result, the aviation industry provides an optimal platform to counter AI
vulnerabilities through its tight regulations, standardization processes, and
certification of new technologies. Keywords: AI in aviation; aviation safety;
standardization; certifiable AI; regulations"
Using Artificial Intelligence to Improve Classroom Learning Experience,"This paper explores advancements in Artificial Intelligence technologies to
enhance classroom learning, highlighting contributions from companies like IBM,
Microsoft, Google, and ChatGPT, as well as the potential of brain signal
analysis. The focus is on improving students learning experiences by using
Machine Learning algorithms to : identify a student preferred learning style
and predict academic dropout risk. A Logistic Regression algorithm is applied
for binary classification using six predictor variables, such as assessment
scores, lesson duration, and preferred learning style, to accurately identify
learning preferences. A case study, with 76,519 candidates and 35 predictor
variables, assesses academic dropout risk using Logistic Regression, achieving
a test accuracy of 87.39%. In comparison, the Stochastic Gradient Descent
classifier achieved an accuracy of 83.1% on the same dataset."
Mestersges Intelligencia Kutatsok Magyarorszgon,"Artificial intelligence (AI) has undergone remarkable development since the
mid-2000s, particularly in the fields of machine learning and deep learning,
driven by the explosive growth of large databases and computational capacity.
Hungarian researchers recognized the significance of AI early on, actively
participating in international research and achieving significant results in
both theoretical and practical domains. This article presents some key
achievements in Hungarian AI research. It highlights the results from the
period before the rise of deep learning (the early 2010s), then discusses major
theoretical advancements in Hungary after 2010. Finally, it provides a brief
overview of AI-related applied scientific achievements from 2010 onward."
"Analysis of AI Effectiveness in Reducing Human Errors in Processing
  Transportation Requests","This article examines the characteristics of human errors in processing
transportation requests. The role of artificial intelligence (AI) in maritime
transportation is explored. The main methods and technologies used for
automating and optimizing the handling of transportation requests are analyzed,
along with their impact on reducing the number of errors. Examples of
successful AI implementation in large companies are provided, confirming the
positive influence of these technologies on overall operational efficiency and
customer service levels."
"Attentional Triple-Encoder Network in Spatiospectral Domains for Medical
  Image Segmentation","Retinal Optical Coherence Tomography (OCT) segmentation is essential for
diagnosing pathology. Traditional methods focus on either spatial or spectral
domains, overlooking their combined dependencies. We propose a triple-encoder
network that integrates CNNs for spatial features, Fast Fourier Convolution
(FFC) for spectral features, and attention mechanisms to capture global
relationships across both domains. Attention fusion modules integrate
convolution and cross-attention to further enhance features. Our method
achieves an average Dice score improvement from 0.855 to 0.864, outperforming
prior work."
Generative AI for Validating Physics Laws,"We present generative artificial intelligence (AI) to empirically validate
fundamental laws of physics, focusing on the Stefan-Boltzmann law linking
stellar temperature and luminosity. Our approach simulates counterfactual
luminosities under hypothetical temperature regimes for each individual star
and iteratively refines the temperature-luminosity relationship in a deep
learning architecture. We use Gaia DR3 data and find that, on average,
temperature's effect on luminosity increases with stellar radius and decreases
with absolute magnitude, consistent with theoretical predictions. By framing
physics laws as causal problems, our method offers a novel, data-driven
approach to refine theoretical understanding and inform evidence-based policy
and practice."
"Systematic Literature Review of Automation and Artificial Intelligence
  in Usability Issue Detection","Usability issues can hinder the effective use of software. Therefore, various
techniques are deployed to diagnose and mitigate them. However, these
techniques are costly and time-consuming, particularly in iterative design and
development. A substantial body of research indicates that automation and
artificial intelligence can enhance the process of obtaining usability
insights. In our systematic review of 155 publications, we offer a
comprehensive overview of the current state of the art for automated usability
issue detection. We analyze trends, paradigms, and the technical context in
which they are applied. Finally, we discuss the implications and potential
directions for future research."
"The Self-Learning Agent with a Progressive Neural Network Integrated
  Transformer","This paper introduces a self-learning agent that integrates LLaMA 3.2 with a
Progressive Neural Network (PNN) for continual learning in conversational AI
and code generation. The framework dynamically collects data, fine-tunes tasks
with minimal samples, and leverages Meta-Learning for rapid adaptation. LoRA
optimizes fine-tuning, while Elastic Weight Consolidation (EWC) enhances
knowledge retention. Experimental results demonstrate improved adaptability and
memory stability, positioning this approach as a scalable step toward
Artificial General Intelligence (AGI)."
"A Framework for the Private Governance of Frontier Artificial
  Intelligence","This paper presents a proposal for the governance of frontier AI systems
through a hybrid public-private system. Private bodies, authorized and overseen
by government, provide certifications to developers of frontier AI systems on
an opt-in basis. In exchange for opting in, frontier AI firms receive
protections from tort liability for customer misuse of their models. Before
detailing the proposal, the paper explores more commonly discussed approaches
to AI governance, analyzing their strengths and flaws. It also examines the
nature of frontier AI governance itself. The paper includes consideration of
the political economic, institutional, legal, safety, and other merits and
tradeoffs inherent in the governance system it proposes."
Hashigo: A Next Generation Sketch Interactive System for Japanese Kanji,"Language students can increase their effectiveness in learning written
Japanese by mastering the visual structure and written technique of Japanese
kanji. Yet, existing kanji handwriting recognition systems do not assess the
written technique sufficiently enough to discourage students from developing
bad learning habits. In this paper, we describe our work on Hashigo, a kanji
sketch interactive system which achieves human instructor-level critique and
feedback on both the visual structure and written technique of students'
sketched kanji. This type of automated critique and feedback allows students to
target and correct specific deficiencies in their sketches that, if left
untreated, are detrimental to effective long-term kanji learning."
Security-First AI: Foundations for Robust and Trustworthy Systems,"The conversation around artificial intelligence (AI) often focuses on safety,
transparency, accountability, alignment, and responsibility. However, AI
security (i.e., the safeguarding of data, models, and pipelines from
adversarial manipulation) underpins all of these efforts. This manuscript
posits that AI security must be prioritized as a foundational layer. We present
a hierarchical view of AI challenges, distinguishing security from safety, and
argue for a security-first approach to enable trustworthy and resilient AI
systems. We discuss core threat models, key attack vectors, and emerging
defense mechanisms, concluding that a metric-driven approach to AI security is
essential for robust AI safety, transparency, and accountability."
Evaluating AI-Driven Automated Map Digitization in QGIS,"Map digitization is an important process that converts maps into digital
formats that can be used for further analysis. This process typically requires
a deep human involvement because of the need for interpretation and
decision-making when translating complex features. With the advancement of
artificial intelligence, there is an alternative to conducting map digitization
with the help of machine learning techniques. Deepness, or Deep Neural Remote
Sensing, is an advanced AI-driven tool designed and integrated as a plugin in
QGIS application. This research focuses on assessing the effectiveness of
Deepness in automated digitization. This study analyses AI-generated
digitization results from Google Earth imagery and compares them with digitized
outputs from OpenStreetMap (OSM) to evaluate performance."
"AI from concrete to abstract: demystifying artificial intelligence to
  the general public","Artificial Intelligence (AI) has been adopted in a wide range of domains.
This shows the imperative need to develop means to endow common people with a
minimum understanding of what AI means. Combining visual programming and WiSARD
weightless artificial neural networks, this article presents a new methodology,
AI from concrete to abstract (AIcon2abs), to enable general people (including
children) to achieve this goal. The main strategy adopted by is to promote a
demystification of artificial intelligence via practical activities related to
the development of learning machines, as well as through the observation of
their learning process. Thus, it is possible to provide subjects with skills
that contributes to making them insightful actors in debates and decisions
involving the adoption of artificial intelligence mechanisms. Currently,
existing approaches to the teaching of basic AI concepts through programming
treat machine intelligence as an external element/module. After being trained,
that external module is coupled to the main application being developed by the
learners. In the methodology herein presented, both training and classification
tasks are blocks that compose the main program, just as the other programming
constructs. As a beneficial side effect of AIcon2abs, the difference between a
program capable of learning from data and a conventional computer program
becomes more evident. In addition, the simplicity of the WiSARD weightless
artificial neural network model enables easy visualization and understanding of
training and classification tasks internal realization."
Aligning Superhuman AI with Human Behavior: Chess as a Model System,"As artificial intelligence becomes increasingly intelligent---in some cases,
achieving superhuman performance---there is growing potential for humans to
learn from and collaborate with algorithms. However, the ways in which AI
systems approach problems are often different from the ways people do, and thus
may be uninterpretable and hard to learn from. A crucial step in bridging this
gap between human and artificial intelligence is modeling the granular actions
that constitute human behavior, rather than simply matching aggregate human
performance.
  We pursue this goal in a model system with a long history in artificial
intelligence: chess. The aggregate performance of a chess player unfolds as
they make decisions over the course of a game. The hundreds of millions of
games played online by players at every skill level form a rich source of data
in which these decisions, and their exact context, are recorded in minute
detail. Applying existing chess engines to this data, including an open-source
implementation of AlphaZero, we find that they do not predict human moves well.
  We develop and introduce Maia, a customized version of Alpha-Zero trained on
human chess games, that predicts human moves at a much higher accuracy than
existing engines, and can achieve maximum accuracy when predicting decisions
made by players at a specific skill level in a tuneable way. For a dual task of
predicting whether a human will make a large mistake on the next move, we
develop a deep neural network that significantly outperforms competitive
baselines. Taken together, our results suggest that there is substantial
promise in designing artificial intelligence systems with human collaboration
in mind by first accurately modeling granular human decision-making."
"The Future of Office and Administrative Support Occupations in the Era
  of Artificial Intelligence: A Bibliometric Analysis","The U.S. Bureau of Labor Statistics projects that by the year 2029, the
United States will lose a million jobs in the office and administrative support
occupations because technology, automation, and artificial intelligence (AI)
have the potential to substitute or replace the office and administrative
functions performed by office workers. Despite the potential impact AI will
have on office work and the important role office workers play in the American
economy, we have limited knowledge of the state of the art research in office
work at the intersection of emerging artificial intelligence technologies. In
this study, we conducted a bibliometric analysis of the scholarly literature at
the intersection of office work and artificial intelligence. We extracted
literature sources from Compendex and Scopus databases and used VOSviewer for
visualizing and quantifying our bibliometric analyses. Our findings from
keywords analysis indicate that office automation, humans, human-computer
interaction, and artificial intelligence occurred more frequently in the
scholarly literature and had high link strengths. Keyword clusters from
co-occurrence analysis indicate that intelligent buildings, robotics, and the
internet of things are emerging topics in the office work domain. The two
clusters related to ergonomics, worker characteristics, human performance, and
safety indicate the types of human factors concerns that are more widely
studied in office work settings. In summary, our findings on the
state-of-the-art research in office work indicate that more studies have been
conducted on smart buildings, robotics, and technology development for office
work, compared to studies on office workers and their professional development."
On Artificial Life and Emergent Computation in Physical Substrates,"In living systems, we often see the emergence of the ingredients necessary
for computation -- the capacity for information transmission, storage, and
modification -- begging the question of how we may exploit or imitate such
biological systems in unconventional computing applications. What can we gain
from artificial life in the advancement of computing technology? Artificial
life provides us with powerful tools for understanding the dynamic behavior of
biological systems and capturing this behavior in manmade substrates. With this
approach, we can move towards a new computing paradigm concerned with
harnessing emergent computation in physical substrates not governed by the
constraints of Moore's law and ultimately realize massively parallel and
distributed computing technology. In this paper, we argue that the lens of
artificial life offers valuable perspectives for the advancement of
high-performance computing technology. We first present a brief foundational
background on artificial life and some relevant tools that may be applicable to
unconventional computing. Two specific substrates are then discussed in detail:
biological neurons and ensembles of nanomagnets. These substrates are the focus
of the authors' ongoing work, and they are illustrative of the two sides of the
approach outlined here -- the close study of living systems and the
construction of artificial systems to produce life-like behaviors. We conclude
with a philosophical discussion on what we can learn from approaching
computation with the curiosity inherent to the study of artificial life. The
main contribution of this paper is to present the great potential of using
artificial life methodologies to uncover and harness the inherent computational
power of physical substrates toward applications in unconventional
high-performance computing."
Textbook examples of recursion,"We discuss properties of recursive schemas related to McCarthy's ``91
function'' and to Takeuchi's triple recursion. Several theorems are proposed as
interesting candidates for machine verification, and some intriguing open
questions are raised."
To Preference via Entrenchment,"We introduce a simple generalization of Gardenfors and Makinson's epistemic
entrenchment called partial entrenchment. We show that preferential inference
can be generated as the sceptical counterpart of an inference mechanism defined
directly on partial entrenchment."
The Logic Programming Paradigm and Prolog,"This is a tutorial on logic programming and Prolog appropriate for a course
on programming languages for students familiar with imperative programming."
A theory of experiment,"This article aims at clarifying the language and practice of scientific
experiment, mainly by hooking observability on calculability."
Value Based Argumentation Frameworks,"This paper introduces the notion of value-based argumentation frameworks, an
extension of the standard argumentation frameworks proposed by Dung, which are
able toshow how rational decision is possible in cases where arguments derive
their force from the social values their acceptance would promote."
Knowledge Representation,"This work analyses main features that should be present in knowledge
representation. It suggests a model for representation and a way to implement
this model in software. Representation takes care of both low-level sensor
information and high-level concepts."
"Toward the Implementation of Functions in the DLV System (Preliminary
  Technical Report)","This document describes the functions as they are treated in the DLV system.
We give first the language, then specify the main implementation issues."
Quantum Computers,"This research paper gives an overview of quantum computers - description of
their operation, differences between quantum and silicon computers, major
construction problems of a quantum computer and many other basic aspects. No
special scientific knowledge is necessary for the reader."
Self-organizing neural networks in classification and image recognition,"Self-organizing neural networks are used for brick finding in OPERA
experiment. Self-organizing neural networks and wavelet analysis used for
recognition and extraction of car numbers from images."
A Note on the PAC Bayesian Theorem,"We prove general exponential moment inequalities for averages of [0,1]-valued
iid random variables and use them to tighten the PAC Bayesian Theorem. The
logarithmic dependence on the sample count in the enumerator of the PAC
Bayesian bound is halved."
Inferring knowledge from a large semantic network,"In this paper, we present a rich semantic network based on a differential
analysis. We then detail implemented measures that take into account common and
differential features between words. In a last section, we describe some
industrial applications."
Self-Organizing Multilayered Neural Networks of Optimal Complexity,"The principles of self-organizing the neural networks of optimal complexity
is considered under the unrepresentative learning set. The method of
self-organizing the multi-layered neural networks is offered and used to train
the logical neural networks which were applied to the medical diagnostics."
Redundancy in Logic III: Non-Mononotonic Reasoning,"Results about the redundancy of circumscriptive and default theories are
presented. In particular, the complexity of establishing whether a given theory
is redundant is establihsed."
Yet Another Efficient Unification Algorithm,"The unification algorithm is at the core of the logic programming paradigm,
the first unification algorithm being developed by Robinson [5]. More efficient
algorithms were developed later [3] and I introduce here yet another efficient
unification algorithm centered on a specific data structure, called the
Unification Table."
Islands for SAT,"In this note we introduce the notion of islands for restricting local search.
We show how we can construct islands for CNF SAT problems, and how much search
space can be eliminated by restricting search to the island."
"Using Answer Set Programming in an Inference-Based approach to Natural
  Language Semantics","Using Answer Set Programming in an Inference-Based approach to Natural
Language Semantics"
Solving planning domains with polytree causal graphs is NP-complete,"We show that solving planning domains on binary variables with polytree
causal graph is \NP-complete. This is in contrast to a polynomial-time
algorithm of Domshlak and Brafman that solves these planning domains for
polytree causal graphs of bounded indegree."
"DIANA, a program for Feynman Diagram Evaluation","A C-program DIANA (DIagram ANAlyser) for the automatic Feynman diagram
evaluation is presented."
Quantum Artificial Intelligence,"This report introduces researchers in AI to some of the concepts in quantum
heurisitics and quantum AI."
Calculating Valid Domains for BDD-Based Interactive Configuration,"In these notes we formally describe the functionality of Calculating Valid
Domains from the BDD representing the solution space of valid configurations.
The formalization is largely based on the CLab configuration framework."
"How to realize ""a sense of humour"" in computers ?","Computer model of a ""sense of humour"" suggested previously [arXiv:0711.2058,
0711.2061, 0711.2270] is raised to the level of a realistic algorithm."
"Geometric Data Analysis, From Correspondence Analysis to Structured Data
  Analysis (book review)","Review of: Brigitte Le Roux and Henry Rouanet, Geometric Data Analysis, From
Correspondence Analysis to Structured Data Analysis, Kluwer, Dordrecht, 2004,
xi+475 pp."
Temporized Equilibria,"This paper has been withdrawn by the author due to a crucial error in the
submission action."
The model of quantum evolution,"This paper has been withdrawn by the author due to extremely unscientific
errors."
"Quantum robot: structure, algorithms and applications",This paper has been withdrawn.
"Identification of parameters underlying emotions and a classification of
  emotions","The standard classification of emotions involves categorizing the expression
of emotions. In this paper, parameters underlying some emotions are identified
and a new classification based on these parameters is suggested."
A remark on higher order RUE-resolution with EXTRUE,"We show that a prominent counterexample for the completeness of first order
RUE-resolution does not apply to the higher order RUE-resolution approach
EXTRUE."
Time Hopping technique for faster reinforcement learning in simulations,This preprint has been withdrawn by the author for revision
Fuzzy Mnesors,"A fuzzy mnesor space is a semimodule over the positive real numbers. It can
be used as theoretical framework for fuzzy sets. Hence we can prove a great
number of properties for fuzzy sets without refering to the membership
functions."
"Comments on ""A new combination of evidence based on compromise"" by K.
  Yamada","Comments on ``A new combination of evidence based on compromise'' by K.
Yamada"
The Soft Cumulative Constraint,"This research report presents an extension of Cumulative of Choco constraint
solver, which is useful to encode over-constrained cumulative problems. This
new global constraint uses sweep and task interval violation-based algorithms."
Modelling Concurrent Behaviors in the Process Specification Language,"In this paper, we propose a first-order ontology for generalized stratified
order structure. We then classify the models of the theory using
model-theoretic techniques. An ontology mapping from this ontology to the core
theory of Process Specification Language is also discussed."
Beyond Turing Machines,"This paper discusses ""computational"" systems capable of ""computing"" functions
not computable by predefined Turing machines if the systems are not isolated
from their environment. Roughly speaking, these systems can change their finite
descriptions by interacting with their environment."
ABC-LogitBoost for Multi-class Classification,"We develop abc-logitboost, based on the prior work on abc-boost and robust
logitboost. Our extensive experiments on a variety of datasets demonstrate the
considerable improvement of abc-logitboost over logitboost and abc-mart."
Algorithms for finding dispensable variables,"This short note reviews briefly three algorithms for finding the set of
dispensable variables of a boolean formula. The presentation is light on proofs
and heavy on intuitions."
"Acquisition d'informations lexicales  partir de corpus Cdric
  Messiant et Thierry Poibeau","This paper is about automatic acquisition of lexical information from
corpora, especially subcategorization acquisition."
Dominion -- A constraint solver generator,"This paper proposes a design for a system to generate constraint solvers that
are specialised for specific problem models. It describes the design in detail
and gives preliminary experimental results showing the feasibility and
effectiveness of the approach."
A Formalization of the Turing Test,"The paper offers a mathematical formalization of the Turing test. This
formalization makes it possible to establish the conditions under which some
Turing machine will pass the Turing test and the conditions under which every
Turing machine (or every Turing machine of the special class) will fail the
Turing test."
"Approximated Structured Prediction for Learning Large Scale Graphical
  Models","This manuscripts contains the proofs for ""A Primal-Dual Message-Passing
Algorithm for Approximated Large Scale Structured Prediction""."
Creating a new Ontology: a Modular Approach,Creating a new Ontology: a Modular Approach
"Planning to Be Surprised: Optimal Bayesian Exploration in Dynamic
  Environments","To maximize its success, an AGI typically needs to explore its initially
unknown world. Is there an optimal way of doing so? Here we derive an
affirmative answer for a broad class of environments."
Xapagy: a cognitive architecture for narrative reasoning,"We introduce the Xapagy cognitive architecture: a software system designed to
perform narrative reasoning. The architecture has been designed from scratch to
model and mimic the activities performed by humans when witnessing, reading,
recalling, narrating and talking about stories."
Predicting growth fluctuation in network economy,"This study presents a method to predict the growth fluctuation of firms
interdependent in a network economy. The risk of downward growth fluctuation of
firms is calculated from the statistics on Japanese industry."
Instantiation Schemes for Nested Theories,"This paper investigates under which conditions instantiation-based proof
procedures can be combined in a nested way, in order to mechanically construct
new instantiation procedures for richer theories. Interesting applications in
the field of verification are emphasized, particularly for handling extensions
of the theory of arrays."
'Just Enough' Ontology Engineering,"This paper introduces 'just enough' principles and 'systems engineering'
approach to the practice of ontology development to provide a minimal yet
complete, lightweight, agile and integrated development process, supportive of
stakeholder management and implementation independence."
Fuzzy Inference Systems Optimization,"This paper compares various optimization methods for fuzzy inference system
optimization. The optimization methods compared are genetic algorithm, particle
swarm optimization and simulated annealing. When these techniques were
implemented it was observed that the performance of each technique within the
fuzzy inference system classification was context dependent."
Kernel diff-hash,"This paper presents a kernel formulation of the recently introduced diff-hash
algorithm for the construction of similarity-sensitive hash functions. Our
kernel diff-hash algorithm that shows superior performance on the problem of
image feature descriptor matching."
Principles of Solomonoff Induction and AIXI,"We identify principles characterizing Solomonoff Induction by demands on an
agent's external behaviour. Key concepts are rationality, computability,
indifference and time consistency. Furthermore, we discuss extensions to the
full AI case to derive AIXI."
Efficient Methods for Unsupervised Learning of Probabilistic Models,"In this thesis I develop a variety of techniques to train, evaluate, and
sample from intractable and high dimensional probabilistic models. Abstract
exceeds arXiv space limitations -- see PDF."
"An example illustrating the imprecision of the efficient approach for
  diagnosis of Petri nets via integer linear programming","This document demonstrates that the efficient approach for diagnosis of Petri
nets via integer linear programming may be unable to detect a fault even if the
system is diagnosable."
"Towards common-sense reasoning via conditional simulation: legacies of
  Turing in Artificial Intelligence","The problem of replicating the flexibility of human common-sense reasoning
has captured the imagination of computer scientists since the early days of
Alan Turing's foundational work on computation and the philosophy of artificial
intelligence. In the intervening years, the idea of cognition as computation
has emerged as a fundamental tenet of Artificial Intelligence (AI) and
cognitive science. But what kind of computation is cognition?
  We describe a computational formalism centered around a probabilistic Turing
machine called QUERY, which captures the operation of probabilistic
conditioning via conditional simulation. Through several examples and analyses,
we demonstrate how the QUERY abstraction can be used to cast common-sense
reasoning as probabilistic inference in a statistical model of our observations
and the uncertain structure of the world that generated that experience. This
formulation is a recent synthesis of several research programs in AI and
cognitive science, but it also represents a surprising convergence of several
of Turing's pioneering insights in AI, the foundations of computation, and
statistics."
"Modification of conceptual clustering algorithm Cobweb for numerical
  data using fuzzy membership function","Modification of a conceptual clustering algorithm Cobweb for the purpose of
its application for numerical data is offered. Keywords: clustering, algorithm
Cobweb, numerical data, fuzzy membership function."
The Doxastic Interpretation of Team Semantics,"We advance a doxastic interpretation for many of the logical connectives
considered in Dependence Logic and in its extensions, and we argue that Team
Semantics is a natural framework for reasoning about beliefs and belief
updates."
Introduction to Judea Pearl's Do-Calculus,"This is a purely pedagogical paper with no new results. The goal of the paper
is to give a fairly self-contained introduction to Judea Pearl's do-calculus,
including proofs of his 3 rules."
Lambda Dependency-Based Compositional Semantics,"This short note presents a new formal language, lambda dependency-based
compositional semantics (lambda DCS) for representing logical forms in semantic
parsing. By eliminating variables and making existential quantification
implicit, lambda DCS logical forms are generally more compact than those in
lambda calculus."
A short note on the axiomatic requirements of uncertainty measure,"In this note, we argue that the axiomatic requirement of range to the measure
of aggregated total uncertainty (ATU) in Dempster-Shafer theory is not
reasonable."
Logic in the Lab,"This file summarizes the plenary talk on laboratory experiments on logic at
the TARK 2013 - 14th Conference on Theoretical Aspects of Rationality and
Knowledge."
Free-configuration Biased Sampling for Motion Planning: Errata,"This document contains improved and updated proofs of convergence for the
sampling method presented in our paper ""Free-configuration Biased Sampling for
Motion Planning""."
"Cortex simulation system proposal using distributed computer network
  environments","In the dawn of computer science and the eve of neuroscience we participate in
rebirth of neuroscience due to new technology that allows us to deeply and
precisely explore whole new world that dwells in our brains."
"Enhancements to ACL2 in Versions 6.2, 6.3, and 6.4",We report on improvements to ACL2 made since the 2013 ACL2 Workshop.
Dynamic Sweep Filtering Algorithm for FlexC,"We investigate cumulative scheduling in uncertain environments, using
constraint programming. We detail in this paper the dynamic sweep filtering
algorithm of the FlexC global constraint."
"AI Evaluation: past, present and future","Artificial intelligence develops techniques and systems whose performance
must be evaluated on a regular basis in order to certify and foster progress in
the discipline. We will describe and critically assess the different ways AI
systems are evaluated. We first focus on the traditional task-oriented
evaluation approach. We see that black-box (behavioural evaluation) is becoming
more and more common, as AI systems are becoming more complex and
unpredictable. We identify three kinds of evaluation: Human discrimination,
problem benchmarks and peer confrontation. We describe the limitations of the
many evaluation settings and competitions in these three categories and propose
several ideas for a more systematic and robust evaluation. We then focus on a
less customary (and challenging) ability-oriented evaluation approach, where a
system is characterised by its (cognitive) abilities, rather than by the tasks
it is designed to solve. We discuss several possibilities: the adaptation of
cognitive tests used for humans and animals, the development of tests derived
from algorithmic information theory or more general approaches under the
perspective of universal psychometrics."
Deontic modality based on preference,"Deontic modalities are here defined in terms of the preference relation
explored in our previous work (Osherson and Weinstein, 2012). Some consequences
of the system are discussed."
"Qualitative shape representation based on the qualitative relative
  direction and distance calculus eOPRAm","This document serves as a brief technical report, detailing the processes
used to represent and reconstruct simplified polygons using qualitative spatial
descriptions, as defined by the eOPRAm qualitative spatial calculus."
About Tau-Chain,"Tau-chain is a decentralized peer-to-peer network having three unified faces:
Rules, Proofs, and Computer Programs, allowing a generalization of virtually
any centralized or decentralized P2P network, together with many new abilities,
as we present on this note."
Norm-Based Capacity Control in Neural Networks,"We investigate the capacity, convexity and characterization of a general
family of norm-constrained feed-forward networks."
A Note on Information-Directed Sampling and Thompson Sampling,"This note introduce three Bayesian style Multi-armed bandit algorithms:
Information-directed sampling, Thompson Sampling and Generalized Thompson
Sampling. The goal is to give an intuitive explanation for these three
algorithms and their regret bounds, and provide some derivations that are
omitted in the original papers."
Why Bother With Syntax?,"This short note discusses the role of syntax vs. semantics and the interplay
between logic, philosophy, and language in computer science and game theory."
Neurocontrol methods review,"Methods of applying neural networks to control plants are considered. Methods
and schemes are described, their advantages and disadvantages are discussed."
"An Application of the Generalized Rectangular Fuzzy Model to Critical
  Thinking Assessment","The authors apply the Generalized Rectangular Model to assessing critical
thinking skills and its relations with their language competency."
Concept Generation in Language Evolution,"This thesis investigates the generation of new concepts from combinations of
existing concepts as a language evolves. We give a method for combining
concepts, and will be investigating the utility of composite concepts in
language evolution and thence the utility of concept generation."
"Obstacle evasion using fuzzy logic in a sliding blades problem
  environment","This paper discusses obstacle avoidance using fuzzy logic and shortest path
algorithm. This paper also introduces the sliding blades problem and
illustrates how a drone can navigate itself through the swinging blade
obstacles while tracing a semi-optimal path and also maintaining constant
velocity"
A note on adjusting $R^2$ for using with cross-validation,"We show how to adjust the coefficient of determination ($R^2$) when used for
measuring predictive accuracy via leave-one-out cross-validation."
How to avoid ethically relevant Machine Consciousness,"This paper discusses the root cause of systems perceiving the self experience
and how to exploit adaptive and learning features without introducing ethically
problematic system properties."
"How to advance general game playing artificial intelligence by player
  modelling","General game playing artificial intelligence has recently seen important
advances due to the various techniques known as 'deep learning'. However the
advances conceal equally important limitations in their reliance on: massive
data sets; fortuitously constructed problems; and absence of any human-level
complexity, including other human opponents. On the other hand, deep learning
systems which do beat human champions, such as in Go, do not generalise well.
The power of deep learning simultaneously exposes its weakness. Given that deep
learning is mostly clever reconfigurations of well-established methods, moving
beyond the state of art calls for forward-thinking visionary solutions, not
just more of the same. I present the argument that general game playing
artificial intelligence will require a generalised player model. This is
because games are inherently human artefacts which therefore, as a class of
problems, contain cases which require a human-style problem solving approach. I
relate this argument to the performance of state of art general game playing
agents. I then describe a concept for a formal category theoretic basis to a
generalised player model. This formal model approach integrates my existing
'Behavlets' method for psychologically-derived player modelling:
  Cowley, B., Charles, D. (2016). Behavlets: a Method for Practical Player
Modelling using Psychology-Based Player Traits and Domain Specific Features.
User Modeling and User-Adapted Interaction, 26(2), 257-306."
Simplified Boardgames,"We formalize Simplified Boardgames language, which describes a subclass of
arbitrary board games. The language structure is based on the regular
expressions, which makes the rules easily machine-processable while keeping the
rules concise and fairly human-readable."
Lattice Structure of Variable Precision Rough Sets,"The main purpose of this paper is to study the lattice structure of variable
precision rough sets. The notion of variation in precision of rough sets have
been further extended to variable precision rough set with variable
classification error and its algebraic properties are also studied."
"Divisive-agglomerative algorithm and complexity of automatic
  classification problems","An algorithm of solution of the Automatic Classification (AC for brevity)
problem is set forth in the paper. In the AC problem, it is required to find
one or several artitions, starting with the given pattern matrix or
dissimilarity, similarity matrix."
"Proceedings of the Second Summer School on Argumentation: Computational
  and Linguistic Perspectives (SSA'16)","This volume contains the thesis abstracts presented at the Second Summer
School on Argumentation: Computational and Linguistic Perspectives (SSA'2016)
held on September 8-12 in Potsdam, Germany."
Expressibility of norms in temporal logic,"In this short note we address the issue of expressing norms (such as
obligations and prohibitions) in temporal logic. In particular, we address the
argument from [Governatori 2015] that norms cannot be expressed in Linear Time
Temporal Logic (LTL)."
On Seeking Consensus Between Document Similarity Measures,"This paper investigates the application of consensus clustering and
meta-clustering to the set of all possible partitions of a data set. We show
that when using a ""complement"" of Rand Index as a measure of cluster
similarity, the total-separation partition, putting each element in a separate
set, is chosen."
"Resolving the Complexity of Some Fundamental Problems in Computational
  Social Choice","This thesis is in the area called computational social choice which is an
intersection area of algorithms and social choice theory."
"Brief Notes on Hard Takeoff, Value Alignment, and Coherent Extrapolated
  Volition","I make some basic observations about hard takeoff, value alignment, and
coherent extrapolated volition, concepts which have been central in analyses of
superintelligent AI systems."
A History of Metaheuristics,"This chapter describes the history of metaheuristics in five distinct
periods, starting long before the first use of the term and ending a long time
in the future."
"Embodied Artificial Intelligence through Distributed Adaptive Control:
  An Integrated Framework","In this paper, we argue that the future of Artificial Intelligence research
resides in two keywords: integration and embodiment. We support this claim by
analyzing the recent advances of the field. Regarding integration, we note that
the most impactful recent contributions have been made possible through the
integration of recent Machine Learning methods (based in particular on Deep
Learning and Recurrent Neural Networks) with more traditional ones (e.g.
Monte-Carlo tree search, goal babbling exploration or addressable memory
systems). Regarding embodiment, we note that the traditional benchmark tasks
(e.g. visual classification or board games) are becoming obsolete as
state-of-the-art learning algorithms approach or even surpass human performance
in most of them, having recently encouraged the development of first-person 3D
game platforms embedding realistic physics. Building upon this analysis, we
first propose an embodied cognitive architecture integrating heterogenous
sub-fields of Artificial Intelligence into a unified framework. We demonstrate
the utility of our approach by showing how major contributions of the field can
be expressed within the proposed framework. We then claim that benchmarking
environments need to reproduce ecologically-valid conditions for bootstrapping
the acquisition of increasingly complex cognitive skills through the concept of
a cognitive arms race between embodied agents."
The Causality/Repair Connection in Databases: Causality-Programs,"In this work, answer-set programs that specify repairs of databases are used
as a basis for solving computational and reasoning problems about causes for
query answers from databases."
"Policy Gradient Methods for Reinforcement Learning with Function
  Approximation and Action-Dependent Baselines","We show how an action-dependent baseline can be used by the policy gradient
theorem using function approximation, originally presented with
action-independent baselines by (Sutton et al. 2000)."
The Complex Negotiation Dialogue Game,"This position paper formalises an abstract model for complex negotiation
dialogue. This model is to be used for the benchmark of optimisation algorithms
ranging from Reinforcement Learning to Stochastic Games, through Transfer
Learning, One-Shot Learning or others."
"Investigating Reinforcement Learning Agents for Continuous State Space
  Environments","Given an environment with continuous state spaces and discrete actions, we
investigate using a Double Deep Q-learning Reinforcement Agent to find optimal
policies using the LunarLander-v2 OpenAI gym environment."
"Artificial Neural Networks-Based Machine Learning for Wireless Networks:
  A Tutorial","Next-generation wireless networks must support ultra-reliable, low-latency
communication and intelligently manage a massive number of Internet of Things
(IoT) devices in real-time, within a highly dynamic environment. This need for
stringent communication quality-of-service (QoS) requirements as well as mobile
edge and core intelligence can only be realized by integrating fundamental
notions of artificial intelligence (AI) and machine learning across the
wireless infrastructure and end-user devices. In this context, this paper
provides a comprehensive tutorial that introduces the main concepts of machine
learning, in general, and artificial neural networks (ANNs), in particular, and
their potential applications in wireless communications. For this purpose, we
present a comprehensive overview on a number of key types of neural networks
that include feed-forward, recurrent, spiking, and deep neural networks. For
each type of neural network, we present the basic architecture and training
procedure, as well as the associated challenges and opportunities. Then, we
provide an in-depth overview on the variety of wireless communication problems
that can be addressed using ANNs, ranging from communication using unmanned
aerial vehicles to virtual reality and edge caching.For each individual
application, we present the main motivation for using ANNs along with the
associated challenges while also providing a detailed example for a use case
scenario and outlining future works that can be addressed using ANNs. In a
nutshell, this article constitutes one of the first holistic tutorials on the
development of machine learning techniques tailored to the needs of future
wireless networks."
Scientists in silico?,"The end (for human scientists) is nigh? The posit of this discourse is that
the majority, if not all, scientific research will eventually be undertaken by
one, or a number of, weak artificial intelligences."
"MagNet and ""Efficient Defenses Against Adversarial Attacks"" are Not
  Robust to Adversarial Examples","MagNet and ""Efficient Defenses..."" were recently proposed as a defense to
adversarial examples. We find that we can construct adversarial examples that
defeat these defenses with only a slight increase in distortion."
Network Analysis for Explanation,"Safety critical systems strongly require the quality aspects of artificial
intelligence including explainability. In this paper, we analyzed a trained
network to extract features which mainly contribute the inference. Based on the
analysis, we developed a simple solution to generate explanations of the
inference processes."
Paranom: A Parallel Anomaly Dataset Generator,"In this paper, we present Paranom, a parallel anomaly dataset generator. We
discuss its design and provide brief experimental results demonstrating its
usefulness in improving the classification correctness of LSTM-AD, a
state-of-the-art anomaly detection model."
"Can Autism be Catered with Artificial Intelligence-Assisted Intervention
  Technology? A Literature Review","This article presents an extensive literature review of technology based
intervention methodologies for individuals facing Autism Spectrum Disorder
(ASD). Reviewed methodologies include: contemporary Computer Aided Systems
(CAS), Computer Vision Assisted Technologies (CVAT) and Virtual Reality (VR) or
Artificial Intelligence (AI)-Assisted interventions. The research over the past
decade has provided enough demonstrations that individuals with ASD have a
strong interest in technology based interventions, which are useful in both,
clinical settings as well as at home and classrooms. Despite showing great
promise, research in developing an advanced technology based intervention that
is clinically quantitative for ASD is minimal. Moreover, the clinicians are
generally not convinced about the potential of the technology based
interventions due to non-empirical nature of published results. A major reason
behind this lack of acceptability is that a vast majority of studies on
distinct intervention methodologies do not follow any specific standard or
research design. We conclude from our findings that there remains a gap between
the research community of computer science, psychology and neuroscience to
develop an AI assisted intervention technology for individuals suffering from
ASD. Following the development of a standardized AI based intervention
technology, a database needs to be developed, to devise effective AI
algorithms."
Solving Sudoku with Ant Colony Optimisation,"In this paper we present a new Ant Colony Optimisation-based algorithm for
Sudoku, which out-performs existing methods on large instances. Our method
includes a novel anti-stagnation operator, which we call Best Value
Evaporation."
Experimental Tests of Spirituality,"We currently harness technologies that could shed new light on old
philosophical questions, such as whether our mind entails anything beyond our
body or whether our moral values reflect universal truth."
A Proof of the Front-Door Adjustment Formula,"We provide a proof of the the Front-Door adjustment formula using the
do-calculus."
Victory Probability in the Fire Emblem Arena,"We demonstrate how to efficiently compute the probability of victory in Fire
Emblem arena battles. The probability can be expressed in terms of a
multivariate recurrence relation which lends itself to a straightforward
dynamic programming solution. Some implementation issues are addressed, and a
full implementation is provided in code."
A Framework for Approval-based Budgeting Methods,"We define and study a general framework for approval-based budgeting methods
and compare certain methods within this framework by their axiomatic and
computational properties. Furthermore, we visualize their behavior on certain
Euclidean distributions and analyze them experimentally."
"Using Artificial Intelligence to Support Compliance with the General
  Data Protection Regulation","The General Data Protection Regulation (GDPR) is a European Union regulation
that will replace the existing Data Protection Directive on 25 May 2018. The
most significant change is a huge increase in the maximum fine that can be
levied for breaches of the regulation. Yet fewer than half of UK companies are
fully aware of GDPR - and a number of those who were preparing for it stopped
doing so when the Brexit vote was announced. A last-minute rush to become
compliant is therefore expected, and numerous companies are starting to offer
advice, checklists and consultancy on how to comply with GDPR. In such an
environment, artificial intelligence technologies ought to be able to assist by
providing best advice; asking all and only the relevant questions; monitoring
activities; and carrying out assessments. The paper considers four areas of
GDPR compliance where rule based technologies and/or machine learning
techniques may be relevant: * Following compliance checklists and codes of
conduct; * Supporting risk assessments; * Complying with the new regulations
regarding technologies that perform automatic profiling; * Complying with the
new regulations concerning recognising and reporting breaches of security. It
concludes that AI technology can support each of these four areas. The
requirements that GDPR (or organisations that need to comply with GDPR) state
for explanation and justification of reasoning imply that rule-based approaches
are likely to be more helpful than machine learning approaches. However, there
may be good business reasons to take a different approach in some
circumstances."
Shannon Entropy for Neutrosophic Information,"The paper presents an extension of Shannon entropy for neutrosophic
information. This extension uses a new formula for distance between two
neutrosophic triplets. In addition, the obtained results are particularized for
bifuzzy, intuitionistic and paraconsistent fuzzy information."
Wikistat 2.0: Educational Resources for Artificial Intelligence,"Big data, data science, deep learning, artificial intelligence are the key
words of intense hype related with a job market in full evolution, that impose
to adapt the contents of our university professional trainings. Which
artificial intelligence is mostly concerned by the job offers? Which
methodologies and technologies should be favored in the training programs?
Which objectives, tools and educational resources do we needed to put in place
to meet these pressing needs? We answer these questions in describing the
contents and operational resources in the Data Science orientation of the
specialty Applied Mathematics at INSA Toulouse. We focus on basic mathematics
training (Optimization, Probability, Statistics), associated with the practical
implementation of the most performing statistical learning algorithms, with the
most appropriate technologies and on real examples. Considering the huge
volatility of the technologies, it is imperative to train students in
seft-training, this will be their technological watch tool when they will be in
professional activity. This explains the structuring of the educational site
github.com/wikistat into a set of tutorials. Finally, to motivate the thorough
practice of these tutorials, a serious game is organized each year in the form
of a prediction contest between students of Master degrees in Applied
Mathematics for IA."
"A Minesweeper Solver Using Logic Inference, CSP and Sampling","Minesweeper as a puzzle video game and is proved that it is an NPC problem.
We use CSP, Logic Inference and Sampling to make a minesweeper solver and we
limit us each select in 5 seconds."
Geometry of Friston's active inference,"We reconstruct Karl Friston's active inference and give a geometrical
interpretation of it."
"Protection of an information system by artificial intelligence: a
  three-phase approach based on behaviour analysis to detect a hostile scenario","The analysis of the behaviour of individuals and entities (UEBA) is an area
of artificial intelligence that detects hostile actions (e.g. attacks, fraud,
influence, poisoning) due to the unusual nature of observed events, by affixing
to a signature-based operation. A UEBA process usually involves two phases,
learning and inference. Intrusion detection systems (IDS) available still
suffer from bias, including over-simplification of problems, underexploitation
of the AI potential, insufficient consideration of the temporality of events,
and perfectible management of the memory cycle of behaviours. In addition,
while an alert generated by a signature-based IDS can refer to the signature on
which the detection is based, the IDS in the UEBA domain produce results, often
associated with a score, whose explainable character is less obvious. Our
unsupervised approach is to enrich this process by adding a third phase to
correlate events (incongruities, weak signals) that are presumed to be linked
together, with the benefit of a reduction of false positives and negatives. We
also seek to avoid a so-called ""boiled frog"" bias inherent in continuous
learning. Our first results are interesting and have an explainable character,
both on synthetic and real data."
Proceedings of the 2018 XCSP3 Competition,"This document represents the proceedings of the 2018 XCSP3 Competition. The
results of this competition of constraint solvers were presented at CP'18, the
24th International Conference on Principles and Practice of Constraint
Programming, held in Lille, France from 27th August 2018 to 31th August, 2018."
"Comparing Knowledge-based Reinforcement Learning to Neural Networks in a
  Strategy Game","The paper reports on an experiment, in which a Knowledge-Based Reinforcement
Learning (KB-RL) method was compared to a Neural Network (NN) approach in
solving a classical Artificial Intelligence (AI) task. In contrast to NNs,
which require a substantial amount of data to learn a good policy, the KB-RL
method seeks to encode human knowledge into the solution, considerably reducing
the amount of data needed for a good policy. By means of Reinforcement Learning
(RL), KB-RL learns to optimize the model and improves the output of the system.
Furthermore, KB-RL offers the advantage of a clear explanation of the taken
decisions as well as transparent reasoning behind the solution.
  The goal of the reported experiment was to examine the performance of the
KB-RL method in contrast to the Neural Network and to explore the capabilities
of KB-RL to deliver a strong solution for the AI tasks. The results show that,
within the designed settings, KB-RL outperformed the NN, and was able to learn
a better policy from the available amount of data. These results support the
opinion that Artificial Intelligence can benefit from the discovery and study
of alternative approaches, potentially extending the frontiers of AI."
Single cell data explosion: Deep learning to the rescue,"The plethora of single-cell multi-omics data is getting treatment with deep
learning, a revolutionary method in artificial intelligence, which has been
increasingly expanding its reign over the bioscience frontiers."
"Proceedings of AAAI 2019 Workshop on Network Interpretability for Deep
  Learning","This is the Proceedings of AAAI 2019 Workshop on Network Interpretability for
Deep Learning"
Probabilistic Relational Agent-based Models,"PRAM puts agent-based models on a sound probabilistic footing as a basis for
integrating agent-based and probabilistic models. It extends the themes of
probabilistic relational models and lifted inference to incorporate dynamical
models and simulation. It can also be much more efficient than agent-based
simulation."
Turing-Completeness of Dynamics in Abstract Persuasion Argumentation,"Abstract Persuasion Argumentation (APA) is a dynamic argumentation formalism
that extends Dung argumentation with persuasion relations. In this work, we
show through two-counter Minsky machine encoding that APA dynamics is
Turing-complete."
Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound,"We improve the over-parametrization size over two beautiful results [Li and
Liang' 2018] and [Du, Zhai, Poczos and Singh' 2019] in deep learning theory."
Subsumption-driven clause learning with DPLL+restarts,"We propose to use a DPLL+restart to solve SAT instances by successive
simplifications based on the production of clauses that subsume the initial
clauses. We show that this approach allows the refutation of pebbling formulae
in polynomial time and linear space, as effectively as with a CDCL solver."
Evolving Robust Neural Architectures to Defend from Adversarial Attacks,"Neural networks are prone to misclassify slightly modified input images.
Recently, many defences have been proposed, but none have improved the
robustness of neural networks consistently. Here, we propose to use adversarial
attacks as a function evaluation to search for neural architectures that can
resist such attacks automatically. Experiments on neural architecture search
algorithms from the literature show that although accurate, they are not able
to find robust architectures. A significant reason for this lies in their
limited search space. By creating a novel neural architecture search with
options for dense layers to connect with convolution layers and vice-versa as
well as the addition of concatenation layers in the search, we were able to
evolve an architecture that is inherently accurate on adversarial samples.
Interestingly, this inherent robustness of the evolved architecture rivals
state-of-the-art defences such as adversarial training while being trained only
on the non-adversarial samples. Moreover, the evolved architecture makes use of
some peculiar traits which might be useful for developing even more robust
ones. Thus, the results here confirm that more robust architectures exist as
well as opens up a new realm of feasibilities for the development and
exploration of neural networks.
  Code available at http://bit.ly/RobustArchitectureSearch."
"DeepHealth: Review and challenges of artificial intelligence in health
  informatics","Artificial intelligence has provided us with an exploration of a whole new
research era. As more data and better computational power become available, the
approach is being implemented in various fields. The demand for it in health
informatics is also increasing, and we can expect to see the potential benefits
of its applications in healthcare. It can help clinicians diagnose disease,
identify drug effects for each patient, understand the relationship between
genotypes and phenotypes, explore new phenotypes or treatment recommendations,
and predict infectious disease outbreaks with high accuracy. In contrast to
traditional models, recent artificial intelligence approaches do not require
domain-specific data pre-processing, and it is expected that it will ultimately
change life in the future. Despite its notable advantages, there are some key
challenges on data (high dimensionality, heterogeneity, time dependency,
sparsity, irregularity, lack of label, bias) and model (reliability,
interpretability, feasibility, security, scalability) for practical use. This
article presents a comprehensive review of research applying artificial
intelligence in health informatics, focusing on the last seven years in the
fields of medical imaging, electronic health records, genomics, sensing, and
online communication health, as well as challenges and promising directions for
future research. We highlight ongoing popular approaches' research and identify
several challenges in building models."
A Note On k-Means Probabilistic Poverty,"It is proven, by example, that the version of $k$-means with random
initialization does not have the property probabilistic k-richness."
"Designing Trustworthy AI: A Human-Machine Teaming Framework to Guide
  Development","Artificial intelligence (AI) holds great promise to empower us with knowledge
and augment our effectiveness. We can -- and must -- ensure that we keep humans
safe and in control, particularly with regard to government and public sector
applications that affect broad populations. How can AI development teams
harness the power of AI systems and design them to be valuable to humans?
Diverse teams are needed to build trustworthy artificial intelligent systems,
and those teams need to coalesce around a shared set of ethics. There are many
discussions in the AI field about ethics and trust, but there are few
frameworks available for people to use as guidance when creating these systems.
The Human-Machine Teaming (HMT) Framework for Designing Ethical AI Experiences
described in this paper, when used with a set of technical ethics, will guide
AI development teams to create AI systems that are accountable, de-risked,
respectful, secure, honest, and usable. To support the team's efforts,
activities to understand people's needs and concerns will be introduced along
with the themes to support the team's efforts. For example, usability testing
can help determine if the audience understands how the AI system works and
complies with the HMT Framework. The HMT Framework is based on reviews of
existing ethical codes and best practices in human-computer interaction and
software development. Human-machine teams are strongest when human users can
trust AI systems to behave as expected, safely, securely, and understandably.
Using the HMT Framework to design trustworthy AI systems will provide support
to teams in identifying potential issues ahead of time and making great
experiences for humans."
"Artificial Intelligence and the Future of Psychiatry: Qualitative
  Findings from a Global Physician Survey","The potential for machine learning to disrupt the medical profession is the
subject of ongoing debate within biomedical informatics. This study aimed to
explore psychiatrists' opinions about the potential impact of innovations in
artificial intelligence and machine learning on psychiatric practice. In Spring
2019, we conducted a web-based survey of 791 psychiatrists from 22 countries
worldwide. The survey measured opinions about the likelihood future technology
would fully replace physicians in performing ten key psychiatric tasks. This
study involved qualitative descriptive analysis of written response to three
open-ended questions in the survey. Comments were classified into four major
categories in relation to the impact of future technology on
patient-psychiatric interactions, the quality of patient medical care, the
profession of psychiatry, and health systems. Overwhelmingly, psychiatrists
were skeptical that technology could fully replace human empathy. Many
predicted that 'man and machine' would increasingly collaborate in undertaking
clinical decisions, with mixed opinions about the benefits and harms of such an
arrangement. Participants were optimistic that technology might improve
efficiencies and access to care, and reduce costs. Ethical and regulatory
considerations received limited attention. This study presents timely
information of psychiatrists' view about the scope of artificial intelligence
and machine learning on psychiatric practice. Psychiatrists expressed divergent
views about the value and impact of future technology with worrying omissions
about practice guidelines, and ethical and regulatory issues."
Information Retrieval and Its Sister Disciplines,"This article presents a summary graph to show the relationships between
Information Retrieval (IR) and other related disciplines. The figure tells the
key differences between them and the conditions under which one would
transition into another."
"A Stable Nuclear Future? The Impact of Autonomous Systems and Artificial
  Intelligence","The potential for advances in information-age technologies to undermine
nuclear deterrence and influence the potential for nuclear escalation
represents a critical question for international politics. One challenge is
that uncertainty about the trajectory of technologies such as autonomous
systems and artificial intelligence (AI) makes assessments difficult. This
paper evaluates the relative impact of autonomous systems and artificial
intelligence in three areas: nuclear command and control, nuclear delivery
platforms and vehicles, and conventional applications of autonomous systems
with consequences for nuclear stability. We argue that countries may be more
likely to use risky forms of autonomy when they fear that their second-strike
capabilities will be undermined. Additionally, the potential deployment of
uninhabited, autonomous nuclear delivery platforms and vehicles could raise the
prospect for accidents and miscalculation. Conventional military applications
of autonomous systems could simultaneously influence nuclear force postures and
first-strike stability in previously unanticipated ways. In particular, the
need to fight at machine speed and the cognitive risk introduced by automation
bias could increase the risk of unintended escalation. Finally, used properly,
there should be many applications of more autonomous systems in nuclear
operations that can increase reliability, reduce the risk of accidents, and buy
more time for decision-makers in a crisis."
"Proceedings of the twelfth Workshop on Answer Set Programming and Other
  Computing Paradigms 2019","This is the Proceedings of the twelfth Workshop on Answer Set Programming and
Other Computing Paradigms (ASPOCP) 2019, which was held in Philadelphia, USA,
June 3rd , 2019."
Monte Carlo Game Solver,"We present a general algorithm to order moves so as to speedup exact game
solvers. It uses online learning of playout policies and Monte Carlo Tree
Search. The learned policy and the information in the Monte Carlo tree are used
to order moves in game solvers. They improve greatly the solving time for
multiple games."
Topologically sensitive metaheuristics,"This paper proposes topologically sensitive metaheuristics, and describes
conceptual design of topologically sensitive Variable Neighborhood Search
method (TVNS) and topologically sensitive Electromagnetism Metaheuristic (TEM)."
"Beyond STEM, How Can Women Engage Big Data, Analytics, Robotics and
  Artificial Intelligence? An Exploratory Analysis of Confidence and
  Educational Factors in the Emerging Technology Waves Influencing the Role of,
  and Impact Upon, Women","In spite of the rapidly advancing global technological environment, the
professional participation of women in technology, big data, analytics,
artificial intelligence and information systems related domains remains
proportionately low. Furthermore, it is of no less concern that the number of
women in leadership in these domains are in even lower proportions. In spite of
numerous initiatives to improve the participation of women in technological
domains, there is an increasing need to gain additional insights into this
phenomenon especially since it occurs in nations and geographies which have
seen a sharp rise in overall female education, without such increase
translating into a corresponding spurt in information systems and technological
roles for women. The present paper presents findings from an exploratory
analysis and outlines a framework to gain insights into educational factors in
the emerging technology waves influencing the role of, and impact upon, women.
We specifically identify ways for learning and self-efficacy as key factors,
which together lead us to the Advancement of Women in Technology (AWT) insights
framework. Based on the AWT framework, we also proposition principles that can
be used to encourage higher professional engagement of women in emerging and
advanced technologies. Key Words- Women's Education, Technology, Artificial
Intelligence, Knowing, Confidence, Self-Efficacy, Learning."
Trust-based Multiagent Consensus or Weightings Aggregation,"We introduce a framework for reaching a consensus amongst several agents
communicating via a trust network on conflicting information about their
environment. We formalise our approach and provide an empirical and theoretical
analysis of its properties."
"SAIA: Split Artificial Intelligence Architecture for Mobile Healthcare
  System","As the advancement of deep learning (DL), the Internet of Things and cloud
computing techniques for biomedical and healthcare problems, mobile healthcare
systems have received unprecedented attention. Since DL techniques usually
require enormous amount of computation, most of them cannot be directly
deployed on the resource-constrained mobile and IoT devices. Hence, most of the
mobile healthcare systems leverage the cloud computing infrastructure, where
the data collected by the mobile and IoT devices would be transmitted to the
cloud computing platforms for analysis. However, in the contested environments,
relying on the cloud might not be practical at all times. For instance, the
satellite communication might be denied or disrupted. We propose SAIA, a Split
Artificial Intelligence Architecture for mobile healthcare systems. Unlike
traditional approaches for artificial intelligence (AI) which solely exploits
the computational power of the cloud server, SAIA could not only relies on the
cloud computing infrastructure while the wireless communication is available,
but also utilizes the lightweight AI solutions that work locally on the client
side, hence, it can work even when the communication is impeded. In SAIA, we
propose a meta-information based decision unit, that could tune whether a
sample captured by the client should be operated by the embedded AI (i.e.,
keeping on the client) or the networked AI (i.e., sending to the server), under
different conditions. In our experimental evaluation, extensive experiments
have been conducted on two popular healthcare datasets. Our results show that
SAIA consistently outperforms its baselines in terms of both effectiveness and
efficiency."
Neural heuristics for SAT solving,"We use neural graph networks with a message-passing architecture and an
attention mechanism to enhance the branching heuristic in two SAT-solving
algorithms. We report improvements of learned neural heuristics compared with
two standard human-designed heuristics."
Constraint Reductions,"This is a commentary on the CP 2003 paper ""Efficient cnf encoding of boolean
cardinality constraints"". After recalling its context, we outline a
classification of Constraints with respect to their deductive power regarding
General Arc Consistency (GAC)."
"Analyzing Power Grid, ICT, and Market Without Domain Knowledge Using
  Distributed Artificial Intelligence","Modern cyber-physical systems (CPS), such as our energy infrastructure, are
becoming increasingly complex: An ever-higher share of Artificial Intelligence
(AI)-based technologies use the Information and Communication Technology (ICT)
facet of energy systems for operation optimization, cost efficiency, and to
reach CO2 goals worldwide. At the same time, markets with increased flexibility
and ever shorter trade horizons enable the multi-stakeholder situation that is
emerging in this setting. These systems still form critical infrastructures
that need to perform with highest reliability. However, today's CPS are
becoming too complex to be analyzed in the traditional monolithic approach,
where each domain, e.g., power grid and ICT as well as the energy market, are
considered as separate entities while ignoring dependencies and side-effects.
To achieve an overall analysis, we introduce the concept for an application of
distributed artificial intelligence as a self-adaptive analysis tool that is
able to analyze the dependencies between domains in CPS by attacking them. It
eschews pre-configured domain knowledge, instead exploring the CPS domains for
emergent risk situations and exploitable loopholes in codices, with a focus on
rational market actors that exploit the system while still following the market
rules."
"Opportunities and Challenges in Explainable Artificial Intelligence
  (XAI): A Survey","Nowadays, deep neural networks are widely used in mission critical systems
such as healthcare, self-driving vehicles, and military which have direct
impact on human lives. However, the black-box nature of deep neural networks
challenges its use in mission critical applications, raising ethical and
judicial concerns inducing lack of trust. Explainable Artificial Intelligence
(XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools,
techniques, and algorithms that can generate high-quality interpretable,
intuitive, human-understandable explanations of AI decisions. In addition to
providing a holistic view of the current XAI landscape in deep learning, this
paper provides mathematical summaries of seminal work. We start by proposing a
taxonomy and categorizing the XAI techniques based on their scope of
explanations, methodology behind the algorithms, and explanation level or usage
which helps build trustworthy, interpretable, and self-explanatory deep
learning models. We then describe the main principles used in XAI research and
present the historical timeline for landmark studies in XAI from 2007 to 2020.
After explaining each category of algorithms and approaches in detail, we then
evaluate the explanation maps generated by eight XAI algorithms on image data,
discuss the limitations of this approach, and provide potential future
directions to improve XAI evaluation."
Dung's semantics satisfy attack removal monotonicity,"We show that preferred, stable, complete, and grounded semantics satisfy
attack removal monotonicity. This means that if an attack from b to a is
removed, the status of a cannot worsen, e.g. if a was skeptically accepted, it
cannot become rejected."
Is there a role for statistics in artificial intelligence?,"The research on and application of artificial intelligence (AI) has triggered
a comprehensive scientific, economic, social and political discussion. Here we
argue that statistics, as an interdisciplinary scientific field, plays a
substantial role both for the theoretical and practical understanding of AI and
for its future development. Statistics might even be considered a core element
of AI. With its specialist knowledge of data evaluation, starting with the
precise formulation of the research question and passing through a study design
stage on to analysis and interpretation of the results, statistics is a natural
partner for other disciplines in teaching, research and practice. This paper
aims at contributing to the current discussion by highlighting the relevance of
statistical methodology in the context of AI development. In particular, we
discuss contributions of statistics to the field of artificial intelligence
concerning methodological development, planning and design of studies,
assessment of data quality and data collection, differentiation of causality
and associations and assessment of uncertainty in results. Moreover, the paper
also deals with the equally necessary and meaningful extension of curricula in
schools and universities."
Build Smart Grids on Artificial Intelligence -- A Real-world Example,"Power grid data are going big with the deployment of various sensors. The big
data in power grids creates huge opportunities for applying artificial
intelligence technologies to improve resilience and reliability. This paper
introduces multiple real-world applications based on artificial intelligence to
improve power grid situational awareness and resilience. These applications
include event identification, inertia estimation, event location and magnitude
estimation, data authentication, control, and stability assessment. These
applications are operating on a real-world system called FNET-GridEye, which is
a wide-area measurement network and arguably the world-largest cyber-physical
system that collects power grid big data. These applications showed much better
performance compared with conventional approaches and accomplished new tasks
that are impossible to realized using conventional technologies. These
encouraging results demonstrate that combining power grid big data and
artificial intelligence can uncover and capture the non-linear correlation
between power grid data and its stabilities indices and will potentially enable
many advanced applications that can significantly improve power grid
resilience."
Implementing Behavior Trees using Three-Valued Logic,"With consideration to behavior trees and their relevance to planning and
control, within and without game development, the distinction between stateful
and stateless models is discussed; a three-valued logic bridging traditional
control flow with behavior trees is introduced, and a C# implementation is
presented."
"Artificial Intelligence for COVID-19 Detection -- A state-of-the-art
  review","The emergence of COVID-19 has necessitated many efforts by the scientific
community for its proper management. An urgent clinical reaction is required in
the face of the unending devastation being caused by the pandemic. These
efforts include technological innovations for improvement in screening,
treatment, vaccine development, contact tracing and, survival prediction. The
use of Deep Learning (DL) and Artificial Intelligence (AI) can be sought in all
of the above-mentioned spheres. This paper aims to review the role of Deep
Learning and Artificial intelligence in various aspects of the overall COVID-19
management and particularly for COVID-19 detection and classification. The DL
models are developed to analyze clinical modalities like CT scans and X-Ray
images of patients and predict their pathological condition. A DL model aims to
detect the COVID-19 pneumonia, classify and distinguish between COVID-19,
Community-Acquired Pneumonia (CAP), Viral and Bacterial pneumonia, and normal
conditions. Furthermore, sophisticated models can be built to segment the
affected area in the lungs and quantify the infection volume for a better
understanding of the extent of damage. Many models have been developed either
independently or with the help of pre-trained models like VGG19, ResNet50, and
AlexNet leveraging the concept of transfer learning. Apart from model
development, data preprocessing and augmentation are also performed to cope
with the challenge of insufficient data samples often encountered in medical
applications. It can be evaluated that DL and AI can be effectively implemented
to withstand the challenges posed by the global emergency"
"If You're Happy, Then You Know It: The Logic of Happiness... and Sadness","The article proposes a formal semantics of happiness and sadness modalities
in imperfect information setting. It shows that these modalities are not
definable through each other and gives a sound and complete axiomatization of
their properties."
Logical-Combinatorial Approaches in Dynamic Recognition Problems,"A pattern recognition scenario, where instead of object classification into
the classes by the learning set, the algorithm aims to allocate all objects to
the same, the so-called normal class, is the research objective."
An evolutionary view on the emergence of Artificial Intelligence,"This paper draws upon the evolutionary concepts of technological relatedness
and knowledge complexity to enhance our understanding of the long-term
evolution of Artificial Intelligence (AI). We reveal corresponding patterns in
the emergence of AI - globally and in the context of specific geographies of
the US, Japan, South Korea, and China. We argue that AI emergence is associated
with increasing related variety due to knowledge commonalities as well as
increasing complexity. We use patent-based indicators for the period between
1974-2018 to analyse the evolution of AI's global technological space, to
identify its technological core as well as changes to its overall relatedness
and knowledge complexity. At the national level, we also measure countries'
overall specialisations against AI-specific ones. At the global level, we find
increasing overall relatedness and complexity of AI. However, for the
technological core of AI, which has been stable over time, we find decreasing
related variety and increasing complexity. This evidence points out that AI
innovations related to core technologies are becoming increasingly distinct
from each other. At the country level, we find that the US and Japan have been
increasing the overall relatedness of their innovations. The opposite is the
case for China and South Korea, which we associate with the fact that these
countries are overall less technologically developed than the US and Japan.
Finally, we observe a stable increasing overall complexity for all countries
apart from China, which we explain by the focus of this country in technologies
not strongly linked to AI."
"Artificial Intelligence Advances for De Novo Molecular Structure
  Modeling in Cryo-EM","Cryo-electron microscopy (cryo-EM) has become a major experimental technique
to determine the structures of large protein complexes and molecular
assemblies, as evidenced by the 2017 Nobel Prize. Although cryo-EM has been
drastically improved to generate high-resolution three-dimensional (3D) maps
that contain detailed structural information about macromolecules, the
computational methods for using the data to automatically build structure
models are lagging far behind. The traditional cryo-EM model building approach
is template-based homology modeling. Manual de novo modeling is very
time-consuming when no template model is found in the database. In recent
years, de novo cryo-EM modeling using machine learning (ML) and deep learning
(DL) has ranked among the top-performing methods in macromolecular structure
modeling. Deep-learning-based de novo cryo-EM modeling is an important
application of artificial intelligence, with impressive results and great
potential for the next generation of molecular biomedicine. Accordingly, we
systematically review the representative ML/DL-based de novo cryo-EM modeling
methods. And their significances are discussed from both practical and
methodological viewpoints. We also briefly describe the background of cryo-EM
data processing workflow. Overall, this review provides an introductory guide
to modern research on artificial intelligence (AI) for de novo molecular
structure modeling and future directions in this emerging field."
Four Generations of Control Theory Development ?,"This short article presents an opinion that control system study up to date
can be divided into four generations; namely, 1 transfer function based; 2
state-space based; 3 networked control systems; and 4 control in the new AI
era."
"Testing Lotka's Law and Pattern of Author Productivity in the Scholarly
  Publications of Artificial Intelligence","Artificial intelligence has changed our day to day life in multitude ways. AI
technology is rearing itself as a driving force to be reckoned with in the
largest industries in the world. AI has already engulfed our educational
system, our businesses and our financial establishments. The future is definite
that machines with artificial intelligence will soon be captivating over
trained manual work that now is mostly cared by humans. Machines can carry out
human-like tasks by new inputs as artificial intelligence makes it possible for
machines to learn from experience. AI data from web of science database from
2008 to 2017 have been mapped to depict the average growth rate, relative
growth rate, contribution made by authors in the view of research productivity,
authorship pattern and collaboration of AI literature. The Lotka's law on
authorship productivity of AI literature has been tested to confirm the
applicability of the law to the present data set. A K-S test was applied to
measure the degree of agreement between the distribution of the observed set of
data against the inverse general power relationship and the theoretical value
of {\alpha} =2. It is found that the inverse square law of Lotka follow as
such."
Optimal Linear Combination of Classifiers,"The question of whether to use one classifier or a combination of classifiers
is a central topic in Machine Learning. We propose here a method for finding an
optimal linear combination of classifiers derived from a bias-variance
framework for the classification task."
IoT-Enabled Social Relationships Meet Artificial Social Intelligence,"With the recent advances of the Internet of Things, and the increasing
accessibility of ubiquitous computing resources and mobile devices, the
prevalence of rich media contents, and the ensuing social, economic, and
cultural changes, computing technology and applications have evolved quickly
over the past decade. They now go beyond personal computing, facilitating
collaboration and social interactions in general, causing a quick proliferation
of social relationships among IoT entities. The increasing number of these
relationships and their heterogeneous social features have led to computing and
communication bottlenecks that prevent the IoT network from taking advantage of
these relationships to improve the offered services and customize the delivered
content, known as relationship explosion. On the other hand, the quick advances
in artificial intelligence applications in social computing have led to the
emerging of a promising research field known as Artificial Social Intelligence
(ASI) that has the potential to tackle the social relationship explosion
problem. This paper discusses the role of IoT in social relationships detection
and management, the problem of social relationships explosion in IoT and
reviews the proposed solutions using ASI, including social-oriented
machine-learning and deep-learning techniques."
"A framework for fostering transparency in shared artificial intelligence
  models by increasing visibility of contributions","Increased adoption of artificial intelligence (AI) systems into scientific
workflows will result in an increasing technical debt as the distance between
the data scientists and engineers who develop AI system components and
scientists, researchers and other users grows. This could quickly become
problematic, particularly where guidance or regulations change and
once-acceptable best practice becomes outdated, or where data sources are later
discredited as biased or inaccurate. This paper presents a novel method for
deriving a quantifiable metric capable of ranking the overall transparency of
the process pipelines used to generate AI systems, such that users, auditors
and other stakeholders can gain confidence that they will be able to validate
and trust the data sources and contributors in the AI systems that they rely
on. The methodology for calculating the metric, and the type of criteria that
could be used to make judgements on the visibility of contributions to systems
are evaluated through models published at ModelHub and PyTorch Hub, popular
archives for sharing science resources, and is found to be helpful in driving
consideration of the contributions made to generating AI systems and approaches
towards effective documentation and improving transparency in machine learning
assets shared within scientific communities."
Implementing Automated Market Makers with Constant Circle,"This paper describe the implementation details of constant ellipse based
automated market makers (CoinSwap). A CoinSwap prototype has been implemented
at http://coinswapapp.io/ and the source codes are available at
https://github.com/coinswapapp/"
"Counterfactuals and Causability in Explainable Artificial Intelligence:
  Theory, Algorithms, and Applications","There has been a growing interest in model-agnostic methods that can make
deep learning models more transparent and explainable to a user. Some
researchers recently argued that for a machine to achieve a certain degree of
human-level explainability, this machine needs to provide human causally
understandable explanations, also known as causability. A specific class of
algorithms that have the potential to provide causability are counterfactuals.
This paper presents an in-depth systematic review of the diverse existing body
of literature on counterfactuals and causability for explainable artificial
intelligence. We performed an LDA topic modelling analysis under a PRISMA
framework to find the most relevant literature articles. This analysis resulted
in a novel taxonomy that considers the grounding theories of the surveyed
algorithms, together with their underlying properties and applications in
real-world data. This research suggests that current model-agnostic
counterfactual algorithms for explainable AI are not grounded on a causal
theoretical formalism and, consequently, cannot promote causability to a human
decision-maker. Our findings suggest that the explanations derived from major
algorithms in the literature provide spurious correlations rather than
cause/effects relationships, leading to sub-optimal, erroneous or even biased
explanations. This paper also advances the literature with new directions and
challenges on promoting causability in model-agnostic approaches for
explainable artificial intelligence."
Symmetry-Preserving Paths in Integrated Gradients,"We provide rigorous proofs that the Integrated Gradients (IG) attribution
method for deep networks satisfies completeness and symmetry-preserving
properties. We also study the uniqueness of IG as a path method preserving
symmetry."
"Playing Against the Board: Rolling Horizon Evolutionary Algorithms
  Against Pandemic","Competitive board games have provided a rich and diverse testbed for
artificial intelligence. This paper contends that collaborative board games
pose a different challenge to artificial intelligence as it must balance
short-term risk mitigation with long-term winning strategies. Collaborative
board games task all players to coordinate their different powers or pool their
resources to overcome an escalating challenge posed by the board and a
stochastic ruleset. This paper focuses on the exemplary collaborative board
game Pandemic and presents a rolling horizon evolutionary algorithm designed
specifically for this game. The complex way in which the Pandemic game state
changes in a stochastic but predictable way required a number of specially
designed forward models, macro-action representations for decision-making, and
repair functions for the genetic operations of the evolutionary algorithm.
Variants of the algorithm which explore optimistic versus pessimistic game
state evaluations, different mutation rates and event horizons are compared
against a baseline hierarchical policy agent. Results show that an evolutionary
approach via short-horizon rollouts can better account for the future dangers
that the board may introduce, and guard against them. Results highlight the
types of challenges that collaborative board games pose to artificial
intelligence, especially for handling multi-player collaboration interactions."
Autonomous Driving Data Chain & Interfaces,"Recent developments in autonomous driving technology have proven that map
data may be used, not only for general routing purposes, but also for to
enhance and complement common sensor data. This document reviews the most
commonly used interfaces and formats at each step of a selfhealing map data
chain."
"Transient Information Adaptation of Artificial Intelligence: Towards
  Sustainable Data Processes in Complex Projects","Large scale projects increasingly operate in complicated settings whilst
drawing on an array of complex data-points, which require precise analysis for
accurate control and interventions to mitigate possible project failure.
Coupled with a growing tendency to rely on new information systems and
processes in change projects, 90% of megaprojects globally fail to achieve
their planned objectives. Renewed interest in the concept of Artificial
Intelligence (AI) against a backdrop of disruptive technological innovations,
seeks to enhance project managers cognitive capacity through the project
lifecycle and enhance project excellence. However, despite growing interest
there remains limited empirical insights on project managers ability to
leverage AI for cognitive load enhancement in complex settings. As such this
research adopts an exploratory sequential linear mixed methods approach to
address unresolved empirical issues on transient adaptations of AI in complex
projects, and the impact on cognitive load enhancement. Initial thematic
findings from semi-structured interviews with domain experts, suggest that in
order to leverage AI technologies and processes for sustainable cognitive load
enhancement with complex data over time, project managers require improved
knowledge and access to relevant technologies that mediate data processes in
complex projects, but equally reflect application across different project
phases. These initial findings support further hypothesis testing through a
larger quantitative study incorporating structural equation modelling to
examine the relationship between artificial intelligence and project managers
cognitive load with project data in complex contexts."
"IUPUI Driving Videos and Images in All Weather and Illumination
  Conditions","This document describes an image and video dataset of driving views captured
in all weather and illumination conditions. The data set has been submitted to
CDVL."
"Knstliche Intelligenz, quo vadis?","This paper outlines the state of the art in AI. It then describes basic
machine learning and knowledge processing techniques. Based on this, some
possibilities and limitations of future AI developments are discussed."
Lecture Notes on Voting Theory,"These lecture notes have been developed for the course Computational Social
Choice of the Artificial Intelligence MSc programme at the University of
Groningen. They cover mathematical and algorithmic aspects of voting theory."
Counterfactual Explanations as Interventions in Latent Space,"Explainable Artificial Intelligence (XAI) is a set of techniques that allows
the understanding of both technical and non-technical aspects of Artificial
Intelligence (AI) systems. XAI is crucial to help satisfying the increasingly
important demand of \emph{trustworthy} Artificial Intelligence, characterized
by fundamental characteristics such as respect of human autonomy, prevention of
harm, transparency, accountability, etc. Within XAI techniques, counterfactual
explanations aim to provide to end users a set of features (and their
corresponding values) that need to be changed in order to achieve a desired
outcome. Current approaches rarely take into account the feasibility of actions
needed to achieve the proposed explanations, and in particular they fall short
of considering the causal impact of such actions. In this paper, we present
Counterfactual Explanations as Interventions in Latent Space (CEILS), a
methodology to generate counterfactual explanations capturing by design the
underlying causal relations from the data, and at the same time to provide
feasible recommendations to reach the proposed profile. Moreover, our
methodology has the advantage that it can be set on top of existing
counterfactuals generator algorithms, thus minimising the complexity of
imposing additional causal constrains. We demonstrate the effectiveness of our
approach with a set of different experiments using synthetic and real datasets
(including a proprietary dataset of the financial domain)."
Proof Generation in CDSAT,"The main ideas in the CDSAT (Conflict-Driven Satisfiability) framework for
SMT are summarized, leading to approaches to proof generation in CDSAT."
"Application of artificial intelligence techniques for automated
  detection of myocardial infarction: A review","Myocardial infarction (MI) results in heart muscle injury due to receiving
insufficient blood flow. MI is the most common cause of mortality in
middle-aged and elderly individuals around the world. To diagnose MI,
clinicians need to interpret electrocardiography (ECG) signals, which requires
expertise and is subject to observer bias. Artificial intelligence-based
methods can be utilized to screen for or diagnose MI automatically using ECG
signals. In this work, we conducted a comprehensive assessment of artificial
intelligence-based approaches for MI detection based on ECG as well as other
biophysical signals, including machine learning (ML) and deep learning (DL)
models. The performance of traditional ML methods relies on handcrafted
features and manual selection of ECG signals, whereas DL models can automate
these tasks. The review observed that deep convolutional neural networks
(DCNNs) yielded excellent classification performance for MI diagnosis, which
explains why they have become prevalent in recent years. To our knowledge, this
is the first comprehensive survey of artificial intelligence techniques
employed for MI diagnosis using ECG and other biophysical signals."
Datasets for Studying Generalization from Easy to Hard Examples,"We describe new datasets for studying generalization from easy to hard
examples."
Prof. Schnhage's Mysterious Machines,"We give a simple Sch\""onhage Storage Modification Machine that simulates one
iteration of the Rule 110 cellular automaton. This provides an alternative
construction to Sch\""onhage's original proof of the Turing completeness of the
eponymous machines."
"Proceedings of the 9th International Symposium on Symbolic Computation
  in Software Science","This volume contains papers presented at the Ninth International Symposium on
Symbolic Computation in Software Science, SCSS 2021.
  Symbolic Computation is the science of computing with symbolic objects
(terms, formulae, programs, representations of algebraic objects, etc.).
Powerful algorithms have been developed during the past decades for the major
subareas of symbolic computation: computer algebra and computational logic.
These algorithms and methods are successfully applied in various fields,
including software science, which covers a broad range of topics about software
construction and analysis.
  Meanwhile, artificial intelligence methods and machine learning algorithms
are widely used nowadays in various domains and, in particular, combined with
symbolic computation. Several approaches mix artificial intelligence and
symbolic methods and tools deployed over large corpora to create what is known
as cognitive systems. Cognitive computing focuses on building systems that
interact with humans naturally by reasoning, aiming at learning at scale.
  The purpose of SCSS is to promote research on theoretical and practical
aspects of symbolic computation in software science, combined with modern
artificial intelligence techniques. These proceedings contain the keynote paper
by Bruno Buchberger and ten contributed papers. Besides, the conference program
included three invited talks, nine short and work-in-progress papers, and a
special session on computer algebra and computational logic. Due to the
COVID-19 pandemic, the symposium was held completely online. It was organized
by the Research Institute for Symbolic Computation (RISC) of the Johannes
Kepler University Linz on September 8--10, 2021."
Toward a Perspectivist Turn in Ground Truthing for Predictive Computing,"Most Artificial Intelligence applications are based on supervised machine
learning (ML), which ultimately grounds on manually annotated data. The
annotation process is often performed in terms of a majority vote and this has
been proved to be often problematic, as highlighted by recent studies on the
evaluation of ML models. In this article we describe and advocate for a
different paradigm, which we call data perspectivism, which moves away from
traditional gold standard datasets, towards the adoption of methods that
integrate the opinions and perspectives of the human subjects involved in the
knowledge representation step of ML processes. Drawing on previous works which
inspired our proposal we describe the potential of our proposal for not only
the more subjective tasks (e.g. those related to human language) but also to
tasks commonly understood as objective (e.g. medical decision making), and
present the main advantages of adopting a perspectivist stance in ML, as well
as possible disadvantages, and various ways in which such a stance can be
implemented in practice. Finally, we share a set of recommendations and outline
a research agenda to advance the perspectivist stance in ML."
Label Assignment Distillation for Object Detection,"This article has been removed by arXiv administrators due to a claim of
copyright infringement"
What can we learn from universal Turing machines?,"In the present paper, we construct what we call a pedagogical universal
Turing machine. We try to understand which comparisons with biological
phenomena can be deduced from its encoding and from its working."
"On games and simulators as a platform for development of artificial
  intelligence for command and control","Games and simulators can be a valuable platform to execute complex
multi-agent, multiplayer, imperfect information scenarios with significant
parallels to military applications: multiple participants manage resources and
make decisions that command assets to secure specific areas of a map or
neutralize opposing forces. These characteristics have attracted the artificial
intelligence (AI) community by supporting development of algorithms with
complex benchmarks and the capability to rapidly iterate over new ideas. The
success of artificial intelligence algorithms in real-time strategy games such
as StarCraft II have also attracted the attention of the military research
community aiming to explore similar techniques in military counterpart
scenarios. Aiming to bridge the connection between games and military
applications, this work discusses past and current efforts on how games and
simulators, together with the artificial intelligence algorithms, have been
adapted to simulate certain aspects of military missions and how they might
impact the future battlefield. This paper also investigates how advances in
virtual reality and visual augmentation systems open new possibilities in human
interfaces with gaming platforms and their military parallels."
"Adaptive maximum power point tracking using neural networks for a
  photovoltaic systems according grid","Introduction. This article deals with the optimization of the energy
conversion of a grid-connected photovoltaic system. The novelty is to develop
an intelligent maximum power point tracking technique using artificial neural
network algorithms. Purpose. Intelligent maximum power point tracking technique
is developed in order to improve the photovoltaic system performances under the
variations of the temperature and irradiation. Methods. This work is to
calculate and follow the maximum power point for a photovoltaic system
operating according to the artificial intelligence mechanism is and the latter
is used an adaptive modified perturbation and observation maximum power point
tracking algorithm based on function sign to generate an specify duty cycle
applied to DC-DC converter, where we use the feed forward artificial neural
network type trained by Levenberg-Marquardt backpropagation. Results. The
photovoltaic system that we chose to simulate and apply this intelligent
technique on it is a stand-alone photovoltaic system. According to the results
obtained from simulation of the photovoltaic system using adaptive modified
perturbation and observation artificial neural network the efficiency and the
quality of the production of energy from photovoltaic is increased. Practical
value. The proposed algorithm is validated by a dSPACE DS1104 for different
operating conditions. All practice results confirm the effectiveness of our
proposed algorithm."
"On the Limits of Design: What Are the Conceptual Constraints on
  Designing Artificial Intelligence for Social Good?","Artificial intelligence AI can bring substantial benefits to society by
helping to reduce costs, increase efficiency and enable new solutions to
complex problems. Using Floridi's notion of how to design the 'infosphere' as a
starting point, in this chapter I consider the question: what are the limits of
design, i.e. what are the conceptual constraints on designing AI for social
good? The main argument of this chapter is that while design is a useful
conceptual tool to shape technologies and societies, collective efforts towards
designing future societies are constrained by both internal and external
factors. Internal constraints on design are discussed by evoking Hardin's
thought experiment regarding 'the Tragedy of the Commons'. Further, Hayek's
classical distinction between 'cosmos' and 'taxis' is used to demarcate
external constraints on design. Finally, five design principles are presented
which are aimed at helping policymakers manage the internal and external
constraints on design. A successful approach to designing future societies
needs to account for the emergent properties of complex systems by allowing
space for serendipity and socio-technological coevolution."
Abstractions of General Reinforcement Learning,"The field of artificial intelligence (AI) is devoted to the creation of
artificial decision-makers that can perform (at least) on par with the human
counterparts on a domain of interest. Unlike the agents in traditional AI, the
agents in artificial general intelligence (AGI) are required to replicate human
intelligence in almost every domain of interest. Moreover, an AGI agent should
be able to achieve this without (virtually any) further changes, retraining, or
fine-tuning of the parameters. The real world is non-stationary, non-ergodic,
and non-Markovian: we, humans, can neither revisit our past nor are the most
recent observations sufficient statistics. Yet, we excel at a variety of
complex tasks. Many of these tasks require longterm planning. We can associate
this success to our natural faculty to abstract away task-irrelevant
information from our overwhelming sensory experience. We make task-specific
mental models of the world without much effort. Due to this ability to
abstract, we can plan on a significantly compact representation of a task
without much loss of performance. Not only this, we also abstract our actions
to produce high-level plans: the level of action-abstraction can be anywhere
between small muscle movements to a mental notion of ""doing an action"". It is
natural to assume that any AGI agent competing with humans (at every plausible
domain) should also have these abilities to abstract its experiences and
actions. This thesis is an inquiry into the existence of such abstractions
which aid efficient planing for a wide range of domains, and most importantly,
these abstractions come with some optimality guarantees."
"Explainability Is in the Mind of the Beholder: Establishing the
  Foundations of Explainable Artificial Intelligence","Explainable artificial intelligence and interpretable machine learning are
research domains growing in importance. Yet, the underlying concepts remain
somewhat elusive and lack generally agreed definitions. While recent
inspiration from social sciences has refocused the work on needs and
expectations of human recipients, the field still misses a concrete
conceptualisation. We take steps towards addressing this challenge by reviewing
the philosophical and social foundations of human explainability, which we then
translate into the technological realm. In particular, we scrutinise the notion
of algorithmic black boxes and the spectrum of understanding determined by
explanatory processes and explainees' background knowledge. This approach
allows us to define explainability as (logical) reasoning applied to
transparent insights (into, possibly black-box, predictive systems) interpreted
under background knowledge and placed within a specific context -- a process
that engenders understanding in a selected group of explainees. We then employ
this conceptualisation to revisit strategies for evaluating explainability as
well as the much disputed trade-off between transparency and predictive power,
including its implications for ante-hoc and post-hoc techniques along with
fairness and accountability established by explainability. We furthermore
discuss components of the machine learning workflow that may be in need of
interpretability, building on a range of ideas from human-centred
explainability, with a particular focus on explainees, contrastive statements
and explanatory processes. Our discussion reconciles and complements current
research to help better navigate open questions -- rather than attempting to
address any individual issue -- thus laying a solid foundation for a grounded
discussion and future progress of explainable artificial intelligence and
interpretable machine learning."
"StyleM: Stylized Metrics for Image Captioning Built with Contrastive
  N-grams","In this paper, we build two automatic evaluation metrics for evaluating the
association between a machine-generated caption and a ground truth stylized
caption: OnlyStyle and StyleCIDEr."
"Arguments about Highly Reliable Agent Designs as a Useful Path to
  Artificial Intelligence Safety","Several different approaches exist for ensuring the safety of future
Transformative Artificial Intelligence (TAI) or Artificial Superintelligence
(ASI) systems, and proponents of different approaches have made different and
debated claims about the importance or usefulness of their work in the near
term, and for future systems. Highly Reliable Agent Designs (HRAD) is one of
the most controversial and ambitious approaches, championed by the Machine
Intelligence Research Institute, among others, and various arguments have been
made about whether and how it reduces risks from future AI systems. In order to
reduce confusion in the debate about AI safety, here we build on a previous
discussion by Rice which collects and presents four central arguments which are
used to justify HRAD as a path towards safety of AI systems.
  We have titled the arguments (1) incidental utility,(2) deconfusion, (3)
precise specification, and (4) prediction. Each of these makes different,
partly conflicting claims about how future AI systems can be risky. We have
explained the assumptions and claims based on a review of published and
informal literature, along with consultation with experts who have stated
positions on the topic. Finally, we have briefly outlined arguments against
each approach and against the agenda overall."
"Can Machines Generate Personalized Music? A Hybrid Favorite-aware Method
  for User Preference Music Transfer","User preference music transfer (UPMT) is a new problem in music style
transfer that can be applied to many scenarios but remains understudied."
"Investigating the fidelity of explainable artificial intelligence
  methods for applications of convolutional neural networks in geoscience","Convolutional neural networks (CNNs) have recently attracted great attention
in geoscience due to their ability to capture non-linear system behavior and
extract predictive spatiotemporal patterns. Given their black-box nature
however, and the importance of prediction explainability, methods of
explainable artificial intelligence (XAI) are gaining popularity as a means to
explain the CNN decision-making strategy. Here, we establish an intercomparison
of some of the most popular XAI methods and investigate their fidelity in
explaining CNN decisions for geoscientific applications. Our goal is to raise
awareness of the theoretical limitations of these methods and gain insight into
the relative strengths and weaknesses to help guide best practices. The
considered XAI methods are first applied to an idealized attribution benchmark,
where the ground truth of explanation of the network is known a priori, to help
objectively assess their performance. Secondly, we apply XAI to a
climate-related prediction setting, namely to explain a CNN that is trained to
predict the number of atmospheric rivers in daily snapshots of climate
simulations. Our results highlight several important issues of XAI methods
(e.g., gradient shattering, inability to distinguish the sign of attribution,
ignorance to zero input) that have previously been overlooked in our field and,
if not considered cautiously, may lead to a distorted picture of the CNN
decision-making strategy. We envision that our analysis will motivate further
investigation into XAI fidelity and will help towards a cautious implementation
of XAI in geoscience, which can lead to further exploitation of CNNs and deep
learning for prediction problems."
"Breast cancer detection using artificial intelligence techniques: A
  systematic literature review","Cancer is one of the most dangerous diseases to humans, and yet no permanent
cure has been developed for it. Breast cancer is one of the most common cancer
types. According to the National Breast Cancer foundation, in 2020 alone, more
than 276,000 new cases of invasive breast cancer and more than 48,000
non-invasive cases were diagnosed in the US. To put these figures in
perspective, 64% of these cases are diagnosed early in the disease's cycle,
giving patients a 99% chance of survival. Artificial intelligence and machine
learning have been used effectively in detection and treatment of several
dangerous diseases, helping in early diagnosis and treatment, and thus
increasing the patient's chance of survival. Deep learning has been designed to
analyze the most important features affecting detection and treatment of
serious diseases. For example, breast cancer can be detected using genes or
histopathological imaging. Analysis at the genetic level is very expensive, so
histopathological imaging is the most common approach used to detect breast
cancer. In this research work, we systematically reviewed previous work done on
detection and treatment of breast cancer using genetic sequencing or
histopathological imaging with the help of deep learning and machine learning.
We also provide recommendations to researchers who will work in this field"
No Efficient Disjunction or Conjunction of Switch-Lists,"It is shown that disjunction of two switch-lists can blow up the
representation size exponentially. Since switch-lists can be negated without
any increase in size, this shows that conjunction of switch-lists also leads to
an exponential blow-up in general."
"Solving Disjunctive Temporal Networks with Uncertainty under Restricted
  Time-Based Controllability using Tree Search and Graph Neural Networks","Planning under uncertainty is an area of interest in artificial intelligence.
We present a novel approach based on tree search and graph machine learning for
the scheduling problem known as Disjunctive Temporal Networks with Uncertainty
(DTNU). Dynamic Controllability (DC) of DTNUs seeks a reactive scheduling
strategy to satisfy temporal constraints in response to uncontrollable action
durations. We introduce new semantics for reactive scheduling: Time-based
Dynamic Controllability (TDC) and a restricted subset of TDC, R-TDC. We design
a tree search algorithm to determine whether or not a DTNU is R-TDC. Moreover,
we leverage a graph neural network as a heuristic for tree search guidance.
Finally, we conduct experiments on a known benchmark on which we show R-TDC to
retain significant completeness with regard to DC, while being faster to prove.
This results in the tree search processing fifty percent more DTNU problems in
R-TDC than the state-of-the-art DC solver does in DC with the same time budget.
We also observe that graph neural network search guidance leads to substantial
performance gains on benchmarks of more complex DTNUs, with up to eleven times
more problems solved than the baseline tree search."
"Contextualizing Artificially Intelligent Morality: A Meta-Ethnography of
  Top-Down, Bottom-Up, and Hybrid Models for Theoretical and Applied Ethics in
  Artificial Intelligence","In this meta-ethnography, we explore three different angles of ethical
artificial intelligence (AI) design implementation including the philosophical
ethical viewpoint, the technical perspective, and framing through a political
lens. Our qualitative research includes a literature review that highlights the
cross-referencing of these angles by discussing the value and drawbacks of
contrastive top-down, bottom-up, and hybrid approaches previously published.
The novel contribution to this framework is the political angle, which
constitutes ethics in AI either being determined by corporations and
governments and imposed through policies or law (coming from the top), or
ethics being called for by the people (coming from the bottom), as well as
top-down, bottom-up, and hybrid technicalities of how AI is developed within a
moral construct and in consideration of its users, with expected and unexpected
consequences and long-term impact in the world. There is a focus on
reinforcement learning as an example of a bottom-up applied technical approach
and AI ethics principles as a practical top-down approach. This investigation
includes real-world case studies to impart a global perspective, as well as
philosophical debate on the ethics of AI and theoretical future thought
experimentation based on historical facts, current world circumstances, and
possible ensuing realities."
"Adaptive cognitive fit: Artificial intelligence augmented management of
  information facets and representations","Explosive growth in big data technologies and artificial intelligence [AI]
applications have led to increasing pervasiveness of information facets and a
rapidly growing array of information representations. Information facets, such
as equivocality and veracity, can dominate and significantly influence human
perceptions of information and consequently affect human performance. Extant
research in cognitive fit, which preceded the big data and AI era, focused on
the effects of aligning information representation and task on performance,
without sufficient consideration to information facets and attendant cognitive
challenges. Therefore, there is a compelling need to understand the interplay
of these dominant information facets with information representations and
tasks, and their influence on human performance. We suggest that artificially
intelligent technologies that can adapt information representations to overcome
cognitive limitations are necessary for these complex information environments.
To this end, we propose and test a novel *Adaptive Cognitive Fit* [ACF]
framework that explains the influence of information facets and AI-augmented
information representations on human performance. We draw on information
processing theory and cognitive dissonance theory to advance the ACF framework
and a set of propositions. We empirically validate the ACF propositions with an
economic experiment that demonstrates the influence of information facets, and
a machine learning simulation that establishes the viability of using AI to
improve human performance."
"Expert Systems with Logic#. A Novel Modeling Framework for Logic
  Programming in an Object-Oriented Context of C#","We present a novel approach how logic programming for expert systems can be
declared directly in an object-oriented language."
"Detection of Fibrosis in Cine Magnetic Resonance Images Using Artificial
  Intelligence Techniques","Background: Artificial intelligence techniques have demonstrated great
potential in cardiology, especially to detect imperceptible patterns for the
human eye. In this sense, these techniques seem to be adequate to identify
patterns in the myocardial texture which could lead to characterize and
quantify fibrosis. Purpose: The aim of this study was to postulate a new
artificial intelligence method to identify fibrosis in cine cardiac magnetic
resonance (CMR) imaging. Methods: A retrospective observational study was
carried out in a population of 75 subjects from a clinical center of San Carlos
de Bariloche. The proposed method analyzes the myocardial texture in cine CMR
images using a convolutional neural network to determine local myocardial
tissue damage. Results: An accuracy of 89% for quantifying local tissue damage
was observed for the validation data set and 70% for the test set. In addition,
the qualitative analysis showed a high spatial correlation in lesion location.
Conclusions: The postulated method enables to spatially identify fibrosis using
only the information from cine nuclear magnetic resonance studies,
demonstrating the potential of this technique to quantify myocardial viability
in the future or to study the lesions etiology"
"Automatic Quantification of Volumes and Biventricular Function in
  Cardiac Resonance. Validation of a New Artificial Intelligence Approach","Background: Artificial intelligence techniques have shown great potential in
cardiology, especially in quantifying cardiac biventricular function, volume,
mass, and ejection fraction (EF). However, its use in clinical practice is not
straightforward due to its poor reproducibility with cases from daily practice,
among other reasons. Objectives: To validate a new artificial intelligence tool
in order to quantify the cardiac biventricular function (volume, mass, and EF).
To analyze its robustness in the clinical area, and the computational times
compared with conventional methods. Methods: A total of 189 patients were
analyzed: 89 from a regional center and 100 from a public center. The method
proposes two convolutional networks that include anatomical information of the
heart to reduce classification errors. Results: A high concordance (Pearson
coefficient) was observed between manual quantification and the proposed
quantification of cardiac function (0.98, 0.92, 0.96 and 0.8 for volumes and
biventricular EF) in about 5 seconds per study. Conclusions: This method
quantifies biventricular function and volumes in seconds with an accuracy
equivalent to that of a specialist."
"Explainable Artificial Intelligence (XAI) for Internet of Things: A
  Survey","Black-box nature of Artificial Intelligence (AI) models do not allow users to
comprehend and sometimes trust the output created by such model. In AI
applications, where not only the results but also the decision paths to the
results are critical, such black-box AI models are not sufficient. Explainable
Artificial Intelligence (XAI) addresses this problem and defines a set of AI
models that are interpretable by the users. Recently, several number of XAI
models have been to address the issues surrounding by lack of interpretability
and explainability of black-box models in various application areas such as
healthcare, military, energy, financial and industrial domains. Although the
concept of XAI has gained great deal of attention recently, its integration
into the IoT domain has not yet been fully defined. In this paper, we provide
an in-depth and systematic review of recent studies using XAI models in the
scope of IoT domain. We categorize the studies according to their methodology
and applications areas. In addition, we aim to focus on the challenging
problems and open issues and give future directions to guide the developers and
researchers for prospective future investigations."
"Malware Detection and Prevention using Artificial Intelligence
  Techniques","With the rapid technological advancement, security has become a major issue
due to the increase in malware activity that poses a serious threat to the
security and safety of both computer systems and stakeholders. To maintain
stakeholders, particularly, end users security, protecting the data from
fraudulent efforts is one of the most pressing concerns. A set of malicious
programming code, scripts, active content, or intrusive software that is
designed to destroy intended computer systems and programs or mobile and web
applications is referred to as malware. According to a study, naive users are
unable to distinguish between malicious and benign applications. Thus, computer
systems and mobile applications should be designed to detect malicious
activities towards protecting the stakeholders. A number of algorithms are
available to detect malware activities by utilizing novel concepts including
Artificial Intelligence, Machine Learning, and Deep Learning. In this study, we
emphasize Artificial Intelligence (AI) based techniques for detecting and
preventing malware activity. We present a detailed review of current malware
detection technologies, their shortcomings, and ways to improve efficiency. Our
study shows that adopting futuristic approaches for the development of malware
detection applications shall provide significant advantages. The comprehension
of this synthesis shall help researchers for further research on malware
detection and prevention using AI."
"Automated Systems For Diagnosis of Dysgraphia in Children: A Survey and
  Novel Framework","Learning disabilities, which primarily interfere with the basic learning
skills such as reading, writing and math, are known to affect around 10% of
children in the world. The poor motor skills and motor coordination as part of
the neurodevelopmental disorder can become a causative factor for the
difficulty in learning to write (dysgraphia), hindering the academic track of
an individual. The signs and symptoms of dysgraphia include but are not limited
to irregular handwriting, improper handling of writing medium, slow or labored
writing, unusual hand position, etc. The widely accepted assessment criterion
for all the types of learning disabilities is the examination performed by
medical experts. The few available artificial intelligence-powered screening
systems for dysgraphia relies on the distinctive features of handwriting from
the corresponding images.This work presents a review of the existing automated
dysgraphia diagnosis systems for children in the literature. The main focus of
the work is to review artificial intelligence-based systems for dysgraphia
diagnosis in children. This work discusses the data collection method,
important handwriting features, machine learning algorithms employed in the
literature for the diagnosis of dysgraphia. Apart from that, this article
discusses some of the non-artificial intelligence-based automated systems also.
Furthermore, this article discusses the drawbacks of existing systems and
proposes a novel framework for dysgraphia diagnosis."
"FAIR principles for AI models with a practical application for
  accelerated high energy diffraction microscopy","A concise and measurable set of FAIR (Findable, Accessible, Interoperable and
Reusable) principles for scientific data is transforming the state-of-practice
for data management and stewardship, supporting and enabling discovery and
innovation. Learning from this initiative, and acknowledging the impact of
artificial intelligence (AI) in the practice of science and engineering, we
introduce a set of practical, concise, and measurable FAIR principles for AI
models. We showcase how to create and share FAIR data and AI models within a
unified computational framework combining the following elements: the Advanced
Photon Source at Argonne National Laboratory, the Materials Data Facility, the
Data and Learning Hub for Science, and funcX, and the Argonne Leadership
Computing Facility (ALCF), in particular the ThetaGPU supercomputer and the
SambaNova DataScale system at the ALCF AI Testbed. We describe how this
domain-agnostic computational framework may be harnessed to enable autonomous
AI-driven discovery."
Automatic Differentiation: Theory and Practice,"We present the classical coordinate-free formalism for forward and backward
mode ad in the real and complex setting. We show how to formally derive the
forward and backward formulae for a number of matrix functions starting from
basic principles."
"Advances of Artificial Intelligence in Classical and Novel
  Spectroscopy-Based Approaches for Cancer Diagnostics. A Review","Cancer is one of the leading causes of death worldwide. Fast and safe
early-stage, pre- and intra-operative diagnostics can significantly contribute
to successful cancer identification and treatment. Artificial intelligence has
played an increasing role in the enhancement of cancer diagnostics techniques
in the last 15 years. This review covers the advances of artificial
intelligence applications in well-established techniques such as MRI and CT.
Also, it shows its high potential in combination with optical
spectroscopy-based approaches that are under development for mobile,
ultra-fast, and low-invasive diagnostics. I will show how spectroscopy-based
approaches can reduce the time of tissue preparation for pathological analysis
by making thin-slicing or haematoxylin-and-eosin staining obsolete. I will
present examples of spectroscopic tools for fast and low-invasive ex- and
in-vivo tissue classification for the determination of a tumour and its
boundaries. Also, I will discuss that, contrary to MRI and CT, spectroscopic
measurements do not require the administration of chemical agents to enhance
the quality of cancer imaging which contributes to the development of more
secure diagnostic methods. Overall, we will see that the combination of
spectroscopy and artificial intelligence constitutes a highly promising and
fast-developing field of medical technology that will soon augment available
cancer diagnostic methods."
"Intelligent problem-solving as integrated hierarchical reinforcement
  learning","According to cognitive psychology and related disciplines, the development of
complex problem-solving behaviour in biological agents depends on hierarchical
cognitive mechanisms. Hierarchical reinforcement learning is a promising
computational approach that may eventually yield comparable problem-solving
behaviour in artificial agents and robots. However, to date the problem-solving
abilities of many human and non-human animals are clearly superior to those of
artificial systems. Here, we propose steps to integrate biologically inspired
hierarchical mechanisms to enable advanced problem-solving skills in artificial
agents. Therefore, we first review the literature in cognitive psychology to
highlight the importance of compositional abstraction and predictive
processing. Then we relate the gained insights with contemporary hierarchical
reinforcement learning methods. Interestingly, our results suggest that all
identified cognitive mechanisms have been implemented individually in isolated
computational architectures, raising the question of why there exists no single
unifying architecture that integrates them. As our final contribution, we
address this question by providing an integrative perspective on the
computational challenges to develop such a unifying architecture. We expect our
results to guide the development of more sophisticated cognitively inspired
hierarchical machine learning architectures."
Subsampling for Knowledge Graph Embedding Explained,"In this article, we explain the recent advance of subsampling methods in
knowledge graph embedding (KGE) starting from the original one used in
word2vec."
Toward the application of XAI methods in EEG-based systems,"An interesting case of the well-known Dataset Shift Problem is the
classification of Electroencephalogram (EEG) signals in the context of
Brain-Computer Interface (BCI). The non-stationarity of EEG signals can lead to
poor generalisation performance in BCI classification systems used in different
sessions, also from the same subject. In this paper, we start from the
hypothesis that the Dataset Shift problem can be alleviated by exploiting
suitable eXplainable Artificial Intelligence (XAI) methods to locate and
transform the relevant characteristics of the input for the goal of
classification. In particular, we focus on an experimental analysis of
explanations produced by several XAI methods on an ML system trained on a
typical EEG dataset for emotion recognition. Results show that many relevant
components found by XAI methods are shared across the sessions and can be used
to build a system able to generalise better. However, relevant components of
the input signal also appear to be highly dependent on the input itself."
"Artificial intelligence and renegotiation of commercial lease contracts
  affected by pandemic-related contingencies from Covid-19. The project
  A.I.A.Co","This paper aims to investigate the possibility of using artificial
intelligence (AI) to resolve the legal issues raised by the Covid-19 emergency
about the fate of continuing execution contracts, or those with deferred or
periodic execution, as well as, more generally, to deal with exceptional events
and contingencies. We first study whether the Italian legal system allows for
''maintenance'' remedies to cope with contingencies and to avoid the
termination of the contract, while ensuring effective protection of the
interests of both parties. We then give a complete and technical description of
an AI-based predictive framework, aimed at assisting both the Magistrate (in
the course of litigation) and the parties themselves (in out-of-court
proceedings) in the redetermination of the rent of commercial lease contracts.
This framework, called A.I.A.Co. for Artificial Intelligence for contract law
Against Covid-19, has been developed under the Italian grant ''Fondo
Integrativo Speciale per la Ricerca''."
Parametric PDF for Goodness of Fit,"The goodness of fit methods for classification problems relies traditionally
on confusion matrices. This paper aims to enrich these methods with a risk
evaluation and stability analysis tools. For this purpose, we present a
parametric PDF framework."
Artificial Intelligence and Arms Control,"Potential advancements in artificial intelligence (AI) could have profound
implications for how countries research and develop weapons systems, and how
militaries deploy those systems on the battlefield. The idea of AI-enabled
military systems has motivated some activists to call for restrictions or bans
on some weapon systems, while others have argued that AI may be too diffuse to
control. This paper argues that while a ban on all military applications of AI
is likely infeasible, there may be specific cases where arms control is
possible. Throughout history, the international community has attempted to ban
or regulate weapons or military systems for a variety of reasons. This paper
analyzes both successes and failures and offers several criteria that seem to
influence why arms control works in some cases and not others. We argue that
success or failure depends on the desirability (i.e., a weapon's military value
versus its perceived horribleness) and feasibility (i.e., sociopolitical
factors that influence its success) of arms control. Based on these criteria,
and the historical record of past attempts at arms control, we analyze the
potential for AI arms control in the future and offer recommendations for what
policymakers can do today."
"Liability regimes in the age of AI: a use-case driven analysis of the
  burden of proof","New emerging technologies powered by Artificial Intelligence (AI) have the
potential to disruptively transform our societies for the better. In
particular, data-driven learning approaches (i.e., Machine Learning (ML)) have
been a true revolution in the advancement of multiple technologies in various
application domains. But at the same time there is growing concern about
certain intrinsic characteristics of these methodologies that carry potential
risks to both safety and fundamental rights. Although there are mechanisms in
the adoption process to minimize these risks (e.g., safety regulations), these
do not exclude the possibility of harm occurring, and if this happens, victims
should be able to seek compensation. Liability regimes will therefore play a
key role in ensuring basic protection for victims using or interacting with
these systems. However, the same characteristics that make AI systems
inherently risky, such as lack of causality, opacity, unpredictability or their
self and continuous learning capabilities, may lead to considerable
difficulties when it comes to proving causation. This paper presents three case
studies, as well as the methodology to reach them, that illustrate these
difficulties. Specifically, we address the cases of cleaning robots, delivery
drones and robots in education. The outcome of the proposed analysis suggests
the need to revise liability regimes to alleviate the burden of proof on
victims in cases involving AI technologies."
Bayesian Brain: Computation with Perception to Recognize 3D Objects,"We mimic the cognitive ability of Human perception, based on Bayesian
hypothesis, to recognize view-based 3D objects. We consider approximate
Bayesian (Empirical Bayesian) for perceptual inference for recognition. We
essentially handle computation with perception."
"Image-based Artificial Intelligence empowered surrogate model and shape
  morpher for real-time blank shape optimisation in the hot stamping process","As the complexity of modern manufacturing technologies increases, traditional
trial-and-error design, which requires iterative and expensive simulations,
becomes unreliable and time-consuming. This difficulty is especially
significant for the design of hot-stamped safety-critical components, such as
ultra-high-strength-steel (UHSS) B-pillars. To reduce design costs and ensure
manufacturability, scalar-based Artificial-Intelligence-empowered surrogate
modelling (SAISM) has been investigated and implemented, which can allow
real-time manufacturability-constrained structural design optimisation.
However, SAISM suffers from low accuracy and generalisability, and usually
requires a high volume of training samples. To solve this problem, an
image-based Artificial-intelligence-empowered surrogate modelling (IAISM)
approach is developed in this research, in combination with an
auto-decoder-based blank shape generator. The IAISM, which is based on a
Mask-Res-SE-U-Net architecture, is trained to predict the full thinning field
of the as-formed component given an arbitrary blank shape. Excellent prediction
performance of IAISM is achieved with only 256 training samples, which
indicates the small-data learning nature of engineering AI tasks using
structured data representations. The trained auto-decoder, trained
Mask-Res-SE-U-Net, and Adam optimiser are integrated to conduct blank
optimisation by modifying the latent vector. The optimiser can rapidly find
blank shapes that satisfy manufacturability criteria. As a high-accuracy and
generalisable surrogate modelling and optimisation tool, the proposed pipeline
is promising to be integrated into a full-chain digital twin to conduct
real-time, multi-objective design optimisation."
"IMAGINE: An Integrated Model of Artificial Intelligence-Mediated
  Communication Effects","Artificial Intelligence (AI) is transforming all fields of knowledge and
production. From surgery, autonomous driving, to image and video creation, AI
seems to make possible hitherto unimaginable processes of automation and
efficient creation. Media and communication are not an exception, and we are
currently witnessing the dawn of powerful AI tools capable of creating artistic
images from simple keywords, or to capture emotions from facial expression.
These examples may be only the beginning of what can be in the future the
engines for automatic AI real time creation of media content linked to the
emotional and behavioural responses of individuals. Although it may seem we are
still far from there, it is already the moment to adapt our theories about
media to the hypothetical scenario in which content production can be done
without human intervention, and governed by the controlled any reactions of the
individual to the exposure to media content. Following that, I propose the
definition of the Integrated Model of Artificial Intelligence-Mediated
Communication Effects (IMAGINE), and its consequences on the way we understand
media evolution (Scolari, 2012) and we think about media effects (Potter,
2010). The conceptual framework proposed is aimed to help scholars theorizing
and doing research in a scenario of continuous real-time connection between AI
measurement of people's responses to media, and the AI creation of content,
with the objective of optimizing and maximizing the processes of influence.
Parasocial interaction and real-time beautification are used as examples to
model the functioning of the IMAGINE process."
A Succinct Summary of Reinforcement Learning,"This document is a concise summary of many key results in single-agent
reinforcement learning (RL). The intended audience are those who already have
some familiarity with RL and are looking to review, reference and/or remind
themselves of important ideas in the field."
"Robofriend: An Adpative Storytelling Robotic Teddy Bear -- Technical
  Report","In this paper we describe Robofriend, a robotic teddy bear for telling
stories to young children. Robofriend adapts its behavior to keep the
childrens' attention using reinforcement learning."
"Conceptual Framework and Documentation Standards of Cystoscopic Media
  Content for Artificial Intelligence","Background: The clinical documentation of cystoscopy includes visual and
textual materials. However, the secondary use of visual cystoscopic data for
educational and research purposes remains limited due to inefficient data
management in routine clinical practice. Methods: A conceptual framework was
designed to document cystoscopy in a standardized manner with three major
sections: data management, annotation management, and utilization management. A
Swiss-cheese model was proposed for quality control and root cause analyses. We
defined the infrastructure required to implement the framework with respect to
FAIR (findable, accessible, interoperable, re-usable) principles. We applied
two scenarios exemplifying data sharing for research and educational projects
to ensure the compliance with FAIR principles. Results: The framework was
successfully implemented while following FAIR principles. The cystoscopy atlas
produced from the framework could be presented in an educational web portal; a
total of 68 full-length qualitative videos and corresponding annotation data
were sharable for artificial intelligence projects covering frame
classification and segmentation problems at case, lesion and frame levels.
Conclusion: Our study shows that the proposed framework facilitates the storage
of the visual documentation in a standardized manner and enables FAIR data for
education and artificial intelligence research."
"Modeling Moral Choices in Social Dilemmas with Multi-Agent Reinforcement
  Learning","Practical uses of Artificial Intelligence (AI) in the real world have
demonstrated the importance of embedding moral choices into intelligent agents.
They have also highlighted that defining top-down ethical constraints on AI
according to any one type of morality is extremely challenging and can pose
risks. A bottom-up learning approach may be more appropriate for studying and
developing ethical behavior in AI agents. In particular, we believe that an
interesting and insightful starting point is the analysis of emergent behavior
of Reinforcement Learning (RL) agents that act according to a predefined set of
moral rewards in social dilemmas.
  In this work, we present a systematic analysis of the choices made by
intrinsically-motivated RL agents whose rewards are based on moral theories. We
aim to design reward structures that are simplified yet representative of a set
of key ethical systems. Therefore, we first define moral reward functions that
distinguish between consequence- and norm-based agents, between morality based
on societal norms or internal virtues, and between single- and mixed-virtue
(e.g., multi-objective) methodologies. Then, we evaluate our approach by
modeling repeated dyadic interactions between learning moral agents in three
iterated social dilemma games (Prisoner's Dilemma, Volunteer's Dilemma and Stag
Hunt). We analyze the impact of different types of morality on the emergence of
cooperation, defection or exploitation, and the corresponding social outcomes.
Finally, we discuss the implications of these findings for the development of
moral agents in artificial and mixed human-AI societies."
"The Past, Current, and Future of Neonatal Intensive Care Units with
  Artificial Intelligence","Machine learning and deep learning are two subsets of artificial intelligence
that involve teaching computers to learn and make decisions from any sort of
data. Most recent developments in artificial intelligence are coming from deep
learning, which has proven revolutionary in almost all fields, from computer
vision to health sciences. The effects of deep learning in medicine have
changed the conventional ways of clinical application significantly. Although
some sub-fields of medicine, such as pediatrics, have been relatively slow in
receiving the critical benefits of deep learning, related research in
pediatrics has started to accumulate to a significant level, too. Hence, in
this paper, we review recently developed machine learning and deep
learning-based solutions for neonatology applications. We systematically
evaluate the roles of both classical machine learning and deep learning in
neonatology applications, define the methodologies, including algorithmic
developments, and describe the remaining challenges in the assessment of
neonatal diseases by using PRISMA 2020 guidelines. To date, the primary areas
of focus in neonatology regarding AI applications have included survival
analysis, neuroimaging, analysis of vital parameters and biosignals, and
retinopathy of prematurity diagnosis. We have categorically summarized 106
research articles from 1996 to 2022 and discussed their pros and cons,
respectively. In this systematic review, we aimed to further enhance the
comprehensiveness of the study. We also discuss possible directions for new AI
models and the future of neonatology with the rising power of AI, suggesting
roadmaps for the integration of AI into neonatal intensive care units."
"Algebraic characterizations of least model and uniform equivalence of
  propositional Krom logic programs","This research note provides algebraic characterizations of the least model,
subsumption, and uniform equivalence of propositional Krom logic programs."
"Affordable Artificial Intelligence -- Augmenting Farmer Knowledge with
  AI","Farms produce hundreds of thousands of data points on the ground daily.
Farming technique which combines farming practices with the insights uncovered
in these data points using AI technology is called precision farming. Precision
farming technology augments and extends farmers' deep knowledge about their
land, making production more sustainable and profitable. As part of the larger
effort at Microsoft for empowering agricultural labor force to be more
productive and sustainable, this paper presents the AI technology for
predicting micro-climate conditions on the farm.
  This article is a chapter in publication by Food and Agriculture Organization
of the United Nations and International Telecommunication Union Bangkok, 2021.
This publication on artificial intelligence (AI) for agriculture is the fifth
in the E-agriculture in Action series, launched in 2016 and jointly produced by
FAO and ITU. It aims to raise awareness about existing AI applications in
agriculture and to inspire stakeholders to develop and replicate the new ones.
Improvement of capacity and tools for capturing and processing data and
substantial advances in the field of machine learning open new horizons for
data-driven solutions that can support decision-making, facilitate supervision
and monitoring, improve the timeliness and effectiveness of safety measures
(e.g. use of pesticides), and support automation of many resource-consuming
tasks in agriculture. This publication presents the reader with a collection of
informative applications highlighting various ways AI is used in agriculture
and offering valuable insights on the implementation process, success factors,
and lessons learnt."
Logic-based similarity,"This paper develops a {\em qualitative} and logic-based notion of similarity
from the ground up using only elementary concepts of first-order logic centered
around the fundamental model-theoretic notion of type."
"What Students Can Learn About Artificial Intelligence -- Recommendations
  for K-12 Computing Education","Technological advances in the context of digital transformation are the basis
for rapid developments in the field of artificial intelligence (AI). Although
AI is not a new topic in computer science (CS), recent developments are having
an immense impact on everyday life and society. In consequence, everyone needs
competencies to be able to adequately and competently analyze, discuss and help
shape the impact, opportunities, and limits of artificial intelligence on their
personal lives and our society. As a result, an increasing number of CS
curricula are being extended to include the topic of AI. However, in order to
integrate AI into existing CS curricula, what students can and should learn in
the context of AI needs to be clarified. This has proven to be particularly
difficult, considering that so far CS education research on central concepts
and principles of AI lacks sufficient elaboration. Therefore, in this paper, we
present a curriculum of learning objectives that addresses digital literacy and
the societal perspective in particular. The learning objectives can be used to
comprehensively design curricula, but also allow for analyzing current
curricula and teaching materials and provide insights into the central concepts
and corresponding competencies of AI."
"Energy-frugal and Interpretable AI Hardware Design using Learning
  Automata","Energy efficiency is a crucial requirement for enabling powerful artificial
intelligence applications at the microedge. Hardware acceleration with frugal
architectural allocation is an effective method for reducing energy. Many
emerging applications also require the systems design to incorporate
interpretable decision models to establish responsibility and transparency. The
design needs to provision for additional resources to provide reachable states
in real-world data scenarios, defining conflicting design tradeoffs between
energy efficiency. is challenging.
  Recently a new machine learning algorithm, called the Tsetlin machine, has
been proposed. The algorithm is fundamentally based on the principles of
finite-state automata and benefits from natural logic underpinning rather than
arithmetic. In this paper, we investigate methods of energy-frugal artificial
intelligence hardware design by suitably tuning the hyperparameters, while
maintaining high learning efficacy. To demonstrate interpretability, we use
reachability and game-theoretic analysis in two simulation environments: a
SystemC model to study the bounded state transitions in the presence of
hardware faults and Nash equilibrium between states to analyze the learning
convergence. Our analyses provides the first insights into conflicting design
tradeoffs involved in energy-efficient and interpretable decision models for
this new artificial intelligence hardware architecture. We show that frugal
resource allocation coupled with systematic prodigality between randomized
reinforcements can provide decisive energy reduction while also achieving
robust and interpretable learning."
Service Composition in the ChatGPT Era,"The paper speculates about how ChatGPT-like systems can support the field of
automated service composition and identifies new research areas to explore in
order to take advantage of such tools in the field of service-oriented
composition."
Remarks on Utility in Repeated Bets,"The use of von Neumann -- Morgenstern utility is examined in the context of
multiple choices between lotteries. Different conclusions are reached if the
choices are simultaneous or sequential. It is demonstrated that utility cannot
be additive."
"For Better or Worse: The Impact of Counterfactual Explanations'
  Directionality on User Behavior in xAI","Counterfactual explanations (CFEs) are a popular approach in explainable
artificial intelligence (xAI), highlighting changes to input data necessary for
altering a model's output. A CFE can either describe a scenario that is better
than the factual state (upward CFE), or a scenario that is worse than the
factual state (downward CFE). However, potential benefits and drawbacks of the
directionality of CFEs for user behavior in xAI remain unclear. The current
user study (N=161) compares the impact of CFE directionality on behavior and
experience of participants tasked to extract new knowledge from an automated
system based on model predictions and CFEs. Results suggest that upward CFEs
provide a significant performance advantage over other forms of counterfactual
feedback. Moreover, the study highlights potential benefits of mixed CFEs
improving user performance compared to downward CFEs or no explanations. In
line with the performance results, users' explicit knowledge of the system is
statistically higher after receiving upward CFEs compared to downward
comparisons. These findings imply that the alignment between explanation and
task at hand, the so-called regulatory fit, may play a crucial role in
determining the effectiveness of model explanations, informing future research
directions in xAI. To ensure reproducible research, the entire code, underlying
models and user data of this study is openly available:
https://github.com/ukuhl/DirectionalAlienZoo"
"A generative artificial intelligence framework based on a molecular
  diffusion model for the design of metal-organic frameworks for carbon capture","Metal-organic frameworks (MOFs) exhibit great promise for CO2 capture.
However, finding the best performing materials poses computational and
experimental grand challenges in view of the vast chemical space of potential
building blocks. Here, we introduce GHP-MOFassemble, a generative artificial
intelligence (AI), high performance framework for the rational and accelerated
design of MOFs with high CO2 adsorption capacity and synthesizable linkers.
GHP-MOFassemble generates novel linkers, assembled with one of three
pre-selected metal nodes (Cu paddlewheel, Zn paddlewheel, Zn tetramer) into
MOFs in a primitive cubic topology. GHP-MOFassemble screens and validates
AI-generated MOFs for uniqueness, synthesizability, structural validity, uses
molecular dynamics simulations to study their stability and chemical
consistency, and crystal graph neural networks and Grand Canonical Monte Carlo
simulations to quantify their CO2 adsorption capacities. We present the top six
AI-generated MOFs with CO2 capacities greater than 2 $m mol/g$, i.e., higher
than 96.9% of structures in the hypothetical MOF dataset."
"What drives the acceptance of AI technology?: the role of expectations
  and experiences","In recent years, Artificial intelligence products and services have been
offered potential users as pilots. The acceptance intention towards artificial
intelligence is greatly influenced by the experience with current AI products
and services, expectations for AI, and past experiences with ICT technology.
This study aims to explore the factors that impact AI acceptance intention and
understand the process of its formation. The analysis results of this study
reveal that AI experience and past ICT experience affect AI acceptance
intention in two ways. Through the direct path, higher AI experience and ICT
experience are associated with a greater intention to accept AI. Additionally,
there is an indirect path where AI experience and ICT experience contribute to
increased expectations for AI, and these expectations, in turn, elevate
acceptance intention. Based on the findings, several recommendations are
suggested for companies and public organizations planning to implement
artificial intelligence in the future. It is crucial to manage the user
experience of ICT services and pilot AI products and services to deliver
positive experiences. It is essential to provide potential AI users with
specific information about the features and benefits of AI products and
services. This will enable them to develop realistic expectations regarding AI
technology."
When Synthetic Data Met Regulation,"In this paper, we argue that synthetic data produced by Differentially
Private generative models can be sufficiently anonymized and, therefore,
anonymous data and regulatory compliant."
What's meant by explainable model: A Scoping Review,"We often see the term explainable in the titles of papers that describe
applications based on artificial intelligence (AI). However, the literature in
explainable artificial intelligence (XAI) indicates that explanations in XAI
are application- and domain-specific, hence requiring evaluation whenever they
are employed to explain a model that makes decisions for a specific application
problem. Additionally, the literature reveals that the performance of post-hoc
methods, particularly feature attribution methods, varies substantially hinting
that they do not represent a solution to AI explainability. Therefore, when
using XAI methods, the quality and suitability of their information outputs
should be evaluated within the specific application. For these reasons, we used
a scoping review methodology to investigate papers that apply AI models and
adopt methods to generate post-hoc explanations while referring to said models
as explainable. This paper investigates whether the term explainable model is
adopted by authors under the assumption that incorporating a post-hoc XAI
method suffices to characterize a model as explainable. To inspect this
problem, our review analyzes whether these papers conducted evaluations. We
found that 81% of the application papers that refer to their approaches as an
explainable model do not conduct any form of evaluation on the XAI method they
used."
"Probabilistic Results on the Architecture of Mathematical Reasoning
  Aligned by Cognitive Alternation","We envision a machine capable of solving mathematical problems. Dividing the
quantitative reasoning system into two parts: thought processes and cognitive
processes, we provide probabilistic descriptions of the architecture."
Neurosymbolic Reinforcement Learning and Planning: A Survey,"The area of Neurosymbolic Artificial Intelligence (Neurosymbolic AI) is
rapidly developing and has become a popular research topic, encompassing
sub-fields such as Neurosymbolic Deep Learning (Neurosymbolic DL) and
Neurosymbolic Reinforcement Learning (Neurosymbolic RL). Compared to
traditional learning methods, Neurosymbolic AI offers significant advantages by
simplifying complexity and providing transparency and explainability.
Reinforcement Learning(RL), a long-standing Artificial Intelligence(AI) concept
that mimics human behavior using rewards and punishment, is a fundamental
component of Neurosymbolic RL, a recent integration of the two fields that has
yielded promising results. The aim of this paper is to contribute to the
emerging field of Neurosymbolic RL by conducting a literature survey. Our
evaluation focuses on the three components that constitute Neurosymbolic RL:
neural, symbolic, and RL. We categorize works based on the role played by the
neural and symbolic parts in RL, into three taxonomies:Learning for Reasoning,
Reasoning for Learning and Learning-Reasoning. These categories are further
divided into sub-categories based on their applications. Furthermore, we
analyze the RL components of each research work, including the state space,
action space, policy module, and RL algorithm. Additionally, we identify
research opportunities and challenges in various applications within this
dynamic field."
Natural Example-Based Explainability: a Survey,"Explainable Artificial Intelligence (XAI) has become increasingly significant
for improving the interpretability and trustworthiness of machine learning
models. While saliency maps have stolen the show for the last few years in the
XAI field, their ability to reflect models' internal processes has been
questioned. Although less in the spotlight, example-based XAI methods have
continued to improve. It encompasses methods that use examples as explanations
for a machine learning model's predictions. This aligns with the psychological
mechanisms of human reasoning and makes example-based explanations natural and
intuitive for users to understand. Indeed, humans learn and reason by forming
mental representations of concepts based on examples.
  This paper provides an overview of the state-of-the-art in natural
example-based XAI, describing the pros and cons of each approach. A ""natural""
example simply means that it is directly drawn from the training data without
involving any generative process. The exclusion of methods that require
generating examples is justified by the need for plausibility which is in some
regards required to gain a user's trust. Consequently, this paper will explore
the following family of methods: similar examples, counterfactual and
semi-factual, influential instances, prototypes, and concepts. In particular,
it will compare their semantic definition, their cognitive impact, and added
values. We hope it will encourage and facilitate future work on natural
example-based XAI."
"Explainable Artificial Intelligence for Drug Discovery and Development
  -- A Comprehensive Survey","The field of drug discovery has experienced a remarkable transformation with
the advent of artificial intelligence (AI) and machine learning (ML)
technologies. However, as these AI and ML models are becoming more complex,
there is a growing need for transparency and interpretability of the models.
Explainable Artificial Intelligence (XAI) is a novel approach that addresses
this issue and provides a more interpretable understanding of the predictions
made by machine learning models. In recent years, there has been an increasing
interest in the application of XAI techniques to drug discovery. This review
article provides a comprehensive overview of the current state-of-the-art in
XAI for drug discovery, including various XAI methods, their application in
drug discovery, and the challenges and limitations of XAI techniques in drug
discovery. The article also covers the application of XAI in drug discovery,
including target identification, compound design, and toxicity prediction.
Furthermore, the article suggests potential future research directions for the
application of XAI in drug discovery. The aim of this review article is to
provide a comprehensive understanding of the current state of XAI in drug
discovery and its potential to transform the field."
A survey of manifold learning and its applications for multimedia,"Manifold learning is an emerging research domain of machine learning. In this
work, we give an introduction into manifold learning and how it is employed for
important application fields in multimedia."
Core Challenge 2023: Solver and Graph Descriptions,"This paper collects all descriptions of solvers and ISR instances submitted
to CoRe Challenge 2023."
"A Review of Digital Twins and their Application in Cybersecurity based
  on Artificial Intelligence","The potential of digital twin technology is yet to be fully realized due to
its diversity and untapped potential. Digital twins enable systems' analysis,
design, optimization, and evolution to be performed digitally or in conjunction
with a cyber-physical approach to improve speed, accuracy, and efficiency over
traditional engineering methods. Industry 4.0, factories of the future, and
digital twins continue to benefit from the technology and provide enhanced
efficiency within existing systems. Due to the lack of information and security
standards associated with the transition to cyber digitization, cybercriminals
have been able to take advantage of the situation. Access to a digital twin of
a product or service is equivalent to threatening the entire collection. There
is a robust interaction between digital twins and artificial intelligence
tools, which leads to strong interaction between these technologies, so it can
be used to improve the cybersecurity of these digital platforms based on their
integration with these technologies. This study aims to investigate the role of
artificial intelligence in providing cybersecurity for digital twin versions of
various industries, as well as the risks associated with these versions. In
addition, this research serves as a road map for researchers and others
interested in cybersecurity and digital security."
"When Side-Channel Attacks Break the Black-Box Property of Embedded
  Artificial Intelligence","Artificial intelligence, and specifically deep neural networks (DNNs), has
rapidly emerged in the past decade as the standard for several tasks from
specific advertising to object detection. The performance offered has led DNN
algorithms to become a part of critical embedded systems, requiring both
efficiency and reliability. In particular, DNNs are subject to malicious
examples designed in a way to fool the network while being undetectable to the
human observer: the adversarial examples. While previous studies propose
frameworks to implement such attacks in black box settings, those often rely on
the hypothesis that the attacker has access to the logits of the neural
network, breaking the assumption of the traditional black box. In this paper,
we investigate a real black box scenario where the attacker has no access to
the logits. In particular, we propose an architecture-agnostic attack which
solve this constraint by extracting the logits. Our method combines hardware
and software attacks, by performing a side-channel attack that exploits
electromagnetic leakages to extract the logits for a given input, allowing an
attacker to estimate the gradients and produce state-of-the-art adversarial
examples to fool the targeted neural network. Through this example of
adversarial attack, we demonstrate the effectiveness of logits extraction using
side-channel as a first step for more general attack frameworks requiring
either the logits or the confidence scores."
"Transdisciplinary AI Education: The Confluence of Curricular and
  Community Needs in the Instruction of Artificial Intelligence","The integration of artificial intelligence (AI) into education has the
potential to transform the way we learn and teach. In this paper, we examine
the current state of AI in education and explore the potential benefits and
challenges of incorporating this technology into the classroom. The approaches
currently available for AI education often present students with experiences
only focusing on discrete computer science concepts agnostic to a larger
curriculum. However, teaching AI must not be siloed or interdisciplinary.
Rather, AI instruction ought to be transdisciplinary, including connections to
the broad curriculum and community in which students are learning. This paper
delves into the AI program currently in development for Neom Community School
and the larger Education, Research, and Innovation Sector in Neom, Saudi Arabia
s new megacity under development. In this program, AI is both taught as a
subject and to learn other subjects within the curriculum through the school
systems International Baccalaureate (IB) approach, which deploys learning
through Units of Inquiry. This approach to education connects subjects across a
curriculum under one major guiding question at a time. The proposed method
offers a meaningful approach to introducing AI to students throughout these
Units of Inquiry, as it shifts AI from a subject that students like or not like
to a subject that is taught throughout the curriculum."
Proceedings of the 2023 XCSP3 Competition,"This document represents the proceedings of the 2023 XCSP3 Competition. The
results of this competition of constraint solvers were presented at CP'23 (the
29th International Conference on Principles and Practice of Constraint
Programming, held in Toronto, Canada from 27th to 31th August, 2023)."
"Multimodality of AI for Education: Towards Artificial General
  Intelligence","This paper presents a comprehensive examination of how multimodal artificial
intelligence (AI) approaches are paving the way towards the realization of
Artificial General Intelligence (AGI) in educational contexts. It scrutinizes
the evolution and integration of AI in educational systems, emphasizing the
crucial role of multimodality, which encompasses auditory, visual, kinesthetic,
and linguistic modes of learning. This research delves deeply into the key
facets of AGI, including cognitive frameworks, advanced knowledge
representation, adaptive learning mechanisms, strategic planning, sophisticated
language processing, and the integration of diverse multimodal data sources. It
critically assesses AGI's transformative potential in reshaping educational
paradigms, focusing on enhancing teaching and learning effectiveness, filling
gaps in existing methodologies, and addressing ethical considerations and
responsible usage of AGI in educational settings. The paper also discusses the
implications of multimodal AI's role in education, offering insights into
future directions and challenges in AGI development. This exploration aims to
provide a nuanced understanding of the intersection between AI, multimodality,
and education, setting a foundation for future research and development in AGI."
"Artificial intelligence optical hardware empowers high-resolution
  hyperspectral video understanding at 1.2 Tb/s","Foundation models, exemplified by GPT technology, are discovering new
horizons in artificial intelligence by executing tasks beyond their designers'
expectations. While the present generation provides fundamental advances in
understanding language and images, the next frontier is video comprehension.
Progress in this area must overcome the 1 Tb/s data rate demanded to grasp
real-time multidimensional video information. This speed limit lies well beyond
the capabilities of the existing generation of hardware, imposing a roadblock
to further advances. This work introduces a hardware-accelerated integrated
optoelectronic platform for multidimensional video understanding in real-time.
The technology platform combines artificial intelligence hardware, processing
information optically, with state-of-the-art machine vision networks, resulting
in a data processing speed of 1.2 Tb/s with hundreds of frequency bands and
megapixel spatial resolution at video rates. Such performance, validated in the
AI tasks of video semantic segmentation and object understanding in indoor and
aerial applications, surpasses the speed of the closest technologies with
similar spectral resolution by three to four orders of magnitude. This platform
opens up new avenues for research in real-time AI video understanding of
multidimensional visual information, helping the empowerment of future
human-machine interactions and cognitive processing developments."
"The Animal-AI Environment: A Virtual Laboratory For Comparative
  Cognition and Artificial Intelligence Research","The Animal-AI Environment is a unique game-based research platform designed
to facilitate collaboration between the artificial intelligence and comparative
cognition research communities. In this paper, we present the latest version of
the Animal-AI Environment, outlining several major features that make the game
more engaging for humans and more complex for AI systems. These features
include interactive buttons, reward dispensers, and player notifications, as
well as an overhaul of the environment's graphics and processing for
significant improvements in agent training time and quality of the human player
experience. We provide detailed guidance on how to build computational and
behavioural experiments with the Animal-AI Environment. We present results from
a series of agents, including the state-of-the-art deep reinforcement learning
agent Dreamer-v3, on newly designed tests and the Animal-AI Testbed of 900
tasks inspired by research in the field of comparative cognition. The Animal-AI
Environment offers a new approach for modelling cognition in humans and
non-human animals, and for building biologically inspired artificial
intelligence."
"Business and ethical concerns in domestic Conversational Generative
  AI-empowered multi-robot systems","Business and technology are intricately connected through logic and design.
They are equally sensitive to societal changes and may be devastated by
scandal. Cooperative multi-robot systems (MRSs) are on the rise, allowing
robots of different types and brands to work together in diverse contexts.
Generative artificial intelligence has been a dominant topic in recent
artificial intelligence (AI) discussions due to its capacity to mimic humans
through the use of natural language and the production of media, including deep
fakes. In this article, we focus specifically on the conversational aspects of
generative AI, and hence use the term Conversational Generative artificial
intelligence (CGI). Like MRSs, CGIs have enormous potential for revolutionizing
processes across sectors and transforming the way humans conduct business. From
a business perspective, cooperative MRSs alone, with potential conflicts of
interest, privacy practices, and safety concerns, require ethical examination.
MRSs empowered by CGIs demand multi-dimensional and sophisticated methods to
uncover imminent ethical pitfalls. This study focuses on ethics in
CGI-empowered MRSs while reporting the stages of developing the MORUL model."
"A systematic review of geospatial location embedding approaches in large
  language models: A path to spatial AI systems","Geospatial Location Embedding (GLE) helps a Large Language Model (LLM)
assimilate and analyze spatial data. GLE emergence in Geospatial Artificial
Intelligence (GeoAI) is precipitated by the need for deeper geospatial
awareness in our complex contemporary spaces and the success of LLMs in
extracting deep meaning in Generative AI. We searched Google Scholar, Science
Direct, and arXiv for papers on geospatial location embedding and LLM and
reviewed articles focused on gaining deeper spatial ""knowing"" through LLMs. We
screened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE
themes - Entity Location Embedding (ELE), Document Location Embedding (DLE),
Sequence Location Embedding (SLE), and Token Location Embedding (TLE).
Synthesis is tabular and narrative, including a dialogic conversation between
""Space"" and ""LLM."" Though GLEs aid spatial understanding by superimposing
spatial data, they emphasize the need to advance in the intricacies of spatial
modalities and generalized reasoning. GLEs signal the need for a Spatial
Foundation/Language Model (SLM) that embeds spatial knowing within the model
architecture. The SLM framework advances Spatial Artificial Intelligence
Systems (SPAIS), establishing a Spatial Vector Space (SVS) that maps to
physical space. The resulting spatially imbued Language Model is unique. It
simultaneously represents actual space and an AI-capable space, paving the way
for AI native geo storage, analysis, and multi-modality as the basis for
Spatial Artificial Intelligence Systems (SPAIS)."
"Advancing Building Energy Modeling with Large Language Models:
  Exploration and Case Studies","The rapid progression in artificial intelligence has facilitated the
emergence of large language models like ChatGPT, offering potential
applications extending into specialized engineering modeling, especially
physics-based building energy modeling. This paper investigates the innovative
integration of large language models with building energy modeling software,
focusing specifically on the fusion of ChatGPT with EnergyPlus. A literature
review is first conducted to reveal a growing trend of incorporating large
language models in engineering modeling, albeit limited research on their
application in building energy modeling. We underscore the potential of large
language models in addressing building energy modeling challenges and outline
potential applications including simulation input generation, simulation output
analysis and visualization, conducting error analysis, co-simulation,
simulation knowledge extraction and training, and simulation optimization.
Three case studies reveal the transformative potential of large language models
in automating and optimizing building energy modeling tasks, underscoring the
pivotal role of artificial intelligence in advancing sustainable building
practices and energy efficiency. The case studies demonstrate that selecting
the right large language model techniques is essential to enhance performance
and reduce engineering efforts. The findings advocate a multidisciplinary
approach in future artificial intelligence research, with implications
extending beyond building energy modeling to other specialized engineering
modeling."
Consciousness qua Mortal Computation,"Computational functionalism posits that consciousness is a computation. Here
we show, perhaps surprisingly, that it cannot be a Turing computation. Rather,
computational functionalism implies that consciousness is a novel type of
computation that has recently been proposed by Geoffrey Hinton, called mortal
computation."
"Developing and Deploying Industry Standards for Artificial Intelligence
  in Education (AIED): Challenges, Strategies, and Future Directions","The adoption of Artificial Intelligence in Education (AIED) holds the promise
of revolutionizing educational practices by offering personalized learning
experiences, automating administrative and pedagogical tasks, and reducing the
cost of content creation. However, the lack of standardized practices in the
development and deployment of AIED solutions has led to fragmented ecosystems,
which presents challenges in interoperability, scalability, and ethical
governance. This article aims to address the critical need to develop and
implement industry standards in AIED, offering a comprehensive analysis of the
current landscape, challenges, and strategic approaches to overcome these
obstacles. We begin by examining the various applications of AIED in various
educational settings and identify key areas lacking in standardization,
including system interoperability, ontology mapping, data integration,
evaluation, and ethical governance. Then, we propose a multi-tiered framework
for establishing robust industry standards for AIED. In addition, we discuss
methodologies for the iterative development and deployment of standards,
incorporating feedback loops from real-world applications to refine and adapt
standards over time. The paper also highlights the role of emerging
technologies and pedagogical theories in shaping future standards for AIED.
Finally, we outline a strategic roadmap for stakeholders to implement these
standards, fostering a cohesive and ethical AIED ecosystem. By establishing
comprehensive industry standards, such as those by IEEE Artificial Intelligence
Standards Committee (AISC) and International Organization for Standardization
(ISO), we can accelerate and scale AIED solutions to improve educational
outcomes, ensuring that technological advances align with the principles of
inclusivity, fairness, and educational excellence."
"An Open-source End-to-End Logic Optimization Framework for Large-scale
  Boolean Network with Reinforcement Learning","We propose an open-source end-to-end logic optimization framework for
large-scale boolean network with reinforcement learning."
"Practical Applications of Advanced Cloud Services and Generative AI
  Systems in Medical Image Analysis","The medical field is one of the important fields in the application of
artificial intelligence technology. With the explosive growth and
diversification of medical data, as well as the continuous improvement of
medical needs and challenges, artificial intelligence technology is playing an
increasingly important role in the medical field. Artificial intelligence
technologies represented by computer vision, natural language processing, and
machine learning have been widely penetrated into diverse scenarios such as
medical imaging, health management, medical information, and drug research and
development, and have become an important driving force for improving the level
and quality of medical services.The article explores the transformative
potential of generative AI in medical imaging, emphasizing its ability to
generate syntheticACM-2 data, enhance images, aid in anomaly detection, and
facilitate image-to-image translation. Despite challenges like model
complexity, the applications of generative models in healthcare, including
Med-PaLM 2 technology, show promising results. By addressing limitations in
dataset size and diversity, these models contribute to more accurate diagnoses
and improved patient outcomes. However, ethical considerations and
collaboration among stakeholders are essential for responsible implementation.
Through experiments leveraging GANs to augment brain tumor MRI datasets, the
study demonstrates how generative AI can enhance image quality and diversity,
ultimately advancing medical diagnostics and patient care."
"Reframing the Mind-Body Picture: Applying Formal Systems to the
  Relationship of Mind and Matter","This paper aims to show that a simple framework, utilizing basic formalisms
from set theory and category theory, can clarify and inform our theories of the
relation between mind and matter."
"Embedding Privacy in Computational Social Science and Artificial
  Intelligence Research","Privacy is a human right. It ensures that individuals are free to engage in
discussions, participate in groups, and form relationships online or offline
without fear of their data being inappropriately harvested, analyzed, or
otherwise used to harm them. Preserving privacy has emerged as a critical
factor in research, particularly in the computational social science (CSS),
artificial intelligence (AI) and data science domains, given their reliance on
individuals' data for novel insights. The increasing use of advanced
computational models stands to exacerbate privacy concerns because, if
inappropriately used, they can quickly infringe privacy rights and lead to
adverse effects for individuals -- especially vulnerable groups -- and society.
We have already witnessed a host of privacy issues emerge with the advent of
large language models (LLMs), such as ChatGPT, which further demonstrate the
importance of embedding privacy from the start. This article contributes to the
field by discussing the role of privacy and the issues that researchers working
in CSS, AI, data science and related domains are likely to face. It then
presents several key considerations for researchers to ensure participant
privacy is best preserved in their research design, data collection and use,
analysis, and dissemination of research results."
Application and practice of AI technology in quantitative investment,"With the continuous development of artificial intelligence technology, using
machine learning technology to predict market trends may no longer be out of
reach. In recent years, artificial intelligence has become a research hotspot
in the academic circle,and it has been widely used in image recognition,
natural language processing and other fields, and also has a huge impact on the
field of quantitative investment. As an investment method to obtain stable
returns through data analysis, model construction and program trading,
quantitative investment is deeply loved by financial institutions and
investors. At the same time, as an important application field of quantitative
investment, the quantitative investment strategy based on artificial
intelligence technology arises at the historic moment.How to apply artificial
intelligence to quantitative investment, so as to better achieve profit and
risk control, has also become the focus and difficulty of the research. From a
global perspective, inflation in the US and the Federal Reserve are the
concerns of investors, which to some extent affects the direction of global
assets, including the Chinese stock market. This paper studies the application
of AI technology, quantitative investment, and AI technology in quantitative
investment, aiming to provide investors with auxiliary decision-making, reduce
the difficulty of investment analysis, and help them to obtain higher returns."
The high dimensional psychological profile and cultural bias of ChatGPT,"Given the rapid advancement of large-scale language models, artificial
intelligence (AI) models, like ChatGPT, are playing an increasingly prominent
role in human society. However, to ensure that artificial intelligence models
benefit human society, we must first fully understand the similarities and
differences between the human-like characteristics exhibited by artificial
intelligence models and real humans, as well as the cultural stereotypes and
biases that artificial intelligence models may exhibit in the process of
interacting with humans. This study first measured ChatGPT in 84 dimensions of
psychological characteristics, revealing differences between ChatGPT and human
norms in most dimensions as well as in high-dimensional psychological
representations. Additionally, through the measurement of ChatGPT in 13
dimensions of cultural values, it was revealed that ChatGPT's cultural value
patterns are dissimilar to those of various countries/regions worldwide.
Finally, an analysis of ChatGPT's performance in eight decision-making tasks
involving interactions with humans from different countries/regions revealed
that ChatGPT exhibits clear cultural stereotypes in most decision-making tasks
and shows significant cultural bias in third-party punishment and ultimatum
games. The findings indicate that, compared to humans, ChatGPT exhibits a
distinct psychological profile and cultural value orientation, and it also
shows cultural biases and stereotypes in interpersonal decision-making. Future
research endeavors should emphasize enhanced technical oversight and augmented
transparency in the database and algorithmic training procedures to foster more
efficient cross-cultural communication and mitigate social disparities."
False Sense of Security in Explainable Artificial Intelligence (XAI),"A cautious interpretation of AI regulations and policy in the EU and the USA
place explainability as a central deliverable of compliant AI systems. However,
from a technical perspective, explainable AI (XAI) remains an elusive and
complex target where even state of the art methods often reach erroneous,
misleading, and incomplete explanations. ""Explainability"" has multiple meanings
which are often used interchangeably, and there are an even greater number of
XAI methods - none of which presents a clear edge. Indeed, there are multiple
failure modes for each XAI method, which require application-specific
development and continuous evaluation. In this paper, we analyze legislative
and policy developments in the United States and the European Union, such as
the Executive Order on the Safe, Secure, and Trustworthy Development and Use of
Artificial Intelligence, the AI Act, the AI Liability Directive, and the
General Data Protection Regulation (GDPR) from a right to explanation
perspective. We argue that these AI regulations and current market conditions
threaten effective AI governance and safety because the objective of
trustworthy, accountable, and transparent AI is intrinsically linked to the
questionable ability of AI operators to provide meaningful explanations. Unless
governments explicitly tackle the issue of explainability through clear
legislative and policy statements that take into account technical realities,
AI governance risks becoming a vacuous ""box-ticking"" exercise where scientific
standards are replaced with legalistic thresholds, providing only a false sense
of security in XAI."
Kolmogorov-Arnold Networks are Radial Basis Function Networks,"This short paper is a fast proof-of-concept that the 3-order B-splines used
in Kolmogorov-Arnold Networks (KANs) can be well approximated by Gaussian
radial basis functions. Doing so leads to FastKAN, a much faster implementation
of KAN which is also a radial basis function (RBF) network."
"Artificial Intelligence Approaches for Predictive Maintenance in the
  Steel Industry: A Survey","Predictive Maintenance (PdM) emerged as one of the pillars of Industry 4.0,
and became crucial for enhancing operational efficiency, allowing to minimize
downtime, extend lifespan of equipment, and prevent failures. A wide range of
PdM tasks can be performed using Artificial Intelligence (AI) methods, which
often use data generated from industrial sensors. The steel industry, which is
an important branch of the global economy, is one of the potential
beneficiaries of this trend, given its large environmental footprint, the
globalized nature of the market, and the demanding working conditions. This
survey synthesizes the current state of knowledge in the field of AI-based PdM
within the steel industry and is addressed to researchers and practitioners. We
identified 219 articles related to this topic and formulated five research
questions, allowing us to gain a global perspective on current trends and the
main research gaps. We examined equipment and facilities subjected to PdM,
determined common PdM approaches, and identified trends in the AI methods used
to develop these solutions. We explored the characteristics of the data used in
the surveyed articles and assessed the practical implications of the research
presented there. Most of the research focuses on the blast furnace or hot
rolling, using data from industrial sensors. Current trends show increasing
interest in the domain, especially in the use of deep learning. The main
challenges include implementing the proposed methods in a production
environment, incorporating them into maintenance plans, and enhancing the
accessibility and reproducibility of the research."
"Artificial Intelligence Satellite Telecommunication Testbed using
  Commercial Off-The-Shelf Chipsets","The Artificial Intelligence Satellite Telecommunications Testbed (AISTT),
part of the ESA project SPAICE, is focused on the transformation of the
satellite payload by using artificial intelligence (AI) and machine learning
(ML) methodologies over available commercial off-the-shelf (COTS) AI-capable
chips for onboard processing. The objectives include validating artificial
intelligence-driven SATCOM scenarios such as interference detection, spectrum
sharing, radio resource management, decoding, and beamforming. The study
highlights hardware selection and payload architecture. Preliminary results
show that ML models significantly improve signal quality, spectral efficiency,
and throughput compared to conventional payload. Moreover, the testbed aims to
evaluate the performance and the use of AI-capable COTS chips in onboard SATCOM
contexts."
"Challenge-Device-Synthesis: A multi-disciplinary approach for the
  development of social innovation competences for students of Artificial
  Intelligence","The advent of Artificial Intelligence is expected to imply profound changes
in the short-term. It is therefore imperative for Academia, and particularly
for the Computer Science scope, to develop cross-disciplinary tools that bond
AI developments to their social dimension. To this aim, we introduce the
Challenge-Device-Synthesis methodology (CDS), in which a specific challenge is
presented to the students of AI, who are required to develop a device as a
solution for the challenge. The device becomes the object of study for the
different dimensions of social transformation, and the conclusions addressed by
the students during the discussion around the device are presented in a
synthesis piece in the shape of a 10-page scientific paper. The latter is
evaluated taking into account both the depth of analysis and the level to which
it genuinely reflects the social transformations associated with the proposed
AI-based device. We provide data obtained during the pilot for the
implementation phase of CDS within the subject of Social Innovation, a 6-ECTS
subject from the 6th semester of the Degree of Artificial Intelligence,
UAB-Barcelona. We provide details on temporalisation, task distribution,
methodological tools used and assessment delivery procedure, as well as
qualitative analysis of the results obtained."
Artificial Intelligence Index Report 2024,"The 2024 Index is our most comprehensive to date and arrives at an important
moment when AI's influence on society has never been more pronounced. This
year, we have broadened our scope to more extensively cover essential trends
such as technical advancements in AI, public perceptions of the technology, and
the geopolitical dynamics surrounding its development. Featuring more original
data than ever before, this edition introduces new estimates on AI training
costs, detailed analyses of the responsible AI landscape, and an entirely new
chapter dedicated to AI's impact on science and medicine. The AI Index report
tracks, collates, distills, and visualizes data related to artificial
intelligence (AI). Our mission is to provide unbiased, rigorously vetted,
broadly sourced data in order for policymakers, researchers, executives,
journalists, and the general public to develop a more thorough and nuanced
understanding of the complex field of AI. The AI Index is recognized globally
as one of the most credible and authoritative sources for data and insights on
artificial intelligence. Previous editions have been cited in major newspapers,
including the The New York Times, Bloomberg, and The Guardian, have amassed
hundreds of academic citations, and been referenced by high-level policymakers
in the United States, the United Kingdom, and the European Union, among other
places. This year's edition surpasses all previous ones in size, scale, and
scope, reflecting the growing significance that AI is coming to hold in all of
our lives."
"Breast Cancer Diagnosis: A Comprehensive Exploration of Explainable
  Artificial Intelligence (XAI) Techniques","Breast cancer (BC) stands as one of the most common malignancies affecting
women worldwide, necessitating advancements in diagnostic methodologies for
better clinical outcomes. This article provides a comprehensive exploration of
the application of Explainable Artificial Intelligence (XAI) techniques in the
detection and diagnosis of breast cancer. As Artificial Intelligence (AI)
technologies continue to permeate the healthcare sector, particularly in
oncology, the need for transparent and interpretable models becomes imperative
to enhance clinical decision-making and patient care. This review discusses the
integration of various XAI approaches, such as SHAP, LIME, Grad-CAM, and
others, with machine learning and deep learning models utilized in breast
cancer detection and classification. By investigating the modalities of breast
cancer datasets, including mammograms, ultrasounds and their processing with
AI, the paper highlights how XAI can lead to more accurate diagnoses and
personalized treatment plans. It also examines the challenges in implementing
these techniques and the importance of developing standardized metrics for
evaluating XAI's effectiveness in clinical settings. Through detailed analysis
and discussion, this article aims to highlight the potential of XAI in bridging
the gap between complex AI models and practical healthcare applications,
thereby fostering trust and understanding among medical professionals and
improving patient outcomes."
"Explain the Black Box for the Sake of Science: the Scientific Method in
  the Era of Generative Artificial Intelligence","The scientific method is the cornerstone of human progress across all
branches of the natural and applied sciences, from understanding the human body
to explaining how the universe works. The scientific method is based on
identifying systematic rules or principles that describe the phenomenon of
interest in a reproducible way that can be validated through experimental
evidence. In the era of generative artificial intelligence, there are
discussions on how AI systems may discover new knowledge. We argue that human
complex reasoning for scientific discovery remains of vital importance, at
least before the advent of artificial general intelligence. Yet, AI can be
leveraged for scientific discovery via explainable AI. More specifically,
knowing the `principles' the AI systems used to make decisions can be a point
of contact with domain experts and scientists, that can lead to divergent or
convergent views on a given scientific problem. Divergent views may spark
further scientific investigations leading to interpretability-guided
explanations (IGEs), and possibly to new scientific knowledge. We define this
field as Explainable AI for Science, where domain experts -- potentially
assisted by generative AI -- formulate scientific hypotheses and explanations
based on the interpretability of a predictive AI system."
Large Language Models Playing Mixed Strategy Nash Equilibrium Games,"Generative artificial intelligence (Generative AI), and in particular Large
Language Models (LLMs) have gained significant popularity among researchers and
industrial communities, paving the way for integrating LLMs in different
domains, such as robotics, telecom, and healthcare. In this paper, we study the
intersection of game theory and generative artificial intelligence, focusing on
the capabilities of LLMs to find the Nash equilibrium in games with a mixed
strategy Nash equilibrium and no pure strategy Nash equilibrium (that we denote
mixed strategy Nash equilibrium games). The study reveals a significant
enhancement in the performance of LLMs when they are equipped with the
possibility to run code and are provided with a specific prompt to incentivize
them to do so. However, our research also highlights the limitations of LLMs
when the randomization strategy of the game is not easy to deduce. It is
evident that while LLMs exhibit remarkable proficiency in well-known standard
games, their performance dwindles when faced with slight modifications of the
same games. This paper aims to contribute to the growing body of knowledge on
the intersection of game theory and generative artificial intelligence while
providing valuable insights into LLMs strengths and weaknesses. It also
underscores the need for further research to overcome the limitations of LLMs,
particularly in dealing with even slightly more complex scenarios, to harness
their full potential."
ClaudesLens: Uncertainty Quantification in Computer Vision Models,"In a world where more decisions are made using artificial intelligence, it is
of utmost importance to ensure these decisions are well-grounded. Neural
networks are the modern building blocks for artificial intelligence. Modern
neural network-based computer vision models are often used for object
classification tasks. Correctly classifying objects with \textit{certainty} has
become of great importance in recent times. However, quantifying the inherent
\textit{uncertainty} of the output from neural networks is a challenging task.
Here we show a possible method to quantify and evaluate the uncertainty of the
output of different computer vision models based on Shannon entropy. By adding
perturbation of different levels, on different parts, ranging from the input to
the parameters of the network, one introduces entropy to the system. By
quantifying and evaluating the perturbed models on the proposed PI and PSI
metrics, we can conclude that our theoretical framework can grant insight into
the uncertainty of predictions of computer vision models. We believe that this
theoretical framework can be applied to different applications for neural
networks. We believe that Shannon entropy may eventually have a bigger role in
the SOTA (State-of-the-art) methods to quantify uncertainty in artificial
intelligence. One day we might be able to apply Shannon entropy to our neural
systems."
"Over the Edge of Chaos? Excess Complexity as a Roadblock to Artificial
  General Intelligence","In this study, we explored the progression trajectories of artificial
intelligence (AI) systems through the lens of complexity theory. We challenged
the conventional linear and exponential projections of AI advancement toward
Artificial General Intelligence (AGI) underpinned by transformer-based
architectures, and posited the existence of critical points, akin to phase
transitions in complex systems, where AI performance might plateau or regress
into instability upon exceeding a critical complexity threshold. We employed
agent-based modelling (ABM) to simulate hypothetical scenarios of AI systems'
evolution under specific assumptions, using benchmark performance as a proxy
for capability and complexity. Our simulations demonstrated how increasing the
complexity of the AI system could exceed an upper criticality threshold,
leading to unpredictable performance behaviours. Additionally, we developed a
practical methodology for detecting these critical thresholds using simulation
data and stochastic gradient descent to fine-tune detection thresholds. This
research offers a novel perspective on AI advancement that has a particular
relevance to Large Language Models (LLMs), emphasising the need for a tempered
approach to extrapolating AI's growth potential and underscoring the importance
of developing more robust and comprehensive AI performance benchmarks."
"Evolutionary Computation for the Design and Enrichment of
  General-Purpose Artificial Intelligence Systems: Survey and Prospects","In Artificial Intelligence, there is an increasing demand for adaptive models
capable of dealing with a diverse spectrum of learning tasks, surpassing the
limitations of systems devised to cope with a single task. The recent emergence
of General-Purpose Artificial Intelligence Systems (GPAIS) poses model
configuration and adaptability challenges at far greater complexity scales than
the optimal design of traditional Machine Learning models. Evolutionary
Computation (EC) has been a useful tool for both the design and optimization of
Machine Learning models, endowing them with the capability to configure and/or
adapt themselves to the task under consideration. Therefore, their application
to GPAIS is a natural choice. This paper aims to analyze the role of EC in the
field of GPAIS, exploring the use of EC for their design or enrichment. We also
match GPAIS properties to Machine Learning areas in which EC has had a notable
contribution, highlighting recent milestones of EC for GPAIS. Furthermore, we
discuss the challenges of harnessing the benefits of EC for GPAIS, presenting
different strategies to both design and improve GPAIS with EC, covering
tangential areas, identifying research niches, and outlining potential research
directions for EC and GPAIS."
"The Pitfalls of Publishing in the Age of LLMs: Strange and Surprising
  Adventures with a High-Impact NLP Journal","We show the fraught side of the academic publishing realm and illustrate it
through a recent case study with an NLP journal."
A Survey of Accessible Explainable Artificial Intelligence Research,"The increasing integration of Artificial Intelligence (AI) into everyday life
makes it essential to explain AI-based decision-making in a way that is
understandable to all users, including those with disabilities. Accessible
explanations are crucial as accessibility in technology promotes digital
inclusion and allows everyone, regardless of their physical, sensory, or
cognitive abilities, to use these technologies effectively. This paper presents
a systematic literature review of the research on the accessibility of
Explainable Artificial Intelligence (XAI), specifically considering persons
with sight loss. Our methodology includes searching several academic databases
with search terms to capture intersections between XAI and accessibility. The
results of this survey highlight the lack of research on Accessible XAI (AXAI)
and stress the importance of including the disability community in XAI
development to promote digital inclusion and accessibility and remove barriers.
Most XAI techniques rely on visual explanations, such as heatmaps or graphs,
which are not accessible to persons who are blind or have low vision.
Therefore, it is necessary to develop explanation methods through non-visual
modalities, such as auditory and tactile feedback, visual modalities accessible
to persons with low vision, and personalized solutions that meet the needs of
individuals, including those with multiple disabilities. We further emphasize
the importance of integrating universal design principles into AI development
practices to ensure that AI technologies are usable by everyone."
The Energy Cost of Artificial Intelligence of Things Lifecycle,"Artificial Intelligence (AI) coupled with the existing Internet of Things
(IoT) enables more autonomous operations across various economic sectors. While
this paradigm shift results in increased energy consumption it is difficult to
quantify the end-to-end energy consumption of such systems with the
conventional metrics as they either focus on the communication, the computation
infrastructure or model development. To address this, we propose a new metric,
the Energy Cost of AI lifecycle (eCAL). eCAL captures the energy consumption
throughout the architectural components and lifecycle of an AI-powered wireless
system by analyzing the complexity of data collection and manipulation in
individual components and deriving overall and per-bit energy consumption. We
show that the better a model and the more it is used, the more energy efficient
an inference is. For an example Artificial Intelligence of Things (AIoT)
configuration, eCAL for making 100 inferences is 2.73 times higher than for
1000 inferences. Additionally, we developed a modular open source simulation
tool to enable researchers, practitioners, and engineers to calculate the
end-to-end energy cost with various configurations and across various systems,
ensuring adaptability to diverse use cases."
Digital Avatars: Framework Development and Their Evaluation,"We present a novel prompting strategy for artificial intelligence driven
digital avatars. To better quantify how our prompting strategy affects
anthropomorphic features like humor, authenticity, and favorability we present
Crowd Vote - an adaptation of Crowd Score that allows for judges to elect a
large language model (LLM) candidate over competitors answering the same or
similar prompts. To visualize the responses of our LLM, and the effectiveness
of our prompting strategy we propose an end-to-end framework for creating
high-fidelity artificial intelligence (AI) driven digital avatars. This
pipeline effectively captures an individual's essence for interaction and our
streaming algorithm delivers a high-quality digital avatar with real-time
audio-video streaming from server to mobile device. Both our visualization
tool, and our Crowd Vote metrics demonstrate our AI driven digital avatars have
state-of-the-art humor, authenticity, and favorability outperforming all
competitors and baselines. In the case of our Donald Trump and Joe Biden
avatars, their authenticity and favorability are rated higher than even their
real-world equivalents."
"A Disease-Specific Foundation Model Using Over 100K Fundus Images:
  Release and Validation for Abnormality and Multi-Disease Classification on
  Downstream Tasks","Artificial intelligence applied to retinal images offers significant
potential for recognizing signs and symptoms of retinal conditions and
expediting the diagnosis of eye diseases and systemic disorders. However,
developing generalized artificial intelligence models for medical data often
requires a large number of labeled images representing various disease signs,
and most models are typically task-specific, focusing on major retinal
diseases. In this study, we developed a Fundus-Specific Pretrained Model
(Image+Fundus), a supervised artificial intelligence model trained to detect
abnormalities in fundus images. A total of 57,803 images were used to develop
this pretrained model, which achieved superior performance across various
downstream tasks, indicating that our proposed model outperforms other general
methods. Our Image+Fundus model offers a generalized approach to improve model
performance while reducing the number of labeled datasets required.
Additionally, it provides more disease-specific insights into fundus images,
with visualizations generated by our model. These disease-specific foundation
models are invaluable in enhancing the performance and efficiency of deep
learning models in the field of fundus imaging."
Possible principles for aligned structure learning agents,"This paper offers a roadmap for the development of scalable aligned
artificial intelligence (AI) from first principle descriptions of natural
intelligence. In brief, a possible path toward scalable aligned AI rests upon
enabling artificial agents to learn a good model of the world that includes a
good model of our preferences. For this, the main objective is creating agents
that learn to represent the world and other agents' world models; a problem
that falls under structure learning (a.k.a. causal representation learning). We
expose the structure learning and alignment problems with this goal in mind, as
well as principles to guide us forward, synthesizing various ideas across
mathematics, statistics, and cognitive science. 1) We discuss the essential
role of core knowledge, information geometry and model reduction in structure
learning, and suggest core structural modules to learn a wide range of
naturalistic worlds. 2) We outline a way toward aligned agents through
structure learning and theory of mind. As an illustrative example, we
mathematically sketch Asimov's Laws of Robotics, which prescribe agents to act
cautiously to minimize the ill-being of other agents. We supplement this
example by proposing refined approaches to alignment. These observations may
guide the development of artificial intelligence in helping to scale existing
-- or design new -- aligned structure learning systems."
"AI Horizon Scanning, White Paper p3395, IEEE-SA. Part I: Areas of
  Attention","Generative Artificial Intelligence (AI) models may carry societal
transformation to an extent demanding a delicate balance between opportunity
and risk. This manuscript is the first of a series of White Papers informing
the development of IEEE-SA's p3995: `Standard for the Implementation of
Safeguards, Controls, and Preventive Techniques for Artificial Intelligence
(AI) Models', Chair: Marina Cort\^{e}s
(https://standards.ieee.org/ieee/3395/11378/). In this first horizon-scanning
we identify key attention areas for standards activities in AI. We examine
different principles for regulatory efforts, and review notions of
accountability, privacy, data rights and mis-use. As a safeguards standard we
devote significant attention to the stability of global infrastructures and
consider a possible overdependence on cloud computing that may result from
densely coupled AI components. We review the recent cascade-failure-like
Crowdstrike event in July 2024, as an illustration of potential impacts on
critical infrastructures from AI-induced incidents in the (near) future. It is
the first of a set of articles intended as White Papers informing the audience
on the standard development. Upcoming articles will focus on regulatory
initiatives, technology evolution and the role of AI in specific domains."
Formal Explanations for Neuro-Symbolic AI,"Despite the practical success of Artificial Intelligence (AI), current neural
AI algorithms face two significant issues. First, the decisions made by neural
architectures are often prone to bias and brittleness. Second, when a chain of
reasoning is required, neural systems often perform poorly. Neuro-symbolic
artificial intelligence is a promising approach that tackles these (and other)
weaknesses by combining the power of neural perception and symbolic reasoning.
Meanwhile, the success of AI has made it critical to understand its behaviour,
leading to the development of explainable artificial intelligence (XAI). While
neuro-symbolic AI systems have important advantages over purely neural AI, we
still need to explain their actions, which are obscured by the interactions of
the neural and symbolic components. To address the issue, this paper proposes a
formal approach to explaining the decisions of neuro-symbolic systems. The
approach hinges on the use of formal abductive explanations and on solving the
neuro-symbolic explainability problem hierarchically. Namely, it first computes
a formal explanation for the symbolic component of the system, which serves to
identify a subset of the individual parts of neural information that needs to
be explained. This is followed by explaining only those individual neural
inputs, independently of each other, which facilitates succinctness of
hierarchical formal explanations and helps to increase the overall performance
of the approach. Experimental results for a few complex reasoning tasks
demonstrate practical efficiency of the proposed approach, in comparison to
purely neural systems, from the perspective of explanation size, explanation
time, training time, model sizes, and the quality of explanations reported."
Artificial Intelligence of Things: A Survey,"The integration of the Internet of Things (IoT) and modern Artificial
Intelligence (AI) has given rise to a new paradigm known as the Artificial
Intelligence of Things (AIoT). In this survey, we provide a systematic and
comprehensive review of AIoT research. We examine AIoT literature related to
sensing, computing, and networking & communication, which form the three key
components of AIoT. In addition to advancements in these areas, we review
domain-specific AIoT systems that are designed for various important
application domains. We have also created an accompanying GitHub repository,
where we compile the papers included in this survey:
https://github.com/AIoT-MLSys-Lab/AIoT-Survey. This repository will be actively
maintained and updated with new research as it becomes available. As both IoT
and AI become increasingly critical to our society, we believe AIoT is emerging
as an essential research field at the intersection of IoT and modern AI. We
hope this survey will serve as a valuable resource for those engaged in AIoT
research and act as a catalyst for future explorations to bridge gaps and drive
advancements in this exciting field."
Artificial Theory of Mind and Self-Guided Social Organisation,"One of the challenges artificial intelligence (AI) faces is how a collection
of agents coordinate their behaviour to achieve goals that are not reachable by
any single agent. In a recent article by Ozmen et al this was framed as one of
six grand challenges: That AI needs to respect human cognitive processes at the
human-AI interaction frontier. We suggest that this extends to the AI-AI
frontier and that it should also reflect human psychology, as it is the only
successful framework we have from which to build out. In this extended abstract
we first make the case for collective intelligence in a general setting,
drawing on recent work from single neuron complexity in neural networks and ant
network adaptability in ant colonies. From there we introduce how species
relate to one another in an ecological network via niche selection, niche
choice, and niche conformity with the aim of forming an analogy with human
social network development as new agents join together and coordinate. From
there we show how our social structures are influenced by our neuro-physiology,
our psychology, and our language. This emphasises how individual people within
a social network influence the structure and performance of that network in
complex tasks, and that cognitive faculties such as Theory of Mind play a
central role. We finish by discussing the current state of the art in AI and
where there is potential for further development of a socially embodied
collective artificial intelligence that is capable of guiding its own social
structures."
"A Bibliometric Analysis of Highly Cited Artificial Intelligence
  Publications in Science Citation Index Expanded","This study aimed to identify and analyze the characteristics of highly cited
publications in the field of artificial intelligence within the Science
Citation Index Expanded from 1991 to 2022. The assessment focused on documents
that garnered 100 citations or more from the Web of Science Core Collection,
spanning from their publication date to the end of 2022. Various aspects of
these documents were analyzed, encompassing document types, the distribution of
annual production, the average number of citations per publication, Web of
Science categories, and journals. Moreover, the publication performance of
countries, institutions, and authors underwent evaluation through six
publication indicators and associated citation metrics. To facilitate a
comprehensive comparison of the authors research performance, the Y-index was
employed. The outcomes of the analysis revealed that a majority of the highly
cited articles were published within the Web of Science categories of
""artificial intelligence computer science"" and ""electrical and electronic
engineering"". Notably, the United States exhibited dominance across all six
publication indicators. Within the realm of average citations per publication,
the United Kingdom emerged as a leader for independent articles, first-author
articles, and corresponding-author articles. Exceptionally, the Chinese Academy
of Sciences in China and the Massachusetts Institute of Technology (MIT) in the
USA, contributed significantly. The significant impact of highly cited articles
extended to the output of Stanford University in the USA. B.L. Bassler
published the most highly cited articles. Upon employing the Y-index analysis,
J.E.P. Santos was identified as having the highest potential for publication.
In addition to the primary analysis, this study also presented nine classic
articles that have left an indelible mark on artificial intelligence research."
A Brief Summary of Explanatory Virtues,"In this report, I provide a brief summary of the literature in philosophy,
psychology and cognitive science about Explanatory Virtues, and link these
concepts to eXplainable AI."
Proceedings of the 2024 XCSP3 Competition,"This document represents the proceedings of the 2024 XCSP3 Competition. The
results of this competition of constraint solvers were presented at CP'24 (30th
International Conference on Principles and Practice of Constraint Programming)."
Reinforcement Learning: A Comprehensive Overview,"This manuscript gives a big-picture, up-to-date overview of the field of
(deep) reinforcement learning and sequential decision making, covering
value-based method, policy-gradient methods, model-based methods, and various
other topics (e.g., multi-agent RL, RL+LLMs, and RL+inference)."
"A comprehensive GeoAI review: Progress, Challenges and Outlooks","In recent years, Geospatial Artificial Intelligence (GeoAI) has gained
traction in the most relevant research works and industrial applications, while
also becoming involved in various fields of use. This paper offers a
comprehensive review of GeoAI as a synergistic concept applying Artificial
Intelligence (AI) methods and models to geospatial data. A preliminary study is
carried out, identifying the methodology of the work, the research motivations,
the issues and the directions to be tracked, followed by exploring how GeoAI
can be used in various interesting fields of application, such as precision
agriculture, environmental monitoring, disaster management and urban planning.
Next, a statistical and semantic analysis is carried out, followed by a clear
and precise presentation of the challenges facing GeoAI. Then, a concrete
exploration of the future prospects is provided, based on several informations
gathered during the census. To sum up, this paper provides a complete overview
of the correlation between AI and the geospatial domain, while mentioning the
researches conducted in this context, and emphasizing the close relationship
linking GeoAI with other advanced concepts such as geographic information
systems (GIS) and large-scale geospatial data, known as big geodata. This will
enable researchers and scientific community to assess the state of progress in
this promising field, and will help other interested parties to gain a better
understanding of the issues involved."
"Integrating Evidence into the Design of XAI and AI-based Decision
  Support Systems: A Means-End Framework for End-users in Construction","A narrative review is used to develop a theoretical evidence-based means-end
framework to build an epistemic foundation to uphold explainable artificial
intelligence instruments so that the reliability of outcomes generated from
decision support systems can be assured and better explained to end-users. The
implications of adopting an evidence-based approach to designing decision
support systems in construction are discussed with emphasis placed on
evaluating the strength, value, and utility of evidence needed to develop
meaningful human explanations for end-users. While the developed means-end
framework is focused on end-users, stakeholders can also utilize it to create
meaningful human explanations. However, they will vary due to their different
epistemic goals. Including evidence in the design and development of
explainable artificial intelligence and decision support systems will improve
decision-making effectiveness, enabling end-users' epistemic goals to be
achieved. The proposed means-end framework is developed from a broad spectrum
of literature. Thus, it is suggested that it can be used in construction and
other engineering domains where there is a need to integrate evidence into the
design of explainable artificial intelligence and decision support systems."
"Doctor-in-the-Loop: An Explainable, Multi-View Deep Learning Framework
  for Predicting Pathological Response in Non-Small Cell Lung Cancer","Non-small cell lung cancer (NSCLC) remains a major global health challenge,
with high post-surgical recurrence rates underscoring the need for accurate
pathological response predictions to guide personalized treatments. Although
artificial intelligence models show promise in this domain, their clinical
adoption is limited by the lack of medically grounded guidance during training,
often resulting in non-explainable intrinsic predictions. To address this, we
propose Doctor-in-the-Loop, a novel framework that integrates expert-driven
domain knowledge with explainable artificial intelligence techniques, directing
the model toward clinically relevant anatomical regions and improving both
interpretability and trustworthiness. Our approach employs a gradual multi-view
strategy, progressively refining the model's focus from broad contextual
features to finer, lesion-specific details. By incorporating domain insights at
every stage, we enhance predictive accuracy while ensuring that the model's
decision-making process aligns more closely with clinical reasoning. Evaluated
on a dataset of NSCLC patients, Doctor-in-the-Loop delivers promising
predictive performance and provides transparent, justifiable outputs,
representing a significant step toward clinically explainable artificial
intelligence in oncology."
A Logic of Uncertain Interpretation,"We introduce a logical framework for reasoning about ""uncertain
interpretations"" and investigate two key applications: a new semantics for
implication capturing a kind of ""meaning entailment"", and a conservative notion
of ""evidentially supported"" belief that takes the form of a Dempster-Shafer
belief function."
"A Study on Neuro-Symbolic Artificial Intelligence: Healthcare
  Perspectives","Over the last few decades, Artificial Intelligence (AI) scientists have been
conducting investigations to attain human-level performance by a machine in
accomplishing a cognitive task. Within machine learning, the ultimate
aspiration is to attain Artificial General Intelligence (AGI) through a
machine. This pursuit has led to the exploration of two distinct AI paradigms.
Symbolic AI, also known as classical or GOFAI (Good Old-Fashioned AI) and
Connectionist (Sub-symbolic) AI, represented by Neural Systems, are two
mutually exclusive paradigms. Symbolic AI excels in reasoning, explainability,
and knowledge representation but faces challenges in processing complex
real-world data with noise. Conversely, deep learning (Black-Box systems)
research breakthroughs in neural networks are notable, yet they lack reasoning
and interpretability. Neuro-symbolic AI (NeSy), an emerging area of AI
research, attempts to bridge this gap by integrating logical reasoning into
neural networks, enabling them to learn and reason with symbolic
representations. While a long path, this strategy has made significant progress
towards achieving common sense reasoning by systems. This article conducts an
extensive review of over 977 studies from prominent scientific databases (DBLP,
ACL, IEEExplore, Scopus, PubMed, ICML, ICLR), thoroughly examining the
multifaceted capabilities of Neuro-Symbolic AI, with a particular focus on its
healthcare applications, particularly in drug discovery, and Protein
engineering research. The survey addresses vital themes, including reasoning,
explainability, integration strategies, 41 healthcare-related use cases,
benchmarking, datasets, current approach limitations from both healthcare and
broader perspectives, and proposed novel approaches for future experiments."
Brain Organoid Computing -- an Overview,"The aim of this paper is to give an overview of brain organoid computing, its
characteristics, as well as possible advantages for future applications in the
field of artificial intelligence. An important part is the extensive
bibliography covering all relevant aspects and questions on this topic."
(How) Do reasoning models reason?,"We will provide a broad unifying perspective on the recent breed of Large
Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek R1, including their
promise, sources of power, misconceptions and limitations."
A model and package for German ColBERT,"In this work, we introduce a German version for ColBERT, a late interaction
multi-dense vector retrieval method, with a focus on RAG applications. We also
present the main features of our package for ColBERT models, supporting both
retrieval and fine-tuning workflows."
"Personalized Artificial General Intelligence (AGI) via
  Neuroscience-Inspired Continuous Learning Systems","Artificial Intelligence has made remarkable advancements in recent years,
primarily driven by increasingly large deep learning models. However, achieving
true Artificial General Intelligence (AGI) demands fundamentally new
architectures rather than merely scaling up existing models. Current approaches
largely depend on expanding model parameters, which improves task-specific
performance but falls short in enabling continuous, adaptable, and generalized
learning. Achieving AGI capable of continuous learning and personalization on
resource-constrained edge devices is an even bigger challenge.
  This paper reviews the state of continual learning and neuroscience-inspired
AI, and proposes a novel architecture for Personalized AGI that integrates
brain-like learning mechanisms for edge deployment. We review literature on
continuous lifelong learning, catastrophic forgetting, and edge AI, and discuss
key neuroscience principles of human learning, including Synaptic Pruning,
Hebbian plasticity, Sparse Coding, and Dual Memory Systems, as inspirations for
AI systems. Building on these insights, we outline an AI architecture that
features complementary fast-and-slow learning modules, synaptic
self-optimization, and memory-efficient model updates to support on-device
lifelong adaptation.
  Conceptual diagrams of the proposed architecture and learning processes are
provided. We address challenges such as catastrophic forgetting, memory
efficiency, and system scalability, and present application scenarios for
mobile AI assistants and embodied AI systems like humanoid robots. We conclude
with key takeaways and future research directions toward truly continual,
personalized AGI on the edge. While the architecture is theoretical, it
synthesizes diverse findings and offers a roadmap for future implementation."
A Generic Model for Swarm Intelligence and Its Validations,"The modeling of emergent swarm intelligence constitutes a major challenge and
it has been tackled in a number of different ways. However, existing approaches
fail to capture the nature of swarm intelligence and they are either too
abstract for practical application or not generic enough to describe the
various types of emergence phenomena. In this paper, a contradiction-centric
model for swarm intelligence is proposed, in which individu-als determine their
behaviors based on their internal contradictions whilst they associate and
interact to update their contradictions. The model hypothesizes that 1) the
emergence of swarm intelligence is rooted in the de-velopment of individuals'
internal contradictions and the interactions taking place between individuals
and the environment, and 2) swarm intelligence is essentially a combinative
reflection of the configurations of individuals' internal contradictions and
the distributions of these contradictions across individuals. The model is
formally described and five swarm intelligence systems are studied to
illustrate its broad applicability. The studies confirm the generic character
of the model and its effectiveness for describing the emergence of various
kinds of swarm intelligence; and they also demonstrate that the model is
straightforward to apply, without the need for complicated computations."
Intelligence Education made in Europe,"Global conflicts and trouble spots have thrown the world into turmoil.
Intelligence services have never been as necessary as they are today when it
comes to providing political decision-makers with concrete, accurate, and
up-to-date decision-making knowledge. This requires a common co-operation, a
common working language and a common understanding of each other. The best way
to create this ""intelligence community"" is through a harmonized intelligence
education.
  In this paper, we show how joint intelligence education can succeed. We draw
on the experience of Germany, where all intelligence services and the
Bundeswehr are academically educated together in a single degree program that
lays the foundations for a common working language. We also show how these
experiences have been successfully transferred to a European level, namely to
ICE, the Intelligence College in Europe. Our experience has shown that three
aspects are particularly important: firstly, interdisciplinarity or better,
transdisciplinarity, secondly, the integration of IT knowhow and thirdly, the
development and learning of methodological skills. Using the example of the
cyber intelligence module with a special focus on data-driven decision support,
additionally with its many points of reference to numerous other academic
modules, we show how the specific analytic methodology presented is embedded in
our specific European teaching context."
An electronic-game framework for evaluating coevolutionary algorithms,"One of the common artificial intelligence applications in electronic games
consists of making an artificial agent learn how to execute some determined
task successfully in a game environment. One way to perform this task is
through machine learning algorithms capable of learning the sequence of actions
required to win in a given game environment. There are several supervised
learning techniques able to learn the correct answer for a problem through
examples. However, when learning how to play electronic games, the correct
answer might only be known by the end of the game, after all the actions were
already taken. Thus, not being possible to measure the accuracy of each
individual action to be taken at each time step. A way for dealing with this
problem is through Neuroevolution, a method which trains Artificial Neural
Networks using evolutionary algorithms. In this article, we introduce a
framework for testing optimization algorithms with artificial agent controllers
in electronic games, called EvoMan, which is inspired in the action-platformer
game Mega Man II. The environment can be configured to run in different
experiment modes, as single evolution, coevolution and others. To demonstrate
some challenges regarding the proposed platform, as initial experiments we
applied Neuroevolution using Genetic Algorithms and the NEAT algorithm, in the
context of competitively coevolving two distinct agents in this game."
"The What, the Why, and the How of Artificial Explanations in Automated
  Decision-Making","The increasing incorporation of Artificial Intelligence in the form of
automated systems into decision-making procedures highlights not only the
importance of decision theory for automated systems but also the need for these
decision procedures to be explainable to the people involved in them.
Traditional realist accounts of explanation, wherein explanation is a relation
that holds (or does not hold) eternally between an explanans and an
explanandum, are not adequate to account for the notion of explanation required
for artificial decision procedures. We offer an alternative account of
explanation as used in the context of automated decision-making that makes
explanation an epistemic phenomenon, and one that is dependent on context. This
account of explanation better accounts for the way that we talk about, and use,
explanations and derived concepts, such as `explanatory power', and also allows
us to differentiate between reasons or causes on the one hand, which do not
need to have an epistemic aspect, and explanations on the other, which do have
such an aspect. Against this theoretical backdrop we then review existing
approaches to explanation in Artificial Intelligence and Machine Learning, and
suggest desiderata which truly explainable decision systems should fulfill."
CloudifierNet -- Deep Vision Models for Artificial Image Processing,"Today, more and more, it is necessary that most applications and documents
developed in previous or current technologies to be accessible online on
cloud-based infrastructures. That is why the migration of legacy systems
including their hosts of documents to new technologies and online
infrastructures, using modern Artificial Intelligence techniques, is absolutely
necessary. With the advancement of Artificial Intelligence and Deep Learning
with its multitude of applications, a new area of research is emerging - that
of automated systems development and maintenance. The underlying work objective
that led to this paper aims to research and develop truly intelligent systems
able to analyze user interfaces from various sources and generate real and
usable inferences ranging from architecture analysis to actual code generation.
One key element of such systems is that of artificial scene detection and
analysis based on deep learning computer vision systems. Computer vision models
and particularly deep directed acyclic graphs based on convolutional modules
are generally constructed and trained based on natural images datasets. Due to
this fact, the models will develop during the training process natural image
feature detectors apart from the base graph modules that will learn basic
primitive features. In the current paper, we will present the base principles
of a deep neural pipeline for computer vision applied to artificial scenes
(scenes generated by user interfaces or similar). Finally, we will present the
conclusions based on experimental development and benchmarking against
state-of-the-art transfer-learning implemented deep vision models."
Language and Culture Internalisation for Human-Like Autotelic AI,"Building autonomous agents able to grow open-ended repertoires of skills
across their lives is a fundamental goal of artificial intelligence (AI). A
promising developmental approach recommends the design of intrinsically
motivated agents that learn new skills by generating and pursuing their own
goals - autotelic agents. But despite recent progress, existing algorithms
still show serious limitations in terms of goal diversity, exploration,
generalisation or skill composition. This perspective calls for the immersion
of autotelic agents into rich socio-cultural worlds, an immensely important
attribute of our environment that shapes human cognition but is mostly omitted
in modern AI. Inspired by the seminal work of Vygotsky, we propose Vygotskian
autotelic agents - agents able to internalise their interactions with others
and turn them into cognitive tools. We focus on language and show how its
structure and informational content may support the development of new
cognitive functions in artificial agents as it does in humans. We justify the
approach by uncovering several examples of new artificial cognitive functions
emerging from interactions between language and embodiment in recent works at
the intersection of deep reinforcement learning and natural language
processing. Looking forward, we highlight future opportunities and challenges
for Vygotskian Autotelic AI research, including the use of language models as
cultural models supporting artificial cognitive development."
"Brain-inspired Graph Spiking Neural Networks for Commonsense Knowledge
  Representation and Reasoning","How neural networks in the human brain represent commonsense knowledge, and
complete related reasoning tasks is an important research topic in
neuroscience, cognitive science, psychology, and artificial intelligence.
Although the traditional artificial neural network using fixed-length vectors
to represent symbols has gained good performance in some specific tasks, it is
still a black box that lacks interpretability, far from how humans perceive the
world. Inspired by the grandmother-cell hypothesis in neuroscience, this work
investigates how population encoding and spiking timing-dependent plasticity
(STDP) mechanisms can be integrated into the learning of spiking neural
networks, and how a population of neurons can represent a symbol via guiding
the completion of sequential firing between different neuron populations. The
neuron populations of different communities together constitute the entire
commonsense knowledge graph, forming a giant graph spiking neural network.
Moreover, we introduced the Reward-modulated spiking timing-dependent
plasticity (R-STDP) mechanism to simulate the biological reinforcement learning
process and completed the related reasoning tasks accordingly, achieving
comparable accuracy and faster convergence speed than the graph convolutional
artificial neural networks. For the fields of neuroscience and cognitive
science, the work in this paper provided the foundation of computational
modeling for further exploration of the way the human brain represents
commonsense knowledge. For the field of artificial intelligence, this paper
indicated the exploration direction for realizing a more robust and
interpretable neural network by constructing a commonsense knowledge
representation and reasoning spiking neural networks with solid biological
plausibility."
"Artificial intelligence is algorithmic mimicry: why artificial ""agents""
  are not (and won't be) proper agents","What is the prospect of developing artificial general intelligence (AGI)? I
investigate this question by systematically comparing living and algorithmic
systems, with a special focus on the notion of ""agency."" There are three
fundamental differences to consider: (1) Living systems are autopoietic, that
is, self-manufacturing, and therefore able to set their own intrinsic goals,
while algorithms exist in a computational environment with target functions
that are both provided by an external agent. (2) Living systems are embodied in
the sense that there is no separation between their symbolic and physical
aspects, while algorithms run on computational architectures that maximally
isolate software from hardware. (3) Living systems experience a large world, in
which most problems are ill-defined (and not all definable), while algorithms
exist in a small world, in which all problems are well-defined. These three
differences imply that living and algorithmic systems have very different
capabilities and limitations. In particular, it is extremely unlikely that true
AGI (beyond mere mimicry) can be developed in the current algorithmic framework
of AI research. Consequently, discussions about the proper development and
deployment of algorithmic tools should be shaped around the dangers and
opportunities of current narrow AI, not the extremely unlikely prospect of the
emergence of true agency in artificial systems."
"Development of Autonomous Artificial Intelligence Systems for Corporate
  Management","The article discusses development of autonomous artificial intelligence
systems for corporate management. The function of a corporate director is still
one of the few that are legislated for execution by a ""natural"" rather than an
""artificial"" person. The main prerequisites for development of systems for full
automation of management decisions made at the level of a board of directors
are formed in the field of corporate law, machine learning, and compliance with
the rules of non-discrimination, transparency, and accountability of decisions
made and algorithms applied. The basic methodological approaches in terms of
corporate law for the ""autonomous director"" have already been developed and do
not get rejection among representatives of the legal sciences. However, there
is an undeniable need for further extensive research in order to amend
corporate law to effectively introduce ""autonomous directors"". In practice,
there are two main options of management decisions automation at the level of
top management and a board of directors: digital command centers or automation
of separate functions. Artificial intelligence systems will be subject to the
same strict requirements for non-discrimination, transparency, and
accountability as ""natural"" directors. At a certain stage, autonomous systems
can be an effective tool for countries, regions, and companies with a shortage
of human capital, equalizing or providing additional chances for such countries
and companies to compete on the global market."
Dissociating Artificial Intelligence from Artificial Consciousness,"Developments in machine learning and computing power suggest that artificial
general intelligence is within reach. This raises the question of artificial
consciousness: if a computer were to be functionally equivalent to a human,
being able to do all we do, would it experience sights, sounds, and thoughts,
as we do when we are conscious? Answering this question in a principled manner
can only be done on the basis of a theory of consciousness that is grounded in
phenomenology and that states the necessary and sufficient conditions for any
system, evolved or engineered, to support subjective experience. Here we employ
Integrated Information Theory (IIT), which provides principled tools to
determine whether a system is conscious, to what degree, and the content of its
experience. We consider pairs of systems constituted of simple Boolean units,
one of which -- a basic stored-program computer -- simulates the other with
full functional equivalence. By applying the principles of IIT, we demonstrate
that (i) two systems can be functionally equivalent without being phenomenally
equivalent, and (ii) that this conclusion is not dependent on the simulated
system's function. We further demonstrate that, according to IIT, it is
possible for a digital computer to simulate our behavior, possibly even by
simulating the neurons in our brain, without replicating our experience. This
contrasts sharply with computational functionalism, the thesis that performing
computations of the right kind is necessary and sufficient for consciousness."
Computing as compression: the SP theory of intelligence,"This paper provides an overview of the SP theory of intelligence and its
central idea that artificial intelligence, mainstream computing, and much of
human perception and cognition, may be understood as information compression.
  The background and origins of the SP theory are described, and the main
elements of the theory, including the key concept of multiple alignment,
borrowed from bioinformatics but with important differences. Associated with
the SP theory is the idea that redundancy in information may be understood as
repetition of patterns, that compression of information may be achieved via the
matching and unification (merging) of patterns, and that computing and
information compression are both fundamentally probabilistic. It appears that
the SP system is Turing-equivalent in the sense that anything that may be
computed with a Turing machine may, in principle, also be computed with an SP
machine.
  One of the main strengths of the SP theory and the multiple alignment concept
is in modelling concepts and phenomena in artificial intelligence. Within that
area, the SP theory provides a simple but versatile means of representing
different kinds of knowledge, it can model both the parsing and production of
natural language, with potential for the understanding and translation of
natural languages, it has strengths in pattern recognition, with potential in
computer vision, it can model several kinds of reasoning, and it has
capabilities in planning, problem solving, and unsupervised learning.
  The paper includes two examples showing how alternative parsings of an
ambiguous sentence may be modelled as multiple alignments, and another example
showing how the concept of multiple alignment may be applied in medical
diagnosis."
"Towards the Augmented Pathologist: Challenges of Explainable-AI in
  Digital Pathology","Digital pathology is not only one of the most promising fields of diagnostic
medicine, but at the same time a hot topic for fundamental research. Digital
pathology is not just the transfer of histopathological slides into digital
representations. The combination of different data sources (images, patient
records, and *omics data) together with current advances in artificial
intelligence/machine learning enable to make novel information accessible and
quantifiable to a human expert, which is not yet available and not exploited in
current medical settings. The grand goal is to reach a level of usable
intelligence to understand the data in the context of an application task,
thereby making machine decisions transparent, interpretable and explainable.
The foundation of such an ""augmented pathologist"" needs an integrated approach:
While machine learning algorithms require many thousands of training examples,
a human expert is often confronted with only a few data points. Interestingly,
humans can learn from such few examples and are able to instantly interpret
complex patterns. Consequently, the grand goal is to combine the possibilities
of artificial intelligence with human intelligence and to find a well-suited
balance between them to enable what neither of them could do on their own. This
can raise the quality of education, diagnosis, prognosis and prediction of
cancer and other diseases. In this paper we describe some (incomplete) research
issues which we believe should be addressed in an integrated and concerted
effort for paving the way towards the augmented pathologist."
Reputation in M2M Economy,"Triggered by modern technologies, our possibilities may now expand beyond the
unthinkable. Cars externally may look similar to decades ago, but a dramatic
revolution happened inside the cabin as a result of their computation,
communications, and storage capabilities. With the advent of Electric
Autonomous Vehicles (EAVs), Artificial Intelligence and ecological technologies
found the best synergy. Several transportation problems may be solved
(accidents, emissions, and congestion among others), and the foundation of
Machine-to-Machine (M2M) economy could be established, in addition to
value-added services such as infotainment (information and entertainment).
  In the world where intelligent technologies are pervading everyday life,
software and algorithms play a major role. Software has been lately introduced
in virtually every technological product available on the market, from phones
to television sets to cars and even housing. Artificial Intelligence is one of
the consequences of this pervasive presence of algorithms. The role of software
is becoming dominant and technology is, at times pervasive, of our existence.
Concerns, such as privacy and security, demand high attention and have been
already explored to some level of detail. However, intelligent agents and
actors are often considered as perfect entities that will overcome human
error-prone nature. This may not always be the case and we advocate that the
notion of reputation is also applicable to intelligent artificial agents, in
particular to EAVs."
"A Smart, Efficient, and Reliable Parking Surveillance System with Edge
  Artificial Intelligence on IoT Devices","Cloud computing has been a main-stream computing service for years. Recently,
with the rapid development in urbanization, massive video surveillance data are
produced at an unprecedented speed. A traditional solution to deal with the big
data would require a large amount of computing and storage resources. With the
advances in Internet of things (IoT), artificial intelligence, and
communication technologies, edge computing offers a new solution to the problem
by processing the data partially or wholly on the edge of a surveillance
system. In this study, we investigate the feasibility of using edge computing
for smart parking surveillance tasks, which is a key component of Smart City.
The system processing pipeline is carefully designed with the consideration of
flexibility, online surveillance, data transmission, detection accuracy, and
system reliability. It enables artificial intelligence at the edge by
implementing an enhanced single shot multibox detector (SSD). A few more
algorithms are developed on both the edge and the server targeting optimal
system efficiency and accuracy. Thorough field tests were conducted in the
Angle Lake parking garage for three months. The experimental results are
promising that the final detection method achieves over 95% accuracy in
real-world scenarios with high efficiency and reliability. The proposed smart
parking surveillance system can be a solid foundation for future applications
of intelligent transportation systems."
An ontology-based chatbot for crises management: use case coronavirus,"Today is the era of intelligence in machines. With the advances in Artificial
Intelligence, machines have started to impersonate different human traits, a
chatbot is the next big thing in the domain of conversational services. A
chatbot is a virtual person who is capable to carry out a natural conversation
with people. They can include skills that enable them to converse with the
humans in audio, visual, or textual formats. Artificial intelligence
conversational entities, also called chatbots, conversational agents, or
dialogue system, are an excellent example of such machines. Obtaining the right
information at the right time and place is the key to effective disaster
management. The term ""disaster management"" encompasses both natural and
human-caused disasters. To assist citizens, our project is to create a COVID
Assistant to provide the need of up to date information to be available 24
hours. With the growth in the World Wide Web, it is quite intelligible that
users are interested in the swift and relatedly correct information for their
hunt. A chatbot can be seen as a question-and-answer system in which experts
provide knowledge to solicit users. This master thesis is dedicated to discuss
COVID Assistant chatbot and explain each component in detail. The design of the
proposed chatbot is introduced by its seven components: Ontology, Web Scraping
module, DB, State Machine, keyword Extractor, Trained chatbot, and User
Interface."
"Bottom-up and top-down approaches for the design of neuromorphic
  processing systems: Tradeoffs and synergies between natural and artificial
  intelligence","While Moore's law has driven exponential computing power expectations, its
nearing end calls for new avenues for improving the overall system performance.
One of these avenues is the exploration of alternative brain-inspired computing
architectures that aim at achieving the flexibility and computational
efficiency of biological neural processing systems. Within this context,
neuromorphic engineering represents a paradigm shift in computing based on the
implementation of spiking neural network architectures in which processing and
memory are tightly co-located. In this paper, we provide a comprehensive
overview of the field, highlighting the different levels of granularity at
which this paradigm shift is realized and comparing design approaches that
focus on replicating natural intelligence (bottom-up) versus those that aim at
solving practical artificial intelligence applications (top-down). First, we
present the analog, mixed-signal and digital circuit design styles, identifying
the boundary between processing and memory through time multiplexing, in-memory
computation, and novel devices. Then, we highlight the key tradeoffs for each
of the bottom-up and top-down design approaches, survey their silicon
implementations, and carry out detailed comparative analyses to extract design
guidelines. Finally, we identify necessary synergies and missing elements
required to achieve a competitive advantage for neuromorphic systems over
conventional machine-learning accelerators in edge computing applications, and
outline the key ingredients for a framework toward neuromorphic intelligence."
"An Ethical Framework for Guiding the Development of Affectively-Aware
  Artificial Intelligence","The recent rapid advancements in artificial intelligence research and
deployment have sparked more discussion about the potential ramifications of
socially- and emotionally-intelligent AI. The question is not if research can
produce such affectively-aware AI, but when it will. What will it mean for
society when machines -- and the corporations and governments they serve -- can
""read"" people's minds and emotions? What should developers and operators of
such AI do, and what should they not do? The goal of this article is to
pre-empt some of the potential implications of these developments, and propose
a set of guidelines for evaluating the (moral and) ethical consequences of
affectively-aware AI, in order to guide researchers, industry professionals,
and policy-makers. We propose a multi-stakeholder analysis framework that
separates the ethical responsibilities of AI Developers vis-\`a-vis the
entities that deploy such AI -- which we term Operators. Our analysis produces
two pillars that clarify the responsibilities of each of these stakeholders:
Provable Beneficence, which rests on proving the effectiveness of the AI, and
Responsible Stewardship, which governs responsible collection, use, and storage
of data and the decisions made from such data. We end with recommendations for
researchers, developers, operators, as well as regulators and law-makers."
"Symmetry-Based Representations for Artificial and Biological General
  Intelligence","Biological intelligence is remarkable in its ability to produce complex
behaviour in many diverse situations through data efficient, generalisable and
transferable skill acquisition. It is believed that learning ""good"" sensory
representations is important for enabling this, however there is little
agreement as to what a good representation should look like. In this review
article we are going to argue that symmetry transformations are a fundamental
principle that can guide our search for what makes a good representation. The
idea that there exist transformations (symmetries) that affect some aspects of
the system but not others, and their relationship to conserved quantities has
become central in modern physics, resulting in a more unified theoretical
framework and even ability to predict the existence of new particles. Recently,
symmetries have started to gain prominence in machine learning too, resulting
in more data efficient and generalisable algorithms that can mimic some of the
complex behaviours produced by biological intelligence. Finally, first
demonstrations of the importance of symmetry transformations for representation
learning in the brain are starting to arise in neuroscience. Taken together,
the overwhelming positive effect that symmetries bring to these disciplines
suggest that they may be an important general framework that determines the
structure of the universe, constrains the nature of natural tasks and
consequently shapes both biological and artificial intelligence."
"A Comparative Study on Approaches to Acoustic Scene Classification using
  CNNs","Acoustic scene classification is a process of characterizing and classifying
the environments from sound recordings. The first step is to generate features
(representations) from the recorded sound and then classify the background
environments. However, different kinds of representations have dramatic effects
on the accuracy of the classification. In this paper, we explored the three
such representations on classification accuracy using neural networks. We
investigated the spectrograms, MFCCs, and embeddings representations using
different CNN networks and autoencoders. Our dataset consists of sounds from
three settings of indoors and outdoors environments - thus the dataset contains
sound from six different kinds of environments. We found that the spectrogram
representation has the highest classification accuracy while MFCC has the
lowest classification accuracy. We reported our findings, insights as well as
some guidelines to achieve better accuracy for environment classification using
sounds."
"Complementary artificial intelligence designed to augment human
  discovery","Neither artificial intelligence designed to play Turing's imitation game, nor
augmented intelligence built to maximize the human manipulation of information
are tuned to accelerate innovation and improve humanity's collective advance
against its greatest challenges. We reconceptualize and pilot beneficial AI to
radically augment human understanding by complementing rather than competing
with human cognitive capacity. Our approach to complementary intelligence
builds on insights underlying the wisdom of crowds, which hinges on the
independence and diversity of crowd members' information and approach. By
programmatically incorporating information on the evolving distribution of
scientific expertise from research papers, our approach follows the
distribution of content in the literature while avoiding the scientific crowd
and the hypotheses cognitively available to it. We use this approach to
generate valuable predictions for what materials possess valuable
energy-related properties (e.g., thermoelectricity), and what compounds possess
valuable medical properties (e.g., asthma) that complement the human scientific
crowd. We demonstrate that our complementary predictions, if identified by
human scientists and inventors at all, are only discovered years further into
the future. When we evaluate the promise of our predictions with
first-principles equations, we demonstrate that increased complementarity of
our predictions does not decrease and in some cases increases the probability
that the predictions possess the targeted properties. In summary, by tuning AI
to avoid the crowd, we can generate hypotheses unlikely to be imagined or
pursued until the distant future and promise to punctuate scientific advance.
By identifying and correcting for collective human bias, these models also
suggest opportunities to improve human prediction by reformulating science
education for discovery."
"Vision Paper: Causal Inference for Interpretable and Robust Machine
  Learning in Mobility Analysis","Artificial intelligence (AI) is revolutionizing many areas of our lives,
leading a new era of technological advancement. Particularly, the
transportation sector would benefit from the progress in AI and advance the
development of intelligent transportation systems. Building intelligent
transportation systems requires an intricate combination of artificial
intelligence and mobility analysis. The past few years have seen rapid
development in transportation applications using advanced deep neural networks.
However, such deep neural networks are difficult to interpret and lack
robustness, which slows the deployment of these AI-powered algorithms in
practice. To improve their usability, increasing research efforts have been
devoted to developing interpretable and robust machine learning methods, among
which the causal inference approach recently gained traction as it provides
interpretable and actionable information. Moreover, most of these methods are
developed for image or sequential data which do not satisfy specific
requirements of mobility data analysis. This vision paper emphasizes research
challenges in deep learning-based mobility analysis that require
interpretability and robustness, summarizes recent developments in using causal
inference for improving the interpretability and robustness of machine learning
methods, and highlights opportunities in developing causally-enabled machine
learning models tailored for mobility analysis. This research direction will
make AI in the transportation sector more interpretable and reliable, thus
contributing to safer, more efficient, and more sustainable future
transportation systems."
Selected Trends in Artificial Intelligence for Space Applications,"The development and adoption of artificial intelligence (AI) technologies in
space applications is growing quickly as the consensus increases on the
potential benefits introduced. As more and more aerospace engineers are
becoming aware of new trends in AI, traditional approaches are revisited to
consider the applications of emerging AI technologies. Already at the time of
writing, the scope of AI-related activities across academia, the aerospace
industry and space agencies is so wide that an in-depth review would not fit in
these pages. In this chapter we focus instead on two main emerging trends we
believe capture the most relevant and exciting activities in the field:
differentiable intelligence and on-board machine learning. Differentiable
intelligence, in a nutshell, refers to works making extensive use of automatic
differentiation frameworks to learn the parameters of machine learning or
related models. Onboard machine learning considers the problem of moving
inference, as well as learning, onboard. Within these fields, we discuss a few
selected projects originating from the European Space Agency's (ESA) Advanced
Concepts Team (ACT), giving priority to advanced topics going beyond the
transposition of established AI techniques and practices to the space domain."
Decision-making and control with diffractive optical networks,"The ultimate goal of artificial intelligence is to mimic the human brain to
perform decision-making and control directly from high-dimensional sensory
input. Diffractive optical networks provide a promising solution for
implementing artificial intelligence with high-speed and low-power consumption.
Most of the reported diffractive optical networks focus on single or multiple
tasks that do not involve environmental interaction, such as object recognition
and image classification. In contrast, the networks capable of performing
decision-making and control have not yet been developed to our knowledge. Here,
we propose using deep reinforcement learning to implement diffractive optical
networks that imitate human-level decision-making and control capability. Such
networks taking advantage of a residual architecture, allow for finding optimal
control policies through interaction with the environment and can be readily
implemented with existing optical devices. The superior performance of these
networks is verified by engaging three types of classic games, Tic-Tac-Toe,
Super Mario Bros., and Car Racing. Finally, we present an experimental
demonstration of playing Tic-Tac-Toe by leveraging diffractive optical networks
based on a spatial light modulator. Our work represents a solid step forward in
advancing diffractive optical networks, which promises a fundamental shift from
the target-driven control of a pre-designed state for simple recognition or
classification tasks to the high-level sensory capability of artificial
intelligence. It may find exciting applications in autonomous driving,
intelligent robots, and intelligent manufacturing."
"AI-Based Energy Transportation Safety: Pipeline Radial Threat Estimation
  Using Intelligent Sensing System","The application of artificial intelligence technology has greatly enhanced
and fortified the safety of energy pipelines, particularly in safeguarding
against external threats. The predominant methods involve the integration of
intelligent sensors to detect external vibration, enabling the identification
of event types and locations, thereby replacing manual detection methods.
However, practical implementation has exposed a limitation in current methods -
their constrained ability to accurately discern the spatial dimensions of
external signals, which complicates the authentication of threat events. Our
research endeavors to overcome the above issues by harnessing deep learning
techniques to achieve a more fine-grained recognition and localization process.
This refinement is crucial in effectively identifying genuine threats to
pipelines, thus enhancing the safety of energy transportation. This paper
proposes a radial threat estimation method for energy pipelines based on
distributed optical fiber sensing technology. Specifically, we introduce a
continuous multi-view and multi-domain feature fusion methodology to extract
comprehensive signal features and construct a threat estimation and recognition
network. The utilization of collected acoustic signal data is optimized, and
the underlying principle is elucidated. Moreover, we incorporate the concept of
transfer learning through a pre-trained model, enhancing both recognition
accuracy and training efficiency. Empirical evidence gathered from real-world
scenarios underscores the efficacy of our method, notably in its substantial
reduction of false alarms and remarkable gains in recognition accuracy. More
generally, our method exhibits versatility and can be extrapolated to a broader
spectrum of recognition tasks and scenarios."
Intelligence and Motion Models of Continuum Robots: an Overview,"Many technical solutions are bio-inspired. Octopus-inspired robotic arms
belong to continuum robots which are used in minimally invasive surgery or for
technical system restoration in areas difficult-toaccess. Continuum robot
missions are bounded with their motions, whereby the motion of the robots is
controlled by humans via wireless communication. In case of a lost connection,
robot autonomy is required. Distributed control and distributed decision-making
mechanisms based on artificial intelligence approaches can be a promising
solution to achieve autonomy of technical systems and to increase their
resilience. However these methods are not well investigated yet. Octopuses are
the living example of natural distributed intelligence but their learning and
decision-making mechanisms are also not fully investigated and understood yet.
Our major interest is investigating mechanisms of Distributed Artificial
Intelligence as a basis for improving resilience of complex systems. We decided
to use a physical continuum robot prototype that is able to perform some basic
movements for our research. The idea is to research how a technical system can
be empowered to combine movements into sequences of motions by itself. For the
experimental investigations a suitable physical prototype has to be selected,
its motion control has to be implemented and automated. In this paper, we give
an overview combining different fields of research, such as Distributed
Artificial Intelligence and continuum robots based on 98 publications. We
provide a detailed description of the basic motion control models of continuum
robots based on the literature reviewed, discuss different aspects of autonomy
and give an overview of physical prototypes of continuum robots."
CrowdTransfer: Enabling Crowd Knowledge Transfer in AIoT Community,"Artificial Intelligence of Things (AIoT) is an emerging frontier based on the
deep fusion of Internet of Things (IoT) and Artificial Intelligence (AI)
technologies. Although advanced deep learning techniques enhance the efficient
data processing and intelligent analysis of complex IoT data, they still suffer
from notable challenges when deployed to practical AIoT applications, such as
constrained resources, and diverse task requirements. Knowledge transfer is an
effective method to enhance learning performance by avoiding the exorbitant
costs associated with data recollection and model retraining. Notably, although
there are already some valuable and impressive surveys on transfer learning,
these surveys introduce approaches in a relatively isolated way and lack the
recent advances of various knowledge transfer techniques for AIoT field. This
survey endeavors to introduce a new concept of knowledge transfer, referred to
as Crowd Knowledge Transfer (CrowdTransfer), which aims to transfer prior
knowledge learned from a crowd of agents to reduce the training cost and as
well as improve the performance of the model in real-world complicated
scenarios. Particularly, we present four transfer modes from the perspective of
crowd intelligence, including derivation, sharing, evolution and fusion modes.
Building upon conventional transfer learning methods, we further delve into
advanced crowd knowledge transfer models from three perspectives for various
AIoT applications. Furthermore, we explore some applications of AIoT areas,
such as human activity recognition, urban computing, multi-robot system, and
smart factory. Finally, we discuss the open issues and outline future research
directions of knowledge transfer in AIoT community."
"Intelligent Cross-Organizational Process Mining: A Survey and New
  Perspectives","Process mining, as a high-level field in data mining, plays a crucial role in
enhancing operational efficiency and decision-making across organizations. In
this survey paper, we delve into the growing significance and ongoing trends in
the field of process mining, advocating a specific viewpoint on its contents,
application, and development in modern businesses and process management,
particularly in cross-organizational settings. We first summarize the framework
of process mining, common industrial applications, and the latest advances
combined with artificial intelligence, such as workflow optimization,
compliance checking, and performance analysis. Then, we propose a holistic
framework for intelligent process analysis and outline initial methodologies in
cross-organizational settings, highlighting both challenges and opportunities.
This particular perspective aims to revolutionize process mining by leveraging
artificial intelligence to offer sophisticated solutions for complex,
multi-organizational data analysis. By integrating advanced machine learning
techniques, we can enhance predictive capabilities, streamline processes, and
facilitate real-time decision-making. Furthermore, we pinpoint avenues for
future investigations within the research community, encouraging the
exploration of innovative algorithms, data integration strategies, and
privacy-preserving methods to fully harness the potential of process mining in
diverse, interconnected business environments."
"Integrating Artificial Intelligence into Operating Systems: A
  Comprehensive Survey on Techniques, Applications, and Future Directions","In the era of the Internet of Everything, operating systems (OSs) face
unprecedented challenges posed by an evolving application landscape and
increasingly heterogeneous hardware ecosystems. This shift toward increasingly
dynamic and unpredictable operational contexts presents significant challenges
for both OS developers and users. Against this backdrop, the fusion of
Artificial Intelligence (AI) with Operating Systems emerges as a critical
frontier for innovation. This survey delves into the intricate interplay
between AI and OSs, illustrating how existing OS mechanisms combined with AI
significantly elevate the performance, security, and efficiency of modern
operating systems. We investigate a range of AI methodologies applied to
optimize core OS functionalities and clarify the correlation to related
studies. Our analysis touches on the existing hurdles and prospective avenues
in this interdisciplinary domain, underscoring the imperative for robust and
seamless integration of AI capabilities into OS architectures.
  Through an examination of illustrative case studies and cutting-edge
developments, we offer a thorough review of the current status of AI-OS
integration, accentuating its pivotal role in steering the evolution of
advanced computing paradigms. We also envision the promising prospects of
Intelligent Operating Systems, debating how groundbreaking OS designs will
usher in novel possibilities and highlight the central role that AI will assume
in propelling these next-generation systems forward. This forward-thinking
outlook illuminates the profound influence of AI on the foundational elements
of computing, heralding the advent of a new epoch characterized by intelligent,
self-adapting, and highly adaptive software ecosystems."
Artificial Intelligence in Landscape Architecture: A Survey,"The development history of landscape architecture (LA) reflects the human
pursuit of environmental beautification and ecological balance. With the
advancement of artificial intelligence (AI) technologies that simulate and
extend human intelligence, immense opportunities have been provided for LA,
offering scientific and technological support throughout the entire workflow.
In this article, we comprehensively review the applications of AI technology in
the field of LA. First, we introduce the many potential benefits that AI brings
to the design, planning, and management aspects of LA. Secondly, we discuss how
AI can assist the LA field in solving its current development problems,
including urbanization, environmental degradation and ecological decline,
irrational planning, insufficient management and maintenance, and lack of
public participation. Furthermore, we summarize the key technologies and
practical cases of applying AI in the LA domain, from design assistance to
intelligent management, all of which provide innovative solutions for the
planning, design, and maintenance of LA. Finally, we look ahead to the problems
and opportunities in LA, emphasizing the need to combine human expertise and
judgment for rational decision-making. This article provides both theoretical
and practical guidance for LA designers, researchers, and technology
developers. The successful integration of AI technology into LA holds great
promise for enhancing the field's capabilities and achieving more sustainable,
efficient, and user-friendly outcomes."
"Privacy-Preserving SAM Quantization for Efficient Edge Intelligence in
  Healthcare","The disparity in healthcare personnel expertise and medical resources across
different regions of the world is a pressing social issue. Artificial
intelligence technology offers new opportunities to alleviate this issue.
Segment Anything Model (SAM), which excels in intelligent image segmentation,
has demonstrated exceptional performance in medical monitoring and assisted
diagnosis. Unfortunately, the huge computational and storage overhead of SAM
poses significant challenges for deployment on resource-limited edge devices.
Quantization is an effective solution for model compression; however,
traditional methods rely heavily on original data for calibration, which raises
widespread concerns about medical data privacy and security. In this paper, we
propose a data-free quantization framework for SAM, called DFQ-SAM, which
learns and calibrates quantization parameters without any original data, thus
effectively preserving data privacy during model compression. Specifically, we
propose pseudo-positive label evolution for segmentation, combined with patch
similarity, to fully leverage the semantic and distribution priors in
pre-trained models, which facilitates high-quality data synthesis as a
substitute for real data. Furthermore, we introduce scale reparameterization to
ensure the accuracy of low-bit quantization. We perform extensive segmentation
experiments on various datasets, and DFQ-SAM consistently provides significant
performance on low-bit quantization. DFQ-SAM eliminates the need for data
transfer in cloud-edge collaboration, thereby protecting sensitive data from
potential attacks. It enables secure, fast, and personalized healthcare
services at the edge, which enhances system efficiency and optimizes resource
allocation, and thus facilitating the pervasive application of artificial
intelligence in worldwide healthcare."
"Artificial Intelligence for Collective Intelligence: A National-Scale
  Research Strategy","Advances in artificial intelligence (AI) have great potential to help address
societal challenges that are both collective in nature and present at national
or trans-national scale. Pressing challenges in healthcare, finance,
infrastructure and sustainability, for instance, might all be productively
addressed by leveraging and amplifying AI for national-scale collective
intelligence. The development and deployment of this kind of AI faces
distinctive challenges, both technical and socio-technical. Here, a research
strategy for mobilising inter-disciplinary research to address these challenges
is detailed and some of the key issues that must be faced are outlined."
"Augmented Intelligence for Multimodal Virtual Biopsy in Breast Cancer
  Using Generative Artificial Intelligence","Full-Field Digital Mammography (FFDM) is the primary imaging modality for
routine breast cancer screening; however, its effectiveness is limited in
patients with dense breast tissue or fibrocystic conditions. Contrast-Enhanced
Spectral Mammography (CESM), a second-level imaging technique, offers enhanced
accuracy in tumor detection. Nonetheless, its application is restricted due to
higher radiation exposure, the use of contrast agents, and limited
accessibility. As a result, CESM is typically reserved for select cases,
leaving many patients to rely solely on FFDM despite the superior diagnostic
performance of CESM. While biopsy remains the gold standard for definitive
diagnosis, it is an invasive procedure that can cause discomfort for patients.
We introduce a multimodal, multi-view deep learning approach for virtual
biopsy, integrating FFDM and CESM modalities in craniocaudal and mediolateral
oblique views to classify lesions as malignant or benign. To address the
challenge of missing CESM data, we leverage generative artificial intelligence
to impute CESM images from FFDM scans. Experimental results demonstrate that
incorporating the CESM modality is crucial to enhance the performance of
virtual biopsy. When real CESM data is missing, synthetic CESM images proved
effective, outperforming the use of FFDM alone, particularly in multimodal
configurations that combine FFDM and CESM modalities. The proposed approach has
the potential to improve diagnostic workflows, providing clinicians with
augmented intelligence tools to improve diagnostic accuracy and patient care.
Additionally, as a contribution to the research community, we publicly release
the dataset used in our experiments, facilitating further advancements in this
field."
"VEGAS: Towards Visually Explainable and Grounded Artificial Social
  Intelligence","Social Intelligence Queries (Social-IQ) serve as the primary multimodal
benchmark for evaluating a model's social intelligence level. While impressive
multiple-choice question(MCQ) accuracy is achieved by current solutions,
increasing evidence shows that they are largely, and in some cases entirely,
dependent on language modality, overlooking visual context. Additionally, the
closed-set nature further prevents the exploration of whether and to what
extent the reasoning path behind selection is correct. To address these
limitations, we propose the Visually Explainable and Grounded Artificial Social
Intelligence (VEGAS) model. As a generative multimodal model, VEGAS leverages
open-ended answering to provide explainable responses, which enhances the
clarity and evaluation of reasoning paths. To enable visually grounded
answering, we propose a novel sampling strategy to provide the model with more
relevant visual frames. We then enhance the model's interpretation of these
frames through Generalist Instruction Fine-Tuning (GIFT), which aims to: i)
learn multimodal-language transformations for fundamental emotional social
traits, and ii) establish multimodal joint reasoning capabilities. Extensive
experiments, comprising modality ablation, open-ended assessments, and
supervised MCQ evaluations, consistently show that VEGAS effectively utilizes
visual information in reasoning to produce correct and also credible answers.
We expect this work to of fer a new perspective on Social-IQ and advance the
development of human-like social AI."
A Hierarchy of Tractable Subsets for Computing Stable Models,"Finding the stable models of a knowledge base is a significant computational
problem in artificial intelligence. This task is at the computational heart of
truth maintenance systems, autoepistemic logic, and default logic.
Unfortunately, it is NP-hard. In this paper we present a hierarchy of classes
of knowledge bases, Omega_1,Omega_2,..., with the following properties: first,
Omega_1 is the class of all stratified knowledge bases; second, if a knowledge
base Pi is in Omega_k, then Pi has at most k stable models, and all of them may
be found in time O(lnk), where l is the length of the knowledge base and n the
number of atoms in Pi; third, for an arbitrary knowledge base Pi, we can find
the minimum k such that Pi belongs to Omega_k in time polynomial in the size of
Pi; and, last, where K is the class of all knowledge bases, it is the case that
union{i=1 to infty} Omega_i = K, that is, every knowledge base belongs to some
class in the hierarchy."
Monotonicity and Persistence in Preferential Logics,"An important characteristic of many logics for Artificial Intelligence is
their nonmonotonicity. This means that adding a formula to the premises can
invalidate some of the consequences. There may, however, exist formulae that
can always be safely added to the premises without destroying any of the
consequences: we say they respect monotonicity. Also, there may be formulae
that, when they are a consequence, can not be invalidated when adding any
formula to the premises: we call them conservative. We study these two classes
of formulae for preferential logics, and show that they are closely linked to
the formulae whose truth-value is preserved along the (preferential) ordering.
We will consider some preferential logics for illustration, and prove syntactic
characterization results for them. The results in this paper may improve the
efficiency of theorem provers for preferential logics."
On the accuracy and running time of GSAT,"Randomized algorithms for deciding satisfiability were shown to be effective
in solving problems with thousands of variables. However, these algorithms are
not complete. That is, they provide no guarantee that a satisfying assignment,
if one exists, will be found. Thus, when studying randomized algorithms, there
are two important characteristics that need to be considered: the running time
and, even more importantly, the accuracy --- a measure of likelihood that a
satisfying assignment will be found, provided one exists. In fact, we argue
that without a reference to the accuracy, the notion of the running time for
randomized algorithms is not well-defined. In this paper, we introduce a formal
notion of accuracy. We use it to define a concept of the running time. We use
both notions to study the random walk strategy GSAT algorithm. We investigate
the dependence of accuracy on properties of input formulas such as
clause-to-variable ratio and the number of satisfying assignments. We
demonstrate that the running time of GSAT grows exponentially in the number of
variables of the input formula for randomly generated 3-CNF formulas and for
the formulas encoding 3- and 4-colorability of graphs."
"Towards a Universal Theory of Artificial Intelligence based on
  Algorithmic Probability and Sequential Decision Theory","Decision theory formally solves the problem of rational agents in uncertain
worlds if the true environmental probability distribution is known.
Solomonoff's theory of universal induction formally solves the problem of
sequence prediction for unknown distribution. We unify both theories and give
strong arguments that the resulting universal AIXI model behaves optimal in any
computable environment. The major drawback of the AIXI model is that it is
uncomputable. To overcome this problem, we construct a modified algorithm
AIXI^tl, which is still superior to any other time t and space l bounded agent.
The computation time of AIXI^tl is of the order t x 2^l."
Annotated revision programs,"Revision programming is a formalism to describe and enforce updates of belief
sets and databases. That formalism was extended by Fitting who assigned
annotations to revision atoms. Annotations provide a way to quantify the
confidence (probability) that a revision atom holds. The main goal of our paper
is to reexamine the work of Fitting, argue that his semantics does not always
provide results consistent with intuition, and to propose an alternative
treatment of annotated revision programs. Our approach differs from that
proposed by Fitting in two key aspects: we change the notion of a model of a
program and we change the notion of a justified revision. We show that under
this new approach fundamental properties of justified revisions of standard
revision programs extend to the annotated case."
Robust Feature Selection by Mutual Information Distributions,"Mutual information is widely used in artificial intelligence, in a
descriptive way, to measure the stochastic dependence of discrete random
variables. In order to address questions such as the reliability of the
empirical value, one must consider sample-to-population inferential approaches.
This paper deals with the distribution of mutual information, as obtained in a
Bayesian framework by a second-order Dirichlet prior distribution. The exact
analytical expression for the mean and an analytical approximation of the
variance are reported. Asymptotic approximations of the distribution are
proposed. The results are applied to the problem of selecting features for
incremental learning and classification of the naive Bayes classifier. A fast,
newly defined method is shown to outperform the traditional approach based on
empirical mutual information on a number of real data sets. Finally, a
theoretical development is reported that allows one to efficiently extend the
above methods to incomplete samples in an easy and effective way."
The New AI: General & Sound & Relevant for Physics,"Most traditional artificial intelligence (AI) systems of the past 50 years
are either very limited, or based on heuristics, or both. The new millennium,
however, has brought substantial progress in the field of theoretically optimal
and practically feasible algorithms for prediction, search, inductive inference
based on Occam's razor, problem solving, decision making, and reinforcement
learning in environments of a very general type. Since inductive inference is
at the heart of all inductive sciences, some of the results are relevant not
only for AI and computer science but also for physics, provoking nontraditional
predictions based on Zuse's thesis of the computer-generated universe."
Anusaaraka: Machine Translation in Stages,"Fully-automatic general-purpose high-quality machine translation systems
(FGH-MT) are extremely difficult to build. In fact, there is no system in the
world for any pair of languages which qualifies to be called FGH-MT. The
reasons are not far to seek. Translation is a creative process which involves
interpretation of the given text by the translator. Translation would also vary
depending on the audience and the purpose for which it is meant. This would
explain the difficulty of building a machine translation system. Since, the
machine is not capable of interpreting a general text with sufficient accuracy
automatically at present - let alone re-expressing it for a given audience, it
fails to perform as FGH-MT. FOOTNOTE{The major difficulty that the machine
faces in interpreting a given text is the lack of general world knowledge or
common sense knowledge.}"
Neuro Fuzzy Systems: Sate-of-the-Art Modeling Techniques,"Fusion of Artificial Neural Networks (ANN) and Fuzzy Inference Systems (FIS)
have attracted the growing interest of researchers in various scientific and
engineering areas due to the growing need of adaptive intelligent systems to
solve the real world problems. ANN learns from scratch by adjusting the
interconnections between layers. FIS is a popular computing framework based on
the concept of fuzzy set theory, fuzzy if-then rules, and fuzzy reasoning. The
advantages of a combination of ANN and FIS are obvious. There are several
approaches to integrate ANN and FIS and very often it depends on the
application. We broadly classify the integration of ANN and FIS into three
categories namely concurrent model, cooperative model and fully fused model.
This paper starts with a discussion of the features of each model and
generalize the advantages and deficiencies of each model. We further focus the
review on the different types of fused neuro-fuzzy systems and citing the
advantages and disadvantages of each model."
"The Integration of Connectionism and First-Order Knowledge
  Representation and Reasoning as a Challenge for Artificial Intelligence","Intelligent systems based on first-order logic on the one hand, and on
artificial neural networks (also called connectionist systems) on the other,
differ substantially. It would be very desirable to combine the robust neural
networking machinery with symbolic knowledge representation and reasoning
paradigms like logic programming in such a way that the strengths of either
paradigm will be retained. Current state-of-the-art research, however, fails by
far to achieve this ultimate goal. As one of the main obstacles to be overcome
we perceive the question how symbolic knowledge can be encoded by means of
connectionist systems: Satisfactory answers to this will naturally lead the way
to knowledge extraction algorithms and to integrated neural-symbolic systems."
Default reasoning over domains and concept hierarchies,"W.C. Rounds and G.-Q. Zhang (2001) have proposed to study a form of
disjunctive logic programming generalized to algebraic domains. This system
allows reasoning with information which is hierarchically structured and forms
a (suitable) domain. We extend this framework to include reasoning with default
negation, giving rise to a new nonmonotonic reasoning framework on hierarchical
knowledge which encompasses answer set programming with extended disjunctive
logic programs. We also show that the hierarchically structured knowledge on
which programming in this paradigm can be done, arises very naturally from
formal concept analysis. Together, we obtain a default reasoning paradigm for
conceptual knowledge which is in accordance with mainstream developments in
nonmonotonic reasoning."
"Artificial Neural Networks and Support Vector Machines for Water Demand
  Time Series Forecasting","Water plays a pivotal role in many physical processes, and most importantly
in sustaining human life, animal life and plant life. Water supply entities
therefore have the responsibility to supply clean and safe water at the rate
required by the consumer. It is therefore necessary to implement mechanisms and
systems that can be employed to predict both short-term and long-term water
demands. The increasingly growing field of computational intelligence
techniques has been proposed as an efficient tool in the modelling of dynamic
phenomena. The primary objective of this paper is to compare the efficiency of
two computational intelligence techniques in water demand forecasting. The
techniques under comparison are the Artificial Neural Networks (ANNs) and the
Support Vector Machines (SVMs). In this study it was observed that the ANNs
perform better than the SVMs. This performance is measured against the
generalisation ability of the two."
"2006: Celebrating 75 years of AI - History and Outlook: the Next 25
  Years","When Kurt Goedel layed the foundations of theoretical computer science in
1931, he also introduced essential concepts of the theory of Artificial
Intelligence (AI). Although much of subsequent AI research has focused on
heuristics, which still play a major role in many practical AI applications, in
the new millennium AI theory has finally become a full-fledged formal science,
with important optimality results for embodied agents living in unknown
environments, obtained through a combination of theory a la Goedel and
probability theory. Here we look back at important milestones of AI history,
mention essential recent results, and speculate about what we may expect from
the next 25 years, emphasizing the significance of the ongoing dramatic
hardware speedups, and discussing Goedel-inspired, self-referential,
self-improving universal problem solvers."
Local search heuristics: Fitness Cloud versus Fitness Landscape,"This paper introduces the concept of fitness cloud as an alternative way to
visualize and analyze search spaces than given by the geographic notion of
fitness landscape. It is argued that the fitness cloud concept overcomes
several deficiencies of the landscape representation. Our analysis is based on
the correlation between fitness of solutions and fitnesses of nearest solutions
according to some neighboring. We focus on the behavior of local search
heuristics, such as hill climber, on the well-known NK fitness landscape. In
both cases the fitness vs. fitness correlation is shown to be related to the
epistatic parameter K."
"A Reactive Tabu Search Algorithm for Stimuli Generation in
  Psycholinguistics","The generation of meaningless ""words"" matching certain statistical and/or
linguistic criteria is frequently needed for experimental purposes in
Psycholinguistics. Such stimuli receive the name of pseudowords or nonwords in
the Cognitive Neuroscience literatue. The process for building nonwords
sometimes has to be based on linguistic units such as syllables or morphemes,
resulting in a numerical explosion of combinations when the size of the
nonwords is increased. In this paper, a reactive tabu search scheme is proposed
to generate nonwords of variables size. The approach builds pseudowords by
using a modified Metaheuristic algorithm based on a local search procedure
enhanced by a feedback-based scheme. Experimental results show that the new
algorithm is a practical and effective tool for nonword generation."
Reasoning about Cardinal Directions between Extended Objects,"Direction relations between extended spatial objects are important
commonsense knowledge. Recently, Goyal and Egenhofer proposed a formal model,
known as Cardinal Direction Calculus (CDC), for representing direction
relations between connected plane regions. CDC is perhaps the most expressive
qualitative calculus for directional information, and has attracted increasing
interest from areas such as artificial intelligence, geographical information
science, and image retrieval. Given a network of CDC constraints, the
consistency problem is deciding if the network is realizable by connected
regions in the real plane. This paper provides a cubic algorithm for checking
consistency of basic CDC constraint networks, and proves that reasoning with
CDC is in general an NP-Complete problem. For a consistent network of basic CDC
constraints, our algorithm also returns a 'canonical' solution in cubic time.
This cubic algorithm is also adapted to cope with cardinal directions between
possibly disconnected regions, in which case currently the best algorithm is of
time complexity O(n^5)."
"Life, the Universe, and almost Everything: Signs of Cosmic Design?","Why did the big bang occur, why do the laws and constants of nature as well
as the boundary conditions seem so fine-tuned for life, what is the role of
intelligence and self-consciousness in the universe, and how can it escape
cosmic doomsday? The hypothesis of Cosmological Artificial Selection (CAS)
connects those questions and suggests a far-reaching answer: Our universe might
be understood in terms of vast computer simulations and could even have been
created and transcended by one. - This essay critically discusses some of the
premises and implications of CAS and related problems both with the proposal
itself and its possible physical realization: Is our universe really
fine-tuned, does CAS deserve to be considered as a convincing explanation, and
which other options are available to understand the physical laws, constants
and boundary conditions? Is life incidental, and does CAS revalue it? And is
intelligence and self-consciousness ultimately doomed, or might CAS rescue it?
  Keywords: origin of the universe, big bang, fine-tuning, laws of nature,
physical constants, initial conditions, intelligent life, cosmological natural
selection, cosmological artificial selection, artificial cosmogenesis, deism,
natural theology, far future of the universe, physical eschatology"
"A Monte Carlo Algorithm for Universally Optimal Bayesian Sequence
  Prediction and Planning","The aim of this work is to address the question of whether we can in
principle design rational decision-making agents or artificial intelligences
embedded in computable physics such that their decisions are optimal in
reasonable mathematical senses. Recent developments in rare event probability
estimation, recursive bayesian inference, neural networks, and probabilistic
planning are sufficient to explicitly approximate reinforcement learners of the
AIXI style with non-trivial model classes (here, the class of resource-bounded
Turing machines). Consideration of the effects of resource limitations in a
concrete implementation leads to insights about possible architectures for
learning systems using optimal decision makers as components."
"Feature Importance in Bayesian Assessment of Newborn Brain Maturity from
  EEG","The methodology of Bayesian Model Averaging (BMA) is applied for assessment
of newborn brain maturity from sleep EEG. In theory this methodology provides
the most accurate assessments of uncertainty in decisions. However, the
existing BMA techniques have been shown providing biased assessments in the
absence of some prior information enabling to explore model parameter space in
details within a reasonable time. The lack in details leads to disproportional
sampling from the posterior distribution. In case of the EEG assessment of
brain maturity, BMA results can be biased because of the absence of information
about EEG feature importance. In this paper we explore how the posterior
information about EEG features can be used in order to reduce a negative impact
of disproportional sampling on BMA performance. We use EEG data recorded from
sleeping newborns to test the efficiency of the proposed BMA technique."
Symmetry within and between solutions,"Symmetry can be used to help solve many problems. For instance, Einstein's
famous 1905 paper (""On the Electrodynamics of Moving Bodies"") uses symmetry to
help derive the laws of special relativity. In artificial intelligence,
symmetry has played an important role in both problem representation and
reasoning. I describe recent work on using symmetry to help solve constraint
satisfaction problems. Symmetries occur within individual solutions of problems
as well as between different solutions of the same problem. Symmetry can also
be applied to the constraints in a problem to give new symmetric constraints.
Reasoning about symmetry can speed up problem solving, and has led to the
discovery of new results in both graph and number theory."
Hybrid tractability of soft constraint problems,"The constraint satisfaction problem (CSP) is a central generic problem in
computer science and artificial intelligence: it provides a common framework
for many theoretical problems as well as for many real-life applications. Soft
constraint problems are a generalisation of the CSP which allow the user to
model optimisation problems. Considerable effort has been made in identifying
properties which ensure tractability in such problems. In this work, we
initiate the study of hybrid tractability of soft constraint problems; that is,
properties which guarantee tractability of the given soft constraint problem,
but which do not depend only on the underlying structure of the instance (such
as being tree-structured) or only on the types of soft constraints in the
instance (such as submodularity). We present several novel hybrid classes of
soft constraint problems, which include a machine scheduling problem,
constraint problems of arbitrary arities with no overlapping nogoods, and the
SoftAllDiff constraint with arbitrary unary soft constraints. An important tool
in our investigation will be the notion of forbidden substructures."
"Quantum Interaction Approach in Cognition, Artificial Intelligence and
  Robotics","The mathematical formalism of quantum mechanics has been successfully
employed in the last years to model situations in which the use of classical
structures gives rise to problematical situations, and where typically quantum
effects, such as 'contextuality' and 'entanglement', have been recognized. This
'Quantum Interaction Approach' is briefly reviewed in this paper focusing, in
particular, on the quantum models that have been elaborated to describe how
concepts combine in cognitive science, and on the ensuing identification of a
quantum structure in human thought. We point out that these results provide
interesting insights toward the development of a unified theory for meaning and
knowledge formalization and representation. Then, we analyze the technological
aspects and implications of our approach, and a particular attention is devoted
to the connections with symbolic artificial intelligence, quantum computation
and robotics."
The Complexity of Reasoning about Spatial Congruence,"In the recent literature of Artificial Intelligence, an intensive research
effort has been spent, for various algebras of qualitative relations used in
the representation of temporal and spatial knowledge, on the problem of
classifying the computational complexity of reasoning problems for subsets of
algebras. The main purpose of these researches is to describe a restricted set
of maximal tractable subalgebras, ideally in an exhaustive fashion with respect
to the hosting algebras. In this paper we introduce a novel algebra for
reasoning about Spatial Congruence, show that the satisfiability problem in the
spatial algebra MC-4 is NP-complete, and present a complete classification of
tractability in the algebra, based on the individuation of three maximal
tractable subclasses, one containing the basic relations. The three algebras
are formed by 14, 10 and 9 relations out of 16 which form the full algebra."
"Decentralized Supply Chain Formation: A Market Protocol and Competitive
  Equilibrium Analysis","Supply chain formation is the process of determining the structure and terms
of exchange relationships to enable a multilevel, multiagent production
activity. We present a simple model of supply chains, highlighting two
characteristic features: hierarchical subtask decomposition, and resource
contention. To decentralize the formation process, we introduce a market price
system over the resources produced along the chain. In a competitive
equilibrium for this system, agents choose locally optimal allocations with
respect to prices, and outcomes are optimal overall. To determine prices, we
define a market protocol based on distributed, progressive auctions, and
myopic, non-strategic agent bidding policies. In the presence of resource
contention, this protocol produces better solutions than the greedy protocols
common in the artificial intelligence and multiagent systems literature. The
protocol often converges to high-value supply chains, and when competitive
equilibria exist, typically to approximate competitive equilibria. However,
complementarities in agent production technologies can cause the protocol to
wastefully allocate inputs to agents that do not produce their outputs. A
subsequent decommitment phase recovers a significant fraction of the lost
surplus."
A Maximal Tractable Class of Soft Constraints,"Many researchers in artificial intelligence are beginning to explore the use
of soft constraints to express a set of (possibly conflicting) problem
requirements. A soft constraint is a function defined on a collection of
variables which associates some measure of desirability with each possible
combination of values for those variables. However, the crucial question of the
computational complexity of finding the optimal solution to a collection of
soft constraints has so far received very little attention. In this paper we
identify a class of soft binary constraints for which the problem of finding
the optimal solution is tractable. In other words, we show that for any given
set of such constraints, there exists a polynomial time algorithm to determine
the assignment having the best overall combined measure of desirability. This
tractable class includes many commonly-occurring soft constraints, such as 'as
near as possible' or 'as soon as possible after', as well as crisp constraints
such as 'greater than'. Finally, we show that this tractable class is maximal,
in the sense that adding any other form of soft binary constraint which is not
in the class gives rise to a class of problems which is NP-hard."
"Evaluation of a Simple, Scalable, Parallel Best-First Search Strategy","Large-scale, parallel clusters composed of commodity processors are
increasingly available, enabling the use of vast processing capabilities and
distributed RAM to solve hard search problems. We investigate Hash-Distributed
A* (HDA*), a simple approach to parallel best-first search that asynchronously
distributes and schedules work among processors based on a hash function of the
search state. We use this approach to parallelize the A* algorithm in an
optimal sequential version of the Fast Downward planner, as well as a 24-puzzle
solver. The scaling behavior of HDA* is evaluated experimentally on a shared
memory, multicore machine with 8 cores, a cluster of commodity machines using
up to 64 cores, and large-scale high-performance clusters, using up to 2400
processors. We show that this approach scales well, allowing the effective
utilization of large amounts of distributed memory to optimally solve problems
which require terabytes of RAM. We also compare HDA* to Transposition-table
Driven Scheduling (TDS), a hash-based parallelization of IDA*, and show that,
in planning, HDA* significantly outperforms TDS. A simple hybrid which combines
HDA* and TDS to exploit strengths of both algorithms is proposed and evaluated."
Classification of artificial intelligence ids for smurf attack,"Many methods have been developed to secure the network infrastructure and
communication over the Internet. Intrusion detection is a relatively new
addition to such techniques. Intrusion detection systems (IDS) are used to find
out if someone has intrusion into or is trying to get it the network. One big
problem is amount of Intrusion which is increasing day by day. We need to know
about network attack information using IDS, then analysing the effect. Due to
the nature of IDSs which are solely signature based, every new intrusion cannot
be detected; so it is important to introduce artificial intelligence (AI)
methods / techniques in IDS. Introduction of AI necessitates the importance of
normalization in intrusions. This work is focused on classification of AI based
IDS techniques which will help better design intrusion detection systems in the
future. We have also proposed a support vector machine for IDS to detect Smurf
attack with much reliable accuracy."
Identifying Optimal Sequential Decisions,"We consider conditions that allow us to find an optimal strategy for
sequential decisions from a given data situation. For the case where all
interventions are unconditional (atomic), identifiability has been discussed by
Pearl & Robins (1995). We argue here that an optimal strategy must be
conditional, i.e. take the information available at each decision point into
account. We show that the identification of an optimal sequential decision
strategy is more restrictive, in the sense that conditional interventions might
not always be identified when atomic interventions are. We further demonstrate
that a simple graphical criterion for the identifiability of an optimal
strategy can be given."
"Making life better one large system at a time: Challenges for UAI
  research","The rapid growth and diversity in service offerings and the ensuing
complexity of information technology ecosystems present numerous management
challenges (both operational and strategic). Instrumentation and measurement
technology is, by and large, keeping pace with this development and growth.
However, the algorithms, tools, and technology required to transform the data
into relevant information for decision making are not. The claim in this paper
(and the invited talk) is that the line of research conducted in Uncertainty in
Artificial Intelligence is very well suited to address the challenges and close
this gap. I will support this claim and discuss open problems using recent
examples in diagnosis, model discovery, and policy optimization on three real
life distributed systems."
"A unified setting for inference and decision: An argumentation-based
  approach","Inferring from inconsistency and making decisions are two problems which have
always been treated separately by researchers in Artificial Intelligence.
Consequently, different models have been proposed for each category. Different
argumentation systems [2, 7, 10, 11] have been developed for handling
inconsistency in knowledge bases. Recently, other argumentation systems [3, 4,
8] have been defined for making decisions under uncertainty. The aim of this
paper is to present a general argumentation framework in which both inferring
from inconsistency and decision making are captured. The proposed framework can
be used for decision under uncertainty, multiple criteria decision, rule-based
decision and finally case-based decision. Moreover, works on classical decision
suppose that the information about environment is coherent, and this no longer
required by this general framework."
"Existence and Finiteness Conditions for Risk-Sensitive Planning: Results
  and Conjectures","Decision-theoretic planning with risk-sensitive planning objectives is
important for building autonomous agents or decision-support systems for
real-world applications. However, this line of research has been largely
ignored in the artificial intelligence and operations research communities
since planning with risk-sensitive planning objectives is more complicated than
planning with risk-neutral planning objectives. To remedy this situation, we
derive conditions that guarantee that the optimal expected utilities of the
total plan-execution reward exist and are finite for fully observable Markov
decision process models with non-linear utility functions. In case of Markov
decision process models with both positive and negative rewards, most of our
results hold for stationary policies only, but we conjecture that they can be
generalized to non stationary policies."
Bayes' Bluff: Opponent Modelling in Poker,"Poker is a challenging problem for artificial intelligence, with
non-deterministic dynamics, partial observability, and the added difficulty of
unknown adversaries. Modelling all of the uncertainties in this domain is not
an easy task. In this paper we present a Bayesian probabilistic model for a
broad class of poker games, separating the uncertainty in the game dynamics
from the uncertainty of the opponent's strategy. We then describe approaches to
two key subproblems: (i) inferring a posterior over opponent strategies given a
prior distribution and observations of their play, and (ii) playing an
appropriate response to that distribution. We demonstrate the overall approach
on a reduced version of poker using Dirichlet priors and then on the full game
of Texas hold'em using a more informed prior. We demonstrate methods for
playing effective responses to the opponent, based on the posterior."
"FHHOP: A Factored Hybrid Heuristic Online Planning Algorithm for Large
  POMDPs","Planning in partially observable Markov decision processes (POMDPs) remains a
challenging topic in the artificial intelligence community, in spite of recent
impressive progress in approximation techniques. Previous research has
indicated that online planning approaches are promising in handling large-scale
POMDP domains efficiently as they make decisions ""on demand"" instead of
proactively for the entire state space. We present a Factored Hybrid Heuristic
Online Planning (FHHOP) algorithm for large POMDPs. FHHOP gets its power by
combining a novel hybrid heuristic search strategy with a recently developed
factored state representation. On several benchmark problems, FHHOP
substantially outperformed state-of-the-art online heuristic search approaches
in terms of both scalability and quality."
Understanding (dis)similarity measures,"Intuitively, the concept of similarity is the notion to measure an inexact
matching between two entities of the same reference set. The notions of
similarity and its close relative dissimilarity are widely used in many fields
of Artificial Intelligence. Yet they have many different and often partial
definitions or properties, usually restricted to one field of application and
thus incompatible with other uses. This paper contributes to the design and
understanding of similarity and dissimilarity measures for Artificial
Intelligence. A formal dual definition for each concept is proposed, joined
with a set of fundamental properties. The behavior of the properties under
several transformations is studied and revealed as an important matter to bear
in mind. We also develop several practical examples that work out the proposed
approach."
Efficient Approximation for Triangulation of Minimum Treewidth,"We present four novel approximation algorithms for finding triangulation of
minimum treewidth. Two of the algorithms improve on the running times of
algorithms by Robertson and Seymour, and Becker and Geiger that approximate the
optimum by factors of 4 and 3 2/3, respectively. A third algorithm is faster
than those but gives an approximation factor of 4 1/2. The last algorithm is
yet faster, producing factor-O(lg/k) approximations in polynomial time. Finding
triangulations of minimum treewidth for graphs is central to many problems in
computer science. Real-world problems in artificial intelligence, VLSI design
and databases are efficiently solvable if we have an efficient approximation
algorithm for them. We report on experimental results confirming the
effectiveness of our algorithms for large graphs associated with real-world
problems."
Choosing Among Interpretations of Probability,"There is available an ever-increasing variety of procedures for managing
uncertainty. These methods are discussed in the literature of artificial
intelligence, as well as in the literature of philosophy of science. Heretofore
these methods have been evaluated by intuition, discussion, and the general
philosophical method of argument and counterexample. Almost any method of
uncertainty management will have the property that in the long run it will
deliver numbers approaching the relative frequency of the kinds of events at
issue. To find a measure that will provide a meaningful evaluation of these
treatments of uncertainty, we must look, not at the long run, but at the short
or intermediate run. Our project attempts to develop such a measure in terms of
short or intermediate length performance. We represent the effects of practical
choices by the outcomes of bets offered to agents characterized by two
uncertainty management approaches: the subjective Bayesian approach and the
Classical confidence interval approach. Experimental evaluation suggests that
the confidence interval approach can outperform the subjective approach in the
relatively short run."
Expected Utility Networks,"We introduce a new class of graphical representations, expected utility
networks (EUNs), and discuss some of its properties and potential applications
to artificial intelligence and economic theory. In EUNs not only probabilities,
but also utilities enjoy a modular representation. EUNs are undirected graphs
with two types of arc, representing probability and utility dependencies
respectively. The representation of utilities is based on a novel notion of
conditional utility independence, which we introduce and discuss in the context
of other existing proposals. Just as probabilistic inference involves the
computation of conditional probabilities, strategic inference involves the
computation of conditional expected utilities for alternative plans of action.
We define a new notion of conditional expected utility (EU) independence, and
show that in EUNs node separation with respect to the probability and utility
subgraphs implies conditional EU independence."
"Inference Using Message Propagation and Topology Transformation in
  Vector Gaussian Continuous Networks","We extend Gaussian networks - directed acyclic graphs that encode
probabilistic relationships between variables - to its vector form. Vector
Gaussian continuous networks consist of composite nodes representing
multivariates, that take continuous values. These vector or composite nodes can
represent correlations between parents, as opposed to conventional univariate
nodes. We derive rules for inference in these networks based on two methods:
message propagation and topology transformation. These two approaches lead to
the development of algorithms, that can be implemented in either a centralized
or a decentralized manner. The domain of application of these networks are
monitoring and estimation problems. This new representation along with the
rules for inference developed here can be used to derive current Bayesian
algorithms such as the Kalman filter, and provide a rich foundation to develop
new algorithms. We illustrate this process by deriving the decentralized form
of the Kalman filter. This work unifies concepts from artificial intelligence
and modern control theory."
Probabilistic Exploration in Planning while Learning,"Sequential decision tasks with incomplete information are characterized by
the exploration problem; namely the trade-off between further exploration for
learning more about the environment and immediate exploitation of the accrued
information for decision-making. Within artificial intelligence, there has been
an increasing interest in studying planning-while-learning algorithms for these
decision tasks. In this paper we focus on the exploration problem in
reinforcement learning and Q-learning in particular. The existing exploration
strategies for Q-learning are of a heuristic nature and they exhibit limited
scaleability in tasks with large (or infinite) state and action spaces.
Efficient experimentation is needed for resolving uncertainties when possible
plans are compared (i.e. exploration). The experimentation should be sufficient
for selecting with statistical significance a locally optimal plan (i.e.
exploitation). For this purpose, we develop a probabilistic hill-climbing
algorithm that uses a statistical selection procedure to decide how much
exploration is needed for selecting a plan which is, with arbitrarily high
probability, arbitrarily close to a locally optimal one. Due to its generality
the algorithm can be employed for the exploration strategy of robust
Q-learning. An experiment on a relatively complex control task shows that the
proposed exploration strategy performs better than a typical exploration
strategy."
Some Properties of Joint Probability Distributions,"Several Artificial Intelligence schemes for reasoning under uncertainty
explore either explicitly or implicitly asymmetries among probabilities of
various states of their uncertain domain models. Even though the correct
working of these schemes is practically contingent upon the existence of a
small number of probable states, no formal justification has been proposed of
why this should be the case. This paper attempts to fill this apparent gap by
studying asymmetries among probabilities of various states of uncertain models.
By rewriting the joint probability distribution over a model's variables into a
product of individual variables' prior and conditional probability
distributions, and applying central limit theorem to this product, we can
demonstrate that the probabilities of individual states of the model can be
expected to be drawn from highly skewed, log-normal distributions. With
sufficient asymmetry in individual prior and conditional probability
distributions, a small fraction of states can be expected to cover a large
portion of the total probability space with the remaining states having
practically negligible probability. Theoretical discussion is supplemented by
simulation results and an illustrative real-world example."
Three Approaches to Probability Model Selection,"This paper compares three approaches to the problem of selecting among
probability models to fit data (1) use of statistical criteria such as Akaike's
information criterion and Schwarz's ""Bayesian information criterion,"" (2)
maximization of the posterior probability of the model, and (3) maximization of
an effectiveness ratio? trading off accuracy and computational cost. The
unifying characteristic of the approaches is that all can be viewed as
maximizing a penalized likelihood function. The second approach with suitable
prior distributions has been shown to reduce to the first. This paper shows
that the third approach reduces to the second for a particular form of the
effectiveness ratio, and illustrates all three approaches with the problem of
selecting the number of components in a mixture of Gaussian distributions.
Unlike the first two approaches, the third can be used even when the candidate
models are chosen for computational efficiency, without regard to physical
interpretation, so that the likelihood and the prior distribution over models
cannot be interpreted literally. As the most general and computationally
oriented of the approaches, it is especially useful for artificial intelligence
applications."
"Expressing Relational and Temporal Knowledge in Visual Probabilistic
  Networks","Bayesian networks have been used extensively in diagnostic tasks such as
medicine, where they represent the dependency relations between a set of
symptoms and a set of diseases. A criticism of this type of knowledge
representation is that it is restricted to this kind of task, and that it
cannot cope with the knowledge required in other artificial intelligence
applications. For example, in computer vision, we require the ability to model
complex knowledge, including temporal and relational factors. In this paper we
extend Bayesian networks to model relational and temporal knowledge for
high-level vision. These extended networks have a simple structure which
permits us to propagate probability efficiently. We have applied them to the
domain of endoscopy, illustrating how the general modelling principles can be
used in specific cases."
A Sensitivity Analysis of Pathfinder: A Follow-up Study,"At last year?s Uncertainty in AI Conference, we reported the results of a
sensitivity analysis study of Pathfinder. Our findings were quite
unexpected-slight variations to Pathfinder?s parameters appeared to lead to
substantial degradations in system performance. A careful look at our first
analysis, together with the valuable feedback provided by the participants of
last year?s conference, led us to conduct a follow-up study. Our follow-up
differs from our initial study in two ways: (i) the probabilities 0.0 and 1.0
remained unchanged, and (ii) the variations to the probabilities that are close
to both ends (0.0 or 1.0) were less than the ones close to the middle (0.5).
The results of the follow-up study look more reasonable-slight variations to
Pathfinder?s parameters now have little effect on its performance. Taken
together, these two sets of results suggest a viable extension of a common
decision analytic sensitivity analysis to the larger, more complex settings
generally encountered in artificial intelligence."
A Randomized Approximation Algorithm of Logic Sampling,"In recent years, researchers in decision analysis and artificial intelligence
(AI) have used Bayesian belief networks to build models of expert opinion.
Using standard methods drawn from the theory of computational complexity,
workers in the field have shown that the problem of exact probabilistic
inference on belief networks almost certainly requires exponential computation
in the worst ease [3]. We have previously described a randomized approximation
scheme, called BN-RAS, for computation on belief networks [ 1, 2, 4]. We gave
precise analytic bounds on the convergence of BN-RAS and showed how to trade
running time for accuracy in the evaluation of posterior marginal
probabilities. We now extend our previous results and demonstrate the
generality of our framework by applying similar mathematical techniques to the
analysis of convergence for logic sampling [7], an alternative simulation
algorithm for probabilistic inference."
"Integrating Case-Based and Rule-Based Reasoning: the Possibilistic
  Connection","Rule based reasoning (RBR) and case based reasoning (CBR) have emerged as two
important and complementary reasoning methodologies in artificial intelligence
(Al). For problem solving in complex, real world situations, it is useful to
integrate RBR and CBR. This paper presents an approach to achieve a compact and
seamless integration of RBR and CBR within the base architecture of rules. The
paper focuses on the possibilistic nature of the approximate reasoning
methodology common to both CBR and RBR. In CBR, the concept of similarity is
casted as the complement of the distance between cases. In RBR the transitivity
of similarity is the basis for the approximate deductions based on the
generalized modus ponens. It is shown that the integration of CBR and RBR is
possible without altering the inference engine of RBR. This integration is
illustrated in the financial domain of mergers and acquisitions. These ideas
have been implemented in a prototype system called MARS."
"The Effects of Perfect and Sample Information on Fuzzy Utilities in
  Decision-Making","In this paper, we first consider a Bayesian framework and model the ""utility
function"" in terms of fuzzy random variables. On the basis of this model, we
define the ""prior (fuzzy) expected utility"" associated with each action, and
the corresponding ""posterior (fuzzy) expected utility given sample information
from a random experiment"". The aim of this paper is to analyze how sample
information can affect the expected utility. In this way, by using some fuzzy
preference relations, we conclude that sample information allows a decision
maker to increase the expected utility on the average. The upper bound on the
value of the expected utility is when the decision maker has perfect
information. Applications of this work to the field of artificial intelligence
are presented through two examples."
Bayesian Prediction for Artificial Intelligence,"This paper shows that the common method used for making predictions under
uncertainty in A1 and science is in error. This method is to use currently
available data to select the best model from a given class of models-this
process is called abduction-and then to use this model to make predictions
about future data. The correct method requires averaging over all the models to
make a prediction-we call this method transduction. Using transduction, an AI
system will not give misleading results when basing predictions on small
amounts of data, when no model is clearly best. For common classes of models we
show that the optimal solution can be given in closed form."
Knowledge Engineering Within A Generalized Bayesian Framework,"During the ongoing debate over the representation of uncertainty in
Artificial Intelligence, Cheeseman, Lemmer, Pearl, and others have argued that
probability theory, and in particular the Bayesian theory, should be used as
the basis for the inference mechanisms of Expert Systems dealing with
uncertainty. In order to pursue the issue in a practical setting, sophisticated
tools for knowledge engineering are needed that allow flexible and
understandable interaction with the underlying knowledge representation
schemes. This paper describes a Generalized Bayesian framework for building
expert systems which function in uncertain domains, using algorithms proposed
by Lemmer. It is neither rule-based nor frame-based, and requires a new system
of knowledge engineering tools. The framework we describe provides a
knowledge-based system architecture with an inference engine, explanation
capability, and a unique aid for building consistent knowledge bases."
"The Rational and Computational Scope of Probabilistic Rule-Based Expert
  Systems","Belief updating schemes in artificial intelligence may be viewed as three
dimensional languages, consisting of a syntax (e.g. probabilities or certainty
factors), a calculus (e.g. Bayesian or CF combination rules), and a semantics
(i.e. cognitive interpretations of competing formalisms). This paper studies
the rational scope of those languages on the syntax and calculus grounds. In
particular, the paper presents an endomorphism theorem which highlights the
limitations imposed by the conditional independence assumptions implicit in the
CF calculus. Implications of the theorem to the relationship between the CF and
the Bayesian languages and the Dempster-Shafer theory of evidence are
presented. The paper concludes with a discussion of some implications on
rule-based knowledge engineering in uncertain domains."
"Machine Learning, Clustering, and Polymorphy","This paper describes a machine induction program (WITT) that attempts to
model human categorization. Properties of categories to which human subjects
are sensitive includes best or prototypical members, relative contrasts between
putative categories, and polymorphy (neither necessary or sufficient features).
This approach represents an alternative to usual Artificial Intelligence
approaches to generalization and conceptual clustering which tend to focus on
necessary and sufficient feature rules, equivalence classes, and simple search
and match schemes. WITT is shown to be more consistent with human
categorization while potentially including results produced by more traditional
clustering schemes. Applications of this approach in the domains of expert
systems and information retrieval are also discussed."
Probabilistic Conflict Resolution in Hierarchical Hypothesis Spaces,"Artificial intelligence applications such as industrial robotics, military
surveillance, and hazardous environment clean-up, require situation
understanding based on partial, uncertain, and ambiguous or erroneous evidence.
It is necessary to evaluate the relative likelihood of multiple possible
hypotheses of the (current) situation faced by the decision making program.
Often, the evidence and hypotheses are hierarchical in nature. In image
understanding tasks, for example, evidence begins with raw imagery, from which
ambiguous features are extracted which have multiple possible aggregations
providing evidential support for the presence of multiple hypothesis of objects
and terrain, which in turn aggregate in multiple ways to provide partial
evidence for different interpretations of the ambient scene. Information fusion
for military situation understanding has a similar evidence/hypothesis
hierarchy from multiple sensor through message level interpretations, and also
provides evidence at multiple levels of the doctrinal hierarchy of military
forces."
"Projective simulation for classical learning agents: a comprehensive
  investigation","We study the model of projective simulation (PS), a novel approach to
artificial intelligence based on stochastic processing of episodic memory which
was recently introduced [H.J. Briegel and G. De las Cuevas. Sci. Rep. 2, 400,
(2012)]. Here we provide a detailed analysis of the model and examine its
performance, including its achievable efficiency, its learning times and the
way both properties scale with the problems' dimension. In addition, we situate
the PS agent in different learning scenarios, and study its learning abilities.
A variety of new scenarios are being considered, thereby demonstrating the
model's flexibility. Furthermore, to put the PS scheme in context, we compare
its performance with those of Q-learning and learning classifier systems, two
popular models in the field of reinforcement learning. It is shown that PS is a
competitive artificial intelligence model of unique properties and strengths."
Second Order Swarm Intelligence,"An artificial Ant Colony System (ACS) algorithm to solve general-purpose
combinatorial Optimization Problems (COP) that extends previous AC models [21]
by the inclusion of a negative pheromone, is here described. Several Travelling
Salesman Problem (TSP) were used as benchmark. We show that by using two
different sets of pheromones, a second-order co-evolved compromise between
positive and negative feedbacks achieves better results than single positive
feedback systems. The algorithm was tested against known NP-complete
combinatorial Optimization Problems, running on symmetrical TSP's. We show that
the new algorithm compares favourably against these benchmarks, accordingly to
recent biological findings by Robinson [26,27], and Gruter [28] where ""No
entry"" signals and negative feedback allows a colony to quickly reallocate the
majority of its foragers to superior food patches. This is the first time an
extended ACS algorithm is implemented with these successful characteristics."
"Pattern recognition issues on anisotropic smoothed particle
  hydrodynamics","This is a preliminary theoretical discussion on the computational
requirements of the state of the art smoothed particle hydrodynamics (SPH) from
the optics of pattern recognition and artificial intelligence. It is pointed
out in the present paper that, when including anisotropy detection to improve
resolution on shock layer, SPH is a very peculiar case of unsupervised machine
learning. On the other hand, the free particle nature of SPH opens an
opportunity for artificial intelligence to study particles as agents acting in
a collaborative framework in which the timed outcomes of a fluid simulation
forms a large knowledge base, which might be very attractive in computational
astrophysics phenomenological problems like self-propagating star formation."
A brief network analysis of Artificial Intelligence publication,"In this paper, we present an illustration to the history of Artificial
Intelligence(AI) with a statistical analysis of publish since 1940. We
collected and mined through the IEEE publish data base to analysis the
geological and chronological variance of the activeness of research in AI. The
connections between different institutes are showed. The result shows that the
leading community of AI research are mainly in the USA, China, the Europe and
Japan. The key institutes, authors and the research hotspots are revealed. It
is found that the research institutes in the fields like Data Mining, Computer
Vision, Pattern Recognition and some other fields of Machine Learning are quite
consistent, implying a strong interaction between the community of each field.
It is also showed that the research of Electronic Engineering and Industrial or
Commercial applications are very active in California. Japan is also publishing
a lot of papers in robotics. Due to the limitation of data source, the result
might be overly influenced by the number of published articles, which is to our
best improved by applying network keynode analysis on the research community
instead of merely count the number of publish."
The Complexity of Integer Bound Propagation,"Bound propagation is an important Artificial Intelligence technique used in
Constraint Programming tools to deal with numerical constraints. It is
typically embedded within a search procedure (""branch and prune"") and used at
every node of the search tree to narrow down the search space, so it is
critical that it be fast. The procedure invokes constraint propagators until a
common fixpoint is reached, but the known algorithms for this have a
pseudo-polynomial worst-case time complexity: they are fast indeed when the
variables have a small numerical range, but they have the well-known problem of
being prohibitively slow when these ranges are large. An important question is
therefore whether strongly-polynomial algorithms exist that compute the common
bound consistent fixpoint of a set of constraints. This paper answers this
question. In particular we show that this fixpoint computation is in fact
NP-complete, even when restricted to binary linear constraints."
"Combining Evaluation Metrics via the Unanimous Improvement Ratio and its
  Application to Clustering Tasks","Many Artificial Intelligence tasks cannot be evaluated with a single quality
criterion and some sort of weighted combination is needed to provide system
rankings. A problem of weighted combination measures is that slight changes in
the relative weights may produce substantial changes in the system rankings.
This paper introduces the Unanimous Improvement Ratio (UIR), a measure that
complements standard metric combination criteria (such as van Rijsbergen's
F-measure) and indicates how robust the measured differences are to changes in
the relative weights of the individual metrics. UIR is meant to elucidate
whether a perceived difference between two systems is an artifact of how
individual metrics are weighted.
  Besides discussing the theoretical foundations of UIR, this paper presents
empirical results that confirm the validity and usefulness of the metric for
the Text Clustering problem, where there is a tradeoff between precision and
recall based metrics and results are particularly sensitive to the weighting
scheme used to combine them. Remarkably, our experiments show that UIR can be
used as a predictor of how well differences between systems measured on a given
test bed will also hold in a different test bed."
Real Time Strategy Language,"Real Time Strategy (RTS) games provide complex domain to test the latest
artificial intelligence (AI) research. In much of the literature, AI systems
have been limited to playing one game. Although, this specialization has
resulted in stronger AI gaming systems it does not address the key concerns of
AI researcher. AI researchers seek the development of AI agents that can
autonomously interpret learn, and apply new knowledge. To achieve human level
performance, current AI systems rely on game specific knowledge of an expert.
The paper presents the full RTS language in hopes of shifting the current
research focus to the development of general RTS agents. General RTS agents are
AI gaming systems that can play any RTS games, defined in the RTS language.
This prevents game specific knowledge from being hard coded into the system,
thereby facilitating research that addresses the fundamental concerns of
artificial intelligence."
"A hybrid swarm-based algorithm for single-objective optimization
  problems involving high-cost analyses","In many technical fields, single-objective optimization procedures in
continuous domains involve expensive numerical simulations. In this context, an
improvement of the Artificial Bee Colony (ABC) algorithm, called the Artificial
super-Bee enhanced Colony (AsBeC), is presented. AsBeC is designed to provide
fast convergence speed, high solution accuracy and robust performance over a
wide range of problems. It implements enhancements of the ABC structure and
hybridizations with interpolation strategies. The latter are inspired by the
quadratic trust region approach for local investigation and by an efficient
global optimizer for separable problems. Each modification and their combined
effects are studied with appropriate metrics on a numerical benchmark, which is
also used for comparing AsBeC with some effective ABC variants and other
derivative-free algorithms. In addition, the presented algorithm is validated
on two recent benchmarks adopted for competitions in international conferences.
Results show remarkable competitiveness and robustness for AsBeC."
E-Generalization Using Grammars,"We extend the notion of anti-unification to cover equational theories and
present a method based on regular tree grammars to compute a finite
representation of E-generalization sets. We present a framework to combine
Inductive Logic Programming and E-generalization that includes an extension of
Plotkin's lgg theorem to the equational case. We demonstrate the potential
power of E-generalization by three example applications: computation of
suggestions for auxiliary lemmas in equational inductive proofs, computation of
construction laws for given term sequences, and learning of screen editor
command sequences."
Robust Feature Selection by Mutual Information Distributions,"Mutual information is widely used in artificial intelligence, in a
descriptive way, to measure the stochastic dependence of discrete random
variables. In order to address questions such as the reliability of the
empirical value, one must consider sample-to-population inferential approaches.
This paper deals with the distribution of mutual information, as obtained in a
Bayesian framework by a second-order Dirichlet prior distribution. The exact
analytical expression for the mean and an analytical approximation of the
variance are reported. Asymptotic approximations of the distribution are
proposed. The results are applied to the problem of selecting features for
incremental learning and classification of the naive Bayes classifier. A fast,
newly defined method is shown to outperform the traditional approach based on
empirical mutual information on a number of real data sets. Finally, a
theoretical development is reported that allows one to efficiently extend the
above methods to incomplete samples in an easy and effective way."
Friendly Artificial Intelligence: the Physics Challenge,"Relentless progress in artificial intelligence (AI) is increasingly raising
concerns that machines will replace humans on the job market, and perhaps
altogether. Eliezer Yudkowski and others have explored the possibility that a
promising future for humankind could be guaranteed by a superintelligent
""Friendly AI"", designed to safeguard humanity and its values. I argue that,
from a physics perspective where everything is simply an arrangement of
elementary particles, this might be even harder than it appears. Indeed, it may
require thinking rigorously about the meaning of life: What is ""meaning"" in a
particle arrangement? What is ""life""? What is the ultimate ethical imperative,
i.e., how should we strive to rearrange the particles of our Universe and shape
its future? If we fail to answer the last question rigorously, this future is
unlikely to contain humans."
A Quantum Production Model,"The production system is a theoretical model of computation relevant to the
artificial intelligence field allowing for problem solving procedures such as
hierarchical tree search. In this work we explore some of the connections
between artificial intelligence and quantum computation by presenting a model
for a quantum production system. Our approach focuses on initially developing a
model for a reversible production system which is a simple mapping of Bennett's
reversible Turing machine. We then expand on this result in order to
accommodate for the requirements of quantum computation. We present the details
of how our proposition can be used alongside Grover's algorithm in order to
yield a speedup comparatively to its classical counterpart. We discuss the
requirements associated with such a speedup and how it compares against a
similar quantum hierarchical search approach."
Ascribing Consciousness to Artificial Intelligence,"This paper critically assesses the anti-functionalist stance on consciousness
adopted by certain advocates of integrated information theory (IIT), a
corollary of which is that human-level artificial intelligence implemented on
conventional computing hardware is necessarily not conscious. The critique
draws on variations of a well-known gradual neuronal replacement thought
experiment, as well as bringing out tensions in IIT's treatment of
self-knowledge. The aim, though, is neither to reject IIT outright nor to
champion functionalism in particular. Rather, it is suggested that both ideas
have something to offer a scientific understanding of consciousness, as long as
they are not dressed up as solutions to illusory metaphysical problems. As for
human-level AI, we must await its development before we can decide whether or
not to ascribe consciousness to it."
"Artificial general intelligence through recursive data compression and
  grounded reasoning: a position paper","This paper presents a tentative outline for the construction of an
artificial, generally intelligent system (AGI). It is argued that building a
general data compression algorithm solving all problems up to a complexity
threshold should be the main thrust of research. A measure for partial progress
in AGI is suggested. Although the details are far from being clear, some
general properties for a general compression algorithm are fleshed out. Its
inductive bias should be flexible and adapt to the input data while constantly
searching for a simple, orthogonal and complete set of hypotheses explaining
the data. It should recursively reduce the size of its representations thereby
compressing the data increasingly at every iteration.
  Abstract Based on that fundamental ability, a grounded reasoning system is
proposed. It is argued how grounding and flexible feature bases made of
hypotheses allow for resourceful thinking. While the simulation of
representation contents on the mental stage accounts for much of the power of
propositional logic, compression leads to simple sets of hypotheses that allow
the detection and verification of universally quantified statements.
  Abstract Together, it is highlighted how general compression and grounded
reasoning could account for the birth and growth of first concepts about the
world and the commonsense reasoning about them."
Automated Assignment of Backbone NMR Data using Artificial Intelligence,"Nuclear magnetic resonance (NMR) spectroscopy is a powerful method for the
investigation of three-dimensional structures of biological molecules such as
proteins. Determining a protein structure is essential for understanding its
function and alterations in function which lead to disease. One of the major
challenges of the post-genomic era is to obtain structural and functional
information on the many unknown proteins encoded by thousands of newly
identified genes. The goal of this research is to design an algorithm capable
of automating the analysis of backbone protein NMR data by implementing AI
strategies such as greedy and A* search."
A Minimal Architecture for General Cognition,"A minimalistic cognitive architecture called MANIC is presented. The MANIC
architecture requires only three function approximating models, and one state
machine. Even with so few major components, it is theoretically sufficient to
achieve functional equivalence with all other cognitive architectures, and can
be practically trained. Instead of seeking to transfer architectural
inspiration from biology into artificial intelligence, MANIC seeks to minimize
novelty and follow the most well-established constructs that have evolved
within various sub-fields of data science. From this perspective, MANIC offers
an alternate approach to a long-standing objective of artificial intelligence.
This paper provides a theoretical analysis of the MANIC architecture."
"A Topological Approach to Meta-heuristics: Analytical Results on the BFS
  vs. DFS Algorithm Selection Problem","Search is a central problem in artificial intelligence, and breadth-first
search (BFS) and depth-first search (DFS) are the two most fundamental ways to
search. In this paper we derive estimates for average BFS and DFS runtime. The
average runtime estimates can be used to allocate resources or judge the
hardness of a problem. They can also be used for selecting the best graph
representation, and for selecting the faster algorithm out of BFS and DFS. They
may also form the basis for an analysis of more advanced search methods. The
paper treats both tree search and graph search. For tree search, we employ a
probabilistic model of goal distribution; for graph search, the analysis
depends on an additional statistic of path redundancy and average branching
factor. As an application, we use the results to predict BFS and DFS runtime on
two concrete grammar problems and on the N-puzzle. Experimental verification
shows that our analytical approximations come close to empirical reality."
"Finetuning Randomized Heuristic Search For 2D Path Planning: Finding The
  Best Input Parameters For R* Algorithm Through Series Of Experiments","Path planning is typically considered in Artificial Intelligence as a graph
searching problem and R* is state-of-the-art algorithm tailored to solve it.
The algorithm decomposes given path finding task into the series of subtasks
each of which can be easily (in computational sense) solved by well-known
methods (such as A*). Parameterized random choice is used to perform the
decomposition and as a result R* performance largely depends on the choice of
its input parameters. In our work we formulate a range of assumptions
concerning possible upper and lower bounds of R* parameters, their
interdependency and their influence on R* performance. Then we evaluate these
assumptions by running a large number of experiments. As a result we formulate
a set of heuristic rules which can be used to initialize the values of R*
parameters in a way that leads to algorithm's best performance."
Strategic Dialogue Management via Deep Reinforcement Learning,"Artificially intelligent agents equipped with strategic skills that can
negotiate during their interactions with other natural or artificial agents are
still underdeveloped. This paper describes a successful application of Deep
Reinforcement Learning (DRL) for training intelligent agents with strategic
conversational skills, in a situated dialogue setting. Previous studies have
modelled the behaviour of strategic agents using supervised learning and
traditional reinforcement learning techniques, the latter using tabular
representations or learning with linear function approximation. In this study,
we apply DRL with a high-dimensional state space to the strategic board game of
Settlers of Catan---where players can offer resources in exchange for others
and they can also reply to offers made by other players. Our experimental
results report that the DRL-based learnt policies significantly outperformed
several baselines including random, rule-based, and supervised-based
behaviours. The DRL-based policy has a 53% win rate versus 3 automated players
(`bots'), whereas a supervised player trained on a dialogue corpus in this
setting achieved only 27%, versus the same 3 bots. This result supports the
claim that DRL is a promising framework for training dialogue systems, and
strategic agents with negotiation abilities."
Distributed Constraint Optimization Problems and Applications: A Survey,"The field of Multi-Agent System (MAS) is an active area of research within
Artificial Intelligence, with an increasingly important impact in industrial
and other real-world applications. Within a MAS, autonomous agents interact to
pursue personal interests and/or to achieve common objectives. Distributed
Constraint Optimization Problems (DCOPs) have emerged as one of the prominent
agent architectures to govern the agents' autonomous behavior, where both
algorithms and communication models are driven by the structure of the specific
problem. During the last decade, several extensions to the DCOP model have
enabled them to support MAS in complex, real-time, and uncertain environments.
This survey aims at providing an overview of the DCOP model, giving a
classification of its multiple extensions and addressing both resolution
methods and applications that find a natural mapping within each class of
DCOPs. The proposed classification suggests several future perspectives for
DCOP extensions, and identifies challenges in the design of efficient
resolution algorithms, possibly through the adaptation of strategies from
different areas."
Philosophical Fictionalism and Problem of Artificial Intelligence,"The artificial intelligence received broad interpretation as a literary
image. This approach did not have unambiguous refering to the scopes of logical
studies and mathematical investigations. An author applied methods peculiar to
the semiotic approach, offered by Boris Uspensky and Yury Lotman. In addition,
the article presented the criticism of modern versions of educational
technologies, which led to the unconditional expectations for possibilities of
information and telecommunication technologies. Methodological culture's
growth, which was described on the base of semiotics and functional approach to
word formation of new meanings for the description of the studied subjects,
provided the development of pupils' thought. As a result, the research opened
new prospects on understanding of artificial intelligence within educational
practice."
Category Theoretic Analysis of Photon-based Decision Making,"Decision making is a vital function in this age of machine learning and
artificial intelligence, yet its physical realization and theoretical
fundamentals are still not completely understood. In our former study, we
demonstrated that single-photons can be used to make decisions in uncertain,
dynamically changing environments. The two-armed bandit problem was
successfully solved using the dual probabilistic and particle attributes of
single photons. In this study, we present a category theoretic modeling and
analysis of single-photon-based decision making, including a quantitative
analysis that is in agreement with the experimental results. A category
theoretic model reveals the complex interdependencies of subject matter
entities in a simplified manner, even in dynamically changing environments. In
particular, the octahedral and braid structures in triangulated categories
provide a better understanding and quantitative metrics of the underlying
mechanisms of a single-photon decision maker. This study provides both insight
and a foundation for analyzing more complex and uncertain problems, to further
machine learning and artificial intelligence."
Limits to Verification and Validation of Agentic Behavior,"Verification and validation of agentic behavior have been suggested as
important research priorities in efforts to reduce risks associated with the
creation of general artificial intelligence (Russell et al 2015). In this paper
we question the appropriateness of using language of certainty with respect to
efforts to manage that risk. We begin by establishing a very general formalism
to characterize agentic behavior and to describe standards of acceptable
behavior. We show that determination of whether an agent meets any particular
standard is not computable. We discuss the extent of the burden associated with
verification by manual proof and by automated behavioral governance. We show
that to ensure decidability of the behavioral standard itself, one must further
limit the capabilities of the agent. We then demonstrate that if our concerns
relate to outcomes in the physical world, attempts at validation are futile.
Finally, we show that layered architectures aimed at making these challenges
tractable mistakenly equate intentions with actions or outcomes, thereby
failing to provide any guarantees. We conclude with a discussion of why
language of certainty should be eradicated from the conversation about the
safety of general artificial intelligence."
Automatic Bridge Bidding Using Deep Reinforcement Learning,"Bridge is among the zero-sum games for which artificial intelligence has not
yet outperformed expert human players. The main difficulty lies in the bidding
phase of bridge, which requires cooperative decision making under partial
information. Existing artificial intelligence systems for bridge bidding rely
on and are thus restricted by human-designed bidding systems or features. In
this work, we propose a pioneering bridge bidding system without the aid of
human domain knowledge. The system is based on a novel deep reinforcement
learning model, which extracts sophisticated features and learns to bid
automatically based on raw card data. The model includes an
upper-confidence-bound algorithm and additional techniques to achieve a balance
between exploration and exploitation. Our experiments validate the promising
performance of our proposed model. In particular, the model advances from
having no knowledge about bidding to achieving superior performance when
compared with a champion-winning computer bridge program that implements a
human-designed bidding system."
"Pilot Testing an Artificial Intelligence Algorithm That Selects Homeless
  Youth Peer Leaders Who Promote HIV Testing","Objective. To pilot test an artificial intelligence (AI) algorithm that
selects peer change agents (PCA) to disseminate HIV testing messaging in a
population of homeless youth. Methods. We recruited and assessed 62 youth at
baseline, 1 month (n = 48), and 3 months (n = 38). A Facebook app collected
preliminary social network data. Eleven PCAs selected by AI attended a 1-day
training and 7 weekly booster sessions. Mixed-effects models with random
effects were used to assess change over time. Results. Significant change over
time was observed in past 6-month HIV testing (57.9%, 82.4%, 76.3%; p < .05)
but not condom use (63.9%, 65.7%, 65.8%). Most youth reported speaking to a PCA
about HIV prevention (72.0% at 1 month, 61.5% at 3 months). Conclusions. AI is
a promising avenue for implementing PCA models for homeless youth. Increasing
rates of regular HIV testing is critical to HIV prevention and linking homeless
youth to treatment."
From Deterministic ODEs to Dynamic Structural Causal Models,"Structural Causal Models are widely used in causal modelling, but how they
relate to other modelling tools is poorly understood. In this paper we provide
a novel perspective on the relationship between Ordinary Differential Equations
and Structural Causal Models. We show how, under certain conditions, the
asymptotic behaviour of an Ordinary Differential Equation under non-constant
interventions can be modelled using Dynamic Structural Causal Models. In
contrast to earlier work, we study not only the effect of interventions on
equilibrium states; rather, we model asymptotic behaviour that is dynamic under
interventions that vary in time, and include as a special case the study of
static equilibria."
Long-Term Trends in the Public Perception of Artificial Intelligence,"Analyses of text corpora over time can reveal trends in beliefs, interest,
and sentiment about a topic. We focus on views expressed about artificial
intelligence (AI) in the New York Times over a 30-year period. General
interest, awareness, and discussion about AI has waxed and waned since the
field was founded in 1956. We present a set of measures that captures levels of
engagement, measures of pessimism and optimism, the prevalence of specific
hopes and concerns, and topics that are linked to discussions about AI over
decades. We find that discussion of AI has increased sharply since 2009, and
that these discussions have been consistently more optimistic than pessimistic.
However, when we examine specific concerns, we find that worries of loss of
control of AI, ethical concerns for AI, and the negative impact of AI on work
have grown in recent years. We also find that hopes for AI in healthcare and
education have increased over time."
Optimizing positional scoring rules for rank aggregation,"Nowadays, several crowdsourcing projects exploit social choice methods for
computing an aggregate ranking of alternatives given individual rankings
provided by workers. Motivated by such systems, we consider a setting where
each worker is asked to rank a fixed (small) number of alternatives and, then,
a positional scoring rule is used to compute the aggregate ranking. Among the
apparently infinite such rules, what is the best one to use? To answer this
question, we assume that we have partial access to an underlying true ranking.
Then, the important optimization problem to be solved is to compute the
positional scoring rule whose outcome, when applied to the profile of
individual rankings, is as close as possible to the part of the underlying true
ranking we know. We study this fundamental problem from a theoretical viewpoint
and present positive and negative complexity results and, furthermore,
complement our theoretical findings with experiments on real-world and
synthetic data."
"Deep Convolutional Networks as Models of Generalization and Blending
  Within Visual Creativity","We examine two recent artificial intelligence (AI) based deep learning
algorithms for visual blending in convolutional neural networks (Mordvintsev et
al. 2015, Gatys et al. 2015). To investigate the potential value of these
algorithms as tools for computational creativity research, we explain and
schematize the essential aspects of the algorithms' operation and give visual
examples of their output. We discuss the relationship of the two algorithms to
human cognitive science theories of creativity such as conceptual blending
theory and honing theory, and characterize the algorithms with respect to
generation of novelty and aesthetic quality."
"Artificial Intelligence Safety and Cybersecurity: a Timeline of AI
  Failures","In this work, we present and analyze reported failures of artificially
intelligent systems and extrapolate our analysis to future AIs. We suggest that
both the frequency and the seriousness of future AI failures will steadily
increase. AI Safety can be improved based on ideas developed by cybersecurity
experts. For narrow AIs safety failures are at the same, moderate, level of
criticality as in cybersecurity, however for general AI, failures have a
fundamentally different impact. A single failure of a superintelligent system
may cause a catastrophic event without a chance for recovery. The goal of
cybersecurity is to reduce the number of successful attacks on the system; the
goal of AI Safety is to make sure zero attacks succeed in bypassing the safety
mechanisms. Unfortunately, such a level of performance is unachievable. Every
security system will eventually fail; there is no such thing as a 100% secure
system."
Self-Correcting Models for Model-Based Reinforcement Learning,"When an agent cannot represent a perfectly accurate model of its
environment's dynamics, model-based reinforcement learning (MBRL) can fail
catastrophically. Planning involves composing the predictions of the model;
when flawed predictions are composed, even minor errors can compound and render
the model useless for planning. Hallucinated Replay (Talvitie 2014) trains the
model to ""correct"" itself when it produces errors, substantially improving MBRL
with flawed models. This paper theoretically analyzes this approach,
illuminates settings in which it is likely to be effective or ineffective, and
presents a novel error bound, showing that a model's ability to self-correct is
more tightly related to MBRL performance than one-step prediction error. These
results inspire an MBRL algorithm for deterministic MDPs with performance
guarantees that are robust to model class limitations."
"Basic protocols in quantum reinforcement learning with superconducting
  circuits","Superconducting circuit technologies have recently achieved quantum protocols
involving closed feedback loops. Quantum artificial intelligence and quantum
machine learning are emerging fields inside quantum technologies which may
enable quantum devices to acquire information from the outer world and improve
themselves via a learning process. Here we propose the implementation of basic
protocols in quantum reinforcement learning, with superconducting circuits
employing feedback-loop control. We introduce diverse scenarios for
proof-of-principle experiments with state-of-the-art superconducting circuit
technologies and analyze their feasibility in presence of imperfections. The
field of quantum artificial intelligence implemented with superconducting
circuits paves the way for enhanced quantum control and quantum computation
protocols."
"Robust Multilingual Named Entity Recognition with Shallow
  Semi-Supervised Features","We present a multilingual Named Entity Recognition approach based on a robust
and general set of features across languages and datasets. Our system combines
shallow local information with clustering semi-supervised features induced on
large amounts of unlabeled text. Understanding via empirical experimentation
how to effectively combine various types of clustering features allows us to
seamlessly export our system to other datasets and languages. The result is a
simple but highly competitive system which obtains state of the art results
across five languages and twelve datasets. The results are reported on standard
shared task evaluation data such as CoNLL for English, Spanish and Dutch.
Furthermore, and despite the lack of linguistically motivated features, we also
report best results for languages such as Basque and German. In addition, we
demonstrate that our method also obtains very competitive results even when the
amount of supervised data is cut by half, alleviating the dependency on
manually annotated data. Finally, the results show that our emphasis on
clustering features is crucial to develop robust out-of-domain models. The
system and models are freely available to facilitate its use and guarantee the
reproducibility of results."
Who Will Win Practical Artificial Intelligence? AI Engineerings in China,"Currently, Artificial Intelligence (AI) has won unprecedented attention and
is becoming the increasingly popular focus in China. This change can be judged
by the impressive record of academic publications, the amount of state-level
investment and the presence of nation-wide participation and devotion. In this
paper, we place emphasis on discussing the progress of artificial intelligence
engineerings in China. We first introduce the focus on AI in Chinese academia,
including the supercomputing brain system, Cambrian Period supercomputer of
neural networks, and biometric systems. Then, the development of AI in
industrial circles and the latest layout of AI products in companies, such as
Baidu, Tencent, and Alibaba, are introduced. Last, we bring in the opinions and
arguments of the main intelligentsia of China about the future development of
AI, including how to examine the relationship between humanity on one side and
science and technology on the other."
A System for Accessible Artificial Intelligence,"While artificial intelligence (AI) has become widespread, many commercial AI
systems are not yet accessible to individual researchers nor the general public
due to the deep knowledge of the systems required to use them. We believe that
AI has matured to the point where it should be an accessible technology for
everyone. We present an ongoing project whose ultimate goal is to deliver an
open source, user-friendly AI system that is specialized for machine learning
analysis of complex data in the biomedical and health care domains. We discuss
how genetic programming can aid in this endeavor, and highlight specific
examples where genetic programming has automated machine learning analyses in
previous projects."
When Will AI Exceed Human Performance? Evidence from AI Experts,"Advances in artificial intelligence (AI) will transform modern life by
reshaping transportation, health, science, finance, and the military. To adapt
public policy, we need to better anticipate these advances. Here we report the
results from a large survey of machine learning researchers on their beliefs
about progress in AI. Researchers predict AI will outperform humans in many
activities in the next ten years, such as translating languages (by 2024),
writing high-school essays (by 2026), driving a truck (by 2027), working in
retail (by 2031), writing a bestselling book (by 2049), and working as a
surgeon (by 2053). Researchers believe there is a 50% chance of AI
outperforming humans in all tasks in 45 years and of automating all human jobs
in 120 years, with Asian respondents expecting these dates much sooner than
North Americans. These results will inform discussion amongst researchers and
policymakers about anticipating and managing trends in AI."
Ethical Artificial Intelligence - An Open Question,"Artificial Intelligence (AI) is an effective science which employs strong
enough approaches, methods, and techniques to solve unsolvable real world based
problems. Because of its unstoppable rise towards the future, there are also
some discussions about its ethics and safety. Shaping an AI friendly
environment for people and a people friendly environment for AI can be a
possible answer for finding a shared context of values for both humans and
robots. In this context, objective of this paper is to address the ethical
issues of AI and explore the moral dilemmas that arise from ethical algorithms,
from pre set or acquired values. In addition, the paper will also focus on the
subject of AI safety. As general, the paper will briefly analyze the concerns
and potential solutions to solving the ethical issues presented and increase
readers awareness on AI safety as another related research interest."
"A ROS multi-ontology references services: OWL reasoners and application
  prototyping issues","This paper introduces a ROS Multi Ontology References (ARMOR) service, a
general-purpose and scalable interface between robot architectures and OWL
reasoners. ARMOR addresses synchronisation and communication issues among
heterogeneous and distributed software components. As a guiding scenario, we
consider a prototyping approach for the use of symbolic reasoning in
human-robot interaction applications."
OPEB: Open Physical Environment Benchmark for Artificial Intelligence,"Artificial Intelligence methods to solve continuous- control tasks have made
significant progress in recent years. However, these algorithms have important
limitations and still need significant improvement to be used in industry and
real- world applications. This means that this area is still in an active
research phase. To involve a large number of research groups, standard
benchmarks are needed to evaluate and compare proposed algorithms. In this
paper, we propose a physical environment benchmark framework to facilitate
collaborative research in this area by enabling different research groups to
integrate their designed benchmarks in a unified cloud-based repository and
also share their actual implemented benchmarks via the cloud. We demonstrate
the proposed framework using an actual implementation of the classical
mountain-car example and present the results obtained using a Reinforcement
Learning algorithm."
Learning Photography Aesthetics with Deep CNNs,"Automatic photo aesthetic assessment is a challenging artificial intelligence
task. Existing computational approaches have focused on modeling a single
aesthetic score or a class (good or bad), however these do not provide any
details on why the photograph is good or bad, or which attributes contribute to
the quality of the photograph. To obtain both accuracy and human interpretation
of the score, we advocate learning the aesthetic attributes along with the
prediction of the overall score. For this purpose, we propose a novel multitask
deep convolution neural network, which jointly learns eight aesthetic
attributes along with the overall aesthetic score. We report near human
performance in the prediction of the overall aesthetic score. To understand the
internal representation of these attributes in the learned model, we also
develop the visualization technique using back propagation of gradients. These
visualizations highlight the important image regions for the corresponding
attributes, thus providing insights about model's representation of these
attributes. We showcase the diversity and complexity associated with different
attributes through a qualitative analysis of the activation maps."
Discriminant chronicles mining: Application to care pathways analytics,"Pharmaco-epidemiology (PE) is the study of uses and effects of drugs in well
defined populations. As medico-administrative databases cover a large part of
the population, they have become very interesting to carry PE studies. Such
databases provide longitudinal care pathways in real condition containing
timestamped care events, especially drug deliveries. Temporal pattern mining
becomes a strategic choice to gain valuable insights about drug uses. In this
paper we propose DCM, a new discriminant temporal pattern mining algorithm. It
extracts chronicle patterns that occur more in a studied population than in a
control population. We present results on the identification of possible
associations between hospitalizations for seizure and anti-epileptic drug
switches in care pathway of epileptic patients."
Neural Networks for Predicting Algorithm Runtime Distributions,"Many state-of-the-art algorithms for solving hard combinatorial problems in
artificial intelligence (AI) include elements of stochasticity that lead to
high variations in runtime, even for a fixed problem instance. Knowledge about
the resulting runtime distributions (RTDs) of algorithms on given problem
instances can be exploited in various meta-algorithmic procedures, such as
algorithm selection, portfolios, and randomized restarts. Previous work has
shown that machine learning can be used to individually predict mean, median
and variance of RTDs. To establish a new state-of-the-art in predicting RTDs,
we demonstrate that the parameters of an RTD should be learned jointly and that
neural networks can do this well by directly optimizing the likelihood of an
RTD given runtime observations. In an empirical study involving five algorithms
for SAT solving and AI planning, we show that neural networks predict the true
RTDs of unseen instances better than previous methods, and can even do so when
only few runtime observations are available per training instance."
"How Does NLP Benefit Legal System: A Summary of Legal Artificial
  Intelligence","Legal Artificial Intelligence (LegalAI) focuses on applying the technology of
artificial intelligence, especially natural language processing, to benefit
tasks in the legal domain. In recent years, LegalAI has drawn increasing
attention rapidly from both AI researchers and legal professionals, as LegalAI
is beneficial to the legal system for liberating legal professionals from a
maze of paperwork. Legal professionals often think about how to solve tasks
from rule-based and symbol-based methods, while NLP researchers concentrate
more on data-driven and embedding methods. In this paper, we introduce the
history, the current state, and the future directions of research in LegalAI.
We illustrate the tasks from the perspectives of legal professionals and NLP
researchers and show several representative applications in LegalAI. We conduct
experiments and provide an in-depth analysis of the advantages and
disadvantages of existing works to explore possible future directions. You can
find the implementation of our work from https://github.com/thunlp/CLAIM."
Using Artificial Intelligence to Analyze Fashion Trends,"Analyzing fashion trends is essential in the fashion industry. Current
fashion forecasting firms, such as WGSN, utilize the visual information from
around the world to analyze and predict fashion trends. However, analyzing
fashion trends is time-consuming and extremely labor intensive, requiring
individual employees' manual editing and classification. To improve the
efficiency of data analysis of such image-based information and lower the cost
of analyzing fashion images, this study proposes a data-driven quantitative
abstracting approach using an artificial intelligence (A.I.) algorithm.
Specifically, an A.I. model was trained on fashion images from a large-scale
dataset under different scenarios, for example in online stores and street
snapshots. This model was used to detect garments and classify clothing
attributes such as textures, garment style, and details for runway photos and
videos. It was found that the A.I. model can generate rich attribute
descriptions of detected regions and accurately bind the garments in the
images. Adoption of A.I. algorithm demonstrated promising results and the
potential to classify garment types and details automatically, which can make
the process of trend forecasting more cost-effective and faster."
"A multi-component framework for the analysis and design of explainable
  artificial intelligence","The rapid growth of research in explainable artificial intelligence (XAI)
follows on two substantial developments. First, the enormous application
success of modern machine learning methods, especially deep and reinforcement
learning, which have created high expectations for industrial, commercial and
social value. Second, the emergence of concern for creating trusted AI systems,
including the creation of regulatory principles to ensure transparency and
trust of AI systems.These two threads have created a kind of ""perfect storm"" of
research activity, all eager to create and deliver it any set of tools and
techniques to address the XAI demand. As some surveys of current XAI suggest,
there is yet to appear a principled framework that respects the literature of
explainability in the history of science, and which provides a basis for the
development of a framework for transparent XAI. Here we intend to provide a
strategic inventory of XAI requirements, demonstrate their connection to a
history of XAI ideas, and synthesize those ideas into a simple framework to
calibrate five successive levels of XAI."
Regulating Artificial Intelligence: Proposal for a Global Solution,"With increasing ubiquity of artificial intelligence (AI) in modern societies,
individual countries and the international community are working hard to create
an innovation-friendly, yet safe, regulatory environment. Adequate regulation
is key to maximize the benefits and minimize the risks stemming from AI
technologies. Developing regulatory frameworks is, however, challenging due to
AI's global reach and the existence of widespread misconceptions about the
notion of regulation. We argue that AI-related challenges cannot be tackled
effectively without sincere international coordination supported by robust,
consistent domestic and international governance arrangements. Against this
backdrop, we propose the establishment of an international AI governance
framework organized around a new AI regulatory agency that -- drawing on
interdisciplinary expertise -- could help creating uniform standards for the
regulation of AI technologies and inform the development of AI policies around
the world. We also believe that a fundamental change of mindset on what
constitutes regulation is necessary to remove existing barriers that hamper
contemporary efforts to develop AI regulatory regimes, and put forward some
recommendations on how to achieve this, and what opportunities doing so would
present."
"Artificial Intelligence (AI) and IT identity: Antecedents Identifying
  with AI Applications","In the age of Artificial Intelligence and automation, machines have taken
over many key managerial tasks. Replacing managers with AI systems may have a
negative impact on workers outcomes. It is unclear if workers receive the same
benefits from their relationships with AI systems, raising the question: What
degree does the relationship between AI systems and workers impact worker
outcomes? We draw on IT identity to understand the influence of identification
with AI systems on job performance. From this theoretical perspective, we
propose a research model and conduct a survey of 97 MTurk workers to test the
model. The findings reveal that work role identity and organizational identity
are key determinants of identification with AI systems. Furthermore, the
findings show that identification with AI systems does increase job
performance."
Explainable Artificial Intelligence: a Systematic Review,"Explainable Artificial Intelligence (XAI) has experienced a significant
growth over the last few years. This is due to the widespread application of
machine learning, particularly deep learning, that has led to the development
of highly accurate models but lack explainability and interpretability. A
plethora of methods to tackle this problem have been proposed, developed and
tested. This systematic review contributes to the body of knowledge by
clustering these methods with a hierarchical classification system with four
main clusters: review articles, theories and notions, methods and their
evaluation. It also summarises the state-of-the-art in XAI and recommends
future research directions."
Assessing Intelligence in Artificial Neural Networks,"The purpose of this work was to develop of metrics to assess network
architectures that balance neural network size and task performance. To this
end, the concept of neural efficiency is introduced to measure neural layer
utilization, and a second metric called artificial intelligence quotient (aIQ)
was created to balance neural network performance and neural network
efficiency. To study aIQ and neural efficiency, two simple neural networks were
trained on MNIST: a fully connected network (LeNet-300-100) and a convolutional
neural network (LeNet-5). The LeNet-5 network with the highest aIQ was 2.32%
less accurate but contained 30,912 times fewer parameters than the highest
accuracy network. Both batch normalization and dropout layers were found to
increase neural efficiency. Finally, high aIQ networks are shown to be
memorization and overtraining resistant, capable of learning proper digit
classification with an accuracy of 92.51% even when 75% of the class labels are
randomized. These results demonstrate the utility of aIQ and neural efficiency
as metrics for balancing network performance and size."
Principles to Practices for Responsible AI: Closing the Gap,"Companies have considered adoption of various high-level artificial
intelligence (AI) principles for responsible AI, but there is less clarity on
how to implement these principles as organizational practices. This paper
reviews the principles-to-practices gap. We outline five explanations for this
gap ranging from a disciplinary divide to an overabundance of tools. In turn,
we argue that an impact assessment framework which is broad, operationalizable,
flexible, iterative, guided, and participatory is a promising approach to close
the principles-to-practices gap. Finally, to help practitioners with applying
these recommendations, we review a case study of AI's use in forest ecosystem
restoration, demonstrating how an impact assessment framework can translate
into effective and responsible AI practices."
Machine Common Sense,"Machine common sense remains a broad, potentially unbounded problem in
artificial intelligence (AI). There is a wide range of strategies that can be
employed to make progress on this challenge. This article deals with the
aspects of modeling commonsense reasoning focusing on such domain as
interpersonal interactions. The basic idea is that there are several types of
commonsense reasoning: one is manifested at the logical level of physical
actions, the other deals with the understanding of the essence of human-human
interactions. Existing approaches, based on formal logic and artificial neural
networks, allow for modeling only the first type of common sense. To model the
second type, it is vital to understand the motives and rules of human behavior.
This model is based on real-life heuristics, i.e., the rules of thumb,
developed through knowledge and experience of different generations. Such
knowledge base allows for development of an expert system with inference and
explanatory mechanisms (commonsense reasoning algorithms and personal models).
Algorithms provide tools for a situation analysis, while personal models make
it possible to identify personality traits. The system so designed should
perform the function of amplified intelligence for interactions, including
human-machine."
Deep Learning for Abstract Argumentation Semantics,"In this paper, we present a learning-based approach to determining acceptance
of arguments under several abstract argumentation semantics. More specifically,
we propose an argumentation graph neural network (AGNN) that learns a
message-passing algorithm to predict the likelihood of an argument being
accepted. The experimental results demonstrate that the AGNN can almost
perfectly predict the acceptability under different semantics and scales well
for larger argumentation frameworks. Furthermore, analysing the behaviour of
the message-passing algorithm shows that the AGNN learns to adhere to basic
principles of argument semantics as identified in the literature, and can thus
be trained to predict extensions under the different semantics - we show how
the latter can be done for multi-extension semantics by using AGNNs to guide a
basic search. We publish our code at
https://github.com/DennisCraandijk/DL-Abstract-Argumentation"
Towards a new Social Choice Theory,"Social choice is the theory about collective decision towards social welfare
starting from individual opinions, preferences, interests or welfare. The field
of Computational Social Welfare is somewhat recent and it is gaining impact in
the Artificial Intelligence Community. Classical literature makes the
assumption of single-peaked preferences, i.e. there exist a order in the
preferences and there is a global maximum in this order. This year some
theoretical results were published about Two-stage Approval Voting Systems
(TAVs), Multi-winner Selection Rules (MWSR) and Incomplete (IPs) and Circular
Preferences (CPs). The purpose of this paper is three-fold: Firstly, I want to
introduced Social Choice Optimisation as a generalisation of TAVs where there
is a max stage and a min stage implementing thus a Minimax, well-known
Artificial Intelligence decision-making rule to minimize hindering towards a
(Social) Goal. Secondly, I want to introduce, following my Open Standardization
and Open Integration Theory (in refinement process) put in practice in my
dissertation, the Open Standardization of Social Inclusion, as a global social
goal of Social Choice Optimization."
"Artificial Intelligence in Music and Performance: A Subjective
  Art-Research Inquiry","This article presents a five-year collaboration situated at the intersection
of Art practice and Scientific research in Human-Computer Interaction (HCI). At
the core of our collaborative work is a hybrid, Art and Science methodology
that combines computational learning technology -- Machine Learning (ML) and
Artificial Intelligence (AI) -- with interactive music performance and
choreography. This article first exposes our thoughts on combining art,
science, movement and sound research. We then describe two of our artistic
works \textit{Corpus Nil} and \textit{Humane Methods} -- created five years
apart from each other -- that crystallize our collaborative research process.
We present the scientific and artistic motivations, framed through our research
interests and cultural environment of the time. We conclude by reflecting on
the methodology we developed during the collaboration and on the conceptual
shift of computational learning technologies, from ML to AI, and its impact on
Music performance."
"Distributed Linguistic Representations in Decision Making: Taxonomy, Key
  Elements and Applications, and Challenges in Data Science and Explainable
  Artificial Intelligence","Distributed linguistic representations are powerful tools for modelling the
uncertainty and complexity of preference information in linguistic decision
making. To provide a comprehensive perspective on the development of
distributed linguistic representations in decision making, we present the
taxonomy of existing distributed linguistic representations. Then, we review
the key elements of distributed linguistic information processing in decision
making, including the distance measurement, aggregation methods, distributed
linguistic preference relations, and distributed linguistic multiple attribute
decision making models. Next, we provide a discussion on ongoing challenges and
future research directions from the perspective of data science and explainable
artificial intelligence."
"A critical analysis of metrics used for measuring progress in artificial
  intelligence","Comparing model performances on benchmark datasets is an integral part of
measuring and driving progress in artificial intelligence. A model's
performance on a benchmark dataset is commonly assessed based on a single or a
small set of performance metrics. While this enables quick comparisons, it may
entail the risk of inadequately reflecting model performance if the metric does
not sufficiently cover all performance characteristics. It is unknown to what
extent this might impact benchmarking efforts.
  To address this question, we analysed the current landscape of performance
metrics based on data covering 3867 machine learning model performance results
from the open repository 'Papers with Code'. Our results suggest that the large
majority of metrics currently used have properties that may result in an
inadequate reflection of a models' performance. While alternative metrics that
address problematic properties have been proposed, they are currently rarely
used.
  Furthermore, we describe ambiguities in reported metrics, which may lead to
difficulties in interpreting and comparing model performances."
"Explainable Artificial Intelligence Based Fault Diagnosis and Insight
  Harvesting for Steel Plates Manufacturing","With the advent of Industry 4.0, Data Science and Explainable Artificial
Intelligence (XAI) has received considerable intrest in recent literature.
However, the entry threshold into XAI, in terms of computer coding and the
requisite mathematical apparatus, is really high. For fault diagnosis of steel
plates, this work reports on a methodology of incorporating XAI based insights
into the Data Science process of development of high precision classifier.
Using Synthetic Minority Oversampling Technique (SMOTE) and notion of medoids,
insights from XAI tools viz. Ceteris Peribus profiles, Partial Dependence and
Breakdown profiles have been harvested. Additionally, insights in the form of
IF-THEN rules have also been extracted from an optimized Random Forest and
Association Rule Mining. Incorporating all the insights into a single ensemble
classifier, a 10 fold cross validated performance of 94% has been achieved. In
sum total, this work makes three main contributions viz.: methodology based
upon utilization of medoids and SMOTE, of gleaning insights and incorporating
into model development process. Secondly the insights themselves are
contribution, as they benefit the human experts of steel manufacturing
industry, and thirdly a high precision fault diagnosis classifier has been
developed."
"Turing Test and the Practice of Law: The Role of Autonomous Levels of AI
  Legal Reasoning","Artificial Intelligence (AI) is increasingly being applied to law and a
myriad of legal tasks amid attempts to bolster AI Legal Reasoning (AILR)
autonomous capabilities. A major question that has generally been unaddressed
involves how we will know when AILR has achieved autonomous capacities. The
field of AI has grappled with similar quandaries over how to assess the
attainment of Artificial General Intelligence (AGI), a persistently discussed
issue among scholars since the inception of AI, with the Turing Test communally
being considered as the bellwether for ascertaining such matters. This paper
proposes a variant of the Turing Test that is customized for specific use in
the AILR realm, including depicting how this famous gold standard of AI
fulfillment can be robustly applied across the autonomous levels of AI Legal
Reasoning."
"Teaching Tech to Talk: K-12 Conversational Artificial Intelligence
  Literacy Curriculum and Development Tools","With children talking to smart-speakers, smart-phones and even
smart-microwaves daily, it is increasingly important to educate students on how
these agents work-from underlying mechanisms to societal implications.
Researchers are developing tools and curriculum to teach K-12 students broadly
about artificial intelligence (AI); however, few studies have evaluated these
tools with respect to AI-specific learning outcomes, and even fewer have
addressed student learning about AI-based conversational agents. We evaluate
our Conversational Agent Interface for MIT App Inventor and workshop curriculum
with respect to eight AI competencies from the literature. Furthermore, we
analyze teacher (n=9) and student (n=47) feedback from workshops with the
interface and recommend that future work leverages design considerations from
the literature to optimize engagement, collaborates with teachers, and
addresses a range of student abilities through pacing and opportunities for
extension. We found students struggled most with the concepts of AI ethics and
learning, and recommend emphasizing these topics when teaching.
  The appendix, including a demo video, can be found here:
https://gist.github.com/jessvb/1cd959e32415a6ad4389761c49b54bbf"
"Optimal Sepsis Patient Treatment using Human-in-the-loop Artificial
  Intelligence","Sepsis is one of the leading causes of death in Intensive Care Units (ICU).
The strategy for treating sepsis involves the infusion of intravenous (IV)
fluids and administration of antibiotics. Determining the optimal quantity of
IV fluids is a challenging problem due to the complexity of a patient's
physiology. In this study, we develop a data-driven optimization solution that
derives the optimal quantity of IV fluids for individual patients. The proposed
method minimizes the probability of severe outcomes by controlling the
prescribed quantity of IV fluids and utilizes human-in-the-loop artificial
intelligence. We demonstrate the performance of our model on 1122 ICU patients
with sepsis diagnosis extracted from the MIMIC-III dataset. The results show
that, on average, our model can reduce mortality by 22%. This study has the
potential to help physicians synthesize optimal, patient-specific treatment
strategies."
"Constraint Programming Algorithms for Route Planning Exploiting
  Geometrical Information","Problems affecting the transport of people or goods are plentiful in industry
and commerce and they also appear to be at the origin of much more complex
problems. In recent years, the logistics and transport sector keeps growing
supported by technological progress, i.e. companies to be competitive are
resorting to innovative technologies aimed at efficiency and effectiveness.
This is why companies are increasingly using technologies such as Artificial
Intelligence (AI), Blockchain and Internet of Things (IoT). Artificial
intelligence, in particular, is often used to solve optimization problems in
order to provide users with the most efficient ways to exploit available
resources. In this work we present an overview of our current research
activities concerning the development of new algorithms, based on CLP
techniques, for route planning problems exploiting the geometric information
intrinsically present in many of them or in some of their variants. The
research so far has focused in particular on the Euclidean Traveling
Salesperson Problem (Euclidean TSP) with the aim to exploit the results
obtained also to other problems of the same category, such as the Euclidean
Vehicle Routing Problem (Euclidean VRP), in the future."
Variant-based Equational Unification under Constructor Symbols,"Equational unification of two terms consists of finding a substitution that,
when applied to both terms, makes them equal modulo some equational properties.
A narrowing-based equational unification algorithm relying on the concept of
the variants of a term is available in the most recent version of Maude,
version 3.0, which provides quite sophisticated unification features. A variant
of a term t is a pair consisting of a substitution sigma and the canonical form
of tsigma. Variant-based unification is decidable when the equational theory
satisfies the finite variant property. However, this unification procedure does
not take into account constructor symbols and, thus, may compute many more
unifiers than the necessary or may not be able to stop immediately. In this
paper, we integrate the notion of constructor symbol into the variant-based
unification algorithm. Our experiments on positive and negative unification
problems show an impressive speedup."
Artificial Intelligence in Surgery: Neural Networks and Deep Learning,"Deep neural networks power most recent successes of artificial intelligence,
spanning from self-driving cars to computer aided diagnosis in radiology and
pathology. The high-stake data intensive process of surgery could highly
benefit from such computational methods. However, surgeons and computer
scientists should partner to develop and assess deep learning applications of
value to patients and healthcare systems. This chapter and the accompanying
hands-on material were designed for surgeons willing to understand the
intuitions behind neural networks, become familiar with deep learning concepts
and tasks, grasp what implementing a deep learning model in surgery means, and
finally appreciate the specific challenges and limitations of deep neural
networks in surgery. For the associated hands-on material, please see
https://github.com/CAMMA-public/ai4surgery."
Explanation Ontology in Action: A Clinical Use-Case,"We addressed the problem of a lack of semantic representation for
user-centric explanations and different explanation types in our Explanation
Ontology (https://purl.org/heals/eo). Such a representation is increasingly
necessary as explainability has become an important problem in Artificial
Intelligence with the emergence of complex methods and an uptake in
high-precision and user-facing settings. In this submission, we provide
step-by-step guidance for system designers to utilize our ontology, introduced
in our resource track paper, to plan and model for explanations during the
design of their Artificial Intelligence systems. We also provide a detailed
example with our utilization of this guidance in a clinical setting."
Exploring Semantic Capacity of Terms,"We introduce and study semantic capacity of terms. For example, the semantic
capacity of artificial intelligence is higher than that of linear regression
since artificial intelligence possesses a broader meaning scope. Understanding
semantic capacity of terms will help many downstream tasks in natural language
processing. For this purpose, we propose a two-step model to investigate
semantic capacity of terms, which takes a large text corpus as input and can
evaluate semantic capacity of terms if the text corpus can provide enough
co-occurrence information of terms. Extensive experiments in three fields
demonstrate the effectiveness and rationality of our model compared with
well-designed baselines and human-level evaluations."
"Deep RL With Information Constrained Policies: Generalization in
  Continuous Control","Biological agents learn and act intelligently in spite of a highly limited
capacity to process and store information. Many real-world problems involve
continuous control, which represents a difficult task for artificial
intelligence agents. In this paper we explore the potential learning advantages
a natural constraint on information flow might confer onto artificial agents in
continuous control tasks. We focus on the model-free reinforcement learning
(RL) setting and formalize our approach in terms of an information-theoretic
constraint on the complexity of learned policies. We show that our approach
emerges in a principled fashion from the application of rate-distortion theory.
We implement a novel Capacity-Limited Actor-Critic (CLAC) algorithm and situate
it within a broader family of RL algorithms such as the Soft Actor Critic (SAC)
and Mutual Information Reinforcement Learning (MIRL) algorithm. Our experiments
using continuous control tasks show that compared to alternative approaches,
CLAC offers improvements in generalization between training and modified test
environments. This is achieved in the CLAC model while displaying the high
sample efficiency of similar methods."
Helpfulness as a Key Metric of Human-Robot Collaboration,"As robotic teammates become more common in society, people will assess the
robots' roles in their interactions along many dimensions. One such dimension
is effectiveness: people will ask whether their robotic partners are
trustworthy and effective collaborators. This begs a crucial question: how can
we quantitatively measure the helpfulness of a robotic partner for a given task
at hand? This paper seeks to answer this question with regards to the
interactive robot's decision making. We describe a clear, concise, and
task-oriented metric applicable to many different planning and execution
paradigms. The proposed helpfulness metric is fundamental to assessing the
benefit that a partner has on a team for a given task. In this paper, we define
helpfulness, illustrate it on concrete examples from a variety of domains,
discuss its properties and ramifications for planning interactions with humans,
and present preliminary results."
"Axiom Learning and Belief Tracing for Transparent Decision Making in
  Robotics","A robot's ability to provide descriptions of its decisions and beliefs
promotes effective collaboration with humans. Providing such transparency is
particularly challenging in integrated robot systems that include
knowledge-based reasoning methods and data-driven learning algorithms. Towards
addressing this challenge, our architecture couples the complementary strengths
of non-monotonic logical reasoning, deep learning, and decision-tree induction.
During reasoning and learning, the architecture enables a robot to provide
on-demand relational descriptions of its decisions, beliefs, and the outcomes
of hypothetical actions. These capabilities are grounded and evaluated in the
context of scene understanding tasks and planning tasks performed using
simulated images and images from a physical robot manipulating tabletop
objects."
A Weaker Faithfulness Assumption based on Triple Interactions,"One of the core assumptions in causal discovery is the faithfulness
assumption, i.e., assuming that independencies found in the data are due to
separations in the true causal graph. This assumption can, however, be violated
in many ways, including xor connections, deterministic functions or cancelling
paths. In this work, we propose a weaker assumption that we call $2$-adjacency
faithfulness. In contrast to adjacency faithfulness, which assumes that there
is no conditional independence between each pair of variables that are
connected in the causal graph, we only require no conditional independence
between a node and a subset of its Markov blanket that can contain up to two
nodes. Equivalently, we adapt orientation faithfulness to this setting. We
further propose a sound orientation rule for causal discovery that applies
under weaker assumptions. As a proof of concept, we derive a modified Grow and
Shrink algorithm that recovers the Markov blanket of a target node and prove
its correctness under strictly weaker assumptions than the standard
faithfulness assumption."
"An Experimentation Platform for Explainable Coalition Situational
  Understanding","We present an experimentation platform for coalition situational
understanding research that highlights capabilities in explainable artificial
intelligence/machine learning (AI/ML) and integration of symbolic and
subsymbolic AI/ML approaches for event processing. The Situational
Understanding Explorer (SUE) platform is designed to be lightweight, to easily
facilitate experiments and demonstrations, and open. We discuss our
requirements to support coalition multi-domain operations with emphasis on
asset interoperability and ad hoc human-machine teaming in a dense urban
terrain setting. We describe the interface functionality and give examples of
SUE applied to coalition situational understanding tasks."
Artificial Intelligence Systems applied to tourism: A Survey,"Artificial Intelligence (AI) has been improving the performance of systems
for a diverse set of tasks and introduced a more interactive generation of
personal agents. Despite the current trend of applying AI for a great amount of
areas, we have not seen the same quantity of work being developed for the
tourism sector. This paper reports on the main applications of AI systems
developed for tourism and the current state of the art for this sector. The
paper also provides an up-to-date survey of this field regarding several key
works and systems that are applied to tourism, like Personal Agents, for
providing a more interactive experience. We also carried out an in-depth
research on systems for predicting traffic human flow, more accurate
recommendation systems and even how geospatial is trying to display tourism
data in a more informative way and prevent problems before they arise."
Exploring the Nuances of Designing (with/for) Artificial Intelligence,"Solutions relying on artificial intelligence are devised to predict data
patterns and answer questions that are clearly defined, involve an enumerable
set of solutions, clear rules, and inherently binary decision mechanisms. Yet,
as they become exponentially implemented in our daily activities, they begin to
transcend these initial boundaries and to affect the larger sociotechnical
system in which they are situated. In this arrangement, a solution is under
pressure to surpass true or false criteria and move to an ethical evaluation of
right and wrong. Neither algorithmic solutions, nor purely humanistic ones will
be enough to fully mitigate undesirable outcomes in the narrow state of AI or
its future incarnations. We must take a holistic view. In this paper we explore
the construct of infrastructure as a means to simultaneously address
algorithmic and societal issues when designing AI."
"Artificial Intelligence and its impact on the Fourth Industrial
  Revolution: A Review","Artificial Intelligence may revolutionize everything during the so-called
fourth industrial revolution, which carries several emerging technologies and
could progress without precedents in human history due to its speed and scope.
Government, academia, industry, and civil society show interest in
understanding the multidimensional impact of the emerging industrial
revolution; however, its development is hard to predict. Experts consider
emerging technologies could bring tremendous benefits to humanity; at the same
time, they could pose an existential risk. This paper reviews the development
and trends in AI, as well as the benefits, risks, and strategies in the field.
During the course of the emerging industrial revolution, the common good may be
achieved in a collaborative environment of shared interests and the hardest
work will be the implementation and monitoring of projects at a global scale."
Explainable AI meets Healthcare: A Study on Heart Disease Dataset,"With the increasing availability of structured and unstructured data and the
swift progress of analytical techniques, Artificial Intelligence (AI) is
bringing a revolution to the healthcare industry. With the increasingly
indispensable role of AI in healthcare, there are growing concerns over the
lack of transparency and explainability in addition to potential bias
encountered by predictions of the model. This is where Explainable Artificial
Intelligence (XAI) comes into the picture. XAI increases the trust placed in an
AI system by medical practitioners as well as AI researchers, and thus,
eventually, leads to an increasingly widespread deployment of AI in healthcare.
  In this paper, we present different interpretability techniques. The aim is
to enlighten practitioners on the understandability and interpretability of
explainable AI systems using a variety of techniques available which can be
very advantageous in the health-care domain. Medical diagnosis model is
responsible for human life and we need to be confident enough to treat a
patient as instructed by a black-box model. Our paper contains examples based
on the heart disease dataset and elucidates on how the explainability
techniques should be preferred to create trustworthiness while using AI systems
in healthcare."
European Strategy on AI: Are we truly fostering social good?,"Artificial intelligence (AI) is already part of our daily lives and is
playing a key role in defining the economic and social shape of the future. In
2018, the European Commission introduced its AI strategy able to compete in the
next years with world powers such as China and US, but relying on the respect
of European values and fundamental rights. As a result, most of the Member
States have published their own National Strategy with the aim to work on a
coordinated plan for Europe. In this paper, we present an ongoing study on how
European countries are approaching the field of Artificial Intelligence, with
its promises and risks, through the lens of their national AI strategies. In
particular, we aim to investigate how European countries are investing in AI
and to what extent the stated plans can contribute to the benefit of the whole
society. This paper reports the main findings of a qualitative analysis of the
investment plans reported in 15 European National Strategies"
Relation Clustering in Narrative Knowledge Graphs,"When coping with literary texts such as novels or short stories, the
extraction of structured information in the form of a knowledge graph might be
hindered by the huge number of possible relations between the entities
corresponding to the characters in the novel and the consequent hurdles in
gathering supervised information about them. Such issue is addressed here as an
unsupervised task empowered by transformers: relational sentences in the
original text are embedded (with SBERT) and clustered in order to merge
together semantically similar relations. All the sentences in the same cluster
are finally summarized (with BART) and a descriptive label extracted from the
summary. Preliminary tests show that such clustering might successfully detect
similar relations, and provide a valuable preprocessing for semi-supervised
approaches."
"Robust Ultra-wideband Range Error Mitigation with Deep Learning at the
  Edge","Ultra-wideband (UWB) is the state-of-the-art and most popular technology for
wireless localization. Nevertheless, precise ranging and localization in
non-line-of-sight (NLoS) conditions is still an open research topic. Indeed,
multipath effects, reflections, refractions, and complexity of the indoor radio
environment can easily introduce a positive bias in the ranging measurement,
resulting in highly inaccurate and unsatisfactory position estimation. This
article proposes an efficient representation learning methodology that exploits
the latest advancement in deep learning and graph optimization techniques to
achieve effective ranging error mitigation at the edge. Channel Impulse
Response (CIR) signals are directly exploited to extract high semantic features
to estimate corrections in either NLoS or LoS conditions. Extensive
experimentation with different settings and configurations has proved the
effectiveness of our methodology and demonstrated the feasibility of a robust
and low computational power UWB range error mitigation."
"Artificial intelligence techniques for integrative structural biology of
  intrinsically disordered proteins","We outline recent developments in artificial intelligence (AI) and machine
learning (ML) techniques for integrative structural biology of intrinsically
disordered proteins (IDP) ensembles. IDPs challenge the traditional protein
structure-function paradigm by adapting their conformations in response to
specific binding partners leading them to mediate diverse, and often complex
cellular functions such as biological signaling, self organization and
compartmentalization. Obtaining mechanistic insights into their function can
therefore be challenging for traditional structural determination techniques.
Often, scientists have to rely on piecemeal evidence drawn from diverse
experimental techniques to characterize their functional mechanisms. Multiscale
simulations can help bridge critical knowledge gaps about IDP structure
function relationships - however, these techniques also face challenges in
resolving emergent phenomena within IDP conformational ensembles. We posit that
scalable statistical inference techniques can effectively integrate information
gleaned from multiple experimental techniques as well as from simulations, thus
providing access to atomistic details of these emergent phenomena."
Cognitive Capabilities for the CAAI in Cyber-Physical Production Systems,"This paper presents the cognitive module of the cognitive architecture for
artificial intelligence (CAAI) in cyber-physical production systems (CPPS). The
goal of this architecture is to reduce the implementation effort of artificial
intelligence (AI) algorithms in CPPS. Declarative user goals and the provided
algorithm-knowledge base allow the dynamic pipeline orchestration and
configuration. A big data platform (BDP) instantiates the pipelines and
monitors the CPPS performance for further evaluation through the cognitive
module. Thus, the cognitive module is able to select feasible and robust
configurations for process pipelines in varying use cases. Furthermore, it
automatically adapts the models and algorithms based on model quality and
resource consumption. The cognitive module also instantiates additional
pipelines to test algorithms from different classes. CAAI relies on
well-defined interfaces to enable the integration of additional modules and
reduce implementation effort. Finally, an implementation based on Docker,
Kubernetes, and Kafka for the virtualization and orchestration of the
individual modules and as messaging-technology for module communication is used
to evaluate a real-world use case."
"A Knowledge Driven Approach to Adaptive Assistance Using Preference
  Reasoning and Explanation","There is a need for socially assistive robots (SARs) to provide transparency
in their behavior by explaining their reasoning. Additionally, the reasoning
and explanation should represent the user's preferences and goals. To work
towards satisfying this need for interpretable reasoning and representations,
we propose the robot uses Analogical Theory of Mind to infer what the user is
trying to do and uses the Hint Engine to find an appropriate assistance based
on what the user is trying to do. If the user is unsure or confused, the robot
provides the user with an explanation, generated by the Explanation
Synthesizer. The explanation helps the user understand what the robot inferred
about the user's preferences and why the robot decided to provide the
assistance it gave. A knowledge-driven approach provides transparency to
reasoning about preferences, assistance, and explanations, thereby facilitating
the incorporation of user feedback and allowing the robot to learn and adapt to
the user."
Robustness of Model Predictions under Extension,"Mathematical models of the real world are simplified representations of
complex systems. A caveat to using mathematical models is that predicted causal
effects and conditional independences may not be robust under model extensions,
limiting applicability of such models. In this work, we consider conditions
under which qualitative model predictions are preserved when two models are
combined. Under mild assumptions, we show how to use the technique of causal
ordering to efficiently assess the robustness of qualitative model predictions.
We also characterize a large class of model extensions that preserve
qualitative model predictions. For dynamical systems at equilibrium, we
demonstrate how novel insights help to select appropriate model extensions and
to reason about the presence of feedback loops. We illustrate our ideas with a
viral infection model with immune responses."
GAN Ensemble for Anomaly Detection,"When formulated as an unsupervised learning problem, anomaly detection often
requires a model to learn the distribution of normal data. Previous works apply
Generative Adversarial Networks (GANs) to anomaly detection tasks and show good
performances from these models. Motivated by the observation that GAN ensembles
often outperform single GANs in generation tasks, we propose to construct GAN
ensembles for anomaly detection. In the proposed method, a group of generators
and a group of discriminators are trained together, so every generator gets
feedback from multiple discriminators, and vice versa. Compared to a single
GAN, a GAN ensemble can better model the distribution of normal data and thus
better detect anomalies. Our theoretical analysis of GANs and GAN ensembles
explains the role of a GAN discriminator in anomaly detection. In the empirical
study, we evaluate ensembles constructed from four types of base models, and
the results show that these ensembles clearly outperform single models in a
series of tasks of anomaly detection."
ColorShapeLinks: A board game AI competition for educators and students,"ColorShapeLinks is an AI board game competition framework specially designed
for students and educators in videogame development, with openness and
accessibility in mind. The competition is based on an arbitrarily-sized version
of the Simplexity board game, the motto of which, ""simple to learn, complex to
master"", is curiously also applicable to AI agents. ColorShapeLinks offers
graphical and text-based frontends and a completely open and documented
development framework built using industry standard tools and following
software engineering best practices. ColorShapeLinks is not only a competition,
but both a game and a framework which educators and students can extend and use
to host their own competitions. It has been successfully used for running
internal competitions in AI classes, as well as for hosting an international AI
competition at the IEEE Conference on Games."
A Systematic Mapping Study in AIOps,"IT systems of today are becoming larger and more complex, rendering their
human supervision more difficult. Artificial Intelligence for IT Operations
(AIOps) has been proposed to tackle modern IT administration challenges thanks
to AI and Big Data. However, past AIOps contributions are scattered,
unorganized and missing a common terminology convention, which renders their
discovery and comparison impractical. In this work, we conduct an in-depth
mapping study to collect and organize the numerous scattered contributions to
AIOps in a unique reference index. We create an AIOps taxonomy to build a
foundation for future contributions and allow an efficient comparison of AIOps
papers treating similar problems. We investigate temporal trends and classify
AIOps contributions based on the choice of algorithms, data sources and the
target components. Our results show a recent and growing interest towards
AIOps, specifically to those contributions treating failure-related tasks
(62%), such as anomaly detection and root cause analysis."
"Infrastructure for Artificial Intelligence, Quantum and High Performance
  Computing","High Performance Computing (HPC), Artificial Intelligence (AI)/Machine
Learning (ML), and Quantum Computing (QC) and communications offer immense
opportunities for innovation and impact on society. Researchers in these areas
depend on access to computing infrastructure, but these resources are in short
supply and are typically siloed in support of their research communities,
making it more difficult to pursue convergent and interdisciplinary research.
Such research increasingly depends on complex workflows that require different
resources for each stage. This paper argues that a more-holistic approach to
computing infrastructure, one that recognizes both the convergence of some
capabilities and the complementary capabilities from new computing approaches,
be it commercial cloud to Quantum Computing, is needed to support computer
science research."
"A Fast Edge-Based Synchronizer for Tasks in Real-Time Artificial
  Intelligence Applications","Real-time artificial intelligence (AI) applications mapped onto edge
computing need to perform data capture, process data, and device actuation
within given bounds while using the available devices. Task synchronization
across the devices is an important problem that affects the timely progress of
an AI application by determining the quality of the captured data, time to
process the data, and the quality of actuation. In this paper, we develop a
fast edge-based synchronization scheme that can time align the execution of
input-output tasks as well compute tasks. The primary idea of the fast
synchronizer is to cluster the devices into groups that are highly synchronized
in their task executions and statically determine few synchronization points
using a game-theoretic solver. The cluster of devices use a late notification
protocol to select the best point among the pre-computed synchronization points
to reach a time aligned task execution as quickly as possible. We evaluate the
performance of our synchronization scheme using trace-driven simulations and we
compare the performance with existing distributed synchronization schemes for
real-time AI application tasks. We implement our synchronization scheme and
compare its training accuracy and training time with other parameter server
synchronization frameworks."
The Last State of Artificial Intelligence in Project Management,"Artificial intelligence (AI) has been used to advance different fields, such
as education, healthcare, and finance. However, the application of AI in the
field of project management (PM) has not progressed equally. This paper reports
on a systematic review of the published studies used to investigate the
application of AI in PM. This systematic review identified relevant papers
using Web of Science, Science Direct, and Google Scholar databases. Of the 652
articles found, 58 met the predefined criteria and were included in the review.
Included papers were classified per the following dimensions: PM knowledge
areas, PM processes, and AI techniques. The results indicated that the
application of AI in PM was in its early stages and AI models have not applied
for multiple PM processes especially in processes groups of project stakeholder
management, project procurements management, and project communication
management. However, the most popular PM processes among included papers were
project effort prediction and cost estimation, and the most popular AI
techniques were support vector machines, neural networks, and genetic
algorithms."
"Antitrust and Artificial Intelligence (AAI): Antitrust Vigilance
  Lifecycle and AI Legal Reasoning Autonomy","There is an increasing interest in the entwining of the field of antitrust
with the field of Artificial Intelligence (AI), frequently referred to jointly
as Antitrust and AI (AAI) in the research literature. This study focuses on the
synergies entangling antitrust and AI, doing so to extend the literature by
proffering the primary ways that these two fields intersect, consisting of: (1)
the application of antitrust to AI, and (2) the application of AI to antitrust.
To date, most of the existing research on this intermixing has concentrated on
the former, namely the application of antitrust to AI, entailing how the
marketplace will be altered by the advent of AI and the potential for adverse
antitrust behaviors arising accordingly. Opting to explore more deeply the
other side of this coin, this research closely examines the application of AI
to antitrust and establishes an antitrust vigilance lifecycle to which AI is
predicted to be substantively infused for purposes of enabling and bolstering
antitrust detection, enforcement, and post-enforcement monitoring. Furthermore,
a gradual and incremental injection of AI into antitrust vigilance is
anticipated to occur as significant advances emerge amidst the Levels of
Autonomy (LoA) for AI Legal Reasoning (AILR)."
"Understanding Team Collaboration in Artificial Intelligence from the
  perspective of Geographic Distance","This paper analyzes team collaboration in the field of Artificial
Intelligence (AI) from the perspective of geographic distance. We obtained
1,584,175 AI related publications during 1950-2019 from the Microsoft Academic
Graph. Three latitude-and-longitude-based indicators were employed to quantify
the geographic distance of collaborations in AI over time at domestic and
international levels. The results show team collaborations in AI has been more
popular in the field over time with around 42,000 (38.4%) multiple-affiliation
AI publications in 2019. The changes in geographic distances of team
collaborations indicate the increase of breadth and density for both domestic
and international collaborations in AI over time. In addition, the United
States produced the largest number of single-country and internationally
collaborated AI publications, and China has played an important role in
international collaborations in AI after 2010."
Artificial Intelligence Methods in In-Cabin Use Cases: A Survey,"As interest in autonomous driving increases, efforts are being made to meet
requirements for the high-level automation of vehicles. In this context, the
functionality inside the vehicle cabin plays a key role in ensuring a safe and
pleasant journey for driver and passenger alike. At the same time, recent
advances in the field of artificial intelligence (AI) have enabled a whole
range of new applications and assistance systems to solve automated problems in
the vehicle cabin. This paper presents a thorough survey on existing work that
utilizes AI methods for use-cases inside the driving cabin, focusing, in
particular, on application scenarios related to (1) driving safety and (2)
driving comfort. Results from the surveyed works show that AI technology has a
promising future in tackling in-cabin tasks within the autonomous driving
aspect."
"Heatmap-based Object Detection and Tracking with a Fully Convolutional
  Neural Network","The main topic of this paper is a brief overview of the field of Artificial
Intelligence. The core of this paper is a practical implementation of an
algorithm for object detection and tracking. The ability to detect and track
fast-moving objects is crucial for various applications of Artificial
Intelligence like autonomous driving, ball tracking in sports, robotics or
object counting. As part of this paper the Fully Convolutional Neural Network
""CueNet"" was developed. It detects and tracks the cueball on a labyrinth game
robustly and reliably. While CueNet V1 has a single input image, the approach
with CueNet V2 was to take three consecutive 240 x 180-pixel images as an input
and transform them into a probability heatmap for the cueball's location. The
network was tested with a separate video that contained all sorts of
distractions to test its robustness. When confronted with our testing data,
CueNet V1 predicted the correct cueball location in 99.6% of all frames, while
CueNet V2 had 99.8% accuracy."
Understanding in Artificial Intelligence,"Current Artificial Intelligence (AI) methods, most based on deep learning,
have facilitated progress in several fields, including computer vision and
natural language understanding. The progress of these AI methods is measured
using benchmarks designed to solve challenging tasks, such as visual question
answering. A question remains of how much understanding is leveraged by these
methods and how appropriate are the current benchmarks to measure understanding
capabilities. To answer these questions, we have analysed existing benchmarks
and their understanding capabilities, defined by a set of understanding
capabilities, and current research streams. We show how progress has been made
in benchmark development to measure understanding capabilities of AI methods
and we review as well how current methods develop understanding capabilities."
"Show or Suppress? Managing Input Uncertainty in Machine Learning Model
  Explanations","Feature attribution is widely used in interpretable machine learning to
explain how influential each measured input feature value is for an output
inference. However, measurements can be uncertain, and it is unclear how the
awareness of input uncertainty can affect the trust in explanations. We propose
and study two approaches to help users to manage their perception of
uncertainty in a model explanation: 1) transparently show uncertainty in
feature attributions to allow users to reflect on, and 2) suppress attribution
to features with uncertain measurements and shift attribution to other features
by regularizing with an uncertainty penalty. Through simulation experiments,
qualitative interviews, and quantitative user evaluations, we identified the
benefits of moderately suppressing attribution uncertainty, and concerns
regarding showing attribution uncertainty. This work adds to the understanding
of handling and communicating uncertainty for model interpretability."
"Toxicity Detection in Drug Candidates using Simplified Molecular-Input
  Line-Entry System","The need for analysis of toxicity in new drug candidates and the requirement
of doing it fast have asked the consideration of scientists towards the use of
artificial intelligence tools to examine toxicity levels and to develop models
to a degree where they can be used commercially to measure toxicity levels
efficiently in upcoming drugs. Artificial Intelligence based models can be used
to predict the toxic nature of a chemical using Quantitative Structure Activity
Relationship techniques. Convolutional Neural Network models have demonstrated
great outcomes in predicting the qualitative analysis of chemicals in order to
determine the toxicity. This paper goes for the study of Simplified Molecular
Input Line-Entry System (SMILES) as a parameter to develop Long short term
memory (LSTM) based models in order to examine the toxicity of a molecule and
the degree to which the need can be fulfilled for practical use alongside its
future outlooks for the purpose of real world applications."
Artificial Intelligence for Satellite Communication: A Review,"Satellite communication offers the prospect of service continuity over
uncovered and under-covered areas, service ubiquity, and service scalability.
However, several challenges must first be addressed to realize these benefits,
as the resource management, network control, network security, spectrum
management, and energy usage of satellite networks are more challenging than
that of terrestrial networks. Meanwhile, artificial intelligence (AI),
including machine learning, deep learning, and reinforcement learning, has been
steadily growing as a research field and has shown successful results in
diverse applications, including wireless communication. In particular, the
application of AI to a wide variety of satellite communication aspects have
demonstrated excellent potential, including beam-hopping, anti-jamming, network
traffic forecasting, channel modeling, telemetry mining, ionospheric
scintillation detecting, interference managing, remote sensing, behavior
modeling, space-air-ground integrating, and energy managing. This work thus
provides a general overview of AI, its diverse sub-fields, and its
state-of-the-art algorithms. Several challenges facing diverse aspects of
satellite communication systems are then discussed, and their proposed and
potential AI-based solutions are presented. Finally, an outlook of field is
drawn, and future steps are suggested."
"Evolution of artificial intelligence languages, a systematic literature
  review","The field of Artificial Intelligence (AI) has undoubtedly received
significant attention in recent years. AI is being adopted to provide solutions
to problems in fields such as medicine, engineering, education, government and
several other domains. In order to analyze the state of the art of research in
the field of AI, we present a systematic literature review focusing on the
Evolution of AI programming languages. We followed the systematic literature
review method by searching relevant databases like SCOPUS, IEEE Xplore and
Google Scholar. EndNote reference manager was used to catalog the relevant
extracted papers. Our search returned a total of 6565 documents, whereof 69
studies were retained. Of the 69 retained studies, 15 documents discussed LISP
programming language, another 34 discussed PROLOG programming language, the
remaining 20 documents were spread between Logic and Object Oriented
Programming (LOOP), ARCHLOG, Epistemic Ontology Language with Constraints
(EOLC), Python, C++, ADA and JAVA programming languages. This review provides
information on the year of implementation, development team, capabilities,
limitations and applications of each of the AI programming languages discussed.
The information in this review could guide practitioners and researchers in AI
to make the right choice of languages to implement their novel AI methods."
A Survey on Personality-Aware Recommendation Systems,"With the emergence of personality computing as a new research field related
to artificial intelligence and personality psychology, we have witnessed an
unprecedented proliferation of personality-aware recommendation systems. Unlike
conventional recommendation systems, these new systems solve traditional
problems such as the cold start and data sparsity problems. This survey aims to
study and systematically classify personality-aware recommendation systems. To
the best of our knowledge, this survey is the first that focuses on
personality-aware recommendation systems. We explore the different design
choices of personality-aware recommendation systems, by comparing their
personality modeling methods, as well as their recommendation techniques.
Furthermore, we present the commonly used datasets and point out some of the
challenges of personality-aware recommendation systems."
Applications of Artificial Intelligence in Particle Radiotherapy,"Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice."
What we are is more than what we do,"If we take the subjective character of consciousness seriously, consciousness
becomes a matter of ""being"" rather than ""doing"". Because ""doing"" can be
dissociated from ""being"", functional criteria alone are insufficient to decide
whether a system possesses the necessary requirements for being a physical
substrate of consciousness. The dissociation between ""being"" and ""doing"" is
most salient in artificial general intelligence, which may soon replicate any
human capacity: computers can perform complex functions (in the limit
resembling human behavior) in the absence of consciousness. Complex behavior
becomes meaningless if it is not performed by a conscious being."
Principles of Explanation in Human-AI Systems,"Explainable Artificial Intelligence (XAI) has re-emerged in response to the
development of modern AI and ML systems. These systems are complex and
sometimes biased, but they nevertheless make decisions that impact our lives.
XAI systems are frequently algorithm-focused; starting and ending with an
algorithm that implements a basic untested idea about explainability. These
systems are often not tested to determine whether the algorithm helps users
accomplish any goals, and so their explainability remains unproven. We propose
an alternative: to start with human-focused principles for the design, testing,
and implementation of XAI systems, and implement algorithms to serve that
purpose. In this paper, we review some of the basic concepts that have been
used for user-centered XAI systems over the past 40 years of research. Based on
these, we describe the ""Self-Explanation Scorecard"", which can help developers
understand how they can empower users by enabling self-explanation. Finally, we
present a set of empirically-grounded, user-centered design principles that may
guide developers to create successful explainable systems."
"Artificial intelligence in communication impacts language and social
  relationships","Artificial intelligence (AI) is now widely used to facilitate social
interaction, but its impact on social relationships and communication is not
well understood. We study the social consequences of one of the most pervasive
AI applications: algorithmic response suggestions (""smart replies""). Two
randomized experiments (n = 1036) provide evidence that a commercially-deployed
AI changes how people interact with and perceive one another in pro-social and
anti-social ways. We find that using algorithmic responses increases
communication efficiency, use of positive emotional language, and positive
evaluations by communication partners. However, consistent with common
assumptions about the negative implications of AI, people are evaluated more
negatively if they are suspected to be using algorithmic responses. Thus, even
though AI can increase communication efficiency and improve interpersonal
perceptions, it risks changing users' language production and continues to be
viewed negatively."
"Artificial Intelligence based Autonomous Molecular Design for Medical
  Therapeutic: A Perspective","Domain-aware machine learning (ML) models have been increasingly adopted for
accelerating small molecule therapeutic design in the recent years. These
models have been enabled by significant advancement in state-of-the-art
artificial intelligence (AI) and computing infrastructures. Several ML
architectures are pre-dominantly and independently used either for predicting
the properties of small molecules, or for generating lead therapeutic
candidates. Synergetically using these individual components along with robust
representation and data generation techniques autonomously in closed loops
holds enormous promise for accelerated drug design which is a time consuming
and expensive task otherwise. In this perspective, we present the most recent
breakthrough achieved by each of the components, and how such autonomous AI and
ML workflow can be realized to radically accelerate the hit identification and
lead optimization. Taken together, this could significantly shorten the
timeline for end-to-end antiviral discovery and optimization times to weeks
upon the arrival of a novel zoonotic transmission event. Our perspective serves
as a guide for researchers to practice autonomous molecular design in
therapeutic discovery."
Symmetry Breaking for k-Robust Multi-Agent Path Finding,"During Multi-Agent Path Finding (MAPF) problems, agents can be delayed by
unexpected events. To address such situations recent work describes k-Robust
Conflict-BasedSearch (k-CBS): an algorithm that produces coordinated and
collision-free plan that is robust for up to k delays. In this work we
introducing a variety of pairwise symmetry breaking constraints, specific to
k-robust planning, that can efficiently find compatible and optimal paths for
pairs of conflicting agents. We give a thorough description of the new
constraints and report large improvements to success rate ina range of domains
including: (i) classic MAPF benchmarks;(ii) automated warehouse domains and;
(iii) on maps from the 2019 Flatland Challenge, a recently introduced railway
domain where k-robust planning can be fruitfully applied to schedule trains."
"Artificial Intelligence Technologies in Education: Benefits, Challenges
  and Strategies of Implementation","Since the education sector is associated with highly dynamic business
environments which are controlled and maintained by information systems, recent
technological advancements and the increasing pace of adopting artificial
intelligence (AI) technologies constitute a need to identify and analyze the
issues regarding their implementation in education sector. However, a study of
the contemporary literature reveled that relatively little research has been
undertaken in this area. To fill this void, we have identified the benefits and
challenges of implementing artificial intelligence in the education sector,
preceded by a short discussion on the concepts of AI and its evolution over
time. Moreover, we have also reviewed modern AI technologies for learners and
educators, currently available on the software market, evaluating their
usefulness. Last but not least, we have developed a strategy implementation
model, described by a five-stage, generic process, along with the corresponding
configuration guide. To verify and validate their design, we separately
developed three implementation strategies for three different higher education
organizations. We believe that the obtained results will contribute to better
understanding the specificities of AI systems, services and tools, and
afterwards pave a smooth way in their implementation."
"Artificial Intelligence Enhanced Rapid and Efficient Diagnosis of
  Mycoplasma Pneumoniae Pneumonia in Children Patients","Artificial intelligence methods have been increasingly turning into a
potentially powerful tool in the diagnosis and management of diseases. In this
study, we utilized logistic regression (LR), decision tree (DT), gradient
boosted decision tree (GBDT), support vector machine (SVM), and multilayer
perceptron (MLP) as machine learning models to rapidly diagnose the mycoplasma
pneumoniae pneumonia (MPP) in children patients. The classification task was
carried out after applying the preprocessing procedure to the MPP dataset. The
most efficient results are obtained by GBDT. It provides the best performance
with an accuracy of 93.7%. In contrast to standard raw feature weighting, the
feature importance takes the underlying correlation structure of the features
into account. The most crucial feature of GBDT is the ""pulmonary infiltrates
range"" with a score of 0.5925, followed by ""cough"" (0.0953) and ""pleural
effusion"" (0.0492). We publicly share our full implementation with the dataset
and trained models at https://github.com/zhenguonie/2021_AI4MPP."
"Artificial Intelligence as an Anti-Corruption Tool (AI-ACT) --
  Potentials and Pitfalls for Top-down and Bottom-up Approaches","Corruption continues to be one of the biggest societal challenges of our
time. New hope is placed in Artificial Intelligence (AI) to serve as an
unbiased anti-corruption agent. Ever more available (open) government data
paired with unprecedented performance of such algorithms render AI the next
frontier in anti-corruption. Summarizing existing efforts to use AI-based
anti-corruption tools (AI-ACT), we introduce a conceptual framework to advance
research and policy. It outlines why AI presents a unique tool for top-down and
bottom-up anti-corruption approaches. For both approaches, we outline in detail
how AI-ACT present different potentials and pitfalls for (a) input data, (b)
algorithmic design, and (c) institutional implementation. Finally, we venture a
look into the future and flesh out key questions that need to be addressed to
develop AI-ACT while considering citizens' views, hence putting ""society in the
loop""."
Reflections on the Clinical Acceptance of Artificial Intelligence,"In this chapter, we reflect on the use of Artificial Intelligence (AI) and
its acceptance in clinical environments. We develop a general view of
hindrances for clinical acceptance in the form of a pipeline model combining AI
and clinical practise. We then link each challenge to the relevant stage in the
pipeline and discuss the necessary requirements in order to overcome each
challenge. We complement this discussion with an overview of opportunities for
AI, which we currently see at the periphery of clinical workflows."
"Have We Learned to Explain?: How Interpretability Methods Can Learn to
  Encode Predictions in their Interpretations","While the need for interpretable machine learning has been established, many
common approaches are slow, lack fidelity, or hard to evaluate. Amortized
explanation methods reduce the cost of providing interpretations by learning a
global selector model that returns feature importances for a single instance of
data. The selector model is trained to optimize the fidelity of the
interpretations, as evaluated by a predictor model for the target. Popular
methods learn the selector and predictor model in concert, which we show allows
predictions to be encoded within interpretations. We introduce EVAL-X as a
method to quantitatively evaluate interpretations and REAL-X as an amortized
explanation method, which learn a predictor model that approximates the true
data generating distribution given any subset of the input. We show EVAL-X can
detect when predictions are encoded in interpretations and show the advantages
of REAL-X through quantitative and radiologist evaluation."
"Expert System Gradient Descent Style Training: Development of a
  Defensible Artificial Intelligence Technique","Artificial intelligence systems, which are designed with a capability to
learn from the data presented to them, are used throughout society. These
systems are used to screen loan applicants, make sentencing recommendations for
criminal defendants, scan social media posts for disallowed content and more.
Because these systems don't assign meaning to their complex learned correlation
network, they can learn associations that don't equate to causality, resulting
in non-optimal and indefensible decisions being made. In addition to making
decisions that are sub-optimal, these systems may create legal liability for
their designers and operators by learning correlations that violate
anti-discrimination and other laws regarding what factors can be used in
different types of decision making. This paper presents the use of a machine
learning expert system, which is developed with meaning-assigned nodes (facts)
and correlations (rules). Multiple potential implementations are considered and
evaluated under different conditions, including different network error and
augmentation levels and different training levels. The performance of these
systems is compared to random and fully connected networks."
"When is it permissible for artificial intelligence to lie? A trust-based
  approach","Conversational Artificial Intelligence (AI) used in industry settings can be
trained to closely mimic human behaviors, including lying and deception.
However, lying is often a necessary part of negotiation. To address this, we
develop a normative framework for when it is ethical or unethical for a
conversational AI to lie to humans, based on whether there is what we call
""invitation of trust"" in a particular scenario. Importantly, cultural norms
play an important role in determining whether there is invitation of trust
across negotiation settings, and thus an AI trained in one culture may not be
generalizable to others. Moreover, individuals may have different expectations
regarding the invitation of trust and propensity to lie for human vs. AI
negotiators, and these expectations may vary across cultures as well. Finally,
we outline how a conversational chatbot can be trained to negotiate ethically
by applying autoregressive models to large dialog and negotiations datasets."
The AI Index 2021 Annual Report,"Welcome to the fourth edition of the AI Index Report. This year we
significantly expanded the amount of data available in the report, worked with
a broader set of external organizations to calibrate our data, and deepened our
connections with the Stanford Institute for Human-Centered Artificial
Intelligence (HAI). The AI Index Report tracks, collates, distills, and
visualizes data related to artificial intelligence. Its mission is to provide
unbiased, rigorously vetted, and globally sourced data for policymakers,
researchers, executives, journalists, and the general public to develop
intuitions about the complex field of AI. The report aims to be the most
credible and authoritative source for data and insights about AI in the world."
"Artificial Intelligence Narratives: An Objective Perspective on Current
  Developments","This work provides a starting point for researchers interested in gaining a
deeper understanding of the big picture of artificial intelligence (AI). To
this end, a narrative is conveyed that allows the reader to develop an
objective view on current developments that is free from false promises that
dominate public communication. An essential takeaway for the reader is that AI
must be understood as an umbrella term encompassing a plethora of different
methods, schools of thought, and their respective historical movements.
Consequently, a bottom-up strategy is pursued in which the field of AI is
introduced by presenting various aspects that are characteristic of the
subject. This paper is structured in three parts: (i) Discussion of current
trends revealing false public narratives, (ii) an introduction to the history
of AI focusing on recurring patterns and main characteristics, and (iii) a
critical discussion on the limitations of current methods in the context of the
potential emergence of a strong(er) AI. It should be noted that this work does
not cover any of these aspects holistically; rather, the content addressed is a
selection made by the author and subject to a didactic strategy."
"Projection: A Mechanism for Human-like Reasoning in Artificial
  Intelligence","Artificial Intelligence systems cannot yet match human abilities to apply
knowledge to situations that vary from what they have been programmed for, or
trained for. In visual object recognition methods of inference exploiting
top-down information (from a model) have been shown to be effective for
recognising entities in difficult conditions. Here this type of inference,
called `projection', is shown to be a key mechanism to solve the problem of
applying knowledge to varied or challenging situations, across a range of AI
domains, such as vision, robotics, or language. Finally the relevance of
projection to tackling the commonsense knowledge problem is discussed."
"Local Explanations via Necessity and Sufficiency: Unifying Theory and
  Practice","Necessity and sufficiency are the building blocks of all successful
explanations. Yet despite their importance, these notions have been
conceptually underdeveloped and inconsistently applied in explainable
artificial intelligence (XAI), a fast-growing research area that is so far
lacking in firm theoretical foundations. Building on work in logic,
probability, and causality, we establish the central role of necessity and
sufficiency in XAI, unifying seemingly disparate methods in a single formal
framework. We provide a sound and complete algorithm for computing explanatory
factors with respect to a given context, and demonstrate its flexibility and
competitive performance against state of the art alternatives on various tasks."
"STARdom: an architecture for trusted and secure human-centered
  manufacturing systems","There is a lack of a single architecture specification that addresses the
needs of trusted and secure Artificial Intelligence systems with humans in the
loop, such as human-centered manufacturing systems at the core of the evolution
towards Industry 5.0. To realize this, we propose an architecture that
integrates forecasts, Explainable Artificial Intelligence, supports collecting
users' feedback, and uses Active Learning and Simulated Reality to enhance
forecasts and provide decision-making recommendations. The architecture
security is addressed as a general concern. We align the proposed architecture
with the Big Data Value Association Reference Architecture Model. We tailor it
for the domain of demand forecasting and validate it on a real-world case
study."
"A review of artificial intelligence methods combined with Raman
  spectroscopy to identify the composition of substances","In general, most of the substances in nature exist in mixtures, and the
noninvasive identification of mixture composition with high speed and accuracy
remains a difficult task. However, the development of Raman spectroscopy,
machine learning, and deep learning techniques have paved the way for achieving
efficient analytical tools capable of identifying mixture components, making an
apparent breakthrough in the identification of mixtures beyond the traditional
chemical analysis methods. This article summarizes the work of Raman
spectroscopy in identifying the composition of substances as well as provides
detailed reviews on the preprocessing process of Raman spectroscopy, the
analysis methods and applications of artificial intelligence. This review
summarizes the work of Raman spectroscopy in identifying the composition of
substances and reviews the preprocessing process of Raman spectroscopy, the
analysis methods and applications of artificial intelligence. Finally, the
advantages and disadvantages and development prospects of Raman spectroscopy
are discussed in detail."
"Artificial Intelligence Methods Based Hierarchical Classification of
  Frontotemporal Dementia to Improve Diagnostic Predictability","Patients with Frontotemporal Dementia (FTD) have impaired cognitive
abilities, executive and behavioral traits, loss of language ability, and
decreased memory capabilities. Based on the distinct patterns of cortical
atrophy and symptoms, the FTD spectrum primarily includes three variants:
behavioral variant FTD (bvFTD), non-fluent variant primary progressive aphasia
(nfvPPA), and semantic variant primary progressive aphasia (svPPA). The purpose
of this study is to classify MRI images of every single subject into one of the
spectrums of the FTD in a hierarchical order by applying data-driven techniques
of Artificial Intelligence (AI) on cortical thickness data. This data is
computed by FreeSurfer software. We used the Smallest Univalue Segment
Assimilating Nucleus (SUSAN) technique to minimize the noise in cortical
thickness data. Specifically, we took 204 subjects from the frontotemporal
lobar degeneration neuroimaging initiative (NIFTD) database to validate this
approach, and each subject was diagnosed in one of the diagnostic categories
(bvFTD, svPPA, nfvPPA and cognitively normal). Our proposed automated
classification model yielded classification accuracy of 86.5, 76, and 72.7 with
support vector machine (SVM), linear discriminant analysis (LDA), and Naive
Bayes methods, respectively, in 10-fold cross-validation analysis, which is a
significant improvement on a traditional single multi-class model with an
accuracy of 82.7, 73.4, and 69.2."
"Comprehensive systematic review into combinations of artificial
  intelligence, human factors, and automation","Artificial intelligence (AI)-based models used to improve different fields
including healthcare, and finance. One of the field that receive advantages of
AI is automation. However, it is important to consider human factors in
application of AI in automation. This paper reports on a systematic review of
the published studies used to investigate the application of AI in PM. This
comprehensive systematic review used ScienceDirect to identify relevant
articles. Of the 422 articles found, 40 met the inclusion and exclusion
criteria and were used in the review. Selected articles were classified based
on categories of human factors and areas of application. The results indicated
that application of AI in automation with respect to human factors could be
divided into three areas of physical ergonomics, cognitive ergonomic and
organizational ergonomics. The main areas of application in physical and
cognitive ergonomics are including transportation, User experience, and
human-machine interactions."
TrustyAI Explainability Toolkit,"Artificial intelligence (AI) is becoming increasingly more popular and can be
found in workplaces and homes around the world. The decisions made by such
""black box"" systems are often opaque; that is, so complex as to be functionally
impossible to understand. How do we ensure that these systems are behaving as
desired? TrustyAI is an initiative which looks into explainable artificial
intelligence (XAI) solutions to address this issue of explainability in the
context of both AI models and decision services. This paper presents the
TrustyAI Explainability Toolkit, a Java and Python library that provides XAI
explanations of decision services and predictive models for both enterprise and
data science use-cases. We describe the TrustyAI implementations and extensions
to techniques such as LIME, SHAP and counterfactuals, which are benchmarked
against existing implementations in a variety of experiments."
Controlling earthquake-like instabilities using artificial intelligence,"Earthquakes are lethal and costly. This study aims at avoiding these
catastrophic events by the application of injection policies retrieved through
reinforcement learning. With the rapid growth of artificial intelligence,
prediction-control problems are all the more tackled by function approximation
models that learn how to control a specific task, even for systems with
unmodeled/unknown dynamics and important uncertainties. Here, we show for the
first time the possibility of controlling earthquake-like instabilities using
state-of-the-art deep reinforcement learning techniques. The controller is
trained using a reduced model of the physical system, i.e, the spring-slider
model, which embodies the main dynamics of the physical problem for a given
earthquake magnitude. Its robustness to unmodeled dynamics is explored through
a parametric study. Our study is a first step towards minimizing seismicity in
industrial projects (geothermal energy, hydrocarbons production, CO2
sequestration) while, in a second step for inspiring techniques for natural
earthquakes control and prevention."
"Applications of Artificial Intelligence to aid detection of dementia: a
  narrative review on current capabilities and future directions","With populations ageing, the number of people with dementia worldwide is
expected to triple to 152 million by 2050. Seventy percent of cases are due to
Alzheimer's disease (AD) pathology and there is a 10-20 year 'pre-clinical'
period before significant cognitive decline occurs. We urgently need, cost
effective, objective methods to detect AD, and other dementias, at an early
stage. Risk factor modification could prevent 40% of cases and drug trials
would have greater chances of success if participants are recruited at an
earlier stage. Currently, detection of dementia is largely by pen and paper
cognitive tests but these are time consuming and insensitive to pre-clinical
phases. Specialist brain scans and body fluid biomarkers can detect the
earliest stages of dementia but are too invasive or expensive for widespread
use. With the advancement of technology, Artificial Intelligence (AI) shows
promising results in assisting with detection of early-stage dementia. Existing
AI-aided methods and potential future research directions are reviewed and
discussed."
"Vehicle Emissions Prediction with Physics-Aware AI Models: Preliminary
  Results","Given an on-board diagnostics (OBD) dataset and a physics-based emissions
prediction model, this paper aims to develop an accurate and
computational-efficient AI (Artificial Intelligence) method that predicts
vehicle emissions. The problem is of societal importance because vehicular
emissions lead to climate change and impact human health. This problem is
challenging because the OBD data does not contain enough parameters needed by
high-order physics models. Conversely, related work has shown that low-order
physics models have poor predictive accuracy when using available OBD data.
This paper uses a divergent window co-occurrence pattern detection method to
develop a spatiotemporal variability-aware AI model for predicting emission
values from the OBD datasets. We conducted a case study using real-world OBD
data from a local public transportation agency. Results show that the proposed
AI method has approximately 65% improved predictive accuracy than a non-AI
low-order physics model and is approximately 35% more accurate than a baseline
model."
Graph Learning: A Survey,"Graphs are widely used as a popular representation of the network structure
of connected data. Graph data can be found in a broad spectrum of application
domains such as social systems, ecosystems, biological networks, knowledge
graphs, and information systems. With the continuous penetration of artificial
intelligence technologies, graph learning (i.e., machine learning on graphs) is
gaining attention from both researchers and practitioners. Graph learning
proves effective for many tasks, such as classification, link prediction, and
matching. Generally, graph learning methods extract relevant features of graphs
by taking advantage of machine learning algorithms. In this survey, we present
a comprehensive overview on the state-of-the-art of graph learning. Special
attention is paid to four categories of existing graph learning methods,
including graph signal processing, matrix factorization, random walk, and deep
learning. Major models and algorithms under these categories are reviewed
respectively. We examine graph learning applications in areas such as text,
images, science, knowledge graphs, and combinatorial optimization. In addition,
we discuss several promising research directions in this field."
"ANDREAS: Artificial intelligence traiNing scheDuler foR accElerAted
  resource clusterS","Artificial Intelligence (AI) and Deep Learning (DL) algorithms are currently
applied to a wide range of products and solutions. DL training jobs are highly
resource demanding and they experience great benefits when exploiting AI
accelerators (e.g., GPUs). However, the effective management of GPU-powered
clusters comes with great challenges. Among these, efficient scheduling and
resource allocation solutions are crucial to maximize performance and minimize
Data Centers operational costs. In this paper we propose ANDREAS, an advanced
scheduling solution that tackles these problems jointly, aiming at optimizing
DL training runtime workloads and their energy consumption in accelerated
clusters. Experiments based on simulation demostrate that we can achieve a cost
reduction between 30 and 62% on average with respect to first-principle methods
while the validation on a real cluster shows a worst case deviation below 13%
between actual and predicted costs, proving the effectiveness of ANDREAS
solution in practical scenarios."
Conscious AI,"Recent advances in artificial intelligence (AI) have achieved human-scale
speed and accuracy for classification tasks. In turn, these capabilities have
made AI a viable replacement for many human activities that at their core
involve classification, such as basic mechanical and analytical tasks in
low-level service jobs. Current systems do not need to be conscious to
recognize patterns and classify them. However, for AI to progress to more
complicated tasks requiring intuition and empathy, it must develop capabilities
such as metathinking, creativity, and empathy akin to human self-awareness or
consciousness. We contend that such a paradigm shift is possible only through a
fundamental shift in the state of artificial intelligence toward consciousness,
a shift similar to what took place for humans through the process of natural
selection and evolution. As such, this paper aims to theoretically explore the
requirements for the emergence of consciousness in AI. It also provides a
principled understanding of how conscious AI can be detected and how it might
be manifested in contrast to the dominant paradigm that seeks to ultimately
create machines that are linguistically indistinguishable from humans."
"An Efficient Application of Neuroevolution for Competitive Multiagent
  Learning","Multiagent systems provide an ideal environment for the evaluation and
analysis of real-world problems using reinforcement learning algorithms. Most
traditional approaches to multiagent learning are affected by long training
periods as well as high computational complexity. NEAT (NeuroEvolution of
Augmenting Topologies) is a popular evolutionary strategy used to obtain the
best performing neural network architecture often used to tackle optimization
problems in the field of artificial intelligence. This paper utilizes the NEAT
algorithm to achieve competitive multiagent learning on a modified pong game
environment in an efficient manner. The competing agents abide by different
rules while having similar observation space parameters. The proposed algorithm
utilizes this property of the environment to define a singular
neuroevolutionary procedure that obtains the optimal policy for all the agents.
The compiled results indicate that the proposed implementation achieves ideal
behaviour in a very short training period when compared to existing multiagent
reinforcement learning models."
Towards Knowledge Organization Ecosystems,"It is needless to mention the (already established) overarching importance of
knowledge organization and its tried-and-tested high-quality schemes in
knowledge-based Artificial Intelligence (AI) systems. But equally, it is also
hard to ignore that, increasingly, standalone KOSs are becoming functionally
ineffective components for such systems, given their inability to capture the
continuous facetization and drift of domains. The paper proposes a radical
re-conceptualization of KOSs as a first step to solve such an inability, and,
accordingly, contributes in the form of the following dimensions: (i) an
explicit characterization of Knowledge Organization Ecosystems (KOEs) (possibly
for the first time) and their positioning as pivotal components in realizing
sustainable knowledge-based AI solutions, (ii) as a consequence of such a novel
characterization, a first examination and characterization of KOEs as
Socio-Technical Systems (STSs), thus opening up an entirely new stream of
research in knowledge-based AI, and (iii) motivating KOEs not to be mere STSs
but STSs which are grounded in Ethics and Responsible Artificial Intelligence
cardinals from their very genesis. The paper grounds the above contributions in
relevant research literature in a distributed fashion throughout the paper, and
finally concludes by outlining the future research possibilities."
"Gender-Specific Patterns in the Artificial Intelligence Scientific
  Ecosystem","Gender disparity in science is one of the most focused debating points among
authorities and the scientific community. Over the last few decades, numerous
initiatives have endeavored to accelerate gender equity in academia and
research society. However, despite the ongoing efforts, gaps persist across the
world, and more measures need to be taken. Using social network analysis,
natural language processing, and machine learning, in this study, we
comprehensively analyzed gender-specific patterns in the highly
interdisciplinary and evolving field of artificial intelligence for the period
of 2000-2019. Our findings suggest an overall increasing rate of mixed-gender
collaborations. From the observed gender-specific collaborative patterns, the
existence of disciplinary homophily at both dyadic and team levels is
confirmed. However, a higher preference was observed for female researchers to
form homophilous collaborative links. Our core-periphery analysis indicated a
significant positive association between having diverse collaboration and
scientific performance and experience. We found evidence in support of
expecting the rise of new female superstar researchers in the artificial
intelligence field."
"Explainable Artificial Intelligence (XAI) for Increasing User Trust in
  Deep Reinforcement Learning Driven Autonomous Systems","We consider the problem of providing users of deep Reinforcement Learning
(RL) based systems with a better understanding of when their output can be
trusted. We offer an explainable artificial intelligence (XAI) framework that
provides a three-fold explanation: a graphical depiction of the systems
generalization and performance in the current game state, how well the agent
would play in semantically similar environments, and a narrative explanation of
what the graphical information implies. We created a user-interface for our XAI
framework and evaluated its efficacy via a human-user experiment. The results
demonstrate a statistically significant increase in user trust and acceptance
of the AI system with explanation, versus the AI system without explanation."
"Paradigm selection for Data Fusion of SAR and Multispectral Sentinel
  data applied to Land-Cover Classification","Data fusion is a well-known technique, becoming more and more popular in the
Artificial Intelligence for Earth Observation (AI4EO) domain mainly due to its
ability of reinforcing AI4EO applications by combining multiple data sources
and thus bringing better results. On the other hand, like other methods for
satellite data analysis, data fusion itself is also benefiting and evolving
thanks to the integration of Artificial Intelligence (AI). In this letter, four
data fusion paradigms, based on Convolutional Neural Networks (CNNs), are
analyzed and implemented. The goals are to provide a systematic procedure for
choosing the best data fusion framework, resulting in the best classification
results, once the basic structure for the CNN has been defined, and to help
interested researchers in their work when data fusion applied to remote sensing
is involved. The procedure has been validated for land-cover classification but
it can be transferred to other cases."
"Quantum Computing for Artificial Intelligence Based Mobile Network
  Optimization","In this paper, we discuss how certain radio access network optimization
problems can be modelled using the concept of constraint satisfaction problems
in artificial intelligence, and solved at scale using a quantum computer. As a
case study, we discuss root sequence index (RSI) assignment problem - an
important LTE/NR physical random access channel configuration related
automation use-case. We formulate RSI assignment as quadratic unconstrained
binary optimization (QUBO) problem constructed using data ingested from a
commercial mobile network, and solve it using a cloud-based commercially
available quantum computing platform. Results show that quantum annealing
solver can successfully assign conflict-free RSIs. Comparison with well-known
heuristics reveals that some classic algorithms are even more effective in
terms of solution quality and computation time. The non-quantum advantage is
due to the fact that current implementation is a semi-quantum proof-of-concept
algorithm. Also, the results depend on the type of quantum computer used.
Nevertheless, the proposed framework is highly flexible and holds tremendous
potential for harnessing the power of quantum computing in mobile network
automation."
Demystifying the Draft EU Artificial Intelligence Act,"In April 2021, the European Commission proposed a Regulation on Artificial
Intelligence, known as the AI Act. We present an overview of the Act and
analyse its implications, drawing on scholarship ranging from the study of
contemporary AI practices to the structure of EU product safety regimes over
the last four decades. Aspects of the AI Act, such as different rules for
different risk-levels of AI, make sense. But we also find that some provisions
of the Draft AI Act have surprising legal implications, whilst others may be
largely ineffective at achieving their stated goals. Several overarching
aspects, including the enforcement regime and the risks of maximum
harmonisation pre-empting legitimate national AI policy, engender significant
concern. These issues should be addressed as a priority in the legislative
process."
"MDE4QAI: Towards Model-Driven Engineering for Quantum Artificial
  Intelligence","Over the past decade, Artificial Intelligence (AI) has provided enormous new
possibilities and opportunities, but also new demands and requirements for
software systems. In particular, Machine Learning (ML) has proven useful in
almost every vertical application domain. In the decade ahead, an unprecedented
paradigm shift from classical computing towards Quantum Computing (QC), with
perhaps a quantum-classical hybrid model, is expected. We argue that the
Model-Driven Engineering (MDE) paradigm can be an enabler and a facilitator,
when it comes to the quantum and the quantum-classical hybrid applications.
This includes not only automated code generation, but also automated model
checking and verification, as well as model analysis in the early design
phases, and model-to-model transformations both at the design-time and at the
runtime. In this paper, the vision is focused on MDE for Quantum AI,
particularly Quantum ML for the Internet of Things (IoT) and smart
Cyber-Physical Systems (CPS) applications."
Artificial Intelligence in PET: an Industry Perspective,"Artificial intelligence (AI) has significant potential to positively impact
and advance medical imaging, including positron emission tomography (PET)
imaging applications. AI has the ability to enhance and optimize all aspects of
the PET imaging chain from patient scheduling, patient setup, protocoling, data
acquisition, detector signal processing, reconstruction, image processing and
interpretation. AI poses industry-specific challenges which will need to be
addressed and overcome to maximize the future potentials of AI in PET. This
paper provides an overview of these industry-specific challenges for the
development, standardization, commercialization, and clinical adoption of AI,
and explores the potential enhancements to PET imaging brought on by AI in the
near future. In particular, the combination of on-demand image reconstruction,
AI, and custom designed data processing workflows may open new possibilities
for innovation which would positively impact the industry and ultimately
patients."
Explainable AI: current status and future directions,"Explainable Artificial Intelligence (XAI) is an emerging area of research in
the field of Artificial Intelligence (AI). XAI can explain how AI obtained a
particular solution (e.g., classification or object detection) and can also
answer other ""wh"" questions. This explainability is not possible in traditional
AI. Explainability is essential for critical applications, such as defense,
health care, law and order, and autonomous driving vehicles, etc, where the
know-how is required for trust and transparency. A number of XAI techniques so
far have been purposed for such applications. This paper provides an overview
of these techniques from a multimedia (i.e., text, image, audio, and video)
point of view. The advantages and shortcomings of these techniques have been
discussed, and pointers to some future directions have also been provided."
Abstract Reasoning via Logic-guided Generation,"Abstract reasoning, i.e., inferring complicated patterns from given
observations, is a central building block of artificial general intelligence.
While humans find the answer by either eliminating wrong candidates or first
constructing the answer, prior deep neural network (DNN)-based methods focus on
the former discriminative approach. This paper aims to design a framework for
the latter approach and bridge the gap between artificial and human
intelligence. To this end, we propose logic-guided generation (LoGe), a novel
generative DNN framework that reduces abstract reasoning as an optimization
problem in propositional logic. LoGe is composed of three steps: extract
propositional variables from images, reason the answer variables with a logic
layer, and reconstruct the answer image from the variables. We demonstrate that
LoGe outperforms the black box DNN frameworks for generative abstract reasoning
under the RAVEN benchmark, i.e., reconstructing answers based on capturing
correct rules of various attributes from observations."
Measuring Ethics in AI with AI: A Methodology and Dataset Construction,"Recently, the use of sound measures and metrics in Artificial Intelligence
has become the subject of interest of academia, government, and industry.
Efforts towards measuring different phenomena have gained traction in the AI
community, as illustrated by the publication of several influential field
reports and policy documents. These metrics are designed to help decision
takers to inform themselves about the fast-moving and impacting influences of
key advances in Artificial Intelligence in general and Machine Learning in
particular. In this paper we propose to use such newfound capabilities of AI
technologies to augment our AI measuring capabilities. We do so by training a
model to classify publications related to ethical issues and concerns. In our
methodology we use an expert, manually curated dataset as the training set and
then evaluate a large set of research papers. Finally, we highlight the
implications of AI metrics, in particular their contribution towards developing
trustful and fair AI-based tools and technologies. Keywords: AI Ethics; AI
Fairness; AI Measurement. Ethics in Computer Science."
"The social dilemma in artificial intelligence development and why we
  have to solve it","While the demand for ethical artificial intelligence (AI) systems increases,
the number of unethical uses of AI accelerates, even though there is no
shortage of ethical guidelines. We argue that a possible underlying cause for
this is that AI developers face a social dilemma in AI development ethics,
preventing the widespread adaptation of ethical best practices. We define the
social dilemma for AI development and describe why the current crisis in AI
development ethics cannot be solved without relieving AI developers of their
social dilemma. We argue that AI development must be professionalised to
overcome the social dilemma, and discuss how medicine can be used as a template
in this process."
"Meaning Versus Information, Prediction Versus Memory, and Question
  Versus Answer","Brain science and artificial intelligence have made great progress toward the
understanding and engineering of the human mind. The progress has accelerated
significantly since the turn of the century thanks to new methods for probing
the brain (both structure and function), and rapid development in deep learning
research. However, despite these new developments, there are still many open
questions, such as how to understand the brain at the system level, and various
robustness issues and limitations of deep learning. In this informal essay, I
will talk about some of the concepts that are central to brain science and
artificial intelligence, such as information and memory, and discuss how a
different view on these concepts can help us move forward, beyond current
limits of our understanding in these fields."
Artificial Intelligence in Healthcare: Lost In Translation?,"Artificial intelligence (AI) in healthcare is a potentially revolutionary
tool to achieve improved healthcare outcomes while reducing overall health
costs. While many exploratory results hit the headlines in recent years there
are only few certified and even fewer clinically validated products available
in the clinical setting. This is a clear indication of failing translation due
to shortcomings of the current approach to AI in healthcare. In this work, we
highlight the major areas, where we observe current challenges for translation
in AI in healthcare, namely precision medicine, reproducible science, data
issues and algorithms, causality, and product development. For each field, we
outline possible solutions for these challenges. Our work will lead to improved
translation of AI in healthcare products into the clinical setting"
"The brain is a computer is a brain: neuroscience's internal debate and
  the social significance of the Computational Metaphor","The Computational Metaphor, comparing the brain to the computer and vice
versa, is the most prominent metaphor in neuroscience and artificial
intelligence (AI). Its appropriateness is highly debated in both fields,
particularly with regards to whether it is useful for the advancement of
science and technology. Considerably less attention, however, has been devoted
to how the Computational Metaphor is used outside of the lab, and particularly
how it may shape society's interactions with AI. As such, recently publicized
concerns over AI's role in perpetuating racism, genderism, and ableism suggest
that the term ""artificial intelligence"" is misplaced, and that a new lexicon is
needed to describe these computational systems. Thus, there is an essential
question about the Computational Metaphor that is rarely asked by
neuroscientists: whom does it help and whom does it harm? This essay invites
the neuroscience community to consider the social implications of the field's
most controversial metaphor."
"On Quantifying Literals in Boolean Logic and Its Applications to
  Explainable AI","Quantified Boolean logic results from adding operators to Boolean logic for
existentially and universally quantifying variables. This extends the reach of
Boolean logic by enabling a variety of applications that have been explored
over the decades. The existential quantification of literals (variable states)
and its applications have also been studied in the literature. In this paper,
we complement this by studying universal literal quantification and its
applications, particularly to explainable AI. We also provide a novel semantics
for quantification, discuss the interplay between variable/literal and
existential/universal quantification. We further identify some classes of
Boolean formulas and circuits on which quantification can be done efficiently.
Literal quantification is more fine-grained than variable quantification as the
latter can be defined in terms of the former. This leads to a refinement of
quantified Boolean logic with literal quantification as its primitive."
"On Solving a Stochastic Shortest-Path Markov Decision Process as
  Probabilistic Inference","Previous work on planning as active inference addresses finite horizon
problems and solutions valid for online planning. We propose solving the
general Stochastic Shortest-Path Markov Decision Process (SSP MDP) as
probabilistic inference. Furthermore, we discuss online and offline methods for
planning under uncertainty. In an SSP MDP, the horizon is indefinite and
unknown a priori. SSP MDPs generalize finite and infinite horizon MDPs and are
widely used in the artificial intelligence community. Additionally, we
highlight some of the differences between solving an MDP using dynamic
programming approaches widely used in the artificial intelligence community and
approaches used in the active inference community."
Towards Resilient Artificial Intelligence: Survey and Research Issues,"Artificial intelligence (AI) systems are becoming critical components of
today's IT landscapes. Their resilience against attacks and other environmental
influences needs to be ensured just like for other IT assets. Considering the
particular nature of AI, and machine learning (ML) in particular, this paper
provides an overview of the emerging field of resilient AI and presents
research issues the authors identify as potential future work."
"CC-Cert: A Probabilistic Approach to Certify General Robustness of
  Neural Networks","In safety-critical machine learning applications, it is crucial to defend
models against adversarial attacks -- small modifications of the input that
change the predictions. Besides rigorously studied $\ell_p$-bounded additive
perturbations, recently proposed semantic perturbations (e.g. rotation,
translation) raise a serious concern on deploying ML systems in real-world.
Therefore, it is important to provide provable guarantees for deep learning
models against semantically meaningful input transformations. In this paper, we
propose a new universal probabilistic certification approach based on
Chernoff-Cramer bounds that can be used in general attack settings. We estimate
the probability of a model to fail if the attack is sampled from a certain
distribution. Our theoretical findings are supported by experimental results on
different datasets."
"A User-Centred Framework for Explainable Artificial Intelligence in
  Human-Robot Interaction","State of the art Artificial Intelligence (AI) techniques have reached an
impressive complexity. Consequently, researchers are discovering more and more
methods to use them in real-world applications. However, the complexity of such
systems requires the introduction of methods that make those transparent to the
human user. The AI community is trying to overcome the problem by introducing
the Explainable AI (XAI) field, which is tentative to make AI algorithms less
opaque. However, in recent years, it became clearer that XAI is much more than
a computer science problem: since it is about communication, XAI is also a
Human-Agent Interaction problem. Moreover, AI came out of the laboratories to
be used in real life. This implies the need for XAI solutions tailored to
non-expert users. Hence, we propose a user-centred framework for XAI that
focuses on its social-interactive aspect taking inspiration from cognitive and
social sciences' theories and findings. The framework aims to provide a
structure for interactive XAI solutions thought for non-expert users."
Empowering Local Communities Using Artificial Intelligence,"Artificial Intelligence (AI) is increasingly used to analyze large amounts of
data in various practices, such as object recognition. We are specifically
interested in using AI-powered systems to engage local communities in
developing plans or solutions for pressing societal and environmental concerns.
Such local contexts often involve multiple stakeholders with different and even
contradictory agendas, resulting in mismatched expectations of these systems'
behaviors and desired outcomes. There is a need to investigate if AI models and
pipelines can work as expected in different contexts through co-creation and
field deployment. Based on case studies in co-creating AI-powered systems with
local people, we explain challenges that require more attention and provide
viable paths to bridge AI research with citizen needs. We advocate for
developing new collaboration approaches and mindsets that are needed to
co-create AI-powered systems in multi-stakeholder contexts to address local
concerns."
Bach Style Music Authoring System based on Deep Learning,"With the continuous improvement in various aspects in the field of artificial
intelligence, the momentum of artificial intelligence with deep learning
capabilities into the field of music is coming. The research purpose of this
paper is to design a Bach style music authoring system based on deep learning.
We use a LSTM neural network to train serialized and standardized music feature
data. By repeated experiments, we find the optimal LSTM model which can
generate imitation of Bach music. Finally the generated music is
comprehensively evaluated in the form of online audition and Turing test. The
repertoires which the music generation system constructed in this article are
very close to the style of Bach's original music, and it is relatively
difficult for ordinary people to distinguish the musics Bach authored and AI
created."
"An In-depth Summary of Recent Artificial Intelligence Applications in
  Drug Design","As a promising tool to navigate in the vast chemical space, artificial
intelligence (AI) is leveraged for drug design. From the year 2017 to 2021, the
number of applications of several recent AI models (i.e. graph neural network
(GNN), recurrent neural network (RNN), variation autoencoder (VAE), generative
adversarial network (GAN), flow and reinforcement learning (RL)) in drug design
increases significantly. Many relevant literature reviews exist. However, none
of them provides an in-depth summary of many applications of the recent AI
models in drug design. To complement the existing literature, this survey
includes the theoretical development of the previously mentioned AI models and
detailed summaries of 42 recent applications of AI in drug design. Concretely,
13 of them leverage GNN for molecular property prediction and 29 of them use RL
and/or deep generative models for molecule generation and optimization. In most
cases, the focus of the summary is the models, their variants, and
modifications for specific tasks in drug design. Moreover, 60 additional
applications of AI in molecule generation and optimization are briefly
summarized in a table. Finally, this survey provides a holistic discussion of
the abundant applications so that the tasks, potential solutions, and
challenges in AI-based drug design become evident."
Value alignment: a formal approach,"principles that should govern autonomous AI systems. It essentially states
that a system's goals and behaviour should be aligned with human values. But
how to ensure value alignment? In this paper we first provide a formal model to
represent values through preferences and ways to compute value aggregations;
i.e. preferences with respect to a group of agents and/or preferences with
respect to sets of values. Value alignment is then defined, and computed, for a
given norm with respect to a given value through the increase/decrease that it
results in the preferences of future states of the world. We focus on norms as
it is norms that govern behaviour, and as such, the alignment of a given system
with a given value will be dictated by the norms the system follows."
"Goal Agnostic Planning using Maximum Likelihood Paths in Hypergraph
  World Models","In this paper, we present a hypergraph--based machine learning algorithm, a
datastructure--driven maintenance method, and a planning algorithm based on a
probabilistic application of Dijkstra's algorithm. Together, these form a goal
agnostic automated planning engine for an autonomous learning agent which
incorporates beneficial properties of both classical Machine Learning and
traditional Artificial Intelligence. We prove that the algorithm determines
optimal solutions within the problem space, mathematically bound learning
performance, and supply a mathematical model analyzing system state progression
through time yielding explicit predictions for learning curves, goal
achievement rates, and response to abstractions and uncertainty. To validate
performance, we exhibit results from applying the agent to three archetypal
planning problems, including composite hierarchical domains, and highlight
empirical findings which illustrate properties elucidated in the analysis."
Introducing Variational Autoencoders to High School Students,"Generative Artificial Intelligence (AI) models are a compelling way to
introduce K-12 students to AI education using an artistic medium, and hence
have drawn attention from K-12 AI educators. Previous Creative AI curricula
mainly focus on Generative Adversarial Networks (GANs) while paying less
attention to Autoregressive Models, Variational Autoencoders (VAEs), or other
generative models, which have since become common in the field of generative
AI. VAEs' latent-space structure and interpolation ability could effectively
ground the interdisciplinary learning of AI, creative arts, and philosophy.
Thus, we designed a lesson to teach high school students about VAEs. We
developed a web-based game and used Plato's cave, a philosophical metaphor, to
introduce how VAEs work. We used a Google Colab notebook for students to
re-train VAEs with their hand-written digits to consolidate their
understandings. Finally, we guided the exploration of creative VAE tools such
as SketchRNN and MusicVAE to draw the connection between what they learned and
real-world applications. This paper describes the lesson design and shares
insights from the pilot studies with 22 students. We found that our approach
was effective in teaching students about a novel AI concept."
Quality and Computation Time in Optimization Problems,"Optimization problems are crucial in artificial intelligence. Optimization
algorithms are generally used to adjust the performance of artificial
intelligence models to minimize the error of mapping inputs to outputs. Current
evaluation methods on optimization algorithms generally consider the
performance in terms of quality. However, not all optimization algorithms for
all test cases are evaluated equal from quality, the computation time should be
also considered for optimization tasks. In this paper, we investigate the
quality and computation time of optimization algorithms in optimization
problems, instead of the one-for-all evaluation of quality. We select the
well-known optimization algorithms (Bayesian optimization and evolutionary
algorithms) and evaluate them on the benchmark test functions in terms of
quality and computation time. The results show that BO is suitable to be
applied in the optimization tasks that are needed to obtain desired quality in
the limited function evaluations, and the EAs are suitable to search the
optimal of the tasks that are allowed to find the optimal solution with enough
function evaluations. This paper provides the recommendation to select suitable
optimization algorithms for optimization problems with different numbers of
function evaluations, which contributes to the efficiency that obtains the
desired quality with less computation time for optimization problems."
"Artificial intelligence enabled radio propagation for
  communications-Part I: Channel characterization and antenna-channel
  optimization","To provide higher data rates, as well as better coverage, cost efficiency,
security, adaptability, and scalability, the 5G and beyond 5G networks are
developed with various artificial intelligence techniques. In this two-part
paper, we investigate the application of artificial intelligence (AI) and in
particular machine learning (ML) to the study of wireless propagation channels.
It firstly provides a comprehensive overview of ML for channel characterization
and ML-based antenna-channel optimization in this first part, and then it gives
a state-of-the-art literature review of channel scenario identification and
channel modeling in Part II. Fundamental results and key concepts of ML for
communication networks are presented, and widely used ML methods for channel
data processing, propagation channel estimation, and characterization are
analyzed and compared. A discussion of challenges and future research
directions for ML-enabled next generation networks of the topics covered in
this part rounds off the paper."
"Est-ce que vous compute? Code-switching, cultural identity, and AI","Cultural code-switching concerns how we adjust our overall behaviours,
manners of speaking, and appearance in response to a perceived change in our
social environment. We defend the need to investigate cultural code-switching
capacities in artificial intelligence systems. We explore a series of ethical
and epistemic issues that arise when bringing cultural code-switching to bear
on artificial intelligence. Building upon Dotson's (2014) analysis of
testimonial smothering, we discuss how emerging technologies in AI can give
rise to epistemic oppression, and specifically, a form of self-silencing that
we call 'cultural smothering'. By leaving the socio-dynamic features of
cultural code-switching unaddressed, AI systems risk negatively impacting
already-marginalised social groups by widening opportunity gaps and further
entrenching social inequalities."
Dilemma of the Artificial Intelligence Regulatory Landscape,"As a startup company in the autonomous driving space, we have undergone four
years of painful experiences dealing with a broad spectrum of regulatory
requirements. Compared to the software industry norm, which spends 13% of their
overall budget on compliances, we were forced to spend 42% of our budget on
compliances. Our situation is not alone and, in a way, reflects the dilemma of
the artificial intelligence (AI) regulatory landscape. The root cause is the
lack of AI expertise in the legislative and executive branches, leading to a
lack of standardization for the industry to follow. In this article, we share
our first-hand experiences and advocate for the establishment of an FDA-like
agency to regulate AI properly."
"Towards Understanding Human Functional Brain Development with
  Explainable Artificial Intelligence: Challenges and Perspectives","The last decades have seen significant advancements in non-invasive
neuroimaging technologies that have been increasingly adopted to examine human
brain development. However, these improvements have not necessarily been
followed by more sophisticated data analysis measures that are able to explain
the mechanisms underlying functional brain development. For example, the shift
from univariate (single area in the brain) to multivariate (multiple areas in
brain) analysis paradigms is of significance as it allows investigations into
the interactions between different brain regions. However, despite the
potential of multivariate analysis to shed light on the interactions between
developing brain regions, artificial intelligence (AI) techniques applied
render the analysis non-explainable. The purpose of this paper is to understand
the extent to which current state-of-the-art AI techniques can inform
functional brain development. In addition, a review of which AI techniques are
more likely to explain their learning based on the processes of brain
development as defined by developmental cognitive neuroscience (DCN) frameworks
is also undertaken. This work also proposes that eXplainable AI (XAI) may
provide viable methods to investigate functional brain development as
hypothesised by DCN frameworks."
On some Foundational Aspects of Human-Centered Artificial Intelligence,"The burgeoning of AI has prompted recommendations that AI techniques should
be ""human-centered"". However, there is no clear definition of what is meant by
Human Centered Artificial Intelligence, or for short, HCAI. This paper aims to
improve this situation by addressing some foundational aspects of HCAI. To do
so, we introduce the term HCAI agent to refer to any physical or software
computational agent equipped with AI components and that interacts and/or
collaborates with humans. This article identifies five main conceptual
components that participate in an HCAI agent: Observations, Requirements,
Actions, Explanations and Models. We see the notion of HCAI agent, together
with its components and functions, as a way to bridge the technical and
non-technical discussions on human-centered AI. In this paper, we focus our
analysis on scenarios consisting of a single agent operating in dynamic
environments in presence of humans."
Towards a Shapley Value Graph Framework for Medical peer-influence,"eXplainable Artificial Intelligence (XAI) is a sub-field of Artificial
Intelligence (AI) that is at the forefront of AI research. In XAI, feature
attribution methods produce explanations in the form of feature importance.
People often use feature importance as guidance for intervention. However, a
limitation of existing feature attribution methods is that there is a lack of
explanation towards the consequence of intervention. In other words, although
contribution towards a certain prediction is highlighted by feature attribution
methods, the relation between features and the consequence of intervention is
not studied. The aim of this paper is to introduce a new framework, called a
peer influence framework to look deeper into explanations using graph
representation for feature-to-feature interactions to improve the
interpretability of black-box Machine Learning models and inform intervention."
"Recent Trends in Artificial Intelligence-inspired Electronic Thermal
  Management","The rise of computation-based methods in thermal management has gained
immense attention in recent years due to the ability of deep learning to solve
complex 'physics' problems, which are otherwise difficult to be approached
using conventional techniques. Thermal management is required in electronic
systems to keep them from overheating and burning, enhancing their efficiency
and lifespan. For a long time, numerical techniques have been employed to aid
in the thermal management of electronics. However, they come with some
limitations. To increase the effectiveness of traditional numerical approaches
and address the drawbacks faced in conventional approaches, researchers have
looked at using artificial intelligence at various stages of the thermal
management process. The present study discusses in detail, the current uses of
deep learning in the domain of 'electronic' thermal management."
"Formalising Geometric Axioms for Minkowski Spacetime and
  Without-Loss-of-Generality Theorems","This contribution reports on the continued formalisation of an axiomatic
system for Minkowski spacetime (as used in the study of Special Relativity)
which is closer in spirit to Hilbert's axiomatic approach to Euclidean geometry
than to the vector space approach employed by Minkowski. We present a brief
overview of the axioms as well as of a formalisation of theorems relating to
linear order. Proofs and excerpts of Isabelle/Isar scripts are discussed, with
a focus on the use of symmetry and reasoning ""without loss of generality""."
"From Textual Experiments to Experimental Texts: Expressive Repetition in
  ""Artificial Intelligence Literature""","Since the birth of artificial intelligence 70 years ago, attempts at literary
""creation"" with computers are present in the course of technological
development, creating what one might call ""artificial intelligence literature""
(AI literature). Evolving from ""textual experiments"" conducted by technologists
to ""experimental texts"" that explore the possibilities of conceptions of
literature, AI literature integrates primitive problems including machine
thinking, text generation, and machine creativity, which exhibits the two-way
interaction between social ideas and technology. In the early stage, the mutual
support between technological path and artistic ideas turned out to be a
failure, while AI-driven expressive repetitions are made probable in the
contemporary technological context, paving the way for the transformation of AI
literature from proof for technical possibilities to self-verification of
literary value."
"Flood Prediction and Analysis on the Relevance of Features using
  Explainable Artificial Intelligence","This paper presents flood prediction models for the state of Kerala in India
by analyzing the monthly rainfall data and applying machine learning algorithms
including Logistic Regression, K-Nearest Neighbors, Decision Trees, Random
Forests, and Support Vector Machine. Although these models have shown high
accuracy prediction of the occurrence of flood in a particular year, they do
not quantitatively and qualitatively explain the prediction decision. This
paper shows how the background features are learned that contributed to the
prediction decision and further extended to explain the inner workings with the
development of explainable artificial intelligence modules. The obtained
results have confirmed the validity of the findings uncovered by the explainer
modules basing on the historical flood monthly rainfall data in Kerala."
"Maximizing information from chemical engineering data sets: Applications
  to machine learning","It is well-documented how artificial intelligence can have (and already is
having) a big impact on chemical engineering. But classical machine learning
approaches may be weak for many chemical engineering applications. This review
discusses how challenging data characteristics arise in chemical engineering
applications. We identify four characteristics of data arising in chemical
engineering applications that make applying classical artificial intelligence
approaches difficult: (1) high variance, low volume data, (2) low variance,
high volume data, (3) noisy/corrupt/missing data, and (4) restricted data with
physics-based limitations. For each of these four data characteristics, we
discuss applications where these data characteristics arise and show how
current chemical engineering research is extending the fields of data science
and machine learning to incorporate these challenges. Finally, we identify
several challenges for future research."
Computational Metacognition,"Computational metacognition represents a cognitive systems perspective on
high-order reasoning in integrated artificial systems that seeks to leverage
ideas from human metacognition and from metareasoning approaches in artificial
intelligence. The key characteristic is to declaratively represent and then
monitor traces of cognitive activity in an intelligent system in order to
manage the performance of cognition itself. Improvements in cognition then lead
to improvements in behavior and thus performance. We illustrate these concepts
with an agent implementation in a cognitive architecture called MIDCA and show
the value of metacognition in problem-solving. The results illustrate how
computational metacognition improves performance by changing cognition through
meta-level goal operations and learning."
Submodularity In Machine Learning and Artificial Intelligence,"In this manuscript, we offer a gentle review of submodularity and
supermodularity and their properties. We offer a plethora of submodular
definitions; a full description of a number of example submodular functions and
their generalizations; example discrete constraints; a discussion of basic
algorithms for maximization, minimization, and other operations; a brief
overview of continuous submodular extensions; and some historical applications.
We then turn to how submodularity is useful in machine learning and artificial
intelligence. This includes summarization, and we offer a complete account of
the differences between and commonalities amongst sketching, coresets,
extractive and abstractive summarization in NLP, data distillation and
condensation, and data subset selection and feature selection. We discuss a
variety of ways to produce a submodular function useful for machine learning,
including heuristic hand-crafting, learning or approximately learning a
submodular function or aspects thereof, and some advantages of the use of a
submodular function as a coreset producer. We discuss submodular combinatorial
information functions, and how submodularity is useful for clustering, data
partitioning, parallel machine learning, active and semi-supervised learning,
probabilistic modeling, and structured norms and loss functions."
Tractable Boolean and Arithmetic Circuits,"Tractable Boolean and arithmetic circuits have been studied extensively in AI
for over two decades now. These circuits were initially proposed as ""compiled
objects,"" meant to facilitate logical and probabilistic reasoning, as they
permit various types of inference to be performed in linear-time and a
feed-forward fashion like neural networks. In more recent years, the role of
tractable circuits has significantly expanded as they became a computational
and semantical backbone for some approaches that aim to integrate knowledge,
reasoning and learning. In this article, we review the foundations of tractable
circuits and some associated milestones, while focusing on their core
properties and techniques that make them particularly useful for the broad aims
of neuro-symbolic AI."
"Existence and perception as the basis of AGI (Artificial General
  Intelligence)","As is known, AGI (Artificial General Intelligence), unlike AI, should operate
with meanings. And that's what distinguishes it from AI. Any successful AI
implementations (playing chess, unmanned driving, face recognition etc.) do not
operate with the meanings of the processed objects in any way and do not
recognize the meaning. And they don't need to. But for AGI, which emulates
human thinking, this ability is crucial. Numerous attempts to define the
concept of ""meaning"" have one very significant drawback - all such definitions
are not strict and formalized, so they cannot be programmed. The meaning search
procedure should use a formalized description of its existence and possible
forms of its perception. For the practical implementation of AGI, it is
necessary to develop such ""ready-to-code"" descriptions in the context of their
use for processing the related cognitive concepts of ""meaning"" and ""knowledge"".
An attempt to formalize the definition of such concepts is made in this
article."
Maximum Likelihood Uncertainty Estimation: Robustness to Outliers,"We benchmark the robustness of maximum likelihood based uncertainty
estimation methods to outliers in training data for regression tasks. Outliers
or noisy labels in training data results in degraded performances as well as
incorrect estimation of uncertainty. We propose the use of a heavy-tailed
distribution (Laplace distribution) to improve the robustness to outliers. This
property is evaluated using standard regression benchmarks and on a
high-dimensional regression task of monocular depth estimation, both containing
outliers. In particular, heavy-tailed distribution based maximum likelihood
provides better uncertainty estimates, better separation in uncertainty for
out-of-distribution data, as well as better detection of adversarial attacks in
the presence of outliers."
Needs-aware Artificial Intelligence: AI that 'serves [human] needs',"By defining the current limits (and thereby the frontiers), many boundaries
are shaping, and will continue to shape, the future of Artificial Intelligence
(AI). We push on these boundaries in order to make further progress into what
were yesterday's frontiers. They are both pliable and resilient - always
creating new boundaries of what AI can (or should) achieve. Among these are
technical boundaries (such as processing capacity), psychological boundaries
(such as human trust in AI systems), ethical boundaries (such as with AI
weapons), and conceptual boundaries (such as the AI people can imagine). It is
within this final category while it can play a fundamental role in all other
boundaries} that we find the construct of needs and the limitations that our
current concept of need places on the future AI."
"Artificial Intelligence-Based Smart Grid Vulnerabilities and Potential
  Solutions for Fake-Normal Attacks: A Short Review","Smart grid systems are critical to the power industry, however their
sophisticated architectural design and operations expose them to a number of
cybersecurity threats, such as data tampering, data eavesdropping, and Denial
of Service, among others. Artificial Intelligence (AI)-based technologies are
becoming increasingly popular for detecting cyber assaults in a variety of
computer settings, and several efforts have been made to secure various
systems. The present AI systems are being exposed and vanquished because of the
recent emergence of sophisticated adversarial systems such as Generative
Adversarial Networks (GAN). The purpose of this short review is to outline some
of the initiatives to protect smart grid systems, their obstacles, and what
might be a potential future AI research direction"
"Trustworthy Autonomous Systems (TAS): Engaging TAS experts in curriculum
  design","Recent advances in artificial intelligence, specifically machine learning,
contributed positively to enhancing the autonomous systems industry, along with
introducing social, technical, legal and ethical challenges to make them
trustworthy. Although Trustworthy Autonomous Systems (TAS) is an established
and growing research direction that has been discussed in multiple disciplines,
e.g., Artificial Intelligence, Human-Computer Interaction, Law, and Psychology.
The impact of TAS on education curricula and required skills for future TAS
engineers has rarely been discussed in the literature. This study brings
together the collective insights from a number of TAS leading experts to
highlight significant challenges for curriculum design and potential TAS
required skills posed by the rapid emergence of TAS. Our analysis is of
interest not only to the TAS education community but also to other researchers,
as it offers ways to guide future research toward operationalising TAS
education."
"A Real-time System for Detecting Landslide Reports on Social Media using
  Artificial Intelligence","This paper presents an online system that leverages social media data in real
time to identify landslide-related information automatically using
state-of-the-art artificial intelligence techniques. The designed system can
(i) reduce the information overload by eliminating duplicate and irrelevant
content, (ii) identify landslide images, (iii) infer geolocation of the images,
and (iv) categorize the user type (organization or person) of the account
sharing the information. The system was deployed in February 2020 online at
https://landslide-aidr.qcri.org/landslide_system.php to monitor live Twitter
data stream and has been running continuously since then to provide
time-critical information to partners such as British Geological Survey and
European Mediterranean Seismological Centre. We trust this system can both
contribute to harvesting of global landslide data for further research and
support global landslide maps to facilitate emergency response and decision
making."
Point Cloud Generation with Continuous Conditioning,"Generative models can be used to synthesize 3D objects of high quality and
diversity. However, there is typically no control over the properties of the
generated object.This paper proposes a novel generative adversarial network
(GAN) setup that generates 3D point cloud shapes conditioned on a continuous
parameter. In an exemplary application, we use this to guide the generative
process to create a 3D object with a custom-fit shape. We formulate this
generation process in a multi-task setting by using the concept of auxiliary
classifier GANs. Further, we propose to sample the generator label input for
training from a kernel density estimation (KDE) of the dataset. Our ablations
show that this leads to significant performance increase in regions with few
samples. Extensive quantitative and qualitative experiments show that we gain
explicit control over the object dimensions while maintaining good generation
quality and diversity."
Brain Principles Programming,"In the monograph, STRONG ARTIFICIAL INTELLIGENCE. On the Approaches to
Superintelligence, published by Sberbank, provides a cross-disciplinary review
of general artificial intelligence. As an anthropomorphic direction of
research, it considers Brain Principles Programming, BPP) the formalization of
universal mechanisms (principles) of the brain's work with information, which
are implemented at all levels of the organization of nervous tissue. This
monograph provides a formalization of these principles in terms of the category
theory. However, this formalization is not enough to develop algorithms for
working with information. In this paper, for the description and modeling of
Brain Principles Programming, it is proposed to apply mathematical models and
algorithms developed by us earlier that model cognitive functions, which are
based on well-known physiological, psychological and other natural science
theories. The paper uses mathematical models and algorithms of the following
theories: P.K.Anokhin's Theory of Functional Brain Systems, Eleonor Rosh's
prototypical categorization theory, Bob Rehter's theory of causal models and
natural classification. As a result, the formalization of the BPP is obtained
and computer examples are given that demonstrate the algorithm's operation."
The dangers in algorithms learning humans' values and irrationalities,"For an artificial intelligence (AI) to be aligned with human values (or human
preferences), it must first learn those values. AI systems that are trained on
human behavior, risk miscategorising human irrationalities as human values --
and then optimising for these irrationalities. Simply learning human values
still carries risks: AI learning them will inevitably also gain information on
human irrationalities and human behaviour/policy. Both of these can be
dangerous: knowing human policy allows an AI to become generically more
powerful (whether it is partially aligned or not aligned at all), while
learning human irrationalities allows it to exploit humans without needing to
provide value in return. This paper analyses the danger in developing
artificial intelligence that learns about human irrationalities and human
policy, and constructs a model recommendation system with various levels of
information about human biases, human policy, and human values. It concludes
that, whatever the power and knowledge of the AI, it is more dangerous for it
to know human irrationalities than human values. Thus it is better for the AI
to learn human values directly, rather than learning human biases and then
deducing values from behaviour."
"Compliance Challenges in Forensic Image Analysis Under the Artificial
  Intelligence Act","In many applications of forensic image analysis, state-of-the-art results are
nowadays achieved with machine learning methods. However, concerns about their
reliability and opaqueness raise the question whether such methods can be used
in criminal investigations. So far, this question of legal compliance has
hardly been discussed, also because legal regulations for machine learning
methods were not defined explicitly. To this end, the European Commission
recently proposed the artificial intelligence (AI) act, a regulatory framework
for the trustworthy use of AI. Under the draft AI act, high-risk AI systems for
use in law enforcement are permitted but subject to compliance with mandatory
requirements. In this paper, we review why the use of machine learning in
forensic image analysis is classified as high-risk. We then summarize the
mandatory requirements for high-risk AI systems and discuss these requirements
in light of two forensic applications, license plate recognition and deep fake
detection. The goal of this paper is to raise awareness of the upcoming legal
requirements and to point out avenues for future research."
"Towards a Responsible AI Development Lifecycle: Lessons From Information
  Security","Legislation and public sentiment throughout the world have promoted fairness
metrics, explainability, and interpretability as prescriptions for the
responsible development of ethical artificial intelligence systems. Despite the
importance of these three pillars in the foundation of the field, they can be
challenging to operationalize and attempts to solve the problems in production
environments often feel Sisyphean. This difficulty stems from a number of
factors: fairness metrics are computationally difficult to incorporate into
training and rarely alleviate all of the harms perpetrated by these systems.
Interpretability and explainability can be gamed to appear fair, may
inadvertently reduce the privacy of personal information contained in training
data, and increase user confidence in predictions -- even when the explanations
are wrong. In this work, we propose a framework for responsibly developing
artificial intelligence systems by incorporating lessons from the field of
information security and the secure development lifecycle to overcome
challenges associated with protecting users in adversarial settings. In
particular, we propose leveraging the concepts of threat modeling, design
review, penetration testing, and incident response in the context of developing
AI systems as ways to resolve shortcomings in the aforementioned methods."
"Piloting Diversity and Inclusion Workshops in Artificial Intelligence
  and Robotics for Children","In this paper, we present preliminary work from a pilot workshop that aimed
to promote diversity and inclusion for fundamentals of Artificial Intelligence
and Robotics for Children (air4children) in the context of developing
countries. Considering the scarcity of funding and the little to none
availability of specialised professionals to teach AI and robotics in
developing countries, we present resources based on free open-source hardware
and software, open educational resources, and alternative education programs.
That said, the contribution of this work is the pilot workshop of four lessons
that promote diversity and inclusion on teaching AI and Robotics for children
to a small gender-balanced sample of 14 children of an average age of 7.64
years old. We conclude that participant, instructors, coordinators and parents
engaged well in the pilot workshop noting the various challenges of having the
right resources for the workshops in developing countries and posing future
work. The resources to reproduce this work are available at
https://github.com/air4children/hri2022."
Needs and Artificial Intelligence,"Throughout their history, homo sapiens have used technologies to better
satisfy their needs. The relation between needs and technology is so
fundamental that the US National Research Council defined the distinguishing
characteristic of technology as its goal ""to make modifications in the world to
meet human needs"". Artificial intelligence (AI) is one of the most promising
emerging technologies of our time. Similar to other technologies, AI is
expected ""to meet [human] needs"". In this article, we reflect on the
relationship between needs and AI, and call for the realisation of needs-aware
AI systems. We argue that re-thinking needs for, through, and by AI can be a
very useful means towards the development of realistic approaches for
Sustainable, Human-centric, Accountable, Lawful, and Ethical (HALE) AI systems.
We discuss some of the most critical gaps, barriers, enablers, and drivers of
co-creating future AI-based socio-technical systems in which [human] needs are
well considered and met. Finally, we provide an overview of potential threats
and HALE considerations that should be carefully taken into account, and call
for joint, immediate, and interdisciplinary efforts and collaborations."
"An Online Semantic Mapping System for Extending and Enhancing Visual
  SLAM","We present a real-time semantic mapping approach for mobile vision systems
with a 2D to 3D object detection pipeline and rapid data association for
generated landmarks. Besides the semantic map enrichment the associated
detections are further introduced as semantic constraints into a simultaneous
localization and mapping (SLAM) system for pose correction purposes. This way,
we are able generate additional meaningful information that allows to achieve
higher-level tasks, while simultaneously leveraging the view-invariance of
object detections to improve the accuracy and the robustness of the odometry
estimation. We propose tracklets of locally associated object observations to
handle ambiguous and false predictions and an uncertainty-based greedy
association scheme for an accelerated processing time. Our system reaches
real-time capabilities with an average iteration duration of 65~ms and is able
to improve the pose estimation of a state-of-the-art SLAM by up to 68% on a
public dataset. Additionally, we implemented our approach as a modular ROS
package that makes it straightforward for integration in arbitrary graph-based
SLAM methods."
"Mapping global dynamics of benchmark creation and saturation in
  artificial intelligence","Benchmarks are crucial to measuring and steering progress in artificial
intelligence (AI). However, recent studies raised concerns over the state of AI
benchmarking, reporting issues such as benchmark overfitting, benchmark
saturation and increasing centralization of benchmark dataset creation. To
facilitate monitoring of the health of the AI benchmarking ecosystem, we
introduce methodologies for creating condensed maps of the global dynamics of
benchmark creation and saturation. We curated data for 3765 benchmarks covering
the entire domains of computer vision and natural language processing, and show
that a large fraction of benchmarks quickly trended towards near-saturation,
that many benchmarks fail to find widespread utilization, and that benchmark
performance gains for different AI tasks were prone to unforeseen bursts. We
analyze attributes associated with benchmark popularity, and conclude that
future benchmarks should emphasize versatility, breadth and real-world utility."
"Attack Analysis of Face Recognition Authentication Systems Using Fast
  Gradient Sign Method","Biometric authentication methods, representing the ""something you are""
scheme, are considered the most secure approach for gaining access to protected
resources. Recent attacks using Machine Learning techniques demand a serious
systematic reevaluation of biometric authentication. This paper analyzes and
presents the Fast Gradient Sign Method (FGSM) attack using face recognition for
biometric authentication. Machine Learning techniques have been used to train
and test the model, which can classify and identify different people's faces
and which will be used as a target for carrying out the attack. Furthermore,
the case study will analyze the implementation of the FGSM and the level of
performance reduction that the model will have by applying this method in
attacking. The test results were performed with the change of parameters both
in terms of training and attacking the model, thus showing the efficiency of
applying the FGSM."
A Survey on Infrared Image and Video Sets,"In this survey, we compile a list of publicly available infrared image and
video sets for artificial intelligence and computer vision researchers. We
mainly focus on IR image and video sets which are collected and labelled for
computer vision applications such as object detection, object segmentation,
classification, and motion detection. We categorize 92 different publicly
available or private sets according to their sensor types, image resolution,
and scale. We describe each and every set in detail regarding their collection
purpose, operation environment, optical system properties, and area of
application. We also cover a general overview of fundamental concepts that
relate to IR imagery, such as IR radiation, IR detectors, IR optics and
application fields. We analyse the statistical significance of the entire
corpus from different perspectives. We believe that this survey will be a
guideline for computer vision and artificial intelligence researchers that are
interested in working with the spectra beyond the visible domain."
"Recognising the importance of preference change: A call for a
  coordinated multidisciplinary research effort in the age of AI","As artificial intelligence becomes more powerful and a ubiquitous presence in
daily life, it is imperative to understand and manage the impact of AI systems
on our lives and decisions. Modern ML systems often change user behavior (e.g.
personalized recommender systems learn user preferences to deliver
recommendations that change online behavior). An externality of behavior change
is preference change. This article argues for the establishment of a
multidisciplinary endeavor focused on understanding how AI systems change
preference: Preference Science. We operationalize preference to incorporate
concepts from various disciplines, outlining the importance of meta-preferences
and preference-change preferences, and proposing a preliminary framework for
how preferences change. We draw a distinction between preference change,
permissible preference change, and outright preference manipulation. A
diversity of disciplines contribute unique insights to this framework."
Certified Symmetry and Dominance Breaking for Combinatorial Optimisation,"Symmetry and dominance breaking can be crucial for solving hard combinatorial
search and optimisation problems, but the correctness of these techniques
sometimes relies on subtle arguments. For this reason, it is desirable to
produce efficient, machine-verifiable certificates that solutions have been
computed correctly. Building on the cutting planes proof system, we develop a
certification method for optimisation problems in which symmetry and dominance
breaking are easily expressible. Our experimental evaluation demonstrates that
we can efficiently verify fully general symmetry breaking in Boolean
satisfiability (SAT) solving, thus providing, for the first time, a unified
method to certify a range of advanced SAT techniques that also includes XOR and
cardinality reasoning. In addition, we apply our method to maximum clique
solving and constraint programming as a proof of concept that the approach
applies to a wider range of combinatorial problems."
Concept Embedding Analysis: A Review,"Deep neural networks (DNNs) have found their way into many applications with
potential impact on the safety, security, and fairness of
human-machine-systems. Such require basic understanding and sufficient trust by
the users. This motivated the research field of explainable artificial
intelligence (XAI), i.e. finding methods for opening the ""black-boxes"" DNNs
represent. For the computer vision domain in specific, practical assessment of
DNNs requires a globally valid association of human interpretable concepts with
internals of the model. The research field of concept (embedding) analysis (CA)
tackles this problem: CA aims to find global, assessable associations of
humanly interpretable semantic concepts (e.g., eye, bearded) with internal
representations of a DNN. This work establishes a general definition of CA and
a taxonomy for CA methods, uniting several ideas from literature. That allows
to easily position and compare CA approaches. Guided by the defined notions,
the current state-of-the-art research regarding CA methods and interesting
applications are reviewed. More than thirty relevant methods are discussed,
compared, and categorized. Finally, for practitioners, a survey of fifteen
datasets is provided that have been used for supervised concept analysis. Open
challenges and research directions are pointed out at the end."
WawPart: Workload-Aware Partitioning of Knowledge Graphs,"Large-scale datasets in the form of knowledge graphs are often used in
numerous domains, today. A knowledge graphs size often exceeds the capacity of
a single computer system, especially if the graph must be stored in main
memory. To overcome this, knowledge graphs can be partitioned into multiple
sub-graphs and distributed as shards among many computing nodes. However,
performance of many common tasks performed on graphs, such as querying,
suffers, as a result. This is due to distributed joins mandated by graph edges
crossing (cutting) the partitions. In this paper, we propose a method of
knowledge graph partitioning that takes into account a set of queries
(workload). The resulting partitioning aims to reduces the number of
distributed joins and improve the workload performance. Critical features
identified in the query workload and the knowledge graph are used to cluster
the queries and then partition the graph. Queries are rewritten to account for
the graph partitioning. Our evaluation results demonstrate the performance
improvement in workload processing time."
"Spatiotemporal Patterns in Neurobiology: An Overview for Future
  Artificial Intelligence","In recent years, there has been increasing interest in developing models and
tools to address the complex patterns of connectivity found in brain tissue.
Specifically, this is due to a need to understand how emergent properties
emerge from these network structures at multiple spatiotemporal scales. We
argue that computational models are key tools for elucidating the possible
functionalities that can emerge from interactions of heterogeneous neurons
connected by complex networks on multi-scale temporal and spatial domains. Here
we review several classes of models including spiking neurons, integrate and
fire neurons with short term plasticity (STP), conductance based
integrate-and-fire models with STP, and population density neural field (PDNF)
models using simple examples with emphasis on neuroscience applications while
also providing some potential future research directions for AI. These
computational approaches allow us to explore the impact of changing underlying
mechanisms on resulting network function both experimentally as well as
theoretically. Thus we hope these studies will inform future developments in
artificial intelligence algorithms as well as help validate our understanding
of brain processes based on experiments in animals or humans."
"Artificial Intelligence: Framework of driving triggers to past, present
  and future applications and influencers of industry sector adoption","To gain a sense of the development of Artificial Intelligence (AI), this
research analyzes what has been done in the past, presently in the last decade
and what is predicted for the next several decades. The paper will highlight
the biggest changes in AI and give examples of how these technologies are
applied in several key industry sectors along with influencers that can affect
adoption speed. Lastly, the research examines the driving triggers such as
cost, speed, accuracy, diversity/inclusion and interdisciplinary
research/collaboration that propel AI into an essential transformative
technology."
"Artificial Intelligence Software Structured to Simulate Human Working
  Memory, Mental Imagery, and Mental Continuity","This article presents an artificial intelligence (AI) architecture intended
to simulate the human working memory system as well as the manner in which it
is updated iteratively. It features several interconnected neural networks
designed to emulate the specialized modules of the cerebral cortex. These are
structured hierarchically and integrated into a global workspace. They are
capable of temporarily maintaining high-level patterns akin to the
psychological items maintained in working memory. This maintenance is made
possible by persistent neural activity in the form of two modalities: sustained
neural firing (resulting in a focus of attention) and synaptic potentiation
(resulting in a short-term store). This persistent activity is updated
iteratively resulting in incremental changes to the content of the working
memory system. As the content stored in working memory gradually evolves,
successive states overlap and are continuous with one another. The present
article will explore how this architecture can lead to gradual shift in the
distribution of coactive representations, ultimately leading to mental
continuity between processing states, and thus to human-like cognition."
"A Survey and Perspective on Artificial Intelligence for Security-Aware
  Electronic Design Automation","Artificial intelligence (AI) and machine learning (ML) techniques have been
increasingly used in several fields to improve performance and the level of
automation. In recent years, this use has exponentially increased due to the
advancement of high-performance computing and the ever increasing size of data.
One of such fields is that of hardware design; specifically the design of
digital and analog integrated circuits~(ICs), where AI/ ML techniques have been
extensively used to address ever-increasing design complexity, aggressive
time-to-market, and the growing number of ubiquitous interconnected devices
(IoT). However, the security concerns and issues related to IC design have been
highly overlooked. In this paper, we summarize the state-of-the-art in AL/ML
for circuit design/optimization, security and engineering challenges, research
in security-aware CAD/EDA, and future research directions and needs for using
AI/ML for security-aware circuit design."
Usage-based learning of grammatical categories,"Human languages use a wide range of grammatical categories to constrain which
words or phrases can fill certain slots in grammatical patterns and to express
additional meanings, such as tense or aspect, through morpho-syntactic means.
These grammatical categories, which are most often language-specific and
changing over time, are difficult to define and learn. This paper raises the
question how these categories can be acquired and where they have come from. We
explore a usage-based approach. This means that categories and grammatical
constructions are selected and aligned by their success in language
interactions. We report on a multi-agent experiment in which agents are endowed
with mechanisms for understanding and producing utterances as well as
mechanisms for expanding their inventories using a meta-level learning process
based on pro- and anti-unification. We show that a categorial type network
which has scores based on the success in a language interaction leads to the
spontaneous formation of grammatical categories in tandem with the formation of
grammatical patterns."
"Facilitating automated conversion of scientific knowledge into
  scientific simulation models with the Machine Assisted Generation,
  Calibration, and Comparison (MAGCC) Framework","The Machine Assisted Generation, Comparison, and Calibration (MAGCC)
framework provides machine assistance and automation of recurrent crucial steps
and processes in the development, implementation, testing, and use of
scientific simulation models. MAGCC bridges systems for knowledge extraction
via natural language processing or extracted from existing mathematical models
and provides a comprehensive workflow encompassing the composition of
scientific models and artificial intelligence (AI) assisted code generation.
MAGCC accomplishes this through: 1) the development of a comprehensively
expressive formal knowledge representation knowledgebase, the Structured
Scientific Knowledge Representation (SSKR) that encompasses all the types of
information needed to make any simulation model, 2) the use of an artificially
intelligent logic reasoning system, the Computational Modeling Assistant (CMA),
that takes information from the SSKR and generates, in a traceable fashion,
model specifications across a range of simulation modeling methods, and 3) the
use of the CMA to generate executable code for a simulation model from those
model specifications. The MAGCC framework can be customized any scientific
domain, and future work will integrate newly developed code-generating AI
systems."
A Survey on Word Meta-Embedding Learning,"Meta-embedding (ME) learning is an emerging approach that attempts to learn
more accurate word embeddings given existing (source) word embeddings as the
sole input.
  Due to their ability to incorporate semantics from multiple source embeddings
in a compact manner with superior performance, ME learning has gained
popularity among practitioners in NLP.
  To the best of our knowledge, there exist no prior systematic survey on ME
learning and this paper attempts to fill this need.
  We classify ME learning methods according to multiple factors such as whether
they (a) operate on static or contextualised embeddings, (b) trained in an
unsupervised manner or (c) fine-tuned for a particular task/domain.
  Moreover, we discuss the limitations of existing ME learning methods and
highlight potential future research directions."
"AI-Assisted Authentication: State of the Art, Taxonomy and Future
  Roadmap","Artificial Intelligence (AI) has found its applications in a variety of
environments ranging from data science to cybersecurity. AI helps break through
the limitations of traditional algorithms and provides more efficient and
flexible methods for solving problems. In this paper, we focus on the
applications of artificial intelligence in authentication, which is used in a
wide range of scenarios including facial recognition to access buildings,
keystroke dynamics to unlock smartphones. With the emerging AI-assisted
authentication schemes, our comprehensive survey provides an overall
understanding on a high level, which paves the way for future research in this
area. In contrast to other relevant surveys, our research is the first of its
kind to focus on the roles of AI in authentication."
"Visual Knowledge Discovery with Artificial Intelligence: Challenges and
  Future Directions","This volume is devoted to the emerging field of Integrated Visual Knowledge
Discovery that combines advances in Artificial Intelligence/Machine Learning
(AI/ML) and Visualization/Visual Analytics. Chapters included are extended
versions of the selected AI and Visual Analytics papers and related symposia at
the recent International Information Visualization Conferences (IV2019 and
IV2020). AI/ML face a long-standing challenge of explaining models to humans.
Models explanation is fundamentally human activity, not only an algorithmic
one. In this chapter we aim to present challenges and future directions within
the field of Visual Analytics, Visual Knowledge Discovery and AI/ML, and to
discuss the role of visualization in visual AI/ML. In addition, we describe
progress in emerging Full 2D ML, natural language processing, and AI/ML in
multidimensional data aided by visual means."
"A Meta-Analysis of the Utility of Explainable Artificial Intelligence in
  Human-AI Decision-Making","Research in artificial intelligence (AI)-assisted decision-making is
experiencing tremendous growth with a constantly rising number of studies
evaluating the effect of AI with and without techniques from the field of
explainable AI (XAI) on human decision-making performance. However, as tasks
and experimental setups vary due to different objectives, some studies report
improved user decision-making performance through XAI, while others report only
negligible effects. Therefore, in this article, we present an initial synthesis
of existing research on XAI studies using a statistical meta-analysis to derive
implications across existing research. We observe a statistically positive
impact of XAI on users' performance. Additionally, the first results indicate
that human-AI decision-making tends to yield better task performance on text
data. However, we find no effect of explanations on users' performance compared
to sole AI predictions. Our initial synthesis gives rise to future research
investigating the underlying causes and contributes to further developing
algorithms that effectively benefit human decision-makers by providing
meaningful explanations."
Responsible Artificial Intelligence -- from Principles to Practice,"The impact of Artificial Intelligence does not depend only on fundamental
research and technological developments, but for a large part on how these
systems are introduced into society and used in everyday situations. AI is
changing the way we work, live and solve challenges but concerns about
fairness, transparency or privacy are also growing. Ensuring responsible,
ethical AI is more than designing systems whose result can be trusted. It is
about the way we design them, why we design them, and who is involved in
designing them. In order to develop and use AI responsibly, we need to work
towards technical, societal, institutional and legal methods and tools which
provide concrete support to AI practitioners, as well as awareness and training
to enable participation of all, to ensure the alignment of AI systems with our
societies' principles and values."
"A Deep Learning Approach for Automatic Detection of Qualitative Features
  of Lecturing","Artificial Intelligence in higher education opens new possibilities for
improving the lecturing process, such as enriching didactic materials, helping
in assessing students' works or even providing directions to the teachers on
how to enhance the lectures. We follow this research path, and in this work, we
explore how an academic lecture can be assessed automatically by quantitative
features. First, we prepare a set of qualitative features based on teaching
practices and then annotate the dataset of academic lecture videos collected
for this purpose. We then show how these features could be detected
automatically using machine learning and computer vision techniques. Our
results show the potential usefulness of our work."
"Formalization of the principles of brain Programming (Brain Principles
  Programming)","In the monograph ""Strong artificial intelligence. On the Approaches to
Superintelligence"" contains an overview of general artificial intelligence
(AGI). As an anthropomorphic research area, it includes Brain Principles
Programming (BPP) -- the formalization of universal mechanisms (principles) of
the brain work with information, which are implemented at all levels of the
organization of nervous tissue. This monograph contains a formalization of
these principles in terms of category theory. However, this formalization is
not enough to develop algorithms for working with information. In this paper,
for the description and modeling of BPP, it is proposed to apply mathematical
models and algorithms developed earlier, which modeling cognitive functions and
base on well-known physiological, psychological and other natural science
theories. The paper uses mathematical models and algorithms of the following
theories: P.K.Anokhin Theory of Functional Brain Systems, Eleanor Rosch
prototypical categorization theory, Bob Rehder theory of causal models and
""natural"" classification. As a result, a formalization of BPP is obtained and
computer experiments demonstrating the operation of algorithms are presented."
AAM-Gym: Artificial Intelligence Testbed for Advanced Air Mobility,"We introduce AAM-Gym, a research and development testbed for Advanced Air
Mobility (AAM). AAM has the potential to revolutionize travel by reducing
ground traffic and emissions by leveraging new types of aircraft such as
electric vertical take-off and landing (eVTOL) aircraft and new advanced
artificial intelligence (AI) algorithms. Validation of AI algorithms require
representative AAM scenarios, as well as a fast time simulation testbed to
evaluate their performance. Until now, there has been no such testbed available
for AAM to enable a common research platform for individuals in government,
industry, or academia. MIT Lincoln Laboratory has developed AAM-Gym to address
this gap by providing an ecosystem to develop, train, and validate new and
established AI algorithms across a wide variety of AAM use-cases. In this
paper, we use AAM-Gym to study the performance of two reinforcement learning
algorithms on an AAM use-case, separation assurance in AAM corridors. The
performance of the two algorithms is demonstrated based on a series of metrics
provided by AAM-Gym, showing the testbed's utility to AAM research."
Automated Compliance Blueprint Optimization with Artificial Intelligence,"For highly regulated industries such as banking and healthcare, one of the
major hindrances to the adoption of cloud computing is compliance with
regulatory standards. This is a complex problem due to many regulatory and
technical specification (techspec) documents that the companies need to comply
with. The critical problem is to establish the mapping between techspecs and
regulation controls so that from day one, companies can comply with regulations
with minimal effort. We demonstrate the practicality of an approach to
automatically analyze regulatory standards using Artificial Intelligence (AI)
techniques. We present early results to identify the mapping between techspecs
and regulation controls, and discuss challenges that must be overcome for this
solution to be fully practical."
"Navigating Incommensurability Between Ethnomethodology, Conversation
  Analysis, and Artificial Intelligence","Like many research communities, ethnomethodologists and conversation analysts
have begun to get caught up -- yet again -- in the pervasive spectacle of
surging interests in Artificial Intelligence (AI). Inspired by discussions
amongst a growing network of researchers in ethnomethodology (EM) and
conversation analysis (CA) traditions who nurse such interests, I started
thinking about what things EM and the more EM end of conversation analysis
might be doing about, for, or even with, fields of AI research. So, this piece
is about the disciplinary and conceptual questions that might be encountered,
and -- in my view -- may need addressing for engagements with AI research and
its affiliates. Although I'm mostly concerned with things to be aware of as
well as outright dangers, later on we can think about some opportunities. And
throughout I will keep using 'we' to talk about EM&CA researchers; but this
really is for convenience only -- I don't wish to ventriloquise for our complex
research communities. All of the following should be read as emanating from my
particular research history, standpoint etc., and treated (hopefully) as an
invitation for further discussion amongst EM and CA researchers turning to
technology and AI specifically."
"GAN-based Intrinsic Exploration For Sample Efficient Reinforcement
  Learning","In this study, we address the problem of efficient exploration in
reinforcement learning. Most common exploration approaches depend on random
action selection, however these approaches do not work well in environments
with sparse or no rewards. We propose Generative Adversarial Network-based
Intrinsic Reward Module that learns the distribution of the observed states and
sends an intrinsic reward that is computed as high for states that are out of
distribution, in order to lead agent to unexplored states. We evaluate our
approach in Super Mario Bros for a no reward setting and in Montezuma's Revenge
for a sparse reward setting and show that our approach is indeed capable of
exploring efficiently. We discuss a few weaknesses and conclude by discussing
future works."
"""Explanation"" is Not a Technical Term: The Problem of Ambiguity in XAI","There is broad agreement that Artificial Intelligence (AI) systems,
particularly those using Machine Learning (ML), should be able to ""explain""
their behavior. Unfortunately, there is little agreement as to what constitutes
an ""explanation."" This has caused a disconnect between the explanations that
systems produce in service of explainable Artificial Intelligence (XAI) and
those explanations that users and other audiences actually need, which should
be defined by the full spectrum of functional roles, audiences, and
capabilities for explanation. In this paper, we explore the features of
explanations and how to use those features in evaluating their utility. We
focus on the requirements for explanations defined by their functional role,
the knowledge states of users who are trying to understand them, and the
availability of the information needed to generate them. Further, we discuss
the risk of XAI enabling trust in systems without establishing their
trustworthiness and define a critical next step for the field of XAI to
establish metrics to guide and ground the utility of system-generated
explanations."
"Breaking Bad News in the Era of Artificial Intelligence and Algorithmic
  Medicine: An Exploration of Disclosure and its Ethical Justification using
  the Hedonic Calculus","An appropriate ethical framework around the use of Artificial Intelligence
(AI) in healthcare has become a key desirable with the increasingly widespread
deployment of this technology. Advances in AI hold the promise of improving the
precision of outcome prediction at the level of the individual. However, the
addition of these technologies to patient-clinician interactions, as with any
complex human interaction, has potential pitfalls. While physicians have always
had to carefully consider the ethical background and implications of their
actions, detailed deliberations around fast-moving technological progress may
not have kept up. We use a common but key challenge in healthcare interactions,
the disclosure of bad news (likely imminent death), to illustrate how the
philosophical framework of the 'Felicific Calculus' developed in the 18th
century by Jeremy Bentham, may have a timely quasi-quantitative application in
the age of AI. We show how this ethical algorithm can be used to assess, across
seven mutually exclusive and exhaustive domains, whether an AI-supported action
can be morally justified."
"Creating an Explainable Intrusion Detection System Using Self Organizing
  Maps","Modern Artificial Intelligence (AI) enabled Intrusion Detection Systems (IDS)
are complex black boxes. This means that a security analyst will have little to
no explanation or clarification on why an IDS model made a particular
prediction. A potential solution to this problem is to research and develop
Explainable Intrusion Detection Systems (X-IDS) based on current capabilities
in Explainable Artificial Intelligence (XAI). In this paper, we create a Self
Organizing Maps (SOMs) based X-IDS system that is capable of producing
explanatory visualizations. We leverage SOM's explainability to create both
global and local explanations. An analyst can use global explanations to get a
general idea of how a particular IDS model computes predictions. Local
explanations are generated for individual datapoints to explain why a certain
prediction value was computed. Furthermore, our SOM based X-IDS was evaluated
on both explanation generation and traditional accuracy tests using the NSL-KDD
and the CIC-IDS-2017 datasets."
"Discovering Behavioral Predispositions in Data to Improve Human Activity
  Recognition","The automatic, sensor-based assessment of challenging behavior of persons
with dementia is an important task to support the selection of interventions.
However, predicting behaviors like apathy and agitation is challenging due to
the large inter- and intra-patient variability. Goal of this paper is to
improve the recognition performance by making use of the observation that
patients tend to show specific behaviors at certain times of the day or week.
We propose to identify such segments of similar behavior via clustering the
distributions of annotations of the time segments. All time segments within a
cluster then consist of similar behaviors and thus indicate a behavioral
predisposition (BPD). We utilize BPDs by training a classifier for each BPD.
Empirically, we demonstrate that when the BPD per time segment is known,
activity recognition performance can be substantially improved."
Artificial Intelligence and Machine Learning for Quantum Technologies,"In recent years, the dramatic progress in machine learning has begun to
impact many areas of science and technology significantly. In the present
perspective article, we explore how quantum technologies are benefiting from
this revolution. We showcase in illustrative examples how scientists in the
past few years have started to use machine learning and more broadly methods of
artificial intelligence to analyze quantum measurements, estimate the
parameters of quantum devices, discover new quantum experimental setups,
protocols, and feedback strategies, and generally improve aspects of quantum
computing, quantum communication, and quantum simulation. We highlight open
challenges and future possibilities and conclude with some speculative visions
for the next decade."
"Using Sentence Embeddings and Semantic Similarity for Seeking Consensus
  when Assessing Trustworthy AI","Assessing the trustworthiness of artificial intelligence systems requires
knowledge from many different disciplines. These disciplines do not necessarily
share concepts between them and might use words with different meanings, or
even use the same words differently. Additionally, experts from different
disciplines might not be aware of specialized terms readily used in other
disciplines. Therefore, a core challenge of the assessment process is to
identify when experts from different disciplines talk about the same problem
but use different terminologies. In other words, the problem is to group
problem descriptions (a.k.a. issues) with the same semantic meaning but
described using slightly different terminologies.
  In this work, we show how we employed recent advances in natural language
processing, namely sentence embeddings and semantic textual similarity, to
support this identification process and to bridge communication gaps in
interdisciplinary teams of experts assessing the trustworthiness of an
artificial intelligence system used in healthcare."
A Means-End Account of Explainable Artificial Intelligence,"Explainable artificial intelligence (XAI) seeks to produce explanations for
those machine learning methods which are deemed opaque. However, there is
considerable disagreement about what this means and how to achieve it. Authors
disagree on what should be explained (topic), to whom something should be
explained (stakeholder), how something should be explained (instrument), and
why something should be explained (goal). In this paper, I employ insights from
means-end epistemology to structure the field. According to means-end
epistemology, different means ought to be rationally adopted to achieve
different epistemic ends. Applied to XAI, different topics, stakeholders, and
goals thus require different instruments. I call this the means-end account of
XAI. The means-end account has a descriptive and a normative component: on the
one hand, I show how the specific means-end relations give rise to a taxonomy
of existing contributions to the field of XAI; on the other hand, I argue that
the suitability of XAI methods can be assessed by analyzing whether they are
prescribed by a given topic, stakeholder, and goal."
"AI in Telemedicine: An Appraisal on Deep Learning-Based Approaches to
  Virtual Diagnostic Solutions (VDS)","Advancements in Telemedicine as an approach to healthcare delivery have
heralded a new dawn in modern Medicine. Its fast-paced development in our
contemporary society is credence to the advances in Artificial Intelligence and
Information Technology. This paper carries out a descriptive study to broadly
explore AI's implementations in healthcare delivery with a more holistic view
of the usability of various Telemedical Innovations in enhancing Virtual
Diagnostic Solutions (VDS). This research further explores notable developments
in Deep Learning model optimizations for Virtual Diagnostic Solutions. A
further research review on the prospects of Virtual Diagnostic Solutions (VDS)
and foreseeable challenges was also highlighted. Conclusively, this research
gives a general overview of Artificial Intelligence in Telemedicine with a
central focus on Deep Learning-based approaches to Virtual Diagnostic
Solutions."
"Explainable Artificial Intelligence for Assault Sentence Prediction in
  New Zealand","The judiciary has historically been conservative in its use of Artificial
Intelligence, but recent advances in machine learning have prompted scholars to
reconsider such use in tasks like sentence prediction. This paper investigates
by experimentation the potential use of explainable artificial intelligence for
predicting imprisonment sentences in assault cases in New Zealand's courts. We
propose a proof-of-concept explainable model and verify in practice that it is
fit for purpose, with predicted sentences accurate to within one year. We
further analyse the model to understand the most influential phrases in
sentence length prediction. We conclude the paper with an evaluative discussion
of the future benefits and risks of different ways of using such an AI model in
New Zealand's courts."
"Sound and Relatively Complete Belief Hoare Logic for Statistical
  Hypothesis Testing Programs","We propose a new approach to formally describing the requirement for
statistical inference and checking whether a program uses the statistical
method appropriately. Specifically, we define belief Hoare logic (BHL) for
formalizing and reasoning about the statistical beliefs acquired via hypothesis
testing. This program logic is sound and relatively complete with respect to a
Kripke model for hypothesis tests. We demonstrate by examples that BHL is
useful for reasoning about practical issues in hypothesis testing. In our
framework, we clarify the importance of prior beliefs in acquiring statistical
beliefs through hypothesis testing, and discuss the whole picture of the
justification of statistical inference inside and outside the program logic."
FALSE: Fake News Automatic and Lightweight Solution,"Fake news existed ever since there was news, from rumors to printed media
then radio and television. Recently, the information age, with its
communications and Internet breakthroughs, exacerbated the spread of fake news.
Additionally, aside from e-Commerce, the current Internet economy is dependent
on advertisements, views and clicks, which prompted many developers to bait the
end users to click links or ads. Consequently, the wild spread of fake news
through social media networks has impacted real world issues from elections to
5G adoption and the handling of the Covid- 19 pandemic. Efforts to detect and
thwart fake news has been there since the advent of fake news, from fact
checkers to artificial intelligence-based detectors. Solutions are still
evolving as more sophisticated techniques are employed by fake news
propagators. In this paper, R code have been used to study and visualize a
modern fake news dataset. We use clustering, classification, correlation and
various plots to analyze and present the data. The experiments show high
efficiency of classifiers in telling apart real from fake news."
"Vision for Bosnia and Herzegovina in Artificial Intelligence Age: Global
  Trends, Potential Opportunities, Selected Use-cases and Realistic Goals","Artificial Intelligence (AI) is one of the most promising technologies of the
21. century, with an already noticeable impact on society and the economy. With
this work, we provide a short overview of global trends, applications in
industry and selected use-cases from our international experience and work in
industry and academia. The goal is to present global and regional positive
practices and provide an informed opinion on the realistic goals and
opportunities for positioning B&H on the global AI scene."
"Artificial Intelligence-Based Image Reconstruction in Cardiac Magnetic
  Resonance","Artificial intelligence (AI) and Machine Learning (ML) have shown great
potential in improving the medical imaging workflow, from image acquisition and
reconstruction to disease diagnosis and treatment. Particularly, in recent
years, there has been a significant growth in the use of AI and ML algorithms,
especially Deep Learning (DL) based methods, for medical image reconstruction.
DL techniques have shown to be competitive and often superior over conventional
reconstruction methods in terms of both reconstruction quality and
computational efficiency. The use of DL-based image reconstruction also
provides promising opportunities to transform the way cardiac images are
acquired and reconstructed. In this chapter, we will review recent advances in
DL-based reconstruction techniques for cardiac imaging, with emphasis on
cardiac magnetic resonance (CMR) image reconstruction. We mainly focus on
supervised DL methods for the application, including image post-processing
techniques, model-driven approaches and k-space based methods. Current
limitations, challenges and future opportunities of DL for cardiac image
reconstruction are also discussed."
Survey on Applications of Neurosymbolic Artificial Intelligence,"In recent years, the Neurosymbolic framework has attracted a lot of attention
in various applications, from recommender systems and information retrieval to
healthcare and finance. This success is due to its stellar performance combined
with attractive properties, such as learning and reasoning. The new emerging
Neurosymbolic field is currently experiencing a renaissance, as novel
frameworks and algorithms motivated by various practical applications are being
introduced, building on top of the classical neural and reasoning problem
setting. This article aims to provide a comprehensive review of significant
recent developments in real-world applications of Neurosymbolic Artificial
Intelligence. Specifically, we introduce a taxonomy of common Neurosymbolic
applications and summarize the state-of-the-art for each of those domains.
Furthermore, we identify important current trends and provide new perspectives
pertaining to the future of this burgeoning field."
"Mitigating Attacks on Artificial Intelligence-based Spectrum Sensing for
  Cellular Network Signals","Cellular networks (LTE, 5G, and beyond) are dramatically growing with high
demand from consumers and more promising than the other wireless networks with
advanced telecommunication technologies. The main goal of these networks is to
connect billions of devices, systems, and users with high-speed data
transmission, high cell capacity, and low latency, as well as to support a wide
range of new applications, such as virtual reality, metaverse, telehealth,
online education, autonomous and flying vehicles, advanced manufacturing, and
many more. To achieve these goals, spectrum sensing has been paid more
attention, along with new approaches using artificial intelligence (AI) methods
for spectrum management in cellular networks. This paper provides a
vulnerability analysis of spectrum sensing approaches using AI-based semantic
segmentation models for identifying cellular network signals under adversarial
attacks with and without defensive distillation methods. The results showed
that mitigation methods can significantly reduce the vulnerabilities of
AI-based spectrum sensing models against adversarial attacks."
"Artificial Intelligence Models for Cell Type and Subtype Identification
  Based on Single-Cell RNA Sequencing Data in Vision Science","Single-cell RNA sequencing (scRNA-seq) provides a high throughput,
quantitative and unbiased framework for scientists in many research fields to
identify and characterize cell types within heterogeneous cell populations from
various tissues. However, scRNA-seq based identification of discrete cell-types
is still labor intensive and depends on prior molecular knowledge. Artificial
intelligence has provided faster, more accurate, and user-friendly approaches
for cell-type identification. In this review, we discuss recent advances in
cell-type identification methods using artificial intelligence techniques based
on single-cell and single-nucleus RNA sequencing data in vision science."
Learning When to Advise Human Decision Makers,"Artificial intelligence (AI) systems are increasingly used for providing
advice to facilitate human decision making in a wide range of domains, such as
healthcare, criminal justice, and finance. Motivated by limitations of the
current practice where algorithmic advice is provided to human users as a
constant element in the decision-making pipeline, in this paper we raise the
question of when should algorithms provide advice? We propose a novel design of
AI systems in which the algorithm interacts with the human user in a two-sided
manner and aims to provide advice only when it is likely to be beneficial for
the user in making their decision. The results of a large-scale experiment show
that our advising approach manages to provide advice at times of need and to
significantly improve human decision making compared to fixed, non-interactive,
advising approaches. This approach has additional advantages in facilitating
human learning, preserving complementary strengths of human decision makers,
and leading to more positive responsiveness to the advice."
"MEIM: Multi-partition Embedding Interaction Beyond Block Term Format for
  Efficient and Expressive Link Prediction","Knowledge graph embedding aims to predict the missing relations between
entities in knowledge graphs. Tensor-decomposition-based models, such as
ComplEx, provide a good trade-off between efficiency and expressiveness, that
is crucial because of the large size of real world knowledge graphs. The recent
multi-partition embedding interaction (MEI) model subsumes these models by
using the block term tensor format and provides a systematic solution for the
trade-off. However, MEI has several drawbacks, some of which carried from its
subsumed tensor-decomposition-based models. In this paper, we address these
drawbacks and introduce the Multi-partition Embedding Interaction iMproved
beyond block term format (MEIM) model, with independent core tensor for
ensemble effects and soft orthogonality for max-rank mapping, in addition to
multi-partition embedding. MEIM improves expressiveness while still being
highly efficient, helping it to outperform strong baselines and achieve
state-of-the-art results on difficult link prediction benchmarks using fairly
small embedding sizes. The source code is released at
https://github.com/tranhungnghiep/MEIM-KGE."
BIASeD: Bringing Irrationality into Automated System Design,"Human perception, memory and decision-making are impacted by tens of
cognitive biases and heuristics that influence our actions and decisions.
Despite the pervasiveness of such biases, they are generally not leveraged by
today's Artificial Intelligence (AI) systems that model human behavior and
interact with humans. In this theoretical paper, we claim that the future of
human-machine collaboration will entail the development of AI systems that
model, understand and possibly replicate human cognitive biases. We propose the
need for a research agenda on the interplay between human cognitive biases and
Artificial Intelligence. We categorize existing cognitive biases from the
perspective of AI systems, identify three broad areas of interest and outline
research directions for the design of AI systems that have a better
understanding of our own biases."
"The Influence of Explainable Artificial Intelligence: Nudging Behaviour
  or Boosting Capability?","This article aims to provide a theoretical account and corresponding paradigm
for analysing how explainable artificial intelligence (XAI) influences people's
behaviour and cognition. It uses insights from research on behaviour change.
Two notable frameworks for thinking about behaviour change techniques are
nudges - aimed at influencing behaviour - and boosts - aimed at fostering
capability. It proposes that local and concept-based explanations are more
adjacent to nudges, while global and counterfactual explanations are more
adjacent to boosts. It outlines a method for measuring XAI influence and argues
for the benefits of understanding it for optimal, safe and ethical human-AI
collaboration."
The evolution of AI approaches for motor imagery EEG-based BCIs,"The Motor Imagery (MI) electroencephalography (EEG) based Brain Computer
Interfaces (BCIs) allow the direct communication between humans and machines by
exploiting the neural pathways connected to motor imagination. Therefore, these
systems open the possibility of developing applications that could span from
the medical field to the entertainment industry. In this context, Artificial
Intelligence (AI) approaches become of fundamental importance especially when
wanting to provide a correct and coherent feedback to BCI users. Moreover,
publicly available datasets in the field of MI EEG-based BCIs have been widely
exploited to test new techniques from the AI domain. In this work, AI
approaches applied to datasets collected in different years and with different
devices but with coherent experimental paradigms are investigated with the aim
of providing a concise yet sufficiently comprehensive survey on the evolution
and influence of AI techniques on MI EEG-based BCI data."
"On-Premise Artificial Intelligence as a Service for Small and Medium
  Size Setups","Artificial Intelligence (AI) technologies are moving from customized
deployments in specific domains towards generic solutions horizontally
permeating vertical domains and industries. For instance, decisions on when to
perform maintenance of roads or bridges or how to optimize public lighting in
view of costs and safety in smart cities are increasingly informed by AI
models. While various commercial solutions offer user friendly and easy to use
AI as a Service (AIaaS), functionality-wise enabling the democratization of
such ecosystems, open-source equivalent ecosystems are lagging behind. In this
chapter, we discuss AIaaS functionality and corresponding technology stack and
analyze possible realizations using open source user friendly technologies that
are suitable for on-premise set-ups of small and medium sized users allowing
full control over the data and technological platform without any third-party
dependence or vendor lock-in."
Threshold Treewidth and Hypertree Width,"Treewidth and hypertree width have proven to be highly successful structural
parameters in the context of the Constraint Satisfaction Problem (CSP). When
either of these parameters is bounded by a constant, then CSP becomes solvable
in polynomial time. However, here the order of the polynomial in the running
time depends on the width, and this is known to be unavoidable; therefore, the
problem is not fixed-parameter tractable parameterized by either of these width
measures. Here we introduce an enhancement of tree and hypertree width through
a novel notion of thresholds, allowing the associated decompositions to take
into account information about the computational costs associated with solving
the given CSP instance. Aside from introducing these notions, we obtain
efficient theoretical as well as empirical algorithms for computing threshold
treewidth and hypertree width and show that these parameters give rise to
fixed-parameter algorithms for CSP as well as other, more general problems. We
complement our theoretical results with experimental evaluations in terms of
heuristics as well as exact methods based on SAT/SMT encodings."
Review of the state of the art in autonomous artificial intelligence,"This article presents a new design for autonomous artificial intelligence
(AI), based on the state-of-the-art algorithms, and describes a new autonomous
AI system called AutoAI. The methodology is used to assemble the design founded
on self-improved algorithms that use new and emerging sources of data (NEFD).
The objective of the article is to conceptualise the design of a novel AutoAI
algorithm. The conceptual approach is used to advance into building new and
improved algorithms. The article integrates and consolidates the findings from
existing literature and advances the AutoAI design into (1) using new and
emerging sources of data for teaching and training AI algorithms and (2)
enabling AI algorithms to use automated tools for training new and improved
algorithms. This approach is going beyond the state-of-the-art in AI algorithms
and suggests a design that enables autonomous algorithms to self-optimise and
self-adapt, and on a higher level, be capable to self-procreate."
AI-HRI Brings New Dimensions to Human-Aware Design for Human-Aware AI,"Since the first AI-HRI held at the 2014 AAAI Fall Symposium Series, a lot of
the presented research and discussions have emphasized how artificial
intelligence (AI) developments can benefit human-robot interaction (HRI). This
portrays HRI as an application, a source of domain-specific problems to solve,
to the AI community. Likewise, this portrays AI as a tool, a source of
solutions available for relevant problems, to the HRI community. However,
members of the AI-HRI research community will point out that the relationship
has a deeper synergy than matchmaking problems and solutions -- there are
insights from each field that impact how the other one thinks about the world
and performs scientific research. There is no greater opportunity for sharing
perspectives at the moment than human-aware AI, which studies how to account
for the fact that people are more than a source of data or part of an
algorithm. We will explore how AI-HRI can change the way researchers think
about human-aware AI, from observation through validation, to make even the
algorithmic design process human-aware."
Explainability in autonomous pedagogically structured scenarios,"We present the notion of explainability for decision-making processes in a
pedagogically structured autonomous environment. Multi-agent systems that are
structured pedagogically consist of pedagogical teachers and learners that
operate in environments in which both are sometimes not fully aware of all the
states in the environment and beliefs of other agents thus making it
challenging to explain their decisions and actions with one another. This work
emphasises the need for robust and iterative explanation-based communication
between the pedagogical teacher and the learner. Explaining the rationale
behind multi-agent decisions in an interactive, partially observable
environment is necessary to build trustworthy and reliable communication
between pedagogical teachers and learners. Ongoing research is primarily
focused on explanations of the agents' behaviour towards humans, and there is a
lack of research on inter-agent explainability."
Object Segmentation of Cluttered Airborne LiDAR Point Clouds,"Airborne topographic LiDAR is an active remote sensing technology that emits
near-infrared light to map objects on the Earth's surface. Derived products of
LiDAR are suitable to service a wide range of applications because of their
rich three-dimensional spatial information and their capacity to obtain
multiple returns. However, processing point cloud data still requires a
significant effort in manual editing. Certain human-made objects are difficult
to detect because of their variety of shapes, irregularly-distributed point
clouds, and low number of class samples. In this work, we propose an efficient
end-to-end deep learning framework to automatize the detection and segmentation
of objects defined by an arbitrary number of LiDAR points surrounded by
clutter. Our method is based on a light version of PointNet that achieves good
performance on both object recognition and segmentation tasks. The results are
tested against manually delineated power transmission towers and show promising
accuracy."
Formalizing Statistical Causality via Modal Logic,"We propose a formal language for describing and explaining statistical
causality. Concretely, we define Statistical Causality Language (StaCL) for
expressing causal effects and specifying the requirements for causal inference.
StaCL incorporates modal operators for interventions to express causal
properties between probability distributions in different possible worlds in a
Kripke model. We formalize axioms for probability distributions, interventions,
and causal predicates using StaCL formulas. These axioms are expressive enough
to derive the rules of Pearl's do-calculus. Finally, we demonstrate by examples
that StaCL can be used to specify and explain the correctness of statistical
causal inference."
"Towards Human Cognition Level-based Experiment Design for Counterfactual
  Explanations (XAI)","Explainable Artificial Intelligence (XAI) has recently gained a swell of
interest, as many Artificial Intelligence (AI) practitioners and developers are
compelled to rationalize how such AI-based systems work. Decades back, most XAI
systems were developed as knowledge-based or expert systems. These systems
assumed reasoning for the technical description of an explanation, with little
regard for the user's cognitive capabilities. The emphasis of XAI research
appears to have turned to a more pragmatic explanation approach for better
understanding. An extensive area where cognitive science research may
substantially influence XAI advancements is evaluating user knowledge and
feedback, which are essential for XAI system evaluation. To this end, we
propose a framework to experiment with generating and evaluating the
explanations on the grounds of different cognitive levels of understanding. In
this regard, we adopt Bloom's taxonomy, a widely accepted model for assessing
the user's cognitive capability. We utilize the counterfactual explanations as
an explanation-providing medium encompassed with user feedback to validate the
levels of understanding about the explanation at each cognitive level and
improvise the explanation generation methods accordingly."
"Detecting Emerging Technologies in Artificial Intelligence Scientific
  Ecosystem Using an Indicator-based Model","Early identification of emergent topics is of eminent importance due to their
potential impacts on society. There are many methods for detecting emerging
terms and topics, all with advantages and drawbacks. However, there is no
consensus about the attributes and indicators of emergence. In this study, we
evaluate emerging topic detection in the field of artificial intelligence using
a new method to evaluate emergence. We also introduce two new attributes of
collaboration and technological impact which can help us use both paper and
patent information simultaneously. Our results confirm that the proposed new
method can successfully identify the emerging topics in the period of the
study. Moreover, this new method can provide us with the score of each
attribute and a final emergence score, which enable us to rank the emerging
topics with their emergence scores and each attribute score."
"Issues and Challenges in Applications of Artificial Intelligence to
  Nuclear Medicine -- The Bethesda Report (AI Summit 2022)","The SNMMI Artificial Intelligence (SNMMI-AI) Summit, organized by the SNMMI
AI Task Force, took place in Bethesda, MD on March 21-22, 2022. It brought
together various community members and stakeholders from academia, healthcare,
industry, patient representatives, and government (NIH, FDA), and considered
various key themes to envision and facilitate a bright future for routine,
trustworthy use of AI in nuclear medicine. In what follows, essential issues,
challenges, controversies and findings emphasized in the meeting are
summarized."
"Explainable Artificial Intelligence in Construction: The Content,
  Context, Process, Outcome Evaluation Framework","Explainable artificial intelligence is an emerging and evolving concept. Its
impact on construction, though yet to be realised, will be profound in the
foreseeable future. Still, XAI has received limited attention in construction.
As a result, no evaluation frameworks have been propagated to enable
construction organisations to understand the what, why, how, and when of XAI.
Our paper aims to fill this void by developing a content, context, process, and
outcome evaluation framework that can be used to justify the adoption and
effective management of XAI. After introducing and describing this novel
framework, we discuss its implications for future research. While our novel
framework is conceptual, it provides a frame of reference for construction
organisations to make headway toward realising XAI business value and benefits."
"Explainable Artificial Intelligence: Precepts, Methods, and
  Opportunities for Research in Construction","Explainable artificial intelligence has received limited attention in
construction despite its growing importance in various other industrial
sectors. In this paper, we provide a narrative review of XAI to raise awareness
about its potential in construction. Our review develops a taxonomy of the XAI
literature comprising its precepts and approaches. Opportunities for future XAI
research focusing on stakeholder desiderata and data and information fusion are
identified and discussed. We hope the opportunities we suggest stimulate new
lines of inquiry to help alleviate the scepticism and hesitancy toward AI
adoption and integration in construction."
"Power-law Scaling to Assist with Key Challenges in Artificial
  Intelligence","Power-law scaling, a central concept in critical phenomena, is found to be
useful in deep learning, where optimized test errors on handwritten digit
examples converge as a power-law to zero with database size. For rapid decision
making with one training epoch, each example is presented only once to the
trained network, the power-law exponent increased with the number of hidden
layers. For the largest dataset, the obtained test error was estimated to be in
the proximity of state-of-the-art algorithms for large epoch numbers. Power-law
scaling assists with key challenges found in current artificial intelligence
applications and facilitates an a priori dataset size estimation to achieve a
desired test accuracy. It establishes a benchmark for measuring training
complexity and a quantitative hierarchy of machine learning tasks and
algorithms."
"A Combined Approach of Process Mining and Rule-based AI for Study
  Planning and Monitoring in Higher Education","This paper presents an approach of using methods of process mining and
rule-based artificial intelligence to analyze and understand study paths of
students based on campus management system data and study program models.
Process mining techniques are used to characterize successful study paths, as
well as to detect and visualize deviations from expected plans. These insights
are combined with recommendations and requirements of the corresponding study
programs extracted from examination regulations. Here, event calculus and
answer set programming are used to provide models of the study programs which
support planning and conformance checking while providing feedback on possible
study plan violations. In its combination, process mining and rule-based
artificial intelligence are used to support study planning and monitoring by
deriving rules and recommendations for guiding students to more suitable study
paths with higher success rates. Two applications will be implemented, one for
students and one for study program designers."
"A Streamlit-based Artificial Intelligence Trust Platform for
  Next-Generation Wireless Networks","With the rapid development and integration of artificial intelligence (AI)
methods in next-generation networks (NextG), AI algorithms have provided
significant advantages for NextG in terms of frequency spectrum usage,
bandwidth, latency, and security. A key feature of NextG is the integration of
AI, i.e., self-learning architecture based on self-supervised algorithms, to
improve the performance of the network. A secure AI-powered structure is also
expected to protect NextG networks against cyber-attacks. However, AI itself
may be attacked, i.e., model poisoning targeted by attackers, and it results in
cybersecurity violations. This paper proposes an AI trust platform using
Streamlit for NextG networks that allows researchers to evaluate, defend,
certify, and verify their AI models and applications against adversarial
threats of evasion, poisoning, extraction, and interference."
"Understanding the Energy Consumption of HPC Scale Artificial
  Intelligence","This paper contributes towards better understanding the energy consumption
trade-offs of HPC scale Artificial Intelligence (AI), and more specifically
Deep Learning (DL) algorithms. For this task we developed benchmark-tracker, a
benchmark tool to evaluate the speed and energy consumption of DL algorithms in
HPC environments. We exploited hardware counters and Python libraries to
collect energy information through software, which enabled us to instrument a
known AI benchmark tool, and to evaluate the energy consumption of numerous DL
algorithms and models. Through an experimental campaign, we show a case example
of the potential of benchmark-tracker to measure the computing speed and the
energy consumption for training and inference DL algorithms, and also the
potential of Benchmark-Tracker to help better understanding the energy behavior
of DL algorithms in HPC platforms. This work is a step forward to better
understand the energy consumption of Deep Learning in HPC, and it also
contributes with a new tool to help HPC DL developers to better balance the HPC
infrastructure in terms of speed and energy consumption."
A Rubric for Human-like Agents and NeuroAI,"Researchers across cognitive, neuro-, and computer sciences increasingly
reference human-like artificial intelligence and neuroAI. However, the scope
and use of the terms are often inconsistent. Contributed research ranges widely
from mimicking behaviour, to testing machine learning methods as neurally
plausible hypotheses at the cellular or functional levels, or solving
engineering problems. However, it cannot be assumed nor expected that progress
on one of these three goals will automatically translate to progress in others.
Here a simple rubric is proposed to clarify the scope of individual
contributions, grounded in their commitments to human-like behaviour, neural
plausibility, or benchmark/engineering goals. This is clarified using examples
of weak and strong neuroAI and human-like agents, and discussing the
generative, corroborate, and corrective ways in which the three dimensions
interact with one another. The author maintains that future progress in
artificial intelligence will need strong interactions across the disciplines,
with iterative feedback loops and meticulous validity tests, leading to both
known and yet-unknown advances that may span decades to come."
"Generative artificial intelligence-enabled dynamic detection of
  nicotine-related circuits","The identification of addiction-related circuits is critical for explaining
addiction processes and developing addiction treatments. And models of
functional addiction circuits developed from functional imaging are an
effective tool for discovering and verifying addiction circuits. However,
analyzing functional imaging data of addiction and detecting functional
addiction circuits still have challenges. We have developed a data-driven and
end-to-end generative artificial intelligence(AI) framework to address these
difficulties. The framework integrates dynamic brain network modeling and novel
network architecture networks architecture, including temporal graph
Transformer and contrastive learning modules. A complete workflow is formed by
our generative AI framework: the functional imaging data, from neurobiological
experiments, and computational modeling, to end-to-end neural networks, is
transformed into dynamic nicotine addiction-related circuits. It enables the
detection of addiction-related brain circuits with dynamic properties and
reveals the underlying mechanisms of addiction."
"Towards Sustainable Artificial Intelligence: An Overview of
  Environmental Protection Uses and Issues","Artificial Intelligence (AI) is used to create more sustainable production
methods and model climate change, making it a valuable tool in the fight
against environmental degradation. This paper describes the paradox of an
energy-consuming technology serving the ecological challenges of tomorrow. The
study provides an overview of the sectors that use AI-based solutions for
environmental protection. It draws on numerous examples from AI for Green
players to present use cases and concrete examples. In the second part of the
study, the negative impacts of AI on the environment and the emerging
technological solutions to support Green AI are examined. It is also shown that
the research on less energy-consuming AI is motivated more by cost and energy
autonomy constraints than by environmental considerations. This leads to a
rebound effect that favors an increase in the complexity of models. Finally,
the need to integrate environmental indicators into algorithms is discussed.
The environmental dimension is part of the broader ethical problem of AI, and
addressing it is crucial for ensuring the sustainability of AI in the long
term."
Data-Centric Artificial Intelligence,"Data-centric artificial intelligence (data-centric AI) represents an emerging
paradigm emphasizing that the systematic design and engineering of data is
essential for building effective and efficient AI-based systems. The objective
of this article is to introduce practitioners and researchers from the field of
Information Systems (IS) to data-centric AI. We define relevant terms, provide
key characteristics to contrast the data-centric paradigm to the model-centric
one, and introduce a framework for data-centric AI. We distinguish data-centric
AI from related concepts and discuss its longer-term implications for the IS
community."
"A Theoretical Framework for AI Models Explainability with Application in
  Biomedicine","EXplainable Artificial Intelligence (XAI) is a vibrant research topic in the
artificial intelligence community, with growing interest across methods and
domains. Much has been written about the subject, yet XAI still lacks shared
terminology and a framework capable of providing structural soundness to
explanations. In our work, we address these issues by proposing a novel
definition of explanation that is a synthesis of what can be found in the
literature. We recognize that explanations are not atomic but the combination
of evidence stemming from the model and its input-output mapping, and the human
interpretation of this evidence. Furthermore, we fit explanations into the
properties of faithfulness (i.e., the explanation being a true description of
the model's inner workings and decision-making process) and plausibility (i.e.,
how much the explanation looks convincing to the user). Using our proposed
theoretical framework simplifies how these properties are operationalized and
it provides new insight into common explanation methods that we analyze as case
studies."
Mastering Diverse Domains through World Models,"Developing a general algorithm that learns to solve tasks across a wide range
of applications has been a fundamental challenge in artificial intelligence.
Although current reinforcement learning algorithms can be readily applied to
tasks similar to what they have been developed for, configuring them for new
application domains requires significant human expertise and experimentation.
We present DreamerV3, a general algorithm that outperforms specialized methods
across over 150 diverse tasks, with a single configuration. Dreamer learns a
model of the environment and improves its behavior by imagining future
scenarios. Robustness techniques based on normalization, balancing, and
transformations enable stable learning across domains. Applied out of the box,
Dreamer is the first algorithm to collect diamonds in Minecraft from scratch
without human data or curricula. This achievement has been posed as a
significant challenge in artificial intelligence that requires exploring
farsighted strategies from pixels and sparse rewards in an open world. Our work
allows solving challenging control problems without extensive experimentation,
making reinforcement learning broadly applicable."
"Collective Privacy Recovery: Data-sharing Coordination via Decentralized
  Artificial Intelligence","Collective privacy loss becomes a colossal problem, an emergency for personal
freedoms and democracy. But, are we prepared to handle personal data as scarce
resource and collectively share data under the doctrine: as little as possible,
as much as necessary? We hypothesize a significant privacy recovery if a
population of individuals, the data collective, coordinates to share minimum
data for running online services with the required quality. Here we show how to
automate and scale-up complex collective arrangements for privacy recovery
using decentralized artificial intelligence. For this, we compare for first
time attitudinal, intrinsic, rewarded and coordinated data sharing in a
rigorous living-lab experiment of high realism involving >27,000 real data
disclosures. Using causal inference and cluster analysis, we differentiate
criteria predicting privacy and five key data-sharing behaviors. Strikingly,
data-sharing coordination proves to be a win-win for all: remarkable privacy
recovery for people with evident costs reduction for service providers."
"Enactive Artificial Intelligence: Subverting Gender Norms in Robot-Human
  Interaction","This paper introduces Enactive Artificial Intelligence (eAI) as an
intersectional gender-inclusive stance towards AI. AI design is an enacted
human sociocultural practice that reflects human culture and values.
Unrepresentative AI design could lead to social marginalisation. Section 1,
drawing from radical enactivism, outlines embodied cultural practices. In
Section 2, explores how intersectional gender intertwines with technoscience as
a sociocultural practice. Section 3 focuses on subverting gender norms in the
specific case of Robot-Human Interaction in AI. Finally, Section 4 identifies
four vectors of ethics: explainability, fairness, transparency, and
auditability for adopting an intersectionality-inclusive stance in developing
gender-inclusive AI and subverting existing gender norms in robot design."
"Mathematics, word problems, common sense, and artificial intelligence","The paper discusses the capacities and limitations of current artificial
intelligence (AI) technology to solve word problems that combine elementary
knowledge with commonsense reasoning. No existing AI systems can solve these
reliably. We review three approaches that have been developed, using AI natural
language technology: outputting the answer directly, outputting a computer
program that solves the problem, and outputting a formalized representation
that can be input to an automated theorem verifier. We review some benchmarks
that have been developed to evaluate these systems and some experimental
studies. We discuss the limitations of the existing technology at solving these
kinds of problems. We argue that it is not clear whether these kinds of
limitations will be important in developing AI technology for pure mathematical
research, but that they will be important in applications of mathematics, and
may well be important in developing programs capable of reading and
understanding mathematical content written by humans."
Reflective Artificial Intelligence,"Artificial Intelligence (AI) is about making computers that do the sorts of
things that minds can do, and as we progress towards this goal, we tend to
increasingly delegate human tasks to machines. However, AI systems usually do
these tasks with an unusual imbalance of insight and understanding: new, deeper
insights are present, yet many important qualities that a human mind would have
previously brought to the activity are utterly absent. Therefore, it is crucial
to ask which features of minds have we replicated, which are missing, and if
that matters. One core feature that humans bring to tasks, when dealing with
the ambiguity, emergent knowledge, and social context presented by the world,
is reflection. Yet this capability is utterly missing from current mainstream
AI. In this paper we ask what reflective AI might look like. Then, drawing on
notions of reflection in complex systems, cognitive science, and agents, we
sketch an architecture for reflective AI agents, and highlight ways forward."
BRAIN L: A book recommender system,"Book sales in Spain have fallen progressively, which requires urgent changes
to optimize the sales process as much as possible. This research proposes a new
system, called Base of Reasoning in Artificial Intelligence with Natural
Language (BRAIN L) focused exclusively on the publishing industry. The new
field of knowledge of Artificial Intelligence (AI), Natural Language Processing
(NLP), tecnolog\'ia del Machine Learning is combined with Case-Based Reasoning
(CBR) techniques for book recommendations. A model is developed to retrieve
similar cases/books supported by NLP techniques for decision making. In
addition, policies are implemented to keep the model evaluated by expert
reviews, where the system not only learns with new cases, but these cases are
real."
Data augmentation for machine learning of chemical process flowsheets,"Artificial intelligence has great potential for accelerating the design and
engineering of chemical processes. Recently, we have shown that
transformer-based language models can learn to auto-complete chemical process
flowsheets using the SFILES 2.0 string notation. Also, we showed that language
translation models can be used to translate Process Flow Diagrams (PFDs) into
Process and Instrumentation Diagrams (P&IDs). However, artificial intelligence
methods require big data and flowsheet data is currently limited. To mitigate
this challenge of limited data, we propose a new data augmentation methodology
for flowsheet data that is represented in the SFILES 2.0 notation. We show that
the proposed data augmentation improves the performance of artificial
intelligence-based process design models. In our case study flowsheet data
augmentation improved the prediction uncertainty of the flowsheet
autocompletion model by 14.7%. In the future, our flowsheet data augmentation
can be used for other machine learning algorithms on chemical process
flowsheets that are based on SFILES notation."
Plan-Based Derivation of General Functional Structures in Product Design,"In product design, a decomposition of the overall product function into a set
of smaller, interacting functions is usually considered a crucial first step
for any computer-supported design tool. Here, we propose a new approach for the
decomposition of functions especially suited for later solutions based on
Artificial Intelligence. The presented approach defines the decomposition
problem in terms of a planning problem--a well established field in Artificial
Intelligence. For the planning problem, logic-based solvers can be used to find
solutions that compute a useful function structure for the design process.
Well-known function libraries from engineering are used as atomic planning
steps. The algorithms are evaluated using two different application examples to
ensure the transferability of a general function decomposition."
"On the Applicability of Explainable Artificial Intelligence for Software
  Requirement Analysis","The applications of Artificial Intelligence (AI) methods especially machine
learning techniques have increased in recent years. Classification algorithms
have been successfully applied to different problems such as requirement
classification. Although these algorithms have good performance, most of them
cannot explain how they make a decision. Explainable Artificial Intelligence
(XAI) is a set of new techniques that explain the predictions of machine
learning algorithms. In this work, the applicability of XAI for software
requirement classification is studied. An explainable software requirement
classifier is presented using the LIME algorithm. The explainability of the
proposed method is studied by applying it to the PROMISE software requirement
dataset. The results show that XAI can help the analyst or requirement
specifier to better understand why a specific requirement is classified as
functional or non-functional. The important keywords for such decisions are
identified and analyzed in detail. The experimental study shows that the XAI
can be used to help analysts and requirement specifiers to better understand
the predictions of the classifiers for categorizing software requirements.
Also, the effect of the XAI on feature reduction is analyzed. The results
showed that the XAI model has a positive role in feature analysis."
"A novel approach to generate datasets with XAI ground truth to evaluate
  image models","With the increased usage of artificial intelligence (AI), it is imperative to
understand how these models work internally. These needs have led to the
development of a new field called eXplainable artificial intelligence (XAI).
This field consists of on a set of techniques that allows us to theoretically
determine the cause of the AI decisions. One main issue of XAI is how to verify
the works on this field, taking into consideration the lack of ground truth
(GT). In this study, we propose a new method to generate datasets with GT. We
conducted a set of experiments that compared our GT with real model
explanations and obtained excellent results confirming that our proposed method
is correct."
"Human-Centered Responsible Artificial Intelligence: Current & Future
  Trends","In recent years, the CHI community has seen significant growth in research on
Human-Centered Responsible Artificial Intelligence. While different research
communities may use different terminology to discuss similar topics, all of
this work is ultimately aimed at developing AI that benefits humanity while
being grounded in human rights and ethics, and reducing the potential harms of
AI. In this special interest group, we aim to bring together researchers from
academia and industry interested in these topics to map current and future
research trends to advance this important area of research by fostering
collaboration and sharing ideas."
Machine Learning for Cutting Planes in Integer Programming: A Survey,"We survey recent work on machine learning (ML) techniques for selecting
cutting planes (or cuts) in mixed-integer linear programming (MILP). Despite
the availability of various classes of cuts, the task of choosing a set of cuts
to add to the linear programming (LP) relaxation at a given node of the
branch-and-bound (B&B) tree has defied both formal and heuristic solutions to
date. ML offers a promising approach for improving the cut selection process by
using data to identify promising cuts that accelerate the solution of MILP
instances. This paper presents an overview of the topic, highlighting recent
advances in the literature, common approaches to data collection, evaluation,
and ML model architectures. We analyze the empirical results in the literature
in an attempt to quantify the progress that has been made and conclude by
suggesting avenues for future research."
"Artificial Intelligence Impact On The Labour Force -- Searching For The
  Analytical Skills Of The Future Software Engineers","This systematic literature review aims to investigate the impact of
artificial intelligence (AI) on the labour force in software engineering, with
a particular focus on the skills needed for future software engineers, the
impact of AI on the demand for software engineering skills, and the future of
work for software engineers. The review identified 42 relevant publications
through a comprehensive search strategy and analysed their findings. The
results indicate that future software engineers will need to be competent in
programming and have soft skills such as problem-solving and interpersonal
communication. AI will have a significant impact on the software engineering
workforce, with the potential to automate many jobs currently done by software
engineers. The role of a software engineer is changing and will continue to
change in the future, with AI-assisted software development posing challenges
for the software engineering profession. The review suggests that the software
engineering profession must adapt to the changing landscape to remain relevant
and effective in the future."
Artificial Intelligence: 70 Years Down the Road,"Artificial intelligence (AI) has a history of nearly a century from its
inception to the present day. We have summarized the development trends and
discovered universal rules, including both success and failure. We have
analyzed the reasons from both technical and philosophical perspectives to help
understand the reasons behind the past failures and current successes of AI,
and to provide a basis for thinking and exploring future development.
Specifically, we have found that the development of AI in different fields,
including computer vision, natural language processing, and machine learning,
follows a pattern from rules to statistics to data-driven methods. In the face
of past failures and current successes, we need to think systematically about
the reasons behind them. Given the unity of AI between natural and social
sciences, it is necessary to incorporate philosophical thinking to understand
and solve AI problems, and we believe that starting from the dialectical method
of Marx is a feasible path. We have concluded that the sustainable development
direction of AI should be human-machine collaboration and a technology path
centered on computing power. Finally, we have summarized the impact of AI on
society from this trend."
"Requirements Engineering Framework for Human-centered Artificial
  Intelligence Software Systems","[Context] Artificial intelligence (AI) components used in building software
solutions have substantially increased in recent years. However, many of these
solutions focus on technical aspects and ignore critical human-centered
aspects. [Objective] Including human-centered aspects during requirements
engineering (RE) when building AI-based software can help achieve more
responsible, unbiased, and inclusive AI-based software solutions. [Method] In
this paper, we present a new framework developed based on human-centered AI
guidelines and a user survey to aid in collecting requirements for
human-centered AI-based software. We provide a catalog to elicit these
requirements and a conceptual model to present them visually. [Results] The
framework is applied to a case study to elicit and model requirements for
enhancing the quality of 360 degree~videos intended for virtual reality (VR)
users. [Conclusion] We found that our proposed approach helped the project team
fully understand the human-centered needs of the project to deliver.
Furthermore, the framework helped to understand what requirements need to be
captured at the initial stages against later stages in the engineering process
of AI-based software."
A Comparison of Methods for Neural Network Aggregation,"Deep learning has been successful in the theoretical aspect. For deep
learning to succeed in industry, we need to have algorithms capable of handling
many inconsistencies appearing in real data. These inconsistencies can have
large effects on the implementation of a deep learning algorithm. Artificial
Intelligence is currently changing the medical industry. However, receiving
authorization to use medical data for training machine learning algorithms is a
huge hurdle. A possible solution is sharing the data without sharing the
patient information. We propose a multi-party computation protocol for the deep
learning algorithm. The protocol enables to conserve both the privacy and the
security of the training data. Three approaches of neural networks assembly are
analyzed: transfer learning, average ensemble learning, and series network
learning. The results are compared to approaches based on data-sharing in
different experiments. We analyze the security issues of the proposed protocol.
Although the analysis is based on medical data, the results of multi-party
computation of machine learning training are theoretical and can be implemented
in multiple research areas."
"Analysis and Evaluation of Explainable Artificial Intelligence on
  Suicide Risk Assessment","This study investigates the effectiveness of Explainable Artificial
Intelligence (XAI) techniques in predicting suicide risks and identifying the
dominant causes for such behaviours. Data augmentation techniques and ML models
are utilized to predict the associated risk. Furthermore, SHapley Additive
exPlanations (SHAP) and correlation analysis are used to rank the importance of
variables in predictions. Experimental results indicate that Decision Tree
(DT), Random Forest (RF) and eXtreme Gradient Boosting (XGBoost) models achieve
the best results while DT has the best performance with an accuracy of 95:23%
and an Area Under Curve (AUC) of 0.95. As per SHAP results, anger problems,
depression, and social isolation are the leading variables in predicting the
risk of suicide, and patients with good incomes, respected occupations, and
university education have the least risk. Results demonstrate the effectiveness
of machine learning and XAI framework for suicide risk prediction, and they can
assist psychiatrists in understanding complex human behaviours and can also
assist in reliable clinical decision-making."
"Linking Alternative Fuel Vehicles Adoption with Socioeconomic Status and
  Air Quality Index","This is a study on the potential widespread usage of alternative fuel
vehicles, linking them with the socio-economic status of the respective
consumers as well as the impact on the resulting air quality index. Research in
this area aims to leverage machine learning techniques in order to promote
appropriate policies for the proliferation of alternative fuel vehicles such as
electric vehicles with due justice to different population groups. Pearson
correlation coefficient is deployed in the modeling the relationships between
socio-economic data, air quality index and data on alternative fuel vehicles.
Linear regression is used to conduct predictive modeling on air quality index
as per the adoption of alternative fuel vehicles, based on socio-economic
factors. This work exemplifies artificial intelligence for social good."
Artificial Intelligence and Dual Contract,"This paper explores the capacity of artificial intelligence (AI) algorithms
to autonomously design incentive-compatible contracts in dual-principal-agent
settings, a relatively unexplored aspect of algorithmic mechanism design. We
develop a dynamic model where two principals, each equipped with independent
Q-learning algorithms, interact with a single agent. Our findings reveal that
the strategic behavior of AI principals (cooperation vs. competition) hinges
crucially on the alignment of their profits. Notably, greater profit alignment
fosters collusive strategies, yielding higher principal profits at the expense
of agent incentives. This emergent behavior persists across varying degrees of
principal heterogeneity, multiple principals, and environments with
uncertainty. Our study underscores the potential of AI for contract automation
while raising critical concerns regarding strategic manipulation and the
emergence of unintended collusion in AI-driven systems, particularly in the
context of the broader AI alignment problem."
Unfinished Architectures: A Perspective from Artificial Intelligence,"Unfinished buildings are a constant throughout the history of architecture
and have given rise to intense debates on the opportuneness of their
completion, in addition to offering alibis for theorizing about the
compositional possibilities in coherence with the finished parts. The
development of Artificial Intelligence (AI) opens new avenues for the proposal
of possibilities for the completion of unfinished architectures. Specifically,
with the recent appearance of tools such as DALL-E, capable of completing
images guided by a textual description, it is possible to count on the help of
AI for architectural design tasks. In this article we explore the use of these
new AI tools for the completion of unfinished facades of historical temples and
analyse the still germinal stadium in the field of architectural graphic
composition."
"Pitchclass2vec: Symbolic Music Structure Segmentation with Chord
  Embeddings","Structure perception is a fundamental aspect of music cognition in humans.
Historically, the hierarchical organization of music into structures served as
a narrative device for conveying meaning, creating expectancy, and evoking
emotions in the listener. Thereby, musical structures play an essential role in
music composition, as they shape the musical discourse through which the
composer organises his ideas. In this paper, we present a novel music
segmentation method, pitchclass2vec, based on symbolic chord annotations, which
are embedded into continuous vector representations using both natural language
processing techniques and custom-made encodings. Our algorithm is based on
long-short term memory (LSTM) neural network and outperforms the
state-of-the-art techniques based on symbolic chord annotations in the field."
"Viewpoint: A Theoretical Computer Science Perspective on Consciousness
  and Artificial General Intelligence","We have defined the Conscious Turing Machine (CTM) for the purpose of
investigating a Theoretical Computer Science (TCS) approach to consciousness.
For this, we have hewn to the TCS demand for simplicity and understandability.
The CTM is consequently and intentionally a simple machine. It is not a model
of the brain, though its design has greatly benefited - and continues to
benefit - from neuroscience and psychology. The CTM is a model of and for
consciousness.
  Although it is developed to understand consciousness, the CTM offers a
thoughtful and novel guide to the creation of an Artificial General
Intelligence (AGI). For example, the CTM has an enormous number of powerful
processors, some with specialized expertise, others unspecialized but poised to
develop an expertise. For whatever problem must be dealt with, the CTM has an
excellent way to utilize those processors that have the required knowledge,
ability, and time to work on the problem, even if it is not aware of which ones
these may be."
"Optimizing Data Shapley Interaction Calculation from O(2^n) to O(t n^2)
  for KNN models","With the rapid growth of data availability and usage, quantifying the added
value of each training data point has become a crucial process in the field of
artificial intelligence. The Shapley values have been recognized as an
effective method for data valuation, enabling efficient training set
summarization, acquisition, and outlier removal. In this paper, we introduce
""STI-KNN"", an innovative algorithm that calculates the exact pair-interaction
Shapley values for KNN models in O(t n^2) time, which is a significant
improvement over the O(2^n)$ time complexity of baseline methods. By using
STI-KNN, we can efficiently and accurately evaluate the value of individual
data points, leading to improved training outcomes and ultimately enhancing the
effectiveness of artificial intelligence applications."
Learning Multi-Pursuit Evasion for Safe Targeted Navigation of Drones,"Safe navigation of drones in the presence of adversarial physical attacks
from multiple pursuers is a challenging task. This paper proposes a novel
approach, asynchronous multi-stage deep reinforcement learning (AMS-DRL), to
train adversarial neural networks that can learn from the actions of multiple
evolved pursuers and adapt quickly to their behavior, enabling the drone to
avoid attacks and reach its target. Specifically, AMS-DRL evolves adversarial
agents in a pursuit-evasion game where the pursuers and the evader are
asynchronously trained in a bipartite graph way during multiple stages. Our
approach guarantees convergence by ensuring Nash equilibrium among agents from
the game-theory analysis. We evaluate our method in extensive simulations and
show that it outperforms baselines with higher navigation success rates. We
also analyze how parameters such as the relative maximum speed affect
navigation performance. Furthermore, we have conducted physical experiments and
validated the effectiveness of the trained policies in real-time flights. A
success rate heatmap is introduced to elucidate how spatial geometry influences
navigation outcomes. Project website:
https://github.com/NTU-ICG/AMS-DRL-for-Pursuit-Evasion."
Ontology for Healthcare Artificial Intelligence Privacy in Brazil,"This article details the creation of a novel domain ontology at the
intersection of epidemiology, medicine, statistics, and computer science. Using
the terminology defined by current legislation, the article outlines a
systematic approach to handling hospital data anonymously in preparation for
its use in Artificial Intelligence (AI) applications in healthcare. The
development process consisted of 7 pragmatic steps, including defining scope,
selecting knowledge, reviewing important terms, constructing classes that
describe designs used in epidemiological studies, machine learning paradigms,
types of data and attributes, risks that anonymized data may be exposed to,
privacy attacks, techniques to mitigate re-identification, privacy models, and
metrics for measuring the effects of anonymization. The article concludes by
demonstrating the practical implementation of this ontology in hospital
settings for the development and validation of AI."
Energy Efficiency Considerations for Popular AI Benchmarks,"Advances in artificial intelligence need to become more resource-aware and
sustainable. This requires clear assessment and reporting of energy efficiency
trade-offs, like sacrificing fast running time for higher predictive
performance. While first methods for investigating efficiency have been
proposed, we still lack comprehensive results for popular methods and data
sets. In this work, we attempt to fill this information gap by providing
empiric insights for popular AI benchmarks, with a total of 100 experiments.
Our findings are evidence of how different data sets all have their own
efficiency landscape, and show that methods can be more or less likely to act
efficiently."
"Positive AI: Key Challenges in Designing Artificial Intelligence for
  Wellbeing","Artificial Intelligence (AI) is a double-edged sword: on one hand, AI
promises to provide great advances that could benefit humanity, but on the
other hand, AI poses substantial (even existential) risks. With advancements
happening daily, many people are increasingly worried about AI's impact on
their lives. To ensure AI progresses beneficially, some researchers have
proposed ""wellbeing"" as a key objective to govern AI. This article addresses
key challenges in designing AI for wellbeing. We group these challenges into
issues of modeling wellbeing in context, assessing wellbeing in context,
designing interventions to improve wellbeing, and maintaining AI alignment with
wellbeing over time. The identification of these challenges provides a scope
for efforts to help ensure that AI developments are aligned with human
wellbeing."
"AI-based Predictive Analytic Approaches for safeguarding the Future of
  Electric/Hybrid Vehicles","In response to the global need for sustainable energy, green technology may
help fight climate change. Before green infrastructure to be easily integrated
into the world's energy system, it needs upgrading. By improving energy
infrastructure and decision-making, artificial intelligence (AI) may help solve
this challenge. EHVs have grown in popularity because to concerns about global
warming and the need for more ecologically friendly transportation. EHVs may
work better with cutting-edge technologies like AI. Electric vehicles (EVs)
reduce greenhouse gas emissions and promote sustainable mobility. Electric
automobiles (EVs) are growing in popularity due to their benefits for climate
change mitigation and sustainable mobility. Unfortunately, EV production
consumes a lot of energy and materials, which may harm nature. EV production is
being improved using green technologies like artificial intelligence and
predictive analysis. Electric and hybrid vehicles (EHVs) may help meet the need
for ecologically friendly transportation. However, the Battery Management
System (BMS) controls EHV performance and longevity. AI may improve EHV energy
efficiency, emissions reduction, and sustainability. Remote hijacking, security
breaches, and unauthorized access are EHV cybersecurity vulnerabilities
addressed in the article. AI research and development may help make
transportation more sustainable, as may optimizing EHVs and charging
infrastructure."
"NIMS-OS: An automation software to implement a closed loop between
  artificial intelligence and robotic experiments in materials science","NIMS-OS (NIMS Orchestration System) is a Python library created to realize a
closed loop of robotic experiments and artificial intelligence (AI) without
human intervention for automated materials exploration. It uses various
combinations of modules to operate autonomously. Each module acts as an AI for
materials exploration or a controller for a robotic experiments. As AI
techniques, Bayesian optimization (PHYSBO), boundless objective-free
exploration (BLOX), phase diagram construction (PDC), and random exploration
(RE) methods can be used. Moreover, a system called NIMS automated robotic
electrochemical experiments (NAREE) is available as a set of robotic
experimental equipment. Visualization tools for the results are also included,
which allows users to check the optimization results in real time. Newly
created modules for AI and robotic experiments can be added easily to extend
the functionality of the system. In addition, we developed a GUI application to
control NIMS-OS.To demonstrate the operation of NIMS-OS, we consider an
automated exploration for new electrolytes. NIMS-OS is available at
https://github.com/nimsos-dev/nimsos."
"A Review of ChatGPT Applications in Education, Marketing, Software
  Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions","ChatGPT is a type of artificial intelligence language model that uses deep
learning algorithms to generate human-like responses to text-based prompts. The
introduction of the latest ChatGPT version in November of 2022 has caused
shockwaves in the industrial and academic communities for its powerful
capabilities, plethora of possible applications, and the great possibility for
abuse. At the time of writing this work, several other language models (e.g.,
Google Bard and Meta LLaMA) just came out in an attempt to get a foothold in
the vast possible market. These models have the ability to revolutionize the
way we interact with computers and have potential applications in many fields,
including education, software engineering, healthcare, and marketing. In this
paper, we will discuss the possible applications, drawbacks, and research
directions using advanced language Chatbots (e.g., ChatGPT) in each of these
fields. We first start with a brief introduction and the development timeline
of artificial intelligence based language models, then we go through possible
applications of such models, after that we discuss the limitations and
drawbacks of the current technological state of the art, and finally we point
out future possible research directions."
"A Perspective on Explainable Artificial Intelligence Methods: SHAP and
  LIME","eXplainable artificial intelligence (XAI) methods have emerged to convert the
black box of machine learning (ML) models into a more digestible form. These
methods help to communicate how the model works with the aim of making ML
models more transparent and increasing the trust of end-users into their
output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model
Agnostic Explanation (LIME) are two widely used XAI methods, particularly with
tabular data. In this perspective piece, we discuss the way the explainability
metrics of these two methods are generated and propose a framework for
interpretation of their outputs, highlighting their weaknesses and strengths.
Specifically, we discuss their outcomes in terms of model-dependency and in the
presence of collinearity among the features, relying on a case study from the
biomedical domain (classification of individuals with or without myocardial
infarction). The results indicate that SHAP and LIME are highly affected by the
adopted ML model and feature collinearity, raising a note of caution on their
usage and interpretation."
"Application of Artificial Intelligence in the Classification of
  Microscopical Starch Images for Drug Formulation","Starches are important energy sources found in plants with many uses in the
pharmaceutical industry such as binders, disintegrants, bulking agents in drugs
and thus require very careful physicochemical analysis for proper
identification and verification which includes microscopy. In this work, we
applied artificial intelligence techniques (using transfer learning and deep
convolution neural network CNNs to microscopical images obtained from 9 starch
samples of different botanical sources. Our approach obtained an accuracy of
61% when the machine learning model was pretrained on microscopic images from
MicroNet dataset. However the accuracy jumped to 81% for model pretrained on
random day to day images obtained from Imagenet dataset. The model pretrained
on the imagenet dataset also showed a better precision, recall and f1 score
than that pretrained on the imagenet dataset."
"Exploring outlooks towards generative AI-based assistive technologies
  for people with Autism","The last few years have significantly increased global interest in generative
artificial intelligence. Deepfakes, which are synthetically created videos,
emerged as an application of generative artificial intelligence. Fake news and
pornographic content have been the two most prevalent negative use cases of
deepfakes in the digital ecosystem. Deepfakes have some advantageous
applications that experts in the subject have thought of in the areas of
filmmaking, teaching, etc. Research on the potential of deepfakes among people
with disabilities is, however, scarce or nonexistent. This workshop paper
explores the potential of deepfakes as an assistive technology. We examined
Reddit conversations regarding Nvdia's new videoconferencing feature which
allows participants to maintain eye contact during online meetings. Through
manual web scraping and qualitative coding, we found 162 relevant comments
discussing the relevance and appropriateness of the technology for people with
Autism. The themes identified from the qualitative codes indicate a number of
concerns for technology among the autistic community. We suggest that
developing generative AI-based assistive solutions will have ramifications for
human-computer interaction (HCI), and present open questions that should be
investigated further in this space."
"A Survey on the Role of Artificial Intelligence in the Prediction and
  Diagnosis of Schizophrenia","Machine learning is employed in healthcare to draw approximate conclusions
regarding human diseases and mental health problems. Compared to older
traditional methods, it can help to analyze data more efficiently and produce
better and more dependable results. Millions of people are affected by
schizophrenia, which is a chronic mental disorder that can significantly impact
their lives. Many machine learning algorithms have been developed to predict
and prevent this disease, and they can potentially be implemented in the
diagnosis of individuals who have it. This survey aims to review papers that
have focused on the use of deep learning to detect and predict schizophrenia
using EEG signals, functional magnetic resonance imaging (fMRI), and diffusion
magnetic resonance imaging (dMRI). With our chosen search strategy, we assessed
ten publications from 2019 to 2022. All studies achieved successful predictions
of more than 80%. This review provides summaries of the studies and compares
their notable aspects. In the field of artificial intelligence (AI) and machine
learning (ML) for schizophrenia, significant advances have been made due to the
availability of ML tools, and we are optimistic that this field will continue
to grow."
"Music Representing Corpus Virtual: An Open Sourced Library for
  Explorative Music Generation, Sound Design, and Instrument Creation with
  Artificial Intelligence and Machine Learning","Music Representing Corpus Virtual (MRCV) is an open source software suite
designed to explore the capabilities of Artificial Intelligence (AI) and
Machine Learning (ML) in Music Generation, Sound Design, and Virtual Instrument
Creation (MGSDIC). The software is accessible to users of varying levels of
experience, with an emphasis on providing an explorative approach to MGSDIC.
The main aim of MRCV is to facilitate creativity, allowing users to customize
input datasets for training the neural networks, and offering a range of
options for each neural network (thoroughly documented in the Github Wiki). The
software suite is designed to be accessible to musicians, audio professionals,
sound designers, and composers, regardless of their prior experience in AI or
ML. The documentation is prepared in such a way as to abstract technical
details, thereby making it easy to understand. The software is open source,
meaning users can contribute to its development, and the community can
collectively benefit from the insights and experience of other users."
"Artificial Intelligence-Based Methods for Precision Medicine: Diabetes
  Risk Prediction","The rising prevalence of type 2 diabetes mellitus (T2DM) necessitates the
development of predictive models for T2DM risk assessment. Artificial
intelligence (AI) models are being extensively used for this purpose, but a
comprehensive review of their advancements and challenges is lacking. This
scoping review analyzes existing literature on AI-based models for T2DM risk
prediction. Forty studies were included, mainly published in the past four
years. Traditional machine learning models were more prevalent than deep
learning models. Electronic health records were the most commonly used data
source. Unimodal AI models relying on EHR data were prominent, while only a few
utilized multimodal models. Both unimodal and multimodal models showed
promising performance, with the latter outperforming the former. Internal
validation was common, while external validation was limited. Interpretability
methods were reported in half of the studies. Few studies reported novel
biomarkers, and open-source code availability was limited. This review provides
insights into the current state and limitations of AI-based T2DM risk
prediction models and highlights challenges for their development and clinical
implementation."
"An Experimental Investigation into the Evaluation of Explainability
  Methods","EXplainable Artificial Intelligence (XAI) aims to help users to grasp the
reasoning behind the predictions of an Artificial Intelligence (AI) system.
Many XAI approaches have emerged in recent years. Consequently, a subfield
related to the evaluation of XAI methods has gained considerable attention,
with the aim to determine which methods provide the best explanation using
various approaches and criteria. However, the literature lacks a comparison of
the evaluation metrics themselves, that one can use to evaluate XAI methods.
This work aims to fill this gap by comparing 14 different metrics when applied
to nine state-of-the-art XAI methods and three dummy methods (e.g., random
saliency maps) used as references. Experimental results show which of these
metrics produces highly correlated results, indicating potential redundancy. We
also demonstrate the significant impact of varying the baseline hyperparameter
on the evaluation metric values. Finally, we use dummy methods to assess the
reliability of metrics in terms of ranking, pointing out their limitations."
Towards Cognitive Bots: Architectural Research Challenges,"Software bots operating in multiple virtual digital platforms must understand
the platforms' affordances and behave like human users. Platform affordances or
features differ from one application platform to another or through a life
cycle, requiring such bots to be adaptable. Moreover, bots in such platforms
could cooperate with humans or other software agents for work or to learn
specific behavior patterns. However, present-day bots, particularly chatbots,
other than language processing and prediction, are far from reaching a human
user's behavior level within complex business information systems. They lack
the cognitive capabilities to sense and act in such virtual environments,
rendering their development a challenge to artificial general intelligence
research. In this study, we problematize and investigate assumptions in
conceptualizing software bot architecture by directing attention to significant
architectural research challenges in developing cognitive bots endowed with
complex behavior for operation on information systems. As an outlook, we
propose alternate architectural assumptions to consider in future bot design
and bot development frameworks."
What We Know So Far: Artificial Intelligence in African Healthcare,"Healthcare in Africa is a complex issue influenced by many factors including
poverty, lack of infrastructure, and inadequate funding. However, Artificial
intelligence (AI) applied to healthcare, has the potential to transform
healthcare in Africa by improving the accuracy and efficiency of diagnosis,
enabling earlier detection of diseases, and supporting the delivery of
personalized medicine. This paper reviews the current state of how AI
Algorithms can be used to improve diagnostics, treatment, and disease
monitoring, as well as how AI can be used to improve access to healthcare in
Africa as a low-resource setting and discusses some of the critical challenges
and opportunities for its adoption. As such, there is a need for a
well-coordinated effort by the governments, private sector, healthcare
providers, and international organizations to create sustainable AI solutions
that meet the unique needs of the African healthcare system."
"Mapping ChatGPT in Mainstream Media to Unravel Jobs and Diversity
  Challenges: Early Quantitative Insights through Sentiment Analysis and Word
  Frequency Analysis","The exponential growth in user acquisition and popularity of OpenAIs ChatGPT,
an artificial intelligence(AI) powered chatbot, was accompanied by widespread
mainstream media coverage. This article presents a quantitative data analysis
of the early trends and sentiments revealed by conducting text mining and NLP
methods onto a corpus of 10,902 mainstream news headlines related to the
subject of ChatGPT and artificial intelligence, from the launch of ChatGPT in
November 2022 to March 2023. The findings revealed in sentiment analysis,
ChatGPT and artificial intelligence, were perceived more positively than
negatively in the mainstream media. In regards to word frequency results, over
sixty-five percent of the top frequency words were focused on Big Tech issues
and actors while topics such as jobs, diversity, ethics, copyright, gender and
women were poorly represented or completely absent and only accounted for six
percent of the total corpus. This article is a critical analysis into the power
structures and collusions between Big Tech and Big Media in their hegemonic
exclusion of diversity and job challenges from mainstream media."
"Employing Explainable Artificial Intelligence (XAI) Methodologies to
  Analyze the Correlation between Input Variables and Tensile Strength in
  Additively Manufactured Samples","This research paper explores the impact of various input parameters,
including Infill percentage, Layer Height, Extrusion Temperature, and Print
Speed, on the resulting Tensile Strength in objects produced through additive
manufacturing. The main objective of this study is to enhance our understanding
of the correlation between the input parameters and Tensile Strength, as well
as to identify the key factors influencing the performance of the additive
manufacturing process. To achieve this objective, we introduced the utilization
of Explainable Artificial Intelligence (XAI) techniques for the first time,
which allowed us to analyze the data and gain valuable insights into the
system's behavior. Specifically, we employed SHAP (SHapley Additive
exPlanations), a widely adopted framework for interpreting machine learning
model predictions, to provide explanations for the behavior of a machine
learning model trained on the data. Our findings reveal that the Infill
percentage and Extrusion Temperature have the most significant influence on
Tensile Strength, while the impact of Layer Height and Print Speed is
relatively minor. Furthermore, we discovered that the relationship between the
input parameters and Tensile Strength is highly intricate and nonlinear, making
it difficult to accurately describe using simple linear models."
"Traffic Prediction using Artificial Intelligence: Review of Recent
  Advances and Emerging Opportunities","Traffic prediction plays a crucial role in alleviating traffic congestion
which represents a critical problem globally, resulting in negative
consequences such as lost hours of additional travel time and increased fuel
consumption. Integrating emerging technologies into transportation systems
provides opportunities for improving traffic prediction significantly and
brings about new research problems. In order to lay the foundation for
understanding the open research challenges in traffic prediction, this survey
aims to provide a comprehensive overview of traffic prediction methodologies.
Specifically, we focus on the recent advances and emerging research
opportunities in Artificial Intelligence (AI)-based traffic prediction methods,
due to their recent success and potential in traffic prediction, with an
emphasis on multivariate traffic time series modeling. We first provide a list
and explanation of the various data types and resources used in the literature.
Next, the essential data preprocessing methods within the traffic prediction
context are categorized, and the prediction methods and applications are
subsequently summarized. Lastly, we present primary research challenges in
traffic prediction and discuss some directions for future research."
Accelerating science with human-aware artificial intelligence,"Artificial intelligence (AI) models trained on published scientific findings
have been used to invent valuable materials and targeted therapies, but they
typically ignore the human scientists who continually alter the landscape of
discovery. Here we show that incorporating the distribution of human expertise
by training unsupervised models on simulated inferences cognitively accessible
to experts dramatically improves (up to 400%) AI prediction of future
discoveries beyond those focused on research content alone, especially when
relevant literature is sparse. These models succeed by predicting human
predictions and the scientists who will make them. By tuning human-aware AI to
avoid the crowd, we can generate scientifically promising ""alien"" hypotheses
unlikely to be imagined or pursued without intervention until the distant
future, which hold promise to punctuate scientific advance beyond questions
currently pursued. Accelerating human discovery or probing its blind spots,
human-aware AI enables us to move toward and beyond the contemporary scientific
frontier."
"Quantitative Analysis of Primary Attribution Explainable Artificial
  Intelligence Methods for Remote Sensing Image Classification","We present a comprehensive analysis of quantitatively evaluating explainable
artificial intelligence (XAI) techniques for remote sensing image
classification. Our approach leverages state-of-the-art machine learning
approaches to perform remote sensing image classification across multiple
modalities. We investigate the results of the models qualitatively through XAI
methods. Additionally, we compare the XAI methods quantitatively through
various categories of desired properties. Through our analysis, we offer
insights and recommendations for selecting the most appropriate XAI method(s)
to gain a deeper understanding of the models' decision-making processes. The
code for this work is publicly available."
"Artificial Intelligence can facilitate selfish decisions by altering the
  appearance of interaction partners","The increasing prevalence of image-altering filters on social media and video
conferencing technologies has raised concerns about the ethical and
psychological implications of using Artificial Intelligence (AI) to manipulate
our perception of others. In this study, we specifically investigate the
potential impact of blur filters, a type of appearance-altering technology, on
individuals' behavior towards others. Our findings consistently demonstrate a
significant increase in selfish behavior directed towards individuals whose
appearance is blurred, suggesting that blur filters can facilitate moral
disengagement through depersonalization. These results emphasize the need for
broader ethical discussions surrounding AI technologies that modify our
perception of others, including issues of transparency, consent, and the
awareness of being subject to appearance manipulation by others. We also
emphasize the importance of anticipatory experiments in informing the
development of responsible guidelines and policies prior to the widespread
adoption of such technologies."
Strategies to exploit XAI to improve classification systems,"Explainable Artificial Intelligence (XAI) aims to provide insights into the
decision-making process of AI models, allowing users to understand their
results beyond their decisions. A significant goal of XAI is to improve the
performance of AI models by providing explanations for their decision-making
processes. However, most XAI literature focuses on how to explain an AI system,
while less attention has been given to how XAI methods can be exploited to
improve an AI system. In this work, a set of well-known XAI methods typically
used with Machine Learning (ML) classification tasks are investigated to verify
if they can be exploited, not just to provide explanations but also to improve
the performance of the model itself. To this aim, two strategies to use the
explanation to improve a classification system are reported and empirically
evaluated on three datasets: Fashion-MNIST, CIFAR10, and STL10. Results suggest
that explanations built by Integrated Gradients highlight input features that
can be effectively used to improve classification performance."
"Adversarial attacks and defenses in explainable artificial intelligence:
  A survey","Explainable artificial intelligence (XAI) methods are portrayed as a remedy
for debugging and trusting statistical and deep learning models, as well as
interpreting their predictions. However, recent advances in adversarial machine
learning (AdvML) highlight the limitations and vulnerabilities of
state-of-the-art explanation methods, putting their security and
trustworthiness into question. The possibility of manipulating, fooling or
fairwashing evidence of the model's reasoning has detrimental consequences when
applied in high-stakes decision-making and knowledge discovery. This survey
provides a comprehensive overview of research concerning adversarial attacks on
explanations of machine learning models, as well as fairness metrics. We
introduce a unified notation and taxonomy of methods facilitating a common
ground for researchers and practitioners from the intersecting research fields
of AdvML and XAI. We discuss how to defend against attacks and design robust
interpretation methods. We contribute a list of existing insecurities in XAI
and outline the emerging research directions in adversarial XAI (AdvXAI).
Future work should address improving explanation methods and evaluation
protocols to take into account the reported safety issues."
"Artificial intelligence and radiation protection. A game changer or an
  update?","Artificial intelligence (AI) is regarded as one of the most disruptive
technology of the century and with countless applications. What does it mean
for radiation protection? This article describes the fundamentals of machine
learning (ML) based methods and presents the inaugural applications in
different fields of radiation protection. It is foreseen that the usage of AI
will increase in radiation protection. Consequently, this article explores some
of the benefits and also the potential barriers and questions, including
ethical ones, that can come out. The article proposes that collaboration
between radiation protection professionals and data scientist experts can
accelerate and guide the development of the algorithms for effective scientific
and technological outcomes."
"Trustworthy Artificial Intelligence Framework for Proactive Detection
  and Risk Explanation of Cyber Attacks in Smart Grid","The rapid growth of distributed energy resources (DERs), such as renewable
energy sources, generators, consumers, and prosumers in the smart grid
infrastructure, poses significant cybersecurity and trust challenges to the
grid controller. Consequently, it is crucial to identify adversarial tactics
and measure the strength of the attacker's DER. To enable a trustworthy smart
grid controller, this work investigates a trustworthy artificial intelligence
(AI) mechanism for proactive identification and explanation of the cyber risk
caused by the control/status message of DERs. Thus, proposing and developing a
trustworthy AI framework to facilitate the deployment of any AI algorithms for
detecting potential cyber threats and analyzing root causes based on Shapley
value interpretation while dynamically quantifying the risk of an attack based
on Ward's minimum variance formula. The experiment with a state-of-the-art
dataset establishes the proposed framework as a trustworthy AI by fulfilling
the capabilities of reliability, fairness, explainability, transparency,
reproducibility, and accountability."
Perceptions and Realities of Text-to-Image Generation,"Generative artificial intelligence (AI) is a widely popular technology that
will have a profound impact on society and individuals. Less than a decade ago,
it was thought that creative work would be among the last to be automated - yet
today, we see AI encroaching on many creative domains. In this paper, we
present the findings of a survey study on people's perceptions of text-to-image
generation. We touch on participants' technical understanding of the emerging
technology, their fears and concerns, and thoughts about risks and dangers of
text-to-image generation to the individual and society. We find that while
participants were aware of the risks and dangers associated with the
technology, only few participants considered the technology to be a personal
risk. The risks for others were more easy to recognize for participants.
Artists were particularly seen at risk. Interestingly, participants who had
tried the technology rated its future importance lower than those who had not
tried it. This result shows that many people are still oblivious of the
potential personal risks of generative artificial intelligence and the
impending societal changes associated with this technology."
"Artificial Intelligence for Real Sustainability? -- What is Artificial
  Intelligence and Can it Help with the Sustainability Transformation?","The discussion about the disruptive possibilities of a technology called
artificial intelligence (AI) is on everyone's lips. Companies and countries
alike are running multi-billion-dollar research programmes to ensure they do
not miss out on the global innovation hunt. Among many other applications, AI
is also supposed to aid the large-scale changes needed to achieve sustainable
societies. To assess those claims and possibilities, this article briefly
explains, classifies, and theorises AI technology and then politically
contextualises that analysis in light of the sustainability discourse. Based on
those insights it finally argues, that AI can play a small role in moving
towards sustainable societies, however the fixation on technological
innovation, especially AI, obscures and depoliticises the necessary societal
decisions regarding sustainability goals and means as mere technicalities and
therefore rather obstructs real and effective societal transformation efforts."
Mixed Fair Division: A Survey,"Fair division considers the allocation of scarce resources among agents in
such a way that every agent gets a fair share. It is a fundamental problem in
society and has received significant attention and rapid developments from the
game theory and artificial intelligence communities in recent years. The
majority of the fair division literature can be divided along at least two
orthogonal directions: goods versus chores, and divisible versus indivisible
resources. In this survey, besides describing the state of the art, we outline
a number of interesting open questions and future directions in three mixed
fair division settings: (i) indivisible goods and chores, (ii) divisible and
indivisible goods (mixed goods), and (iii) indivisible goods with subsidy which
can be viewed like a divisible good."
A Shift In Artistic Practices through Artificial Intelligence,"The explosion of content generated by artificial intelligence (AI) models has
initiated a cultural shift in arts, music, and media, whereby roles are
changing, values are shifting, and conventions are challenged. The vast,
readily available dataset of the Internet has created an environment for AI
models to be trained on any content on the Web. With AI models shared openly
and used by many globally, how does this new paradigm shift challenge the
status quo in artistic practices? What kind of changes will AI technology bring
to music, arts, and new media?"
"A Graphical Modeling Language for Artificial Intelligence Applications
  in Automation Systems","Artificial Intelligence (AI) applications in automation systems are usually
distributed systems whose development and integration involve several experts.
Each expert uses its own domain-specific modeling language and tools to model
the system elements. An interdisciplinary graphical modeling language that
enables the modeling of an AI application as an overall system comprehensible
to all disciplines does not yet exist. As a result, there is often a lack of
interdisciplinary system understanding, leading to increased development,
integration, and maintenance efforts. This paper therefore presents a graphical
modeling language that enables consistent and understandable modeling of AI
applications in automation systems at system level. This makes it possible to
subdivide individual subareas into domain specific subsystems and thus reduce
the existing efforts."
Transformers in Healthcare: A Survey,"With Artificial Intelligence (AI) increasingly permeating various aspects of
society, including healthcare, the adoption of the Transformers neural network
architecture is rapidly changing many applications. Transformer is a type of
deep learning architecture initially developed to solve general-purpose Natural
Language Processing (NLP) tasks and has subsequently been adapted in many
fields, including healthcare. In this survey paper, we provide an overview of
how this architecture has been adopted to analyze various forms of data,
including medical imaging, structured and unstructured Electronic Health
Records (EHR), social media, physiological signals, and biomolecular sequences.
Those models could help in clinical diagnosis, report generation, data
reconstruction, and drug/protein synthesis. We identified relevant studies
using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses
(PRISMA) guidelines. We also discuss the benefits and limitations of using
transformers in healthcare and examine issues such as computational cost, model
interpretability, fairness, alignment with human values, ethical implications,
and environmental impact."
Impact of Feature Encoding on Malware Classification Explainability,"This paper investigates the impact of feature encoding techniques on the
explainability of XAI (Explainable Artificial Intelligence) algorithms. Using a
malware classification dataset, we trained an XGBoost model and compared the
performance of two feature encoding methods: Label Encoding (LE) and One Hot
Encoding (OHE). Our findings reveal a marginal performance loss when using OHE
instead of LE. However, the more detailed explanations provided by OHE
compensated for this loss. We observed that OHE enables deeper exploration of
details in both global and local contexts, facilitating more comprehensive
answers. Additionally, we observed that using OHE resulted in smaller
explanation files and reduced analysis time for human analysts. These findings
emphasize the significance of considering feature encoding techniques in XAI
research and suggest potential for further exploration by incorporating
additional encoding methods and innovative visualization approaches."
"Deep Generative Models for Physiological Signals: A Systematic
  Literature Review","In this paper, we present a systematic literature review on deep generative
models for physiological signals, particularly electrocardiogram (ECG),
electroencephalogram (EEG), photoplethysmogram (PPG) and electromyogram (EMG).
Compared to the existing review papers, we present the first review that
summarizes the recent state-of-the-art deep generative models. By analyzing the
state-of-the-art research related to deep generative models along with their
main applications and challenges, this review contributes to the overall
understanding of these models applied to physiological signals. Additionally,
by highlighting the employed evaluation protocol and the most used
physiological databases, this review facilitates the assessment and
benchmarking of deep generative models."
Artificial Intelligence for Drug Discovery: Are We There Yet?,"Drug discovery is adapting to novel technologies such as data science,
informatics, and artificial intelligence (AI) to accelerate effective treatment
development while reducing costs and animal experiments. AI is transforming
drug discovery, as indicated by increasing interest from investors, industrial
and academic scientists, and legislators. Successful drug discovery requires
optimizing properties related to pharmacodynamics, pharmacokinetics, and
clinical outcomes. This review discusses the use of AI in the three pillars of
drug discovery: diseases, targets, and therapeutic modalities, with a focus on
small molecule drugs. AI technologies, such as generative chemistry, machine
learning, and multi-property optimization, have enabled several compounds to
enter clinical trials. The scientific community must carefully vet known
information to address the reproducibility crisis. The full potential of AI in
drug discovery can only be realized with sufficient ground truth and
appropriate human intervention at later pipeline stages."
"Enhancing Evacuation Planning through Multi-Agent Simulation and
  Artificial Intelligence: Understanding Human Behavior in Hazardous
  Environments","This paper focuses on the crucial task of addressing the evacuation of
hazardous places, which holds great importance for coordinators, event hosts,
and authorities. To facilitate the development of effective solutions, the
paper employs Artificial Intelligence (AI) techniques, specifically Multi-Agent
Systems (MAS), to construct a simulation model for evacuation. NetLogo is
selected as the simulation tool of choice due to its ability to provide a
comprehensive understanding of human behaviour in distressing situations within
hazardous environments. The primary objective of this paper is to enhance our
comprehension of how individuals react and respond during such distressing
situations. By leveraging AI and MAS, the simulation model aims to capture the
complex dynamics of evacuation scenarios, enabling policymakers and emergency
planners to make informed decisions and implement more efficient and effective
evacuation strategies. This paper endeavours to contribute to the advancement
of evacuation planning and ultimately improve the safety and well-being of
individuals in hazardous places"
"Revisiting the Performance-Explainability Trade-Off in Explainable
  Artificial Intelligence (XAI)","Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI."
"A New Perspective on Evaluation Methods for Explainable Artificial
  Intelligence (XAI)","Within the field of Requirements Engineering (RE), the increasing
significance of Explainable Artificial Intelligence (XAI) in aligning
AI-supported systems with user needs, societal expectations, and regulatory
standards has garnered recognition. In general, explainability has emerged as
an important non-functional requirement that impacts system quality. However,
the supposed trade-off between explainability and performance challenges the
presumed positive influence of explainability. If meeting the requirement of
explainability entails a reduction in system performance, then careful
consideration must be given to which of these quality aspects takes precedence
and how to compromise between them. In this paper, we critically examine the
alleged trade-off. We argue that it is best approached in a nuanced way that
incorporates resource availability, domain characteristics, and considerations
of risk. By providing a foundation for future research and best practices, this
work aims to advance the field of RE for AI."
"Artificial Intelligence in archival and historical scholarship workflow:
  HTS and ChatGPT","This article examines the impact of Artificial Intelligence on the archival
heritage digitization processes, specifically regarding the manuscripts'
automatic transcription, their correction, and normalization. It highlights how
digitality has compelled scholars to redefine Archive and History field and has
facilitated the accessibility of analogue sources through digitization and
integration into big data. The study focuses on two AI systems, namely
Transkribus and ChatGPT, which enable efficient analysis and transcription of
digitized sources. The article presents a test of ChatGPT, which was utilized
to normalize the text of 366 letters stored in the Correspondence section of
the Biscari Archive (Catania). Although the AI exhibited some limitations that
resulted in inaccuracies, the corrected texts met expectations. Overall, the
article concludes that digitization and AI can significantly enhance archival
and historical research by allowing the analysis of vast amounts of data and
the application of computational linguistic tools."
Designing Fiduciary Artificial Intelligence,"A fiduciary is a trusted agent that has the legal duty to act with loyalty
and care towards a principal that employs them. When fiduciary organizations
interact with users through a digital interface, or otherwise automate their
operations with artificial intelligence, they will need to design these AI
systems to be compliant with their duties. This article synthesizes recent work
in computer science and law to develop a procedure for designing and auditing
Fiduciary AI. The designer of a Fiduciary AI should understand the context of
the system, identify its principals, and assess the best interests of those
principals. Then the designer must be loyal with respect to those interests,
and careful in an contextually appropriate way. We connect the steps in this
procedure to dimensions of Trustworthy AI, such as privacy and alignment.
Fiduciary AI is a promising means to address the incompleteness of data
subject's consent when interacting with complex technical systems."
Current and Future Challenges in Knowledge Representation and Reasoning,"Knowledge Representation and Reasoning is a central, longstanding, and active
area of Artificial Intelligence. Over the years it has evolved significantly;
more recently it has been challenged and complemented by research in areas such
as machine learning and reasoning under uncertainty. In July 2022 a Dagstuhl
Perspectives workshop was held on Knowledge Representation and Reasoning. The
goal of the workshop was to describe the state of the art in the field,
including its relation with other areas, its shortcomings and strengths,
together with recommendations for future progress. We developed this manifesto
based on the presentations, panels, working groups, and discussions that took
place at the Dagstuhl Workshop. It is a declaration of our views on Knowledge
Representation: its origins, goals, milestones, and current foci; its relation
to other disciplines, especially to Artificial Intelligence; and on its
challenges, along with key priorities for the next decade."
"Neuro-Symbolic RDF and Description Logic Reasoners: The State-Of-The-Art
  and Challenges","Ontologies are used in various domains, with RDF and OWL being prominent
standards for ontology development. RDF is favored for its simplicity and
flexibility, while OWL enables detailed domain knowledge representation.
However, as ontologies grow larger and more expressive, reasoning complexity
increases, and traditional reasoners struggle to perform efficiently. Despite
optimization efforts, scalability remains an issue. Additionally, advancements
in automated knowledge base construction have created large and expressive
ontologies that are often noisy and inconsistent, posing further challenges for
conventional reasoners. To address these challenges, researchers have explored
neuro-symbolic approaches that combine neural networks' learning capabilities
with symbolic systems' reasoning abilities. In this chapter,we provide an
overview of the existing literature in the field of neuro-symbolic deductive
reasoning supported by RDF(S), the description logics EL and ALC, and OWL 2 RL,
discussing the techniques employed, the tasks they address, and other relevant
efforts in this area."
"Semantic Communications for Artificial Intelligence Generated Content
  (AIGC) Toward Effective Content Creation","Artificial Intelligence Generated Content (AIGC) Services have significant
potential in digital content creation. The distinctive abilities of AIGC, such
as content generation based on minimal input, hold huge potential, especially
when integrating with semantic communication (SemCom). In this paper, a novel
comprehensive conceptual model for the integration of AIGC and SemCom is
developed. Particularly, a content generation level is introduced on top of the
semantic level that provides a clear outline of how AIGC and SemCom interact
with each other to produce meaningful and effective content. Moreover, a novel
framework that employs AIGC technology is proposed as an encoder and decoder
for semantic information, considering the joint optimization of semantic
extraction and evaluation metrics tailored to AIGC services. The framework can
adapt to different types of content generated, the required quality, and the
semantic information utilized. By employing a Deep Q Network (DQN), a case
study is presented that provides useful insights into the feasibility of the
optimization problem and its convergence characteristics."
"Dialogue Possibilities between a Human Supervisor and UAM Air Traffic
  Management: Route Alteration","This paper introduces a novel approach to detour management in Urban Air
Traffic Management (UATM) using knowledge representation and reasoning. It aims
to understand the complexities and requirements of UAM detours, enabling a
method that quickly identifies safe and efficient routes in a carefully sampled
environment. This method implemented in Answer Set Programming uses
non-monotonic reasoning and a two-phase conversation between a human manager
and the UATM system, considering factors like safety and potential impacts. The
robustness and efficacy of the proposed method were validated through several
queries from two simulation scenarios, contributing to the symbiosis of human
knowledge and advanced AI techniques. The paper provides an introduction,
citing relevant studies, problem formulation, solution, discussions, and
concluding comments."
"AIxArtist: A First-Person Tale of Interacting with Artificial
  Intelligence to Escape Creative Block","The future of the arts and artificial intelligence (AI) is promising as
technology advances. As the use of AI in design becomes more widespread, art
practice may not be a human-only art form and could instead become a digitally
integrated experience. With enhanced creativity and collaboration, arts and AI
could work together towards creating artistic outputs that are visually
appealing and meet the needs of the artist and viewer. While it is uncertain
how far the integration will go, arts and AI will likely influence one another.
This workshop pictorial puts forward first-person research that shares
interactions between an HCI researcher and AI as they try to escape the
creative block. The pictorial paper explores two questions: How can AI support
artists' creativity, and what does it mean to be explainable in this context?
HIs, ChatGPT and Midjourney were engaged; the result was a series of
reflections that require further discussion and explorations in the XAIxArts
community: Transparency of attribution, the creation process, ethics of asking,
and inspiration vs copying."
"CACTUS: a Comprehensive Abstraction and Classification Tool for
  Uncovering Structures","The availability of large data sets is providing an impetus for driving
current artificial intelligent developments. There are, however, challenges for
developing solutions with small data sets due to practical and cost-effective
deployment and the opacity of deep learning models. The Comprehensive
Abstraction and Classification Tool for Uncovering Structures called CACTUS is
presented for improved secure analytics by effectively employing explainable
artificial intelligence. It provides additional support for categorical
attributes, preserving their original meaning, optimising memory usage, and
speeding up the computation through parallelisation. It shows to the user the
frequency of the attributes in each class and ranks them by their
discriminative power. Its performance is assessed by application to the
Wisconsin diagnostic breast cancer and Thyroid0387 data sets."
"The Promise and Peril of Artificial Intelligence -- Violet Teaming
  Offers a Balanced Path Forward","Artificial intelligence (AI) promises immense benefits across sectors, yet
also poses risks from dual-use potentials, biases, and unintended behaviors.
This paper reviews emerging issues with opaque and uncontrollable AI systems
and proposes an integrative framework called violet teaming to develop reliable
and responsible AI. Violet teaming combines adversarial vulnerability probing
(red teaming) with solutions for safety and security (blue teaming) while
prioritizing ethics and social benefit. It emerged from AI safety research to
manage risks proactively by design. The paper traces the evolution of red,
blue, and purple teaming toward violet teaming, and then discusses applying
violet techniques to address biosecurity risks of AI in biotechnology.
Additional sections review key perspectives across law, ethics, cybersecurity,
macrostrategy, and industry best practices essential for operationalizing
responsible AI through holistic technical and social considerations. Violet
teaming provides both philosophy and method for steering AI trajectories toward
societal good. With conscience and wisdom, the extraordinary capabilities of AI
can enrich humanity. But without adequate precaution, the risks could prove
catastrophic. Violet teaming aims to empower moral technology for the common
welfare."
Artificial Intelligence in Career Counseling: A Test Case with ResumAI,"The rise of artificial intelligence (AI) has led to various means of
integration of AI aimed to provide efficiency in tasks, one of which is career
counseling. A key part of getting a job is having a solid resume that passes
through the first round of programs and recruiters. It is difficult to find
good resources or schedule an appointment with a career counselor to help with
editing a resume for a specific role. With the rise of ChatGPT, Bard, and
several other AI chat programs it is possible to provide specific, automated
feedback on various concerns to suggest places for improvement within the
context of career counseling. This paper begins with a quick literature review
on the ethical considerations and limitations of AI in career counseling. The
authors also have created their own website service, called ResumAI, to test
and review the functionality of an AI career counselor. The findings of this
study will contribute to the understanding of chat AI ResumAI reviewer programs
and sites. The implications of the findings for the field of career counseling,
AI development, and ethical practice will be discussed."
Graph Meets LLMs: Towards Large Graph Models,"Large models have emerged as the most recent groundbreaking achievements in
artificial intelligence, and particularly machine learning. However, when it
comes to graphs, large models have not achieved the same level of success as in
other fields, such as natural language processing and computer vision. In order
to promote applying large models for graphs forward, we present a perspective
paper to discuss the challenges and opportunities associated with developing
large graph models. First, we discuss the desired characteristics of large
graph models. Then, we present detailed discussions from three key
perspectives: representation basis, graph data, and graph models. In each
category, we provide a brief overview of recent advances and highlight the
remaining challenges together with our visions. Finally, we discuss valuable
applications of large graph models. We believe this perspective can encourage
further investigations into large graph models, ultimately pushing us one step
closer towards artificial general intelligence (AGI). We are the first to
comprehensively study large graph models, to the best of our knowledge."
"Boosting AND/OR-Based Computational Protein Design: Dynamic Heuristics
  and Generalizable UFO","Scientific computing has experienced a surge empowered by advancements in
technologies such as neural networks. However, certain important tasks are less
amenable to these technologies, benefiting from innovations to traditional
inference schemes. One such task is protein re-design. Recently a new re-design
algorithm, AOBB-K*, was introduced and was competitive with state-of-the-art
BBK* on small protein re-design problems. However, AOBB-K* did not scale well.
In this work we focus on scaling up AOBB-K* and introduce three new versions:
AOBB-K*-b (boosted), AOBB-K*-DH (with dynamic heuristics), and AOBB-K*-UFO
(with underflow optimization) that significantly enhance scalability."
"Advanced Computing and Related Applications Leveraging Brain-inspired
  Spiking Neural Networks","In the rapid evolution of next-generation brain-inspired artificial
intelligence and increasingly sophisticated electromagnetic environment, the
most bionic characteristics and anti-interference performance of spiking neural
networks show great potential in terms of computational speed, real-time
information processing, and spatio-temporal information processing. Data
processing. Spiking neural network is one of the cores of brain-like artificial
intelligence, which realizes brain-like computing by simulating the structure
and information transfer mode of biological neural networks. This paper
summarizes the strengths, weaknesses and applicability of five neuronal models
and analyzes the characteristics of five network topologies; then reviews the
spiking neural network algorithms and summarizes the unsupervised learning
algorithms based on synaptic plasticity rules and four types of supervised
learning algorithms from the perspectives of unsupervised learning and
supervised learning; finally focuses on the review of brain-like neuromorphic
chips under research at home and abroad. This paper is intended to provide
learning concepts and research orientations for the peers who are new to the
research field of spiking neural networks through systematic summaries."
"A compendium of data sources for data science, machine learning, and
  artificial intelligence","Recent advances in data science, machine learning, and artificial
intelligence, such as the emergence of large language models, are leading to an
increasing demand for data that can be processed by such models. While data
sources are application-specific, and it is impossible to produce an exhaustive
list of such data sources, it seems that a comprehensive, rather than complete,
list would still benefit data scientists and machine learning experts of all
levels of seniority. The goal of this publication is to provide just such an
(inevitably incomplete) list -- or compendium -- of data sources across
multiple areas of applications, including finance and economics, legal (laws
and regulations), life sciences (medicine and drug discovery), news sentiment
and social media, retail and ecommerce, satellite imagery, and shipping and
logistics, and sports."
"Adaptive User-centered Neuro-symbolic Learning for Multimodal
  Interaction with Autonomous Systems","Recent advances in machine learning, particularly deep learning, have enabled
autonomous systems to perceive and comprehend objects and their environments in
a perceptual subsymbolic manner. These systems can now perform object
detection, sensor data fusion, and language understanding tasks. However, there
is a growing need to enhance these systems to understand objects and their
environments more conceptually and symbolically. It is essential to consider
both the explicit teaching provided by humans (e.g., describing a situation or
explaining how to act) and the implicit teaching obtained by observing human
behavior (e.g., through the system's sensors) to achieve this level of powerful
artificial intelligence. Thus, the system must be designed with multimodal
input and output capabilities to support implicit and explicit interaction
models. In this position paper, we argue for considering both types of inputs,
as well as human-in-the-loop and incremental learning techniques, for advancing
the field of artificial intelligence and enabling autonomous systems to learn
like humans. We propose several hypotheses and design guidelines and highlight
a use case from related work to achieve this goal."
"Life-inspired Interoceptive Artificial Intelligence for Autonomous and
  Adaptive Agents","Building autonomous -- i.e., choosing goals based on one's needs -- and
adaptive -- i.e., surviving in ever-changing environments -- agents has been a
holy grail of artificial intelligence (AI). A living organism is a prime
example of such an agent, offering important lessons about adaptive autonomy.
Here, we focus on interoception, a process of monitoring one's internal
environment to keep it within certain bounds, which underwrites the survival of
an organism. To develop AI with interoception, we need to factorize the state
variables representing internal environments from external environments and
adopt life-inspired mathematical properties of internal environment states.
This paper offers a new perspective on how interoception can help build
autonomous and adaptive agents by integrating the legacy of cybernetics with
recent advances in theories of life, reinforcement learning, and neuroscience."
"Can humans help BERT gain ""confidence""?","The advancements in artificial intelligence over the last decade have opened
a multitude of avenues for interdisciplinary research. Since the idea of
artificial intelligence was inspired by the working of neurons in the brain, it
seems pretty practical to combine the two fields and take the help of cognitive
data to train AI models. Not only it will help to get a deeper understanding of
the technology, but of the brain as well. In this thesis, I conduct novel
experiments to integrate cognitive features from the Zurich Cognitive Corpus
(ZuCo) (Hollenstein et al., 2018) with a transformer-based encoder model called
BERT. I show how EEG and eye-tracking features from ZuCo can help to increase
the performance of the NLP model. I confirm the performance increase with the
help of a robustness-checking pipeline and derive a word-EEG lexicon to use in
benchmarking on an external dataset that does not have any cognitive features
associated with it. Further, I analyze the internal working mechanism of BERT
and explore a potential method for model explainability by correlating it with
a popular model-agnostic explainability framework called LIME (Ribeiro et al.,
2016). Finally, I discuss the possible directions to take this research
forward."
"Challenges in Annotating Datasets to Quantify Bias in Under-represented
  Society","Recent advances in artificial intelligence, including the development of
highly sophisticated large language models (LLM), have proven beneficial in
many real-world applications. However, evidence of inherent bias encoded in
these LLMs has raised concerns about equity. In response, there has been an
increase in research dealing with bias, including studies focusing on
quantifying bias and developing debiasing techniques. Benchmark bias datasets
have also been developed for binary gender classification and ethical/racial
considerations, focusing predominantly on American demographics. However, there
is minimal research in understanding and quantifying bias related to
under-represented societies. Motivated by the lack of annotated datasets for
quantifying bias in under-represented societies, we endeavoured to create
benchmark datasets for the New Zealand (NZ) population. We faced many
challenges in this process, despite the availability of three annotators. This
research outlines the manual annotation process, provides an overview of the
challenges we encountered and lessons learnt, and presents recommendations for
future research."
"Functional requirements to mitigate the Risk of Harm to Patients from
  Artificial Intelligence in Healthcare","The Directorate General for Parliamentary Research Services of the European
Parliament has prepared a report to the Members of the European Parliament
where they enumerate seven main risks of Artificial Intelligence (AI) in
medicine and healthcare: patient harm due to AI errors, misuse of medical AI
tools, bias in AI and the perpetuation of existing inequities, lack of
transparency, privacy and security issues, gaps in accountability, and
obstacles in implementation.
  In this study, we propose fourteen functional requirements that AI systems
may implement to reduce the risks associated with their medical purpose: AI
passport, User management, Regulation check, Academic use only disclaimer, data
quality assessment, Clinicians double check, Continuous performance evaluation,
Audit trail, Continuous usability test, Review of retrospective/simulated
cases, Bias check, eXplainable AI, Encryption and use of field-tested
libraries, and Semantic interoperability.
  Our intention here is to provide specific high-level specifications of
technical solutions to ensure continuous good performance and use of AI systems
to benefit patients in compliance with the future EU regulatory framework."
"Generative Agent-Based Modeling: Unveiling Social System Dynamics
  through Coupling Mechanistic Models with Generative Artificial Intelligence","We discuss the emerging new opportunity for building feedback-rich
computational models of social systems using generative artificial
intelligence. Referred to as Generative Agent-Based Models (GABMs), such
individual-level models utilize large language models such as ChatGPT to
represent human decision-making in social settings. We provide a GABM case in
which human behavior can be incorporated in simulation models by coupling a
mechanistic model of human interactions with a pre-trained large language
model. This is achieved by introducing a simple GABM of social norm diffusion
in an organization. For educational purposes, the model is intentionally kept
simple. We examine a wide range of scenarios and the sensitivity of the results
to several changes in the prompt. We hope the article and the model serve as a
guide for building useful diffusion models that include realistic human
reasoning and decision-making."
Use Scenarios & Practical Examples of AI Use in Education,"This report presents a set of use scenarios based on existing resources that
teachers can use as inspiration to create their own, with the aim of
introducing artificial intelligence (AI) at different pre-university levels,
and with different goals. The Artificial Intelligence Education field (AIEd) is
very active, with new resources and tools arising continuously. Those included
in this document have already been tested with students and selected by experts
in the field, but they must be taken just as practical examples to guide and
inspire teachers creativity."
"Sampling - Variational Auto Encoder - Ensemble: In the Quest of
  Explainable Artificial Intelligence","Explainable Artificial Intelligence (XAI) models have recently attracted a
great deal of interest from a variety of application sectors. Despite
significant developments in this area, there are still no standardized methods
or approaches for understanding AI model outputs. A systematic and cohesive
framework is also increasingly necessary to incorporate new techniques like
discriminative and generative models to close the gap. This paper contributes
to the discourse on XAI by presenting an empirical evaluation based on a novel
framework: Sampling - Variational Auto Encoder (VAE) - Ensemble Anomaly
Detection (SVEAD). It is a hybrid architecture where VAE combined with ensemble
stacking and SHapley Additive exPlanations are used for imbalanced
classification. The finding reveals that combining ensemble stacking, VAE, and
SHAP can. not only lead to better model performance but also provide an easily
explainable framework. This work has used SHAP combined with Permutation
Importance and Individual Conditional Expectations to create a powerful
interpretability of the model. The finding has an important implication in the
real world, where the need for XAI is paramount to boost confidence in AI
applications."
"Towards A Unified Utilitarian Ethics Framework for Healthcare Artificial
  Intelligence","Artificial Intelligence (AI) aims to elevate healthcare to a pinnacle by
aiding clinical decision support. Overcoming the challenges related to the
design of ethical AI will enable clinicians, physicians, healthcare
professionals, and other stakeholders to use and trust AI in healthcare
settings. This study attempts to identify the major ethical principles
influencing the utility performance of AI at different technological levels
such as data access, algorithms, and systems through a thematic analysis. We
observed that justice, privacy, bias, lack of regulations, risks, and
interpretability are the most important principles to consider for ethical AI.
This data-driven study has analyzed secondary survey data from the Pew Research
Center (2020) of 36 AI experts to categorize the top ethical principles of AI
design. To resolve the ethical issues identified by the meta-analysis and
domain experts, we propose a new utilitarian ethics-based theoretical framework
for designing ethical AI for the healthcare domain."
"User Experience Design Professionals' Perceptions of Generative
  Artificial Intelligence","Among creative professionals, Generative Artificial Intelligence (GenAI) has
sparked excitement over its capabilities and fear over unanticipated
consequences. How does GenAI impact User Experience Design (UXD) practice, and
are fears warranted? We interviewed 20 UX Designers, with diverse experience
and across companies (startups to large enterprises). We probed them to
characterize their practices, and sample their attitudes, concerns, and
expectations. We found that experienced designers are confident in their
originality, creativity, and empathic skills, and find GenAI's role as
assistive. They emphasized the unique human factors of ""enjoyment"" and
""agency"", where humans remain the arbiters of ""AI alignment"". However, skill
degradation, job replacement, and creativity exhaustion can adversely impact
junior designers. We discuss implications for human-GenAI collaboration,
specifically copyright and ownership, human creativity and agency, and AI
literacy and access. Through the lens of responsible and participatory AI, we
contribute a deeper understanding of GenAI fears and opportunities for UXD."
Borges and AI,"Many believe that Large Language Models (LLMs) open the era of Artificial
Intelligence (AI). Some see opportunities while others see dangers. Yet both
proponents and opponents grasp AI through the imagery popularised by science
fiction. Will the machine become sentient and rebel against its creators? Will
we experience a paperclip apocalypse? Before answering such questions, we
should first ask whether this mental imagery provides a good description of the
phenomenon at hand. Understanding weather patterns through the moods of the
gods only goes so far. The present paper instead advocates understanding LLMs
and their connection to AI through the imagery of Jorge Luis Borges, a master
of 20th century literature, forerunner of magical realism, and precursor to
postmodern literature. This exercise leads to a new perspective that
illuminates the relation between language modelling and artificial
intelligence."
Artificial Intelligence Index Report 2023,"Welcome to the sixth edition of the AI Index Report. This year, the report
introduces more original data than any previous edition, including a new
chapter on AI public opinion, a more thorough technical performance chapter,
original analysis about large language and multimodal models, detailed trends
in global AI legislation records, a study of the environmental impact of AI
systems, and more. The AI Index Report tracks, collates, distills, and
visualizes data related to artificial intelligence. Our mission is to provide
unbiased, rigorously vetted, broadly sourced data in order for policymakers,
researchers, executives, journalists, and the general public to develop a more
thorough and nuanced understanding of the complex field of AI. The report aims
to be the world's most credible and authoritative source for data and insights
about AI."
BridgeHand2Vec Bridge Hand Representation,"Contract bridge is a game characterized by incomplete information, posing an
exciting challenge for artificial intelligence methods. This paper proposes the
BridgeHand2Vec approach, which leverages a neural network to embed a bridge
player's hand (consisting of 13 cards) into a vector space. The resulting
representation reflects the strength of the hand in the game and enables
interpretable distances to be determined between different hands. This
representation is derived by training a neural network to estimate the number
of tricks that a pair of players can take. In the remainder of this paper, we
analyze the properties of the resulting vector space and provide examples of
its application in reinforcement learning, and opening bid classification.
Although this was not our main goal, the neural network used for the
vectorization achieves SOTA results on the DDBP2 problem (estimating the number
of tricks for two given hands)."
"If our aim is to build morality into an artificial agent, how might we
  begin to go about doing so?","As Artificial Intelligence (AI) becomes pervasive in most fields, from
healthcare to autonomous driving, it is essential that we find successful ways
of building morality into our machines, especially for decision-making.
However, the question of what it means to be moral is still debated,
particularly in the context of AI. In this paper, we highlight the different
aspects that should be considered when building moral agents, including the
most relevant moral paradigms and challenges. We also discuss the top-down and
bottom-up approaches to design and the role of emotion and sentience in
morality. We then propose solutions including a hybrid approach to design and a
hierarchical approach to combining moral paradigms. We emphasize how governance
and policy are becoming ever more critical in AI Ethics and in ensuring that
the tasks we set for moral agents are attainable, that ethical behavior is
achieved, and that we obtain good AI."
"Multinational AGI Consortium (MAGIC): A Proposal for International
  Coordination on AI","This paper proposes a Multinational Artificial General Intelligence
Consortium (MAGIC) to mitigate existential risks from advanced artificial
intelligence (AI). MAGIC would be the only institution in the world permitted
to develop advanced AI, enforced through a global moratorium by its signatory
members on all other advanced AI development. MAGIC would be exclusive,
safety-focused, highly secure, and collectively supported by member states,
with benefits distributed equitably among signatories. MAGIC would allow narrow
AI models to flourish while significantly reducing the possibility of
misaligned, rogue, breakout, or runaway outcomes of general-purpose systems. We
do not address the political feasibility of implementing a moratorium or
address the specific legislative strategies and rules needed to enforce a ban
on high-capacity AGI training runs. Instead, we propose one positive vision of
the future, where MAGIC, as a global governance regime, can lay the groundwork
for long-term, safe regulation of advanced AI."
Generative artificial intelligence for de novo protein design,"Engineering new molecules with desirable functions and properties has the
potential to extend our ability to engineer proteins beyond what nature has so
far evolved. Advances in the so-called ""de novo"" design problem have recently
been brought forward by developments in artificial intelligence. Generative
architectures, such as language models and diffusion processes, seem adept at
generating novel, yet realistic proteins that display desirable properties and
perform specified functions. State-of-the-art design protocols now achieve
experimental success rates nearing 20%, thus widening the access to de novo
designed proteins. Despite extensive progress, there are clear field-wide
challenges, for example in determining the best in silico metrics to prioritise
designs for experimental testing, and in designing proteins that can undergo
large conformational changes or be regulated by post-translational
modifications and other cellular processes. With an increase in the number of
models being developed, this review provides a framework to understand how
these tools fit into the overall process of de novo protein design. Throughout,
we highlight the power of incorporating biochemical knowledge to improve
performance and interpretability."
Using Slisemap to interpret physical data,"Manifold visualisation techniques are commonly used to visualise
high-dimensional datasets in physical sciences. In this paper we apply a
recently introduced manifold visualisation method, called Slise, on datasets
from physics and chemistry. Slisemap combines manifold visualisation with
explainable artificial intelligence. Explainable artificial intelligence is
used to investigate the decision processes of black box machine learning models
and complex simulators. With Slisemap we find an embedding such that data items
with similar local explanations are grouped together. Hence, Slisemap gives us
an overview of the different behaviours of a black box model. This makes
Slisemap into a supervised manifold visualisation method, where the patterns in
the embedding reflect a target property. In this paper we show how Slisemap can
be used and evaluated on physical data and that Slisemap is helpful in finding
meaningful information on classification and regression models trained on these
datasets."
Critical Role of Artificially Intelligent Conversational Chatbot,"Artificially intelligent chatbot, such as ChatGPT, represents a recent and
powerful advancement in the AI domain. Users prefer them for obtaining quick
and precise answers, avoiding the usual hassle of clicking through multiple
links in traditional searches. ChatGPT's conversational approach makes it
comfortable and accessible for finding answers quickly and in an organized
manner. However, it is important to note that these chatbots have limitations,
especially in terms of providing accurate answers as well as ethical concerns.
In this study, we explore various scenarios involving ChatGPT's ethical
implications within academic contexts, its limitations, and the potential
misuse by specific user groups. To address these challenges, we propose
architectural solutions aimed at preventing inappropriate use and promoting
responsible AI interactions."
"Learning impartial policies for sequential counterfactual explanations
  using Deep Reinforcement Learning","In the field of explainable Artificial Intelligence (XAI), sequential
counterfactual (SCF) examples are often used to alter the decision of a trained
classifier by implementing a sequence of modifications to the input instance.
Although certain test-time algorithms aim to optimize for each new instance
individually, recently Reinforcement Learning (RL) methods have been proposed
that seek to learn policies for discovering SCFs, thereby enhancing
scalability. As is typical in RL, the formulation of the RL problem, including
the specification of state space, actions, and rewards, can often be ambiguous.
In this work, we identify shortcomings in existing methods that can result in
policies with undesired properties, such as a bias towards specific actions. We
propose to use the output probabilities of the classifier to create a more
informative reward, to mitigate this effect."
"Notion of Explainable Artificial Intelligence -- An Empirical
  Investigation from A Users Perspective","The growing attention to artificial intelligence-based applications has led
to research interest in explainability issues. This emerging research attention
on explainable AI (XAI) advocates the need to investigate end user-centric
explainable AI. Thus, this study aims to investigate usercentric explainable AI
and considered recommendation systems as the study context. We conducted focus
group interviews to collect qualitative data on the recommendation system. We
asked participants about the end users' comprehension of a recommended item,
its probable explanation, and their opinion of making a recommendation
explainable. Our findings reveal that end users want a non-technical and
tailor-made explanation with on-demand supplementary information. Moreover, we
also observed users requiring an explanation about personal data usage,
detailed user feedback, and authentic and reliable explanations. Finally, we
propose a synthesized framework that aims at involving the end user in the
development process for requirements collection and validation."
"Deep Image Semantic Communication Model for Artificial Intelligent
  Internet of Things","With the rapid development of Artificial Intelligent Internet of Things
(AIoT), the image data from AIoT devices has been witnessing the explosive
increasing. In this paper, a novel deep image semantic communication model is
proposed for the efficient image communication in AIoT. Particularly, at the
transmitter side, a high-precision image semantic segmentation algorithm is
proposed to extract the semantic information of the image to achieve
significant compression of the image data. At the receiver side, a semantic
image restoration algorithm based on Generative Adversarial Network (GAN) is
proposed to convert the semantic image to a real scene image with detailed
information. Simulation results demonstrate that the proposed image semantic
communication model can improve the image compression ratio and recovery
accuracy by 71.93% and 25.07% on average in comparison with WebP and CycleGAN,
respectively. More importantly, our demo experiment shows that the proposed
model reduces the total delay by 95.26% in the image communication, when
comparing with the original image transmission."
"An attempt to generate new bridge types from latent space of variational
  autoencoder","Try to generate new bridge types using generative artificial intelligence
technology. The grayscale images of the bridge facade with the change of
component width was rendered by 3dsMax animation software, and then the OpenCV
module performed an appropriate amount of geometric transformation (rotation,
horizontal scale, vertical scale) to obtain the image dataset of three-span
beam bridge, arch bridge, cable-stayed bridge and suspension bridge. Based on
Python programming language, TensorFlow and Keras deep learning platform
framework, variational autoencoder was constructed and trained, and
low-dimensional bridge-type latent space that is convenient for vector
operations was obtained. Variational autoencoder can combine two bridge types
on the basis of the original of human into one that is a new bridge type.
Generative artificial intelligence technology can assist bridge designers in
bridge-type innovation, and can be used as copilot."
"Explainable artificial intelligence for Healthcare applications using
  Random Forest Classifier with LIME and SHAP","With the advances in computationally efficient artificial Intelligence (AI)
techniques and their numerous applications in our everyday life, there is a
pressing need to understand the computational details hidden in black box AI
techniques such as most popular machine learning and deep learning techniques;
through more detailed explanations. The origin of explainable AI (xAI) is
coined from these challenges and recently gained more attention by the
researchers by adding explainability comprehensively in traditional AI systems.
This leads to develop an appropriate framework for successful applications of
xAI in real life scenarios with respect to innovations, risk mitigation,
ethical issues and logical values to the users. In this book chapter, an
in-depth analysis of several xAI frameworks and methods including LIME (Local
Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive
exPlanations) are provided. Random Forest Classifier as black box AI is used on
a publicly available Diabetes symptoms dataset with LIME and SHAP for better
interpretations. The results obtained are interesting in terms of transparency,
valid and trustworthiness in diabetes disease prediction."
"""ChatGPT, a Friend or Foe for Education?"" Analyzing the User's
  Perspectives on the Latest AI Chatbot Via Reddit","Latest developments in Artificial Intelligence (AI) and big data gave rise to
Artificial Intelligent agents like Open AI's ChatGPT, which has recently become
the fastest growing application since Facebook and WhatsApp. ChatGPT has
demonstrated its ability to impact students' classroom learning experience and
exam outcomes. However, there is evidence that ChatGPT provides biased and
erroneous information, yet students use ChatGPT in academic tasks. Therefore,
an accurate understanding of ChatGPT user perception is crucial. This study has
analyzed 247 Reddit top posts related to the educational use of ChatGPT from a
prominent subreddit called ""ChatGPT"" for user perception analysis. Descriptive
statistics, sentiment analysis using NLP techniques, and LDA topic modeling
were used for analysis to gather a contextual understanding of the data.
Results show that the majority of the users took a neutral viewpoint. However,
there was more positive perception than negative regarding the usefulness of
ChatGPT in education."
"Advancements in Generative AI: A Comprehensive Review of GANs, GPT,
  Autoencoders, Diffusion Model, and Transformers","The launch of ChatGPT has garnered global attention, marking a significant
milestone in the field of Generative Artificial Intelligence. While Generative
AI has been in effect for the past decade, the introduction of ChatGPT has
ignited a new wave of research and innovation in the AI domain. This surge in
interest has led to the development and release of numerous cutting-edge tools,
such as Bard, Stable Diffusion, DALL-E, Make-A-Video, Runway ML, and Jukebox,
among others. These tools exhibit remarkable capabilities, encompassing tasks
ranging from text generation and music composition, image creation, video
production, code generation, and even scientific work. They are built upon
various state-of-the-art models, including Stable Diffusion, transformer models
like GPT-3 (recent GPT-4), variational autoencoders, and generative adversarial
networks. This advancement in Generative AI presents a wealth of exciting
opportunities and, simultaneously, unprecedented challenges. Throughout this
paper, we have explored these state-of-the-art models, the diverse array of
tasks they can accomplish, the challenges they pose, and the promising future
of Generative Artificial Intelligence."
"Data-Driven Risk Modeling for Infrastructure Projects Using Artificial
  Intelligence Techniques","Managing project risk is a key part of the successful implementation of any
large project and is widely recognized as a best practice for public agencies
to deliver infrastructures. The conventional method of identifying and
evaluating project risks involves getting input from subject matter experts at
risk workshops in the early phases of a project. As a project moves through its
life cycle, these identified risks and their assessments evolve. Some risks are
realized to become issues, some are mitigated, and some are retired as no
longer important. Despite the value provided by conventional expert-based
approaches, several challenges remain due to the time-consuming and expensive
processes involved. Moreover, limited is known about how risks evolve from
ex-ante to ex-post over time. How well does the project team identify and
evaluate risks in the initial phase compared to what happens during project
execution? Using historical data and artificial intelligence techniques, this
study addressed these limitations by introducing a data-driven framework to
identify risks automatically and to examine the quality of early risk registers
and risk assessments. Risk registers from more than 70 U.S. major
transportation projects form the input dataset."
ChatGPT and Beyond: The Generative AI Revolution in Education,"The wide adoption and usage of generative artificial intelligence (AI)
models, particularly ChatGPT, has sparked a surge in research exploring their
potential applications in the educational landscape. This survey examines
academic literature published between November, 2022, and July, 2023,
specifically targeting high-impact research from Scopus-indexed Q1 and Q2
journals. This survey delves into the practical applications and implications
of generative AI models across a diverse range of educational contexts. Through
a comprehensive and rigorous evaluation of recent academic literature, this
survey seeks to illuminate the evolving role of generative AI models,
particularly ChatGPT, in education. By shedding light on the potential
benefits, challenges, and emerging trends in this dynamic field, the survey
endeavors to contribute to the understanding of the nexus between artificial
intelligence and education. The findings of this review will empower educators,
researchers, and policymakers to make informed decisions about the integration
of AI technologies into learning environments."
Advances in 3D Neural Stylization: A Survey,"Modern artificial intelligence offers a novel and transformative approach to
creating digital art across diverse styles and modalities like images, videos
and 3D data, unleashing the power of creativity and revolutionizing the way
that we perceive and interact with visual content. This paper reports on recent
advances in stylized 3D asset creation and manipulation with the expressive
power of neural networks. We establish a taxonomy for neural stylization,
considering crucial design choices such as scene representation, guidance data,
optimization strategies, and output styles. Building on such taxonomy, our
survey first revisits the background of neural stylization on 2D images, and
then presents in-depth discussions on recent neural stylization methods for 3D
data, accompanied by a benchmark evaluating selected mesh and neural field
stylization methods. Based on the insights gained from the survey, we highlight
the practical significance, open challenges, future research, and potential
impacts of neural stylization, which facilitates researchers and practitioners
to navigate the rapidly evolving landscape of 3D content creation using modern
artificial intelligence."
Artificial Intelligence in Sustainable Vertical Farming,"As global challenges of population growth, climate change, and resource
scarcity intensify, the agricultural landscape is at a critical juncture.
Sustainable vertical farming emerges as a transformative solution to address
these challenges by maximizing crop yields in controlled environments. This
paradigm shift necessitates the integration of cutting-edge technologies, with
Artificial Intelligence (AI) at the forefront. The paper provides a
comprehensive exploration of the role of AI in sustainable vertical farming,
investigating its potential, challenges, and opportunities. The review
synthesizes the current state of AI applications, encompassing machine
learning, computer vision, the Internet of Things (IoT), and robotics, in
optimizing resource usage, automating tasks, and enhancing decision-making. It
identifies gaps in research, emphasizing the need for optimized AI models,
interdisciplinary collaboration, and the development of explainable AI in
agriculture. The implications extend beyond efficiency gains, considering
economic viability, reduced environmental impact, and increased food security.
The paper concludes by offering insights for stakeholders and suggesting
avenues for future research, aiming to guide the integration of AI technologies
in sustainable vertical farming for a resilient and sustainable future in
agriculture."
"Retail Analytics in the New Normal: The Influence of Artificial
  Intelligence and the Covid-19 Pandemic","The COVID-19 pandemic has severely disrupted the retail landscape and has
accelerated the adoption of innovative technologies. A striking example relates
to the proliferation of online grocery orders and the technology deployed to
facilitate such logistics. In fact, for many retailers, this disruption was a
wake-up call after which they started recognizing the power of data analytics
and artificial intelligence (AI). In this article, we discuss the opportunities
that AI can offer to retailers in the new normal retail landscape. Some of the
techniques described have been applied at scale to adapt previously deployed AI
models, whereas in other instances, fresh solutions needed to be developed to
help retailers cope with recent disruptions, such as unexpected panic buying,
retraining predictive models, and leveraging online-offline synergies."
"Generative artificial intelligence enhances creativity but reduces the
  diversity of novel content","Creativity is core to being human. Generative artificial intelligence (GenAI)
holds promise for humans to be more creative by offering new ideas, or less
creative by anchoring on GenAI ideas. We study the causal impact of GenAI on
the production of a creative output in an online experimental study where some
writers are could obtain ideas for a story from a GenAI platform. Access to
GenAI ideas causes an increase in the writer's creativity with stories being
evaluated as better written and more enjoyable, especially among less creative
writers. However, GenAI-enabled stories are more similar to each other than
stories by humans alone. Our results have implications for researchers,
policy-makers and practitioners interested in bolstering creativity, but point
to potential downstream consequences from over-reliance."
"Transforming organic chemistry research paradigms: moving from manual
  efforts to the intersection of automation and artificial intelligence","Organic chemistry is undergoing a major paradigm shift, moving from a
labor-intensive approach to a new era dominated by automation and artificial
intelligence (AI). This transformative shift is being driven by technological
advances, the ever-increasing demand for greater research efficiency and
accuracy, and the burgeoning growth of interdisciplinary research. AI models,
supported by computational power and algorithms, are drastically reshaping
synthetic planning and introducing groundbreaking ways to tackle complex
molecular synthesis. In addition, autonomous robotic systems are rapidly
accelerating the pace of discovery by performing tedious tasks with
unprecedented speed and precision. This article examines the multiple
opportunities and challenges presented by this paradigm shift and explores its
far-reaching implications. It provides valuable insights into the future
trajectory of organic chemistry research, which is increasingly defined by the
synergistic interaction of automation and AI."
"Explainable AI is Responsible AI: How Explainability Creates Trustworthy
  and Socially Responsible Artificial Intelligence","Artificial intelligence (AI) has been clearly established as a technology
with the potential to revolutionize fields from healthcare to finance - if
developed and deployed responsibly. This is the topic of responsible AI, which
emphasizes the need to develop trustworthy AI systems that minimize bias,
protect privacy, support security, and enhance transparency and accountability.
Explainable AI (XAI) has been broadly considered as a building block for
responsible AI (RAI), with most of the literature considering it as a solution
for improved transparency. This work proposes that XAI and responsible AI are
significantly more deeply entwined. In this work, we explore state-of-the-art
literature on RAI and XAI technologies. Based on our findings, we demonstrate
that XAI can be utilized to ensure fairness, robustness, privacy, security, and
transparency in a wide range of contexts. Our findings lead us to conclude that
XAI is an essential foundation for every pillar of RAI."
"Artificial Intelligence in the automatic coding of interviews on
  Landscape Quality Objectives. Comparison and case study","In this study, we conducted a comparative analysis of the automated coding
provided by three Artificial Intelligence functionalities (At-las.ti, ChatGPT
and Google Bard) in relation to the manual coding of 12 research interviews
focused on Landscape Quality Objectives for a small island in the north of Cuba
(Cayo Santa Mar\'ia). For this purpose, the following comparison criteria were
established: Accuracy, Comprehensiveness, Thematic Coherence, Redundancy,
Clarity, Detail and Regularity. The analysis showed the usefulness of AI for
the intended purpose, albeit with numerous flaws and shortcomings. In summary,
today the automatic coding of AIs can be considered useful as a guide towards a
subsequent in-depth and meticulous analysis of the information by the
researcher. However, as this is such a recently developed field, rapid
evolution is expected to bring the necessary improvements to these tools."
"RACER: Rational Artificial Intelligence Car-following-model Enhanced by
  Reality","This paper introduces RACER, the Rational Artificial Intelligence
Car-following model Enhanced by Reality, a cutting-edge deep learning
car-following model, that satisfies partial derivative constraints, designed to
predict Adaptive Cruise Control (ACC) driving behavior while staying
theoretically feasible. Unlike conventional models, RACER effectively
integrates Rational Driving Constraints (RDCs), crucial tenets of actual
driving, resulting in strikingly accurate and realistic predictions. Against
established models like the Optimal Velocity Relative Velocity (OVRV), a
car-following Neural Network (NN), and a car-following Physics-Informed Neural
Network (PINN), RACER excels across key metrics, such as acceleration,
velocity, and spacing. Notably, it displays a perfect adherence to the RDCs,
registering zero violations, in stark contrast to other models. This study
highlights the immense value of incorporating physical constraints within AI
models, especially for augmenting safety measures in transportation. It also
paves the way for future research to test these models against human driving
data, with the potential to guide safer and more rational driving behavior. The
versatility of the proposed model, including its potential to incorporate
additional derivative constraints and broader architectural applications,
enhances its appeal and broadens its impact within the scientific community."
"DVQI: A Multi-task, Hardware-integrated Artificial Intelligence System
  for Automated Visual Inspection in Electronics Manufacturing","As electronics manufacturers continue to face pressure to increase production
efficiency amid difficulties with supply chains and labour shortages, many
printed circuit board assembly (PCBA) manufacturers have begun to invest in
automation and technological innovations to remain competitive. One such method
is to leverage artificial intelligence (AI) to greatly augment existing
manufacturing processes. In this paper, we present the DarwinAI Visual Quality
Inspection (DVQI) system, a hardware-integration artificial intelligence system
for the automated inspection of printed circuit board assembly defects in an
electronics manufacturing environment. The DVQI system enables multi-task
inspection via minimal programming and setup for manufacturing engineers while
improving cycle time relative to manual inspection. We also present a case
study of the deployed DVQI system's performance and impact for a top
electronics manufacturer."
AS-XAI: Self-supervised Automatic Semantic Interpretation for CNN,"Explainable artificial intelligence (XAI) aims to develop transparent
explanatory approaches for ""black-box"" deep learning models. However,it remains
difficult for existing methods to achieve the trade-off of the three key
criteria in interpretability, namely, reliability, causality, and usability,
which hinder their practical applications. In this paper, we propose a
self-supervised automatic semantic interpretable explainable artificial
intelligence (AS-XAI) framework, which utilizes transparent orthogonal
embedding semantic extraction spaces and row-centered principal component
analysis (PCA) for global semantic interpretation of model decisions in the
absence of human interference, without additional computational costs. In
addition, the invariance of filter feature high-rank decomposition is used to
evaluate model sensitivity to different semantic concepts. Extensive
experiments demonstrate that robust and orthogonal semantic spaces can be
automatically extracted by AS-XAI, providing more effective global
interpretability for convolutional neural networks (CNNs) and generating
human-comprehensible explanations. The proposed approach offers broad
fine-grained extensible practical applications, including shared semantic
interpretation under out-of-distribution (OOD) categories, auxiliary
explanations for species that are challenging to distinguish, and
classification explanations from various perspectives."
"Diffusion Models for Generative Artificial Intelligence: An Introduction
  for Applied Mathematicians","Generative artificial intelligence (AI) refers to algorithms that create
synthetic but realistic output. Diffusion models currently offer state of the
art performance in generative AI for images. They also form a key component in
more general tools, including text-to-image generators and large language
models. Diffusion models work by adding noise to the available training data
and then learning how to reverse the process. The reverse operation may then be
applied to new random data in order to produce new outputs. We provide a brief
introduction to diffusion models for applied mathematicians and statisticians.
Our key aims are (a) to present illustrative computational examples, (b) to
give a careful derivation of the underlying mathematical formulas involved, and
(c) to draw a connection with partial differential equation (PDE) diffusion
models. We provide code for the computational experiments. We hope that this
topic will be of interest to advanced undergraduate students and postgraduate
students. Portions of the material may also provide useful motivational
examples for those who teach courses in stochastic processes, inference,
machine learning, PDEs or scientific computing."
"Task-Driven Causal Feature Distillation: Towards Trustworthy Risk
  Prediction","Since artificial intelligence has seen tremendous recent successes in many
areas, it has sparked great interest in its potential for trustworthy and
interpretable risk prediction. However, most models lack causal reasoning and
struggle with class imbalance, leading to poor precision and recall. To address
this, we propose a Task-Driven Causal Feature Distillation model (TDCFD) to
transform original feature values into causal feature attributions for the
specific risk prediction task. The causal feature attribution helps describe
how much contribution the value of this feature can make to the risk prediction
result. After the causal feature distillation, a deep neural network is applied
to produce trustworthy prediction results with causal interpretability and high
precision/recall. We evaluate the performance of our TDCFD method on several
synthetic and real datasets, and the results demonstrate its superiority over
the state-of-the-art methods regarding precision, recall, interpretability, and
causality."
"From Bytes to Biases: Investigating the Cultural Self-Perception of
  Large Language Models","Large language models (LLMs) are able to engage in natural-sounding
conversations with humans, showcasing unprecedented capabilities for
information retrieval and automated decision support. They have disrupted
human-technology interaction and the way businesses operate. However,
technologies based on generative artificial intelligence (GenAI) are known to
hallucinate, misinform, and display biases introduced by the massive datasets
on which they are trained. Existing research indicates that humans may
unconsciously internalize these biases, which can persist even after they stop
using the programs. This study explores the cultural self-perception of LLMs by
prompting ChatGPT (OpenAI) and Bard (Google) with value questions derived from
the GLOBE project. The findings reveal that their cultural self-perception is
most closely aligned with the values of English-speaking countries and
countries characterized by sustained economic competitiveness. Recognizing the
cultural biases of LLMs and understanding how they work is crucial for all
members of society because one does not want the black box of artificial
intelligence to perpetuate bias in humans, who might, in turn, inadvertently
create and train even more biased algorithms."
"An attempt to generate new bridge types from latent space of generative
  adversarial network","Try to generate new bridge types using generative artificial intelligence
technology. Symmetric structured image dataset of three-span beam bridge, arch
bridge, cable-stayed bridge and suspension bridge are used . Based on Python
programming language, TensorFlow and Keras deep learning platform framework ,
as well as Wasserstein loss function and Lipschitz constraints, generative
adversarial network is constructed and trained. From the obtained low
dimensional bridge-type latent space sampling, new bridge types with asymmetric
structures can be generated. Generative adversarial network can create new
bridge types by organically combining different structural components on the
basis of human original bridge types. It has a certain degree of human original
ability. Generative artificial intelligence technology can open up imagination
space and inspire humanity."
AI-FLARES: Artificial Intelligence for the Analysis of Solar Flares Data,"AI-FLARES (Artificial Intelligence for the Analysis of Solar Flares Data) is
a research project funded by the Agenzia Spaziale Italiana and by the Istituto
Nazionale di Astrofisica within the framework of the ``Attivit\`a di Studio per
la Comunit\`a Scientifica Nazionale Sole, Sistema Solare ed Esopianeti''
program. The topic addressed by this project was the development and use of
computational methods for the analysis of remote sensing space data associated
to solar flare emission. This paper overviews the main results obtained by the
project, with specific focus on solar flare forecasting, reconstruction of
morphologies of the flaring sources, and interpretation of acceleration
mechanisms triggered by solar flares."
Federated Learning for distribution skewed data using sample weights,"One of the most challenging issues in federated learning is that the data is
often not independent and identically distributed (nonIID). Clients are
expected to contribute the same type of data and drawn from one global
distribution. However, data are often collected in different ways from
different resources. Thus, the data distributions among clients might be
different from the underlying global distribution. This creates a weight
divergence issue and reduces federated learning performance. This work focuses
on improving federated learning performance for skewed data distribution across
clients. The main idea is to adjust the client distribution closer to the
global distribution using sample weights. Thus, the machine learning model
converges faster with higher accuracy. We start from the fundamental concept of
empirical risk minimization and theoretically derive a solution for adjusting
the distribution skewness using sample weights. To determine sample weights, we
implicitly exchange density information by leveraging a neural network-based
density estimation model, MADE. The clients data distribution can then be
adjusted without exposing their raw data. Our experiment results on three
real-world datasets show that the proposed method not only improves federated
learning accuracy but also significantly reduces communication costs compared
to the other experimental methods."
"Synthetic Information towards Maximum Posterior Ratio for deep learning
  on Imbalanced Data","This study examines the impact of class-imbalanced data on deep learning
models and proposes a technique for data balancing by generating synthetic data
for the minority class. Unlike random-based oversampling, our method
prioritizes balancing the informative regions by identifying high entropy
samples. Generating well-placed synthetic data can enhance machine learning
algorithms accuracy and efficiency, whereas poorly-placed ones may lead to
higher misclassification rates. We introduce an algorithm that maximizes the
probability of generating a synthetic sample in the correct region of its class
by optimizing the class posterior ratio. Additionally, to maintain data
topology, synthetic data are generated within each minority sample's
neighborhood. Our experimental results on forty-one datasets demonstrate the
superior performance of our technique in enhancing deep-learning models."
"Artificial Intelligence for Operations Research: Revolutionizing the
  Operations Research Process","The rapid advancement of artificial intelligence (AI) techniques has opened
up new opportunities to revolutionize various fields, including operations
research (OR). This survey paper explores the integration of AI within the OR
process (AI4OR) to enhance its effectiveness and efficiency across multiple
stages, such as parameter generation, model formulation, and model
optimization. By providing a comprehensive overview of the state-of-the-art and
examining the potential of AI to transform OR, this paper aims to inspire
further research and innovation in the development of AI-enhanced OR methods
and tools. The synergy between AI and OR is poised to drive significant
advancements and novel solutions in a multitude of domains, ultimately leading
to more effective and efficient decision-making."
"The Key Artificial Intelligence Technologies in Early Childhood
  Education: A Review","Artificial Intelligence (AI) technologies have been applied in various
domains, including early childhood education (ECE). Integration of AI
educational technology is a recent significant trend in ECE. Currently, there
are more and more studies of AI in ECE. To date, there is a lack of survey
articles that discuss the studies of AI in ECE. In this paper, we provide an
up-to-date and in-depth overview of the key AI technologies in ECE that
provides a historical perspective, summarizes the representative works,
outlines open questions, discusses the trends and challenges through a detailed
bibliometric analysis, and provides insightful recommendations for future
research. We mainly discuss the studies that apply AI-based robots and AI
technologies to ECE, including improving the social interaction of children
with an autism spectrum disorder. This paper significantly contributes to
provide an up-to-date and in-depth survey that is suitable as introductory
material for beginners to AI in ECE, as well as supplementary material for
advanced users."
An attempt to generate new bridge types from latent space of PixelCNN,"Try to generate new bridge types using generative artificial intelligence
technology. Using symmetric structured image dataset of three-span beam bridge,
arch bridge, cable-stayed bridge and suspension bridge , based on Python
programming language, TensorFlow and Keras deep learning platform framework ,
PixelCNN is constructed and trained. The model can capture the statistical
structure of the images and calculate the probability distribution of the next
pixel when the previous pixels are given. From the obtained latent space
sampling, new bridge types different from the training dataset can be
generated. PixelCNN can organically combine different structural components on
the basis of human original bridge types, creating new bridge types that have a
certain degree of human original ability. Autoregressive models cannot
understand the meaning of the sequence, while multimodal models combine
regression and autoregressive models to understand the sequence. Multimodal
models should be the way to achieve artificial general intelligence in the
future."
"Exploring the Reasoning Abilities of Multimodal Large Language Models
  (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning","Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence
(AGI) with abstract reasoning ability is the goal of next-generation AI. Recent
advancements in Large Language Models (LLMs), along with the emerging field of
Multimodal Large Language Models (MLLMs), have demonstrated impressive
capabilities across a wide range of multimodal tasks and applications.
Particularly, various MLLMs, each with distinct model architectures, training
data, and training stages, have been evaluated across a broad range of MLLM
benchmarks. These studies have, to varying degrees, revealed different aspects
of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs
have not been systematically investigated. In this survey, we comprehensively
review the existing evaluation protocols of multimodal reasoning, categorize
and illustrate the frontiers of MLLMs, introduce recent trends in applications
of MLLMs on reasoning-intensive tasks, and finally discuss current practices
and future directions. We believe our survey establishes a solid base and sheds
light on this important topic, multimodal reasoning."
"AttentionLego: An Open-Source Building Block For Spatially-Scalable
  Large Language Model Accelerator With Processing-In-Memory Technology","Large language models (LLMs) with Transformer architectures have become
phenomenal in natural language processing, multimodal generative artificial
intelligence, and agent-oriented artificial intelligence. The self-attention
module is the most dominating sub-structure inside Transformer-based LLMs.
Computation using general-purpose graphics processing units (GPUs) inflicts
reckless demand for I/O bandwidth for transferring intermediate calculation
results between memories and processing units. To tackle this challenge, this
work develops a fully customized vanilla self-attention accelerator,
AttentionLego, as the basic building block for constructing spatially
expandable LLM processors. AttentionLego provides basic implementation with
fully-customized digital logic incorporating Processing-In-Memory (PIM)
technology. It is based on PIM-based matrix-vector multiplication and look-up
table-based Softmax design. The open-source code is available online:
https://bonany.cc/attentionleg."
Domain-Independent Dynamic Programming,"For combinatorial optimization problems, model-based paradigms such as
mixed-integer programming (MIP) and constraint programming (CP) aim to decouple
modeling and solving a problem: the `holy grail' of declarative problem
solving. We propose domain-independent dynamic programming (DIDP), a novel
model-based paradigm based on dynamic programming (DP). While DP is not new, it
has typically been implemented as a problem-specific method. We introduce
Dynamic Programming Description Language (DyPDL), a formalism to define DP
models based on a state transition system, inspired by artificial intelligence
(AI) planning. we show that heuristic search algorithms can be used to solve
DyPDL models and propose seven DIDP solvers. We experimentally compare our DIDP
solvers with commercial MIP and CP solvers (solving MIP and CP models,
respectively) on common benchmark instances of eleven combinatorial
optimization problem classes. We show that DIDP outperforms MIP in nine problem
classes, CP also in nine problem classes, and both MIP and CP in seven. DIDP
also achieves superior performance to existing state-based solvers including
domain-independent AI planners."
Aprendizado de mquina aplicado na eletroqumica,"This systematic review focuses on analyzing the use of machine learning
techniques for identifying and quantifying analytes in various electrochemical
applications, presenting the available applications in the literature. Machine
learning is a tool that can facilitate the analysis and enhance the
understanding of processes involving various analytes. In electrochemical
biosensors, it increases the precision of medical diagnostics, improving the
identification of biomarkers and pathogens with high reliability. It can be
effectively used for the classification of complex chemical products; in
environmental monitoring, using low-cost sensors; in portable devices and
wearable systems; among others. Currently, the analysis of some analytes is
still performed manually, requiring the expertise of a specialist in the field
and thus hindering the generalization of results. In light of the advancements
in artificial intelligence today, this work proposes to carry out a systematic
review of the literature on the applications of artificial intelligence
techniques. A set of articles has been identified that address electrochemical
problems using machine learning techniques, more specifically, supervised
learning."
Artificial Intelligence: Arguments for Catastrophic Risk,"Recent progress in artificial intelligence (AI) has drawn attention to the
technology's transformative potential, including what some see as its prospects
for causing large-scale harm. We review two influential arguments purporting to
show how AI could pose catastrophic risks. The first argument -- the Problem of
Power-Seeking -- claims that, under certain assumptions, advanced AI systems
are likely to engage in dangerous power-seeking behavior in pursuit of their
goals. We review reasons for thinking that AI systems might seek power, that
they might obtain it, that this could lead to catastrophe, and that we might
build and deploy such systems anyway. The second argument claims that the
development of human-level AI will unlock rapid further progress, culminating
in AI systems far more capable than any human -- this is the Singularity
Hypothesis. Power-seeking behavior on the part of such systems might be
particularly dangerous. We discuss a variety of objections to both arguments
and conclude by assessing the state of the debate."
"Le Nozze di Giustizia. Interactions between Artificial Intelligence,
  Law, Logic, Language and Computation with some case studies in Traffic
  Regulations and Health Care","An important aim of this paper is to convey some basics of mathematical logic
to the legal community working with Artificial Intelligence. After analysing
what AI is, we decide to delimit ourselves to rule-based AI leaving Neural
Networks and Machine Learning aside. Rule based AI allows for Formal methods
which are described in a rudimentary form. We will then see how mathematical
logic interacts with legal rule-based AI practice. We shall see how
mathematical logic imposes limitations and complications to AI applications. We
classify the limitations and interactions between mathematical logic and legal
AI in three categories: logical, computational and mathematical. The examples
to showcase the interactions will largely come from European traffic
regulations. The paper closes off with some reflections on how and where AI
could be used and on basic mechanisms that shape society."
Mapping the Ethics of Generative AI: A Comprehensive Scoping Review,"The advent of generative artificial intelligence and the widespread adoption
of it in society engendered intensive debates about its ethical implications
and risks. These risks often differ from those associated with traditional
discriminative machine learning. To synthesize the recent discourse and map its
normative concepts, we conducted a scoping review on the ethics of generative
artificial intelligence, including especially large language models and
text-to-image models. Our analysis provides a taxonomy of 378 normative issues
in 19 topic areas and ranks them according to their prevalence in the
literature. The study offers a comprehensive overview for scholars,
practitioners, or policymakers, condensing the ethical debates surrounding
fairness, safety, harmful content, hallucinations, privacy, interaction risks,
security, alignment, societal impacts, and others. We discuss the results,
evaluate imbalances in the literature, and explore unsubstantiated risk
scenarios."
FGeo-TP: A Language Model-Enhanced Solver for Geometry Problems,"The application of contemporary artificial intelligence techniques to address
geometric problems and automated deductive proof has always been a grand
challenge to the interdiscipline field of mathematics and artificial
Intelligence. This is the fourth article in a series of our works, in our
previous work, we established of a geometric formalized system known as
FormalGeo. Moreover we annotated approximately 7000 geometric problems, forming
the FormalGeo7k dataset. Despite the FGPS (Formal Geometry Problem Solver) can
achieve interpretable algebraic equation solving and human-like deductive
reasoning, it often experiences timeouts due to the complexity of the search
strategy. In this paper, we introduced FGeo-TP (Theorem Predictor), which
utilizes the language model to predict theorem sequences for solving geometry
problems. We compared the effectiveness of various Transformer architectures,
such as BART or T5, in theorem prediction, implementing pruning in the search
process of FGPS, thereby improving its performance in solving geometry
problems. Our results demonstrate a significant increase in the problem-solving
rate of the language model-enhanced FGeo-TP on the FormalGeo7k dataset, rising
from 39.7% to 80.86%. Furthermore, FGeo-TP exhibits notable reductions in
solving time and search steps across problems of varying difficulty levels."
"Data Augmentation and Transfer Learning Approaches Applied to Facial
  Expressions Recognition","The face expression is the first thing we pay attention to when we want to
understand a person's state of mind. Thus, the ability to recognize facial
expressions in an automatic way is a very interesting research field. In this
paper, because the small size of available training datasets, we propose a
novel data augmentation technique that improves the performances in the
recognition task. We apply geometrical transformations and build from scratch
GAN models able to generate new synthetic images for each emotion type. Thus,
on the augmented datasets we fine tune pretrained convolutional neural networks
with different architectures. To measure the generalization ability of the
models, we apply extra-database protocol approach, namely we train models on
the augmented versions of training dataset and test them on two different
databases. The combination of these techniques allows to reach average accuracy
values of the order of 85\% for the InceptionResNetV2 model."
"Search Engines Post-ChatGPT: How Generative Artificial Intelligence
  Could Make Search Less Reliable","In this commentary, we discuss the evolving nature of search engines, as they
begin to generate, index, and distribute content created by generative
artificial intelligence (GenAI). Our discussion highlights challenges in the
early stages of GenAI integration, particularly around factual inconsistencies
and biases. We discuss how output from GenAI carries an unwarranted sense of
credibility, while decreasing transparency and sourcing ability. Furthermore,
search engines are already answering queries with error-laden, generated
content, further blurring the provenance of information and impacting the
integrity of the information ecosystem. We argue how all these factors could
reduce the reliability of search engines. Finally, we summarize some of the
active research directions and open questions."
"LangXAI: Integrating Large Vision Models for Generating Textual
  Explanations to Enhance Explainability in Visual Perception Tasks","LangXAI is a framework that integrates Explainable Artificial Intelligence
(XAI) with advanced vision models to generate textual explanations for visual
recognition tasks. Despite XAI advancements, an understanding gap persists for
end-users with limited domain knowledge in artificial intelligence and computer
vision. LangXAI addresses this by furnishing text-based explanations for
classification, object detection, and semantic segmentation model outputs to
end-users. Preliminary results demonstrate LangXAI's enhanced plausibility,
with high BERTScore across tasks, fostering a more transparent and reliable AI
framework on vision tasks for end-users."
"What Generative Artificial Intelligence Means for Terminological
  Definitions","This paper examines the impact of Generative Artificial Intelligence (GenAI)
tools like ChatGPT on the creation and consumption of terminological
definitions. From the terminologist's point of view, the strategic use of GenAI
tools can streamline the process of crafting definitions, reducing both time
and effort, while potentially enhancing quality. GenAI tools enable AI-assisted
terminography, notably post-editing terminography, where the machine produces a
definition that the terminologist then corrects or refines. However, the
potential of GenAI tools to fulfill all the terminological needs of a user,
including term definitions, challenges the very existence of terminological
definitions and resources as we know them. Unlike terminological definitions,
GenAI tools can describe the knowledge activated by a term in a specific
context. However, a main drawback of these tools is that their output can
contain errors. For this reason, users requiring reliability will likely still
resort to terminological resources for definitions. Nevertheless, with the
inevitable integration of AI into terminology work, the distinction between
human-created and AI-created content will become increasingly blurred."
"Identification of important nodes in the information propagation network
  based on the artificial intelligence method","This study presents an integrated approach for identifying key nodes in
information propagation networks using advanced artificial intelligence
methods. We introduce a novel technique that combines the Decision-making Trial
and Evaluation Laboratory (DEMATEL) method with the Global Structure Model
(GSM), creating a synergistic model that effectively captures both local and
global influences within a network. This method is applied across various
complex networks, such as social, transportation, and communication systems,
utilizing the Global Network Influence Dataset (GNID). Our analysis highlights
the structural dynamics and resilience of these networks, revealing insights
into node connectivity and community formation. The findings demonstrate the
effectiveness of our AI-based approach in offering a comprehensive
understanding of network behavior, contributing significantly to strategic
network analysis and optimization."
"Unraveling the Molecular Magic: AI Insights on the Formation of
  Extraordinarily Stretchable Hydrogels","The deliberate manipulation of ammonium persulfate, methylenebisacrylamide,
dimethyleacrylamide, and polyethylene oxide concentrations resulted in the
development of a hydrogel with an exceptional stretchability, capable of
extending up to 260 times its original length. This study aims to elucidate the
molecular architecture underlying this unique phenomenon by exploring potential
reaction mechanisms, facilitated by an artificial intelligence prediction
system. Artificial intelligence predictor introduces a novel approach to
interlinking two polymers, involving the formation of networks interconnected
with linear chains following random chain scission. This novel configuration
leads to the emergence of a distinct type of hydrogel, herein referred to as a
""Span Network."" Additionally, Fourier-transform infrared spectroscopy (FTIR) is
used to investigate functional groups that may be implicated in the proposed
mechanism, with ester formation confirmed among numerous hydroxyl end groups
obtained from chain scission of PEO and carboxyl groups formed on hydrogel
networks."
Responsible Artificial Intelligence: A Structured Literature Review,"Our research endeavors to advance the concept of responsible artificial
intelligence (AI), a topic of increasing importance within EU policy
discussions. The EU has recently issued several publications emphasizing the
necessity of trust in AI, underscoring the dual nature of AI as both a
beneficial tool and a potential weapon. This dichotomy highlights the urgent
need for international regulation. Concurrently, there is a need for frameworks
that guide companies in AI development, ensuring compliance with such
regulations. Our research aims to assist lawmakers and machine learning
practitioners in navigating the evolving landscape of AI regulation,
identifying focal areas for future attention. This paper introduces a
comprehensive and, to our knowledge, the first unified definition of
responsible AI. Through a structured literature review, we elucidate the
current understanding of responsible AI. Drawing from this analysis, we propose
an approach for developing a future framework centered around this concept. Our
findings advocate for a human-centric approach to Responsible AI. This approach
encompasses the implementation of AI methods with a strong emphasis on ethics,
model explainability, and the pillars of privacy, security, and trust."
On the stochastics of human and artificial creativity,"What constitutes human creativity, and is it possible for computers to
exhibit genuine creativity? We argue that achieving human-level intelligence in
computers, or so-called Artificial General Intelligence, necessitates attaining
also human-level creativity. We contribute to this discussion by developing a
statistical representation of human creativity, incorporating prior insights
from stochastic theory, psychology, philosophy, neuroscience, and chaos theory.
This highlights the stochastic nature of the human creative process, which
includes both a bias guided, random proposal step, and an evaluation step
depending on a flexible or transformable bias structure. The acquired
representation of human creativity is subsequently used to assess the
creativity levels of various contemporary AI systems. Our analysis includes
modern AI algorithms such as reinforcement learning, diffusion models, and
large language models, addressing to what extent they measure up to human level
creativity. We conclude that these technologies currently lack the capability
for autonomous creative action at a human level."
"Legally Binding but Unfair? Towards Assessing Fairness of Privacy
  Policies","Privacy policies are expected to inform data subjects about their data
protection rights and should explain the data controller's data management
practices. Privacy policies only fulfill their purpose, if they are correctly
interpreted, understood, and trusted by the data subject. This implies that a
privacy policy is written in a fair way, e.g., it does not use polarizing
terms, does not require a certain education, or does not assume a particular
social background. We outline our approach to assessing fairness in privacy
policies. We identify from fundamental legal sources and fairness research, how
the dimensions informational fairness, representational fairness and ethics /
morality are related to privacy policies. We propose options to automatically
assess policies in these fairness dimensions, based on text statistics,
linguistic methods and artificial intelligence. We conduct initial experiments
with German privacy policies to provide evidence that our approach is
applicable. Our experiments indicate that there are issues in all three
dimensions of fairness. This is important, as future privacy policies may be
used in a corpus for legal artificial intelligence models."
"Bridging Human Concepts and Computer Vision for Explainable Face
  Verification","With Artificial Intelligence (AI) influencing the decision-making process of
sensitive applications such as Face Verification, it is fundamental to ensure
the transparency, fairness, and accountability of decisions. Although
Explainable Artificial Intelligence (XAI) techniques exist to clarify AI
decisions, it is equally important to provide interpretability of these
decisions to humans. In this paper, we present an approach to combine computer
and human vision to increase the explanation's interpretability of a face
verification algorithm. In particular, we are inspired by the human perceptual
process to understand how machines perceive face's human-semantic areas during
face comparison tasks. We use Mediapipe, which provides a segmentation
technique that identifies distinct human-semantic facial regions, enabling the
machine's perception analysis. Additionally, we adapted two model-agnostic
algorithms to provide human-interpretable insights into the decision-making
processes."
Governance of Generative Artificial Intelligence for Companies,"Generative Artificial Intelligence (GenAI), specifically large language
models like ChatGPT, has swiftly entered organizations without adequate
governance, posing both opportunities and risks. Despite extensive debates on
GenAI's transformative nature and regulatory measures, limited research
addresses organizational governance, encompassing technical and business
perspectives. Although numerous frameworks for governance of AI exist, it is
not clear to what extent they apply to GenAI. Our review paper fills this gap
by surveying recent works with the purpose of better understanding fundamental
characteristics of GenAI and adjusting prior frameworks specifically towards
GenAI governance within companies. To do so, it extends Nickerson's framework
development processes to include prior conceptualizations. Our framework
outlines the scope, objectives, and governance mechanisms tailored to harness
business opportunities as well as mitigate risks associated with GenAI
integration. Our research contributes a focused approach to GenAI governance,
offering practical insights for companies navigating the challenges of GenAI
adoption and highlighting research gaps."
"Toward Sustainable GenAI using Generation Directives for Carbon-Friendly
  Large Language Model Inference","The rapid advancement of Generative Artificial Intelligence (GenAI) across
diverse sectors raises significant environmental concerns, notably the carbon
emissions from their cloud and high performance computing (HPC) infrastructure.
This paper presents Sprout, an innovative framework designed to address these
concerns by reducing the carbon footprint of generative Large Language Model
(LLM) inference services. Sprout leverages the innovative concept of
""generation directives"" to guide the autoregressive generation process, thereby
enhancing carbon efficiency. Our proposed method meticulously balances the need
for ecological sustainability with the demand for high-quality generation
outcomes. Employing a directive optimizer for the strategic assignment of
generation directives to user prompts and an original offline quality
evaluator, Sprout demonstrates a significant reduction in carbon emissions by
over 40% in real-world evaluations using the Llama2 LLM and global electricity
grid data. This research marks a critical step toward aligning AI technology
with sustainable practices, highlighting the potential for mitigating
environmental impacts in the rapidly expanding domain of generative artificial
intelligence."
"Identifying Potential Inlets of Man in the Artificial Intelligence
  Development Process","In this paper we hope to identify how the typical or standard artificial
intelligence development process encourages or facilitates the creation of
racialized technologies. We begin by understanding Sylvia Wynter's definition
of the biocentric Man genre and its exclusion of Blackness from humanness. We
follow this with outlining what we consider to be the typical steps for
developing an AI-based technology, which we have broken down into 6 stages:
identifying a problem, development process and management tool selection,
dataset development and data processing, model development, deployment and risk
assessment, and integration and monitoring. The goal of this paper is to better
understand how Wynter's biocentric Man is being represented and reinforced by
the technologies we are producing in the AI lifecycle and by the lifecycle
itself; we hope to identify ways in which the distinction of Blackness from the
""ideal"" human leads to perpetual punishment at the hands of these technologies.
By deconstructing this development process, we can potentially identify ways in
which humans in general have not been prioritized and how those affects are
disproportionately affecting marginalized people. We hope to offer solutions
that will encourage changes in the AI development cycle."
"End-to-End Mineral Exploration with Artificial Intelligence and Ambient
  Noise Tomography","This paper presents an innovative end-to-end workflow for mineral
exploration, integrating ambient noise tomography (ANT) and artificial
intelligence (AI) to enhance the discovery and delineation of mineral resources
essential for the global transition to a low carbon economy. We focus on copper
as a critical element, required in significant quantities for renewable energy
solutions. We show the benefits of utilising ANT, characterised by its speed,
scalability, depth penetration, resolution, and low environmental impact,
alongside artificial intelligence (AI) techniques to refine a continent-scale
prospectivity model at the deposit scale by fine-tuning our model on local
high-resolution data. We show the promise of the method by first presenting a
new data-driven AI prospectivity model for copper within Australia, which
serves as our foundation model for further fine-tuning. We then focus on the
Hillside IOCG deposit on the prospective Yorke Peninsula. We show that with
relatively few local training samples (orebody intercepts), we can fine tune
the foundation model to provide a good estimate of the Hillside orebody
outline. Our methodology demonstrates how AI can augment geophysical data
interpretation, providing a novel approach to mineral exploration with improved
decision-making capabilities for targeting mineralization, thereby addressing
the urgent need for increased mineral resource discovery."
"Opportunities and challenges in the application of large artificial
  intelligence models in radiology","Influenced by ChatGPT, artificial intelligence (AI) large models have
witnessed a global upsurge in large model research and development. As people
enjoy the convenience by this AI large model, more and more large models in
subdivided fields are gradually being proposed, especially large models in
radiology imaging field. This article first introduces the development history
of large models, technical details, workflow, working principles of multimodal
large models and working principles of video generation large models. Secondly,
we summarize the latest research progress of AI large models in radiology
education, radiology report generation, applications of unimodal and multimodal
radiology. Finally, this paper also summarizes some of the challenges of large
AI models in radiology, with the aim of better promoting the rapid revolution
in the field of radiography."
"All Artificial, Less Intelligence: GenAI through the Lens of Formal
  Verification","Modern hardware designs have grown increasingly efficient and complex.
However, they are often susceptible to Common Weakness Enumerations (CWEs).
This paper is focused on the formal verification of CWEs in a dataset of
hardware designs written in SystemVerilog from Regenerative Artificial
Intelligence (AI) powered by Large Language Models (LLMs). We applied formal
verification to categorize each hardware design as vulnerable or CWE-free. This
dataset was generated by 4 different LLMs and features a unique set of designs
for each of the 10 CWEs we target in our paper. We have associated the
identified vulnerabilities with CWE numbers for a dataset of 60,000 generated
SystemVerilog Register Transfer Level (RTL) code. It was also found that most
LLMs are not aware of any hardware CWEs; hence they are usually not considered
when generating the hardware code. Our study reveals that approximately 60% of
the hardware designs generated by LLMs are prone to CWEs, posing potential
safety and security risks. The dataset could be ideal for training LLMs and
Machine Learning (ML) algorithms to abstain from generating CWE-prone hardware
designs."
"Toward Safe Evolution of Artificial Intelligence (AI) based
  Conversational Agents to Support Adolescent Mental and Sexual Health
  Knowledge Discovery","Following the recent release of various Artificial Intelligence (AI) based
Conversation Agents (CAs), adolescents are increasingly using CAs for
interactive knowledge discovery on sensitive topics, including mental and
sexual health topics. Exploring such sensitive topics through online search has
been an essential part of adolescent development, and CAs can support their
knowledge discovery on such topics through human-like dialogues. Yet,
unintended risks have been documented with adolescents' interactions with
AI-based CAs, such as being exposed to inappropriate content, false
information, and/or being given advice that is detrimental to their mental and
physical well-being (e.g., to self-harm). In this position paper, we discuss
the current landscape and opportunities for CAs to support adolescents' mental
and sexual health knowledge discovery. We also discuss some of the challenges
related to ensuring the safety of adolescents when interacting with CAs
regarding sexual and mental health topics. We call for a discourse on how to
set guardrails for the safe evolution of AI-based CAs for adolescents."
"Unraveling the Dilemma of AI Errors: Exploring the Effectiveness of
  Human and Machine Explanations for Large Language Models","The field of eXplainable artificial intelligence (XAI) has produced a
plethora of methods (e.g., saliency-maps) to gain insight into artificial
intelligence (AI) models, and has exploded with the rise of deep learning (DL).
However, human-participant studies question the efficacy of these methods,
particularly when the AI output is wrong. In this study, we collected and
analyzed 156 human-generated text and saliency-based explanations collected in
a question-answering task (N=40) and compared them empirically to
state-of-the-art XAI explanations (integrated gradients, conservative LRP, and
ChatGPT) in a human-participant study (N=136). Our findings show that
participants found human saliency maps to be more helpful in explaining AI
answers than machine saliency maps, but performance negatively correlated with
trust in the AI model and explanations. This finding hints at the dilemma of AI
errors in explanation, where helpful explanations can lead to lower task
performance when they support wrong AI predictions."
"Interaction as Explanation: A User Interaction-based Method for
  Explaining Image Classification Models","In computer vision, explainable AI (xAI) methods seek to mitigate the
'black-box' problem by making the decision-making process of deep learning
models more interpretable and transparent. Traditional xAI methods concentrate
on visualizing input features that influence model predictions, providing
insights primarily suited for experts. In this work, we present an
interaction-based xAI method that enhances user comprehension of image
classification models through their interaction. Thus, we developed a web-based
prototype allowing users to modify images via painting and erasing, thereby
observing changes in classification results. Our approach enables users to
discern critical features influencing the model's decision-making process,
aligning their mental models with the model's logic. Experiments conducted with
five images demonstrate the potential of the method to reveal feature
importance through user interaction. Our work contributes a novel perspective
to xAI by centering on end-user engagement and understanding, paving the way
for more intuitive and accessible explainability in AI systems."
"CNN-based explanation ensembling for dataset, representation and
  explanations evaluation","Explainable Artificial Intelligence has gained significant attention due to
the widespread use of complex deep learning models in high-stake domains such
as medicine, finance, and autonomous cars. However, different explanations
often present different aspects of the model's behavior. In this research
manuscript, we explore the potential of ensembling explanations generated by
deep classification models using convolutional model. Through experimentation
and analysis, we aim to investigate the implications of combining explanations
to uncover a more coherent and reliable patterns of the model's behavior,
leading to the possibility of evaluating the representation learned by the
model. With our method, we can uncover problems of under-representation of
images in a certain class. Moreover, we discuss other side benefits like
features' reduction by replacing the original image with its explanations
resulting in the removal of some sensitive information. Through the use of
carefully selected evaluation metrics from the Quantus library, we demonstrated
the method's superior performance in terms of Localisation and Faithfulness,
compared to individual explanations."
"Explainable Artificial Intelligence Techniques for Accurate Fault
  Detection and Diagnosis: A Review","As the manufacturing industry advances with sensor integration and
automation, the opaque nature of deep learning models in machine learning poses
a significant challenge for fault detection and diagnosis. And despite the
related predictive insights Artificial Intelligence (AI) can deliver, advanced
machine learning engines often remain a black box. This paper reviews the
eXplainable AI (XAI) tools and techniques in this context. We explore various
XAI methodologies, focusing on their role in making AI decision-making
transparent, particularly in critical scenarios where humans are involved. We
also discuss current limitations and potential future research that aims to
balance explainability with model performance while improving trustworthiness
in the context of AI applications for critical industrial use cases."
Explainable AI for Fair Sepsis Mortality Predictive Model,"Artificial intelligence supports healthcare professionals with predictive
modeling, greatly transforming clinical decision-making. This study addresses
the crucial need for fairness and explainability in AI applications within
healthcare to ensure equitable outcomes across diverse patient demographics. By
focusing on the predictive modeling of sepsis-related mortality, we propose a
method that learns a performance-optimized predictive model and then employs
the transfer learning process to produce a model with better fairness. Our
method also introduces a novel permutation-based feature importance algorithm
aiming at elucidating the contribution of each feature in enhancing fairness on
predictions. Unlike existing explainability methods concentrating on explaining
feature contribution to predictive performance, our proposed method uniquely
bridges the gap in understanding how each feature contributes to fairness. This
advancement is pivotal, given sepsis's significant mortality rate and its role
in one-third of hospital deaths. Our method not only aids in identifying and
mitigating biases within the predictive model but also fosters trust among
healthcare stakeholders by improving the transparency and fairness of model
predictions, thereby contributing to more equitable and trustworthy healthcare
delivery."
Exploring Diverse Methods in Visual Question Answering,"This study explores innovative methods for improving Visual Question
Answering (VQA) using Generative Adversarial Networks (GANs), autoencoders, and
attention mechanisms. Leveraging a balanced VQA dataset, we investigate three
distinct strategies. Firstly, GAN-based approaches aim to generate answer
embeddings conditioned on image and question inputs, showing potential but
struggling with more complex tasks. Secondly, autoencoder-based techniques
focus on learning optimal embeddings for questions and images, achieving
comparable results with GAN due to better ability on complex questions. Lastly,
attention mechanisms, incorporating Multimodal Compact Bilinear pooling (MCB),
address language priors and attention modeling, albeit with a
complexity-performance trade-off. This study underscores the challenges and
opportunities in VQA and suggests avenues for future research, including
alternative GAN formulations and attentional mechanisms."
"Gallbladder Cancer Detection in Ultrasound Images based on YOLO and
  Faster R-CNN","Medical image analysis is a significant application of artificial
intelligence for disease diagnosis. A crucial step in this process is the
identification of regions of interest within the images. This task can be
automated using object detection algorithms. YOLO and Faster R-CNN are renowned
for such algorithms, each with its own strengths and weaknesses. This study
aims to explore the advantages of both techniques to select more accurate
bounding boxes for gallbladder detection from ultrasound images, thereby
enhancing gallbladder cancer classification. A fusion method that leverages the
benefits of both techniques is presented in this study. The proposed method
demonstrated superior classification performance, with an accuracy of 92.62%,
compared to the individual use of Faster R-CNN and YOLOv8, which yielded
accuracies of 90.16% and 82.79%, respectively."
Fiper: a Visual-based Explanation Combining Rules and Feature Importance,"Artificial Intelligence algorithms have now become pervasive in multiple
high-stakes domains. However, their internal logic can be obscure to humans.
Explainable Artificial Intelligence aims to design tools and techniques to
illustrate the predictions of the so-called black-box algorithms. The
Human-Computer Interaction community has long stressed the need for a more
user-centered approach to Explainable AI. This approach can benefit from
research in user interface, user experience, and visual analytics. This paper
proposes a visual-based method to illustrate rules paired with feature
importance. A user study with 15 participants was conducted comparing our
visual method with the original output of the algorithm and textual
representation to test its effectiveness with users."
"Leveraging AI to Generate Audio for User-generated Content in Video
  Games","In video game design, audio (both environmental background music and object
sound effects) play a critical role. Sounds are typically pre-created assets
designed for specific locations or objects in a game. However, user-generated
content is becoming increasingly popular in modern games (e.g. building custom
environments or crafting unique objects). Since the possibilities are virtually
limitless, it is impossible for game creators to pre-create audio for
user-generated content. We explore the use of generative artificial
intelligence to create music and sound effects on-the-fly based on
user-generated content. We investigate two avenues for audio generation: 1)
text-to-audio: using a text description of user-generated content as input to
the audio generator, and 2) image-to-audio: using a rendering of the created
environment or object as input to an image-to-text generator, then piping the
resulting text description into the audio generator. In this paper we discuss
ethical implications of using generative artificial intelligence for
user-generated content and highlight two prototype games where audio is
generated for user-created environments and objects."
DTization: A New Method for Supervised Feature Scaling,"Artificial intelligence is currently a dominant force in shaping various
aspects of the world. Machine learning is a sub-field in artificial
intelligence. Feature scaling is one of the data pre-processing techniques that
improves the performance of machine learning algorithms. The traditional
feature scaling techniques are unsupervised where they do not have influence of
the dependent variable in the scaling process. In this paper, we have presented
a novel feature scaling technique named DTization that employs decision tree
and robust scaler for supervised feature scaling. The proposed method utilizes
decision tree to measure the feature importance and based on the importance,
different features get scaled differently with the robust scaler algorithm. The
proposed method has been extensively evaluated on ten classification and
regression datasets on various evaluation matrices and the results show a
noteworthy performance improvement compared to the traditional feature scaling
methods."
Using artificial intelligence methods for the studyed visual analyzer,"The paper describes how various techniques for applying artificial
intelligence to the study of human eyes are utilized. The first dataset was
collected using computerized perimetry to investigate the visualization of the
human visual field and the diagnosis of glaucoma. A method to analyze the image
using software tools is proposed. The second dataset was obtained, as part of
the implementation of a Russian-Swiss experiment to collect and analyze eye
movement data using the Tobii Pro Glasses 3 device on VR video. Eye movements
and focus on the recorded route of a virtual journey through the canton of Vaud
were investigated. Methods are being developed to investigate the dependencies
of eye pupil movements using mathematical modelling. VR-video users can use
these studies in medicine to assess the course and deterioration of glaucoma
patients and to study the mechanisms of attention to tourist attractions."
"Is Artificial Intelligence the great filter that makes advanced
  technical civilisations rare in the universe?","This study examines the hypothesis that the rapid development of Artificial
Intelligence (AI), culminating in the emergence of Artificial Superintelligence
(ASI), could act as a ""Great Filter"" that is responsible for the scarcity of
advanced technological civilisations in the universe. It is proposed that such
a filter emerges before these civilisations can develop a stable,
multiplanetary existence, suggesting the typical longevity (L) of a technical
civilization is less than 200 years. Such estimates for L, when applied to
optimistic versions of the Drake equation, are consistent with the null results
obtained by recent SETI surveys, and other efforts to detect various
technosignatures across the electromagnetic spectrum. Through the lens of SETI,
we reflect on humanity's current technological trajectory - the modest
projections for L suggested here, underscore the critical need to quickly
establish regulatory frameworks for AI development on Earth and the advancement
of a multiplanetary society to mitigate against such existential threats. The
persistence of intelligent and conscious life in the universe could hinge on
the timely and effective implementation of such international regulatory
measures and technological endeavours."
"An Artificial Intelligence Approach for Interpreting Creative
  Combinational Designs","Combinational creativity, a form of creativity involving the blending of
familiar ideas, is pivotal in design innovation. While most research focuses on
how combinational creativity in design is achieved through blending elements,
this study focuses on the computational interpretation, specifically
identifying the 'base' and 'additive' components that constitute a creative
design. To achieve this goal, the authors propose a heuristic algorithm
integrating computer vision and natural language processing technologies, and
implement multiple approaches based on both discriminative and generative
artificial intelligence architectures. A comprehensive evaluation was conducted
on a dataset created for studying combinational creativity. Among the
implementations of the proposed algorithm, the most effective approach
demonstrated a high accuracy in interpretation, achieving 87.5% for identifying
'base' and 80% for 'additive'. We conduct a modular analysis and an ablation
experiment to assess the performance of each part in our implementations.
Additionally, the study includes an analysis of error cases and bottleneck
issues, providing critical insights into the limitations and challenges
inherent in the computational interpretation of creative designs."
"Opportunities for Persian Digital Humanities Research with Artificial
  Intelligence Language Models; Case Study: Forough Farrokhzad","This study explores the integration of advanced Natural Language Processing
(NLP) and Artificial Intelligence (AI) techniques to analyze and interpret
Persian literature, focusing on the poetry of Forough Farrokhzad. Utilizing
computational methods, we aim to unveil thematic, stylistic, and linguistic
patterns in Persian poetry. Specifically, the study employs AI models including
transformer-based language models for clustering of the poems in an
unsupervised framework. This research underscores the potential of AI in
enhancing our understanding of Persian literary heritage, with Forough
Farrokhzad's work providing a comprehensive case study. This approach not only
contributes to the field of Persian Digital Humanities but also sets a
precedent for future research in Persian literary studies using computational
techniques."
Summarizing Radiology Reports Findings into Impressions,"Patient hand-off and triage are two fundamental problems in health care.
Often doctors must painstakingly summarize complex findings to efficiently
communicate with specialists and quickly make decisions on which patients have
the most urgent cases. In pursuit of these challenges, we present (1) a model
with state-of-art radiology report summarization performance using (2) a novel
method for augmenting medical data, and (3) an analysis of the model
limitations and radiology knowledge gain. We also provide a data processing
pipeline for future models developed on the the MIMIC CXR dataset. Our best
performing model was a fine-tuned BERT-to-BERT encoder-decoder with 58.75/100
ROUGE-L F1, which outperformed specialized checkpoints with more sophisticated
attention mechanisms. We investigate these aspects in this work."
"Evaluating the Explainable AI Method Grad-CAM for Breath Classification
  on Newborn Time Series Data","With the digitalization of health care systems, artificial intelligence
becomes more present in medicine. Especially machine learning shows great
potential for complex tasks such as time series classification, usually at the
cost of transparency and comprehensibility. This leads to a lack of trust by
humans and thus hinders its active usage. Explainable artificial intelligence
tries to close this gap by providing insight into the decision-making process,
the actual usefulness of its different methods is however unclear. This paper
proposes a user study based evaluation of the explanation method Grad-CAM with
application to a neural network for the classification of breaths in time
series neonatal ventilation data. We present the perceived usefulness of the
explainability method by different stakeholders, exposing the difficulty to
achieve actual transparency and the wish for more in-depth explanations by many
of the participants."
From Sora What We Can See: A Survey of Text-to-Video Generation,"With impressive achievements made, artificial intelligence is on the path
forward to artificial general intelligence. Sora, developed by OpenAI, which is
capable of minute-level world-simulative abilities can be considered as a
milestone on this developmental path. However, despite its notable successes,
Sora still encounters various obstacles that need to be resolved. In this
survey, we embark from the perspective of disassembling Sora in text-to-video
generation, and conducting a comprehensive review of literature, trying to
answer the question, \textit{From Sora What We Can See}. Specifically, after
basic preliminaries regarding the general algorithms are introduced, the
literature is categorized from three mutually perpendicular dimensions:
evolutionary generators, excellent pursuit, and realistic panorama.
Subsequently, the widely used datasets and metrics are organized in detail.
Last but more importantly, we identify several challenges and open problems in
this domain and propose potential future directions for research and
development."
"Application of Artificial Intelligence in Schizophrenia Rehabilitation
  Management: A Systematic Scoping Review","This systematic review assessed the current state and future prospects of
artificial intelligence (AI) in schizophrenia rehabilitation management. We
reviewed 61 studies on AI-related data types, feature engineering methods,
algorithmic models, and evaluation metrics published from 2012-2024. The review
categorizes AI applications into the following key application areas: symptom
monitoring, medication management, risk management, functional training, and
psychosocial support. Findings indicate that supervised machine learning
techniques, particularly for symptom monitoring and relapse risk management,
remain the predominant approaches, effectively leveraging structured data while
incorporating interpretable algorithms. This study underscores the potential of
AI in transforming long-term management strategies for schizophrenia, offering
valuable insights into improving the quality of life of patients. Future
research should focus on expanding data sources through multimodal data
integration, exploring deep learning models, and integrating AI-driven
interventions into training tasks to fully capitalize on AI's potential in
schizophrenia rehabilitation."
"Artificial Intelligence in Industry 4.0: A Review of Integration
  Challenges for Industrial Systems","In Industry 4.0, Cyber-Physical Systems (CPS) generate vast data sets that
can be leveraged by Artificial Intelligence (AI) for applications including
predictive maintenance and production planning. However, despite the
demonstrated potential of AI, its widespread adoption in sectors like
manufacturing remains limited. Our comprehensive review of recent literature,
including standards and reports, pinpoints key challenges: system integration,
data-related issues, managing workforce-related concerns and ensuring
trustworthy AI. A quantitative analysis highlights particular challenges and
topics that are important for practitioners but still need to be sufficiently
investigated by academics. The paper briefly discusses existing solutions to
these challenges and proposes avenues for future research. We hope that this
survey serves as a resource for practitioners evaluating the cost-benefit
implications of AI in CPS and for researchers aiming to address these urgent
challenges."
"Strategies to Counter Artificial Intelligence in Law Enforcement:
  Cross-Country Comparison of Citizens in Greece, Italy and Spain","This paper investigates citizens' counter-strategies to the use of Artificial
Intelligence (AI) by law enforcement agencies (LEAs). Based on information from
three countries (Greece, Italy and Spain) we demonstrate disparities in the
likelihood of ten specific counter-strategies. We further identified factors
that increase the propensity for counter-strategies. Our study provides an
important new perspective to societal impacts of security-focused AI
applications by illustrating the conscious, strategic choices by citizens when
confronted with AI capabilities for LEAs."
"Nuclear Medicine Artificial Intelligence in Action: The Bethesda Report
  (AI Summit 2024)","The 2nd SNMMI Artificial Intelligence (AI) Summit, organized by the SNMMI AI
Task Force, took place in Bethesda, MD, on February 29 - March 1, 2024.
Bringing together various community members and stakeholders, and following up
on a prior successful 2022 AI Summit, the summit theme was: AI in Action. Six
key topics included (i) an overview of prior and ongoing efforts by the AI task
force, (ii) emerging needs and tools for computational nuclear oncology, (iii)
new frontiers in large language and generative models, (iv) defining the value
proposition for the use of AI in nuclear medicine, (v) open science including
efforts for data and model repositories, and (vi) issues of reimbursement and
funding. The primary efforts, findings, challenges, and next steps are
summarized in this manuscript."
Transferring Domain Knowledge with (X)AI-Based Learning Systems,"In numerous high-stakes domains, training novices via conventional learning
systems does not suffice. To impart tacit knowledge, experts' hands-on guidance
is imperative. However, training novices by experts is costly and
time-consuming, increasing the need for alternatives. Explainable artificial
intelligence (XAI) has conventionally been used to make black-box artificial
intelligence systems interpretable. In this work, we utilize XAI as an
alternative: An (X)AI system is trained on experts' past decisions and is then
employed to teach novices by providing examples coupled with explanations. In a
study with 249 participants, we measure the effectiveness of such an approach
for a classification task. We show that (X)AI-based learning systems are able
to induce learning in novices and that their cognitive styles moderate
learning. Thus, we take the first steps to reveal the impact of XAI on human
learning and point AI developers to future options to tailor the design of
(X)AI-based learning systems."
"Promoting the Responsible Development of Speech Datasets for Mental
  Health and Neurological Disorders Research","Current research in machine learning and artificial intelligence is largely
centered on modeling and performance evaluation, less so on data collection.
However, recent research demonstrated that limitations and biases in data may
negatively impact trustworthiness and reliability. These aspects are
particularly impactful on sensitive domains such as mental health and
neurological disorders, where speech data are used to develop AI applications
for patients and healthcare providers. In this paper, we chart the landscape of
available speech datasets for this domain, to highlight possible pitfalls and
opportunities for improvement and promote fairness and diversity. We present a
comprehensive list of desiderata for building speech datasets for mental health
and neurological disorders and distill it into an actionable checklist focused
on ethical concerns to foster more responsible research."
Open-Endedness is Essential for Artificial Superhuman Intelligence,"In recent years there has been a tremendous surge in the general capabilities
of AI systems, mainly fuelled by training foundation models on internetscale
data. Nevertheless, the creation of openended, ever self-improving AI remains
elusive. In this position paper, we argue that the ingredients are now in place
to achieve openendedness in AI systems with respect to a human observer.
Furthermore, we claim that such open-endedness is an essential property of any
artificial superhuman intelligence (ASI). We begin by providing a concrete
formal definition of open-endedness through the lens of novelty and
learnability. We then illustrate a path towards ASI via open-ended systems
built on top of foundation models, capable of making novel, humanrelevant
discoveries. We conclude by examining the safety implications of
generally-capable openended AI. We expect that open-ended foundation models
will prove to be an increasingly fertile and safety-critical area of research
in the near future."
Multimodal Approach for Harmonized System Code Prediction,"The rapid growth of e-commerce has placed considerable pressure on customs
representatives, prompting advanced methods. In tackling this, Artificial
intelligence (AI) systems have emerged as a promising approach to minimize the
risks faced. Given that the Harmonized System (HS) code is a crucial element
for an accurate customs declaration, we propose a novel multimodal HS code
prediction approach using deep learning models exploiting both image and text
features obtained through the customs declaration combined with e-commerce
platform information. We evaluated two early fusion methods and introduced our
MultConcat fusion method. To the best of our knowledge, few studies analyze the
featurelevel combination of text and image in the state-of-the-art for HS code
prediction, which heightens interest in our paper and its findings. The
experimental results prove the effectiveness of our approach and fusion method
with a top-3 and top-5 accuracy of 93.5% and 98.2% respectively"
"Transforming Dental Diagnostics with Artificial Intelligence: Advanced
  Integration of ChatGPT and Large Language Models for Patient Care","Artificial intelligence has dramatically reshaped our interaction with
digital technologies, ushering in an era where advancements in AI algorithms
and Large Language Models (LLMs) have natural language processing (NLP) systems
like ChatGPT. This study delves into the impact of cutting-edge LLMs, notably
OpenAI's ChatGPT, on medical diagnostics, with a keen focus on the dental
sector. Leveraging publicly accessible datasets, these models augment the
diagnostic capabilities of medical professionals, streamline communication
between patients and healthcare providers, and enhance the efficiency of
clinical procedures. The advent of ChatGPT-4 is poised to make substantial
inroads into dental practices, especially in the realm of oral surgery. This
paper sheds light on the current landscape and explores potential future
research directions in the burgeoning field of LLMs, offering valuable insights
for both practitioners and developers. Furthermore, it critically assesses the
broad implications and challenges within various sectors, including academia
and healthcare, thus mapping out an overview of AI's role in transforming
dental diagnostics for enhanced patient care."
"Research on Edge Detection of LiDAR Images Based on Artificial
  Intelligence Technology","With the widespread application of Light Detection and Ranging (LiDAR)
technology in fields such as autonomous driving, robot navigation, and terrain
mapping, the importance of edge detection in LiDAR images has become
increasingly prominent. Traditional edge detection methods often face
challenges in accuracy and computational complexity when processing LiDAR
images. To address these issues, this study proposes an edge detection method
for LiDAR images based on artificial intelligence technology. This paper first
reviews the current state of research on LiDAR technology and image edge
detection, introducing common edge detection algorithms and their applications
in LiDAR image processing. Subsequently, a deep learning-based edge detection
model is designed and implemented, optimizing the model training process
through preprocessing and enhancement of the LiDAR image dataset. Experimental
results indicate that the proposed method outperforms traditional methods in
terms of detection accuracy and computational efficiency, showing significant
practical application value. Finally, improvement strategies are proposed for
the current method's shortcomings, and the improvements are validated through
experiments."
"Data Ethics in the Era of Healthcare Artificial Intelligence in Africa:
  An Ubuntu Philosophy Perspective","Data are essential in developing healthcare artificial intelligence (AI)
systems. However, patient data collection, access, and use raise ethical
concerns, including informed consent, data bias, data protection and privacy,
data ownership, and benefit sharing. Various ethical frameworks have been
proposed to ensure the ethical use of healthcare data and AI, however, these
frameworks often align with Western cultural values, social norms, and
institutional contexts emphasizing individual autonomy and well-being. Ethical
guidelines must reflect political and cultural settings to account for cultural
diversity, inclusivity, and historical factors such as colonialism. Thus, this
paper discusses healthcare data ethics in the AI era in Africa from the Ubuntu
philosophy perspective. It focuses on the contrast between individualistic and
communitarian approaches to data ethics. The proposed framework could inform
stakeholders, including AI developers, healthcare providers, the public, and
policy-makers about healthcare data ethical usage in AI in Africa."
Justice in Healthcare Artificial Intelligence in Africa,"There is an ongoing debate on balancing the benefits and risks of artificial
intelligence (AI) as AI is becoming critical to improving healthcare delivery
and patient outcomes. Such improvements are essential in resource-constrained
settings where millions lack access to adequate healthcare services, such as in
Africa. AI in such a context can potentially improve the effectiveness,
efficiency, and accessibility of healthcare services. Nevertheless, the
development and use of AI-driven healthcare systems raise numerous ethical,
legal, and socio-economic issues. Justice is a major concern in AI that has
implications for amplifying social inequities. This paper discusses these
implications and related justice concepts such as solidarity, Common Good,
sustainability, AI bias, and fairness. For Africa to effectively benefit from
AI, these principles should align with the local context while balancing the
risks. Compared to mainstream ethical debates on justice, this perspective
offers context-specific considerations for equitable healthcare AI development
in Africa."
"Explainable Artificial Intelligence and Multicollinearity : A Mini
  Review of Current Approaches","Explainable Artificial Intelligence (XAI) methods help to understand the
internal mechanism of machine learning models and how they reach a specific
decision or made a specific action. The list of informative features is one of
the most common output of XAI methods. Multicollinearity is one of the big
issue that should be considered when XAI generates the explanation in terms of
the most informative features in an AI system. No review has been dedicated to
investigate the current approaches to handle such significant issue. In this
paper, we provide a review of the current state-of-the-art approaches in
relation to the XAI in the context of recent advances in dealing with the
multicollinearity issue. To do so, we searched in three repositories that are:
Web of Science, Scopus and IEEE Xplore to find pertinent published papers.
After excluding irrelevant papers, seven papers were considered in the review.
In addition, we discuss the current XAI methods and their limitations in
dealing with the multicollinearity and suggest future directions."
"Generative AI for Enhancing Active Learning in Education: A Comparative
  Study of GPT-3.5 and GPT-4 in Crafting Customized Test Questions","This study investigates how LLMs, specifically GPT-3.5 and GPT-4, can develop
tailored questions for Grade 9 math, aligning with active learning principles.
By utilizing an iterative method, these models adjust questions based on
difficulty and content, responding to feedback from a simulated 'student'
model. A novel aspect of the research involved using GPT-4 as a 'teacher' to
create complex questions, with GPT-3.5 as the 'student' responding to these
challenges. This setup mirrors active learning, promoting deeper engagement.
The findings demonstrate GPT-4's superior ability to generate precise,
challenging questions and notable improvements in GPT-3.5's ability to handle
more complex problems after receiving instruction from GPT-4. These results
underscore the potential of LLMs to mimic and enhance active learning
scenarios, offering a promising path for AI in customized education. This
research contributes to understanding how AI can support personalized learning
experiences, highlighting the need for further exploration in various
educational contexts"
"When Can You Trust Your Explanations? A Robustness Analysis on Feature
  Importances","Recent legislative regulations have underlined the need for accountable and
transparent artificial intelligence systems and have contributed to a growing
interest in the Explainable Artificial Intelligence (XAI) field. Nonetheless,
the lack of standardized criteria to validate explanation methodologies remains
a major obstacle to developing trustworthy systems. We address a crucial yet
often overlooked aspect of XAI, the robustness of explanations, which plays a
central role in ensuring trust in both the system and the provided explanation.
To this end, we propose a novel approach to analyse the robustness of neural
network explanations to non-adversarial perturbations, leveraging the manifold
hypothesis to produce new perturbed datapoints that resemble the observed data
distribution. We additionally present an ensemble method to aggregate various
explanations, showing how merging explanations can be beneficial for both
understanding the model's decision and evaluating the robustness. The aim of
our work is to provide practitioners with a framework for evaluating the
trustworthiness of model explanations. Experimental results on feature
importances derived from neural networks applied to tabular datasets highlight
the importance of robust explanations in practical applications."
"An updated overview of radiomics-based artificial intelligence (AI)
  methods in breast cancer screening and diagnosis","Current imaging methods for diagnosing BC are associated with limited
sensitivity and specificity and modest positive predictive power. The recent
progress in image analysis using artificial intelligence (AI) has created great
promise to improve breast cancer (BC) diagnosis and subtype differentiation. In
this case, novel quantitative computational methods, such as radiomics, have
been developed to improve the sensitivity and specificity of early BC diagnosis
and classification. The potential of radiomics in improving the diagnostic
efficacy of imaging studies has been shown in several studies. In this review
article, we discuss the radiomics workflow and current hand-crafted radiomics
methods in the diagnosis and classification of BC based on most recent studies
on different imaging modalities, e.g. MRI, mammography, contrast-enhanced
spectral mammography (CESM), ultrasound imaging, and digital breast
tumosynthesis (DBT). We also discuss current challenges and potential
strategies to improve the specificity and sensitivity of radiomics in breast
cancer to help achieve a higher level of BC classification and diagnosis in the
clinical setting. The growing field of AI incorporation with imaging
information has opened a great opportunity to provide a higher level of care
for BC patients."
"Younger: The First Dataset for Artificial Intelligence-Generated Neural
  Network Architecture","Designing and optimizing neural network architectures typically requires
extensive expertise, starting with handcrafted designs and then manual or
automated refinement. This dependency presents a significant barrier to rapid
innovation. Recognizing the complexity of automatically generating neural
network architecture from scratch, we introduce Younger, a pioneering dataset
to advance this ambitious goal. Derived from over 174K real-world models across
more than 30 tasks from various public model hubs, Younger includes 7,629
unique architectures, and each is represented as a directed acyclic graph with
detailed operator-level information. The dataset facilitates two primary design
paradigms: global, for creating complete architectures from scratch, and local,
for detailed architecture component refinement. By establishing these
capabilities, Younger contributes to a new frontier, Artificial
Intelligence-Generated Neural Network Architecture (AIGNNA). Our experiments
explore the potential and effectiveness of Younger for automated architecture
generation and, as a secondary benefit, demonstrate that Younger can serve as a
benchmark dataset, advancing the development of graph neural networks. We
release the dataset and code publicly to lower the entry barriers and encourage
further research in this challenging area."
"Orangutan: A Multiscale Brain Emulation-Based Artificial Intelligence
  Framework for Dynamic Environments","Achieving General Artificial Intelligence (AGI) has long been a grand
challenge in the field of AI, and brain-inspired computing is widely
acknowledged as one of the most promising approaches to realize this goal. This
paper introduces a novel brain-inspired AI framework, Orangutan. It simulates
the structure and computational mechanisms of biological brains on multiple
scales, encompassing multi-compartment neuron architectures, diverse synaptic
connection modalities, neural microcircuits, cortical columns, and brain
regions, as well as biochemical processes including facilitation, feedforward
inhibition, short-term potentiation, and short-term depression, all grounded in
solid neuroscience. Building upon these highly integrated brain-like
mechanisms, I have developed a sensorimotor model that simulates human saccadic
eye movements during object observation. The model's algorithmic efficacy was
validated through testing with the observation of handwritten digit images."
"Quantitative Methods in Research Evaluation Citation Indicators,
  Altmetrics, and Artificial Intelligence","This book critically analyses the value of citation data, altmetrics, and
artificial intelligence to support the research evaluation of articles,
scholars, departments, universities, countries, and funders. It introduces and
discusses indicators that can support research evaluation and analyses their
strengths and weaknesses as well as the generic strengths and weaknesses of the
use of indicators for research assessment. The book includes evidence of the
comparative value of citations and altmetrics in all broad academic fields
primarily through comparisons against article level human expert judgements
from the UK Research Excellence Framework 2021. It also discusses the potential
applications of traditional artificial intelligence and large language models
for research evaluation, with large scale evidence for the former. The book
concludes that citation data can be informative and helpful in some research
fields for some research evaluation purposes but that indicators are never
accurate enough to be described as research quality measures. It also argues
that AI may be helpful in limited circumstances for some types of research
evaluation."
"First Place Solution of 2023 Global Artificial Intelligence Technology
  Innovation Competition Track 1","In this paper, we present our champion solution to the Global Artificial
Intelligence Technology Innovation Competition Track 1: Medical Imaging
Diagnosis Report Generation. We select CPT-BASE as our base model for the text
generation task. During the pre-training stage, we delete the mask language
modeling task of CPT-BASE and instead reconstruct the vocabulary, adopting a
span mask strategy and gradually increasing the number of masking ratios to
perform the denoising auto-encoder pre-training task. In the fine-tuning stage,
we design iterative retrieval augmentation and noise-aware similarity bucket
prompt strategies. The retrieval augmentation constructs a mini-knowledge base,
enriching the input information of the model, while the similarity bucket
further perceives the noise information within the mini-knowledge base, guiding
the model to generate higher-quality diagnostic reports based on the similarity
prompts. Surprisingly, our single model has achieved a score of 2.321 on
leaderboard A, and the multiple model fusion scores are 2.362 and 2.320 on the
A and B leaderboards respectively, securing first place in the rankings."
Synthetic Data in Radiological Imaging: Current State and Future Outlook,"A key challenge for the development and deployment of artificial intelligence
(AI) solutions in radiology is solving the associated data limitations.
Obtaining sufficient and representative patient datasets with appropriate
annotations may be burdensome due to high acquisition cost, safety limitations,
patient privacy restrictions or low disease prevalence rates. In silico data
offers a number of potential advantages to patient data, such as diminished
patient harm, reduced cost, simplified data acquisition, scalability, improved
quality assurance testing, and a mitigation approach to data imbalances. We
summarize key research trends and practical uses for synthetically generated
data for radiological applications of AI. Specifically, we discuss different
types of techniques for generating synthetic examples, their main application
areas, and related quality control assessment issues. We also discuss current
approaches for evaluating synthetic imaging data. Overall, synthetic data holds
great promise in addressing current data availability gaps, but additional work
is needed before its full potential is realized."
"Towards Asimov's Psychohistory: Harnessing Topological Data Analysis,
  Artificial Intelligence and Social Media data to Forecast Societal Trends","In the age of big data and advanced computational methods, the prediction of
large-scale social behaviors, reminiscent of Isaac Asimov's fictional science
of Psychohistory, is becoming increasingly feasible. This paper consists of a
theoretical exploration of the integration of computational power and
mathematical frameworks, particularly through Topological Data Analysis (TDA)
(Carlsson, Vejdemo-Johansson, 2022) and Artificial Intelligence (AI), to
forecast societal trends through social media data analysis. By examining
social media as a reflective surface of collective human behavior through the
systematic behaviorist approach (Glenn, et al., 2016), I argue that these tools
provide unprecedented clarity into the dynamics of large communities. This
study dialogues with Asimov's work, drawing parallels between his visionary
concepts and contemporary methodologies, illustrating how modern computational
techniques can uncover patterns and predict shifts in social behavior,
contributing to the emerging field of digital sociology -- or even,
Psychohistory itself."
"Research on the Acoustic Emission Source Localization Methodology in
  Composite Materials based on Artificial Intelligence","In this study, methodology of acoustic emission source localization in
composite materials based on artificial intelligence was presented. Carbon
fiber reinforced plastic was selected for specimen, and acoustic emission
signal were measured using piezoelectric devices. The measured signal was
wavelet-transformed to obtain scalograms, which were used as training data for
the artificial intelligence model. AESLNet(acoustic emission source
localization network), proposed in this study, was constructed convolutional
layers in parallel due to anisotropy of the composited materials. It is
regression model to detect the coordinates of acoustic emission source
location. Hyper-parameter of network has been optimized by Bayesian
optimization. It has been confirmed that network can detect location of
acoustic emission source with an average error of 3.02mm and a resolution of
20mm."
A Manifesto for a Pro-Actively Responsible AI in Education,"This paper examines the historical foundations, current practices, and
emerging challenges for Artificial Intelligence in Education (AIED) within
broader AI practices. It highlights AIED's unique and rich potential for
contributing to the current AI policy and practices, especially in the context
of responsible AI. It also discusses the key gaps in the AIED field, which need
to be addressed by the community to elevate the field from a cottage industry
to the level where it will deservedly be seen as key to advancin AI research
and practical applications. The paper offers a five-point manifesto aimed to
revitalise AIED' contributions to education and broader AI community,
suggesting enhanced interdisciplinary collaboration, a broadened understanding
of AI's impact on human functioning, and commitment to setting agendas for
human-centred educational innovations.This approach positions AIED to
significantly influence educational technologies to achieve genuine positive
impact across diverse societal segments."
"Are Large Language Models Really Bias-Free? Jailbreak Prompts for
  Assessing Adversarial Robustness to Bias Elicitation","Large Language Models (LLMs) have revolutionized artificial intelligence,
demonstrating remarkable computational power and linguistic capabilities.
However, these models are inherently prone to various biases stemming from
their training data. These include selection, linguistic, and confirmation
biases, along with common stereotypes related to gender, ethnicity, sexual
orientation, religion, socioeconomic status, disability, and age. This study
explores the presence of these biases within the responses given by the most
recent LLMs, analyzing the impact on their fairness and reliability. We also
investigate how known prompt engineering techniques can be exploited to
effectively reveal hidden biases of LLMs, testing their adversarial robustness
against jailbreak prompts specially crafted for bias elicitation. Extensive
experiments are conducted using the most widespread LLMs at different scales,
confirming that LLMs can still be manipulated to produce biased or
inappropriate responses, despite their advanced capabilities and sophisticated
alignment processes. Our findings underscore the importance of enhancing
mitigation techniques to address these safety issues, toward a more sustainable
and inclusive artificial intelligence."
"Application of Artificial Intelligence in Supporting Healthcare
  Professionals and Caregivers in Treatment of Autistic Children","Autism Spectrum Disorder (ASD) represents a multifaceted neurodevelopmental
condition marked by difficulties in social interaction, communication
impediments, and repetitive behaviors. Despite progress in understanding ASD,
its diagnosis and treatment continue to pose significant challenges due to the
variability in symptomatology and the necessity for multidisciplinary care
approaches. This paper investigates the potential of Artificial Intelligence
(AI) to augment the capabilities of healthcare professionals and caregivers in
managing ASD. We have developed a sophisticated algorithm designed to analyze
facial and bodily expressions during daily activities of both autistic and
non-autistic children, leading to the development of a powerful deep
learning-based autism detection system. Our study demonstrated that AI models,
specifically the Xception and ResNet50V2 architectures, achieved high accuracy
in diagnosing Autism Spectrum Disorder (ASD). This research highlights the
transformative potential of AI in improving the diagnosis, treatment, and
comprehensive management of ASD. Our study revealed that AI models, notably the
Xception and ResNet50V2 architectures, demonstrated high accuracy in diagnosing
ASD."
Procedural Content Generation via Generative Artificial Intelligence,"The attempt to utilize machine learning in PCG has been made in the past. In
this survey paper, we investigate how generative artificial intelligence (AI),
which saw a significant increase in interest in the mid-2010s, is being used
for PCG. We review applications of generative AI for the creation of various
types of content, including terrains, items, and even storylines. While
generative AI is effective for PCG, one significant issues it faces is that
building high-performance generative AI requires vast amounts of training data.
Because content generally highly customized, domain-specific training data is
scarce, and straightforward approaches to generative AI models may not work
well. For PCG research to advance further, issues related to limited training
data must be overcome. Thus, we also give special consideration to research
that addresses the challenges posed by limited training data."
"What is Reproducibility in Artificial Intelligence and Machine Learning
  Research?","In the rapidly evolving fields of Artificial Intelligence (AI) and Machine
Learning (ML), the reproducibility crisis underscores the urgent need for clear
validation methodologies to maintain scientific integrity and encourage
advancement. The crisis is compounded by the prevalent confusion over
validation terminology. In response to this challenge, we introduce a framework
that clarifies the roles and definitions of key validation efforts:
repeatability, dependent and independent reproducibility, and direct and
conceptual replicability. This structured framework aims to provide AI/ML
researchers with the necessary clarity on these essential concepts,
facilitating the appropriate design, conduct, and interpretation of validation
studies. By articulating the nuances and specific roles of each type of
validation study, we aim to enhance the reliability and trustworthiness of
research findings and support the community's efforts to address
reproducibility challenges effectively."
"Strategic Integration of Artificial Intelligence in the C-Suite: The
  Role of the Chief AI Officer","The integration of Artificial Intelligence (AI) into corporate strategy has
become a pivotal focus for organizations aiming to maintain a competitive
advantage in the digital age. As AI reshapes business operations and drives
innovation, the need for specialized leadership to effectively manage these
changes becomes increasingly apparent. In this paper, I explore the role of the
Chief AI Officer (CAIO) within the C-suite, emphasizing the necessity of this
position for successful AI strategy, integration, and governance. I analyze
future scenarios based on current trends in three key areas: the AI Economy, AI
Organization, and Competition in the Age of AI. These explorations lay the
foundation for identifying the antecedents (environmental, structural, and
strategic factors) that justify the inclusion of a CAIO in top management
teams. This sets the stage for a comprehensive examination of the CAIO's role
and the broader implications of AI leadership. This paper advances the
discussion on AI leadership by providing a rationale for the strategic
integration of AI at the executive level and examining the role of the Chief AI
Officer within organizations."
"Recent Advances in Generative AI and Large Language Models: Current
  Status, Challenges, and Perspectives","The emergence of Generative Artificial Intelligence (AI) and Large Language
Models (LLMs) has marked a new era of Natural Language Processing (NLP),
introducing unprecedented capabilities that are revolutionizing various
domains. This paper explores the current state of these cutting-edge
technologies, demonstrating their remarkable advancements and wide-ranging
applications. Our paper contributes to providing a holistic perspective on the
technical foundations, practical applications, and emerging challenges within
the evolving landscape of Generative AI and LLMs. We believe that understanding
the generative capabilities of AI systems and the specific context of LLMs is
crucial for researchers, practitioners, and policymakers to collaboratively
shape the responsible and ethical integration of these technologies into
various domains. Furthermore, we identify and address main research gaps,
providing valuable insights to guide future research endeavors within the AI
research community."
A Measure for Level of Autonomy Based on Observable System Behavior,"Contemporary artificial intelligence systems are pivotal in enhancing human
efficiency and safety across various domains. One such domain is autonomous
systems, especially in automotive and defense use cases. Artificial
intelligence brings learning and enhanced decision-making to autonomy system
goal-oriented behaviors and human independence. However, the lack of clear
understanding of autonomy system capabilities hampers human-machine or
machine-machine interaction and interdiction. This necessitates varying degrees
of human involvement for safety, accountability, and explainability purposes.
Yet, measuring the level autonomous capability in an autonomous system presents
a challenge. Two scales of measurement exist, yet measuring autonomy
presupposes a variety of elements not available in the wild. This is why
existing measures for level of autonomy are operationalized only during design
or test and evaluation phases. No measure for level of autonomy based on
observed system behavior exists at this time. To address this, we outline a
potential measure for predicting level of autonomy using observable actions. We
also present an algorithm incorporating the proposed measure. The measure and
algorithm have significance to researchers and practitioners interested in a
method to blind compare autonomous systems at runtime. Defense-based
implementations are likewise possible because counter-autonomy depends on
robust identification of autonomous systems."
"Problems in AI, their roots in philosophy, and implications for science
  and society","Artificial Intelligence (AI) is one of today's most relevant emergent
technologies. In view thereof, this paper proposes that more attention should
be paid to the philosophical aspects of AI technology and its use. It is argued
that this deficit is generally combined with philosophical misconceptions about
the growth of knowledge. To identify these misconceptions, reference is made to
the ideas of the philosopher of science Karl Popper and the physicist David
Deutsch. The works of both thinkers aim against mistaken theories of knowledge,
such as inductivism, empiricism, and instrumentalism. This paper shows that
these theories bear similarities to how current AI technology operates. It also
shows that these theories are very much alive in the (public) discourse on AI,
often called Bayesianism. In line with Popper and Deutsch, it is proposed that
all these theories are based on mistaken philosophies of knowledge. This
includes an analysis of the implications of these mistaken philosophies for the
use of AI in science and society, including some of the likely problem
situations that will arise. This paper finally provides a realistic outlook on
Artificial General Intelligence (AGI) and three propositions on A(G)I and
philosophy (i.e., epistemology)."
"A Survey of Explainable Artificial Intelligence (XAI) in Financial Time
  Series Forecasting","Artificial Intelligence (AI) models have reached a very significant level of
accuracy. While their superior performance offers considerable benefits, their
inherent complexity often decreases human trust, which slows their application
in high-risk decision-making domains, such as finance. The field of eXplainable
AI (XAI) seeks to bridge this gap, aiming to make AI models more
understandable. This survey, focusing on published work from the past five
years, categorizes XAI approaches that predict financial time series. In this
paper, explainability and interpretability are distinguished, emphasizing the
need to treat these concepts separately as they are not applied the same way in
practice. Through clear definitions, a rigorous taxonomy of XAI approaches, a
complementary characterization, and examples of XAI's application in the
finance industry, this paper provides a comprehensive view of XAI's current
role in finance. It can also serve as a guide for selecting the most
appropriate XAI approach for future applications."
Capital as Artificial Intelligence,"We gather many perspectives on Capital and synthesize their commonalities. We
provide a characterization of Capital as a historical agential system and
propose a model of Capital using tools from computer science. Our model
consists of propositions which, if satisfied by a specific grounding,
constitute a valid model of Capital. We clarify the manners in which Capital
can evolve. We claim that, when its evolution is driven by quantitative
optimization processes, Capital can possess qualities of Artificial
Intelligence. We find that Capital may not uniquely represent meaning, in the
same way that optimization is not intentionally meaningful. We find that
Artificial Intelligences like modern day Large Language Models are a part of
Capital. We link our readers to a web-interface where they can interact with a
part of Capital."
"The Potential and Perils of Generative Artificial Intelligence for
  Quality Improvement and Patient Safety","Generative artificial intelligence (GenAI) has the potential to improve
healthcare through automation that enhances the quality and safety of patient
care. Powered by foundation models that have been pretrained and can generate
complex content, GenAI represents a paradigm shift away from the more
traditional focus on task-specific classifiers that have dominated the AI
landscape thus far. We posit that the imminent application of GenAI in
healthcare will be through well-defined, low risk, high value, and narrow
applications that automate healthcare workflows at the point of care using
smaller foundation models. These models will be finetuned for different
capabilities and application specific scenarios and will have the ability to
provide medical explanations, reference evidence within a retrieval augmented
framework and utilizing external tools. We contrast this with a general,
all-purpose AI model for end-to-end clinical decision making that improves
clinician performance, including safety-critical diagnostic tasks, which will
require greater research prior to implementation. We consider areas where
'human in the loop' Generative AI can improve healthcare quality and safety by
automating mundane tasks. Using the principles of implementation science will
be critical for integrating 'end to end' GenAI systems that will be accepted by
healthcare teams."
"Unexplainability of Artificial Intelligence Judgments in Kant's
  Perspective","Kant's Critique of Pure Reason, a major contribution to the history of
epistemology, proposes a table of categories to elucidate the structure of the
a priori principle of human judgment. The technology of artificial intelligence
(AI), based on functionalism, claims to simulate or replicate human judgment.
To assess this claim, it is necessary to study whether AI judgment possesses
the characteristics of human judgment. This paper argues that AI judgments
exhibit a form that cannot be understood in terms of the characteristics of
human judgments according to Kant. Because the characteristics of judgment
overlap, we can call this AI's uncertainty. Then, I show that concepts without
physical intuitions are not easy to explain when their functions are shown
through vision. Finally, I illustrate that even if AI makes sentences through
subject and predicate in natural language, which are components of judgment, it
is difficult to determine whether AI understands the concepts to the level
humans can accept. This shows that it is questionable whether the explanation
through natural language is reliable."
"Artificial Intelligence from Idea to Implementation. How Can AI Reshape
  the Education Landscape?","This introductory chapter provides an overview of the evolution and impact of
Artificial Intelligence technologies in today society. Beginning with a
historical context while exploring a few general definitions of AI, the author
provides a timeline of the used technologies, highlighting its periods of
stagnation, commonly referred to as AI winters, and the subsequent resurgence
fueled by relentless enthusiasm and investment. The narrative then transitions
to focus on the transformative effects of AI on society at large, with a
particular emphasis on educational applications. Through examples, the paper
shows how AI technologies have moved from theoretical constructs to practical
tools that are reshaping pedagogical approaches and student engagement. The
essay concludes by discussing the prospects of AI in education, emphasizing the
need for a balanced approach that considers both technological advancements and
societal implications."
"Rico: extended TIAGo robot towards up-to-date social and assistive robot
  usage scenarios","Social and assistive robotics have vastly increased in popularity in recent
years. Due to the wide range of usage, robots executing such tasks must be
highly reliable and possess enough functions to satisfy multiple scenarios.
This article describes a mobile, artificial intelligence-driven, robotic
platform Rico. Its prior usage in similar scenarios, the number of its
capabilities, and the experiments it presented should qualify it as a proper
arm-less platform for social and assistive circumstances."
Human interaction classifier for LLM based chatbot,"This study investigates different approaches to classify human interactions
in an artificial intelligence-based environment, specifically for Applus+
IDIADA's intelligent agent AIDA. The main objective is to develop a classifier
that accurately identifies the type of interaction received (Conversation,
Services, or Document Translation) to direct requests to the appropriate
channel and provide a more specialized and efficient service. Various models
are compared, including LLM-based classifiers, KNN using Titan and Cohere
embeddings, SVM, and artificial neural networks. Results show that SVM and ANN
models with Cohere embeddings achieve the best overall performance, with
superior F1 scores and faster execution times compared to LLM-based approaches.
The study concludes that the SVM model with Cohere embeddings is the most
suitable option for classifying human interactions in the AIDA environment,
offering an optimal balance between accuracy and computational efficiency."
Future of Artificial Intelligence in Agile Software Development,"The advent of Artificial intelligence has promising advantages that can be
utilized to transform the landscape of software project development. The
Software process framework consists of activities that constantly require
routine human interaction, leading to the possibility of errors and
uncertainties. AI can assist software development managers, software testers,
and other team members by leveraging LLMs, GenAI models, and AI agents to
perform routine tasks, risk analysis and prediction, strategy recommendations,
and support decision making. AI has the potential to increase efficiency and
reduce the risks encountered by the project management team while increasing
the project success rates. Additionally, it can also break down complex notions
and development processes for stakeholders to make informed decisions. In this
paper, we propose an approach in which AI tools and technologies can be
utilized to bestow maximum assistance for agile software projects, which have
become increasingly favored in the industry in recent years."
Learning Brave Assumption-Based Argumentation Frameworks via ASP,"Assumption-based Argumentation (ABA) is advocated as a unifying formalism for
various forms of non-monotonic reasoning, including logic programming. It
allows capturing defeasible knowledge, subject to argumentative debate. While,
in much existing work, ABA frameworks are given up-front, in this paper we
focus on the problem of automating their learning from background knowledge and
positive/negative examples. Unlike prior work, we newly frame the problem in
terms of brave reasoning under stable extensions for ABA. We present a novel
algorithm based on transformation rules (such as Rote Learning, Folding,
Assumption Introduction and Fact Subsumption) and an implementation thereof
that makes use of Answer Set Programming. Finally, we compare our technique to
state-of-the-art ILP systems that learn defeasible knowledge."
"Understanding the Skills Gap between Higher Education and Industry in
  the UK in Artificial Intelligence Sector","As Artificial Intelligence (AI) changes how businesses work, there is a
growing need for people who can work in this sector. This paper investigates
how well universities in United Kingdom offering courses in AI, prepare
students for jobs in the real world. To gain insight into the differences
between university curricula and industry demands we review the contents of
taught courses and job advertisement portals. By using custom data scraping
tools to gather information from job advertisements and university curricula,
and frequency and Naive Bayes classifier analysis, this study will show exactly
what skills industry is looking for. In this study we identified 12 skill
categories that were used for mapping. The study showed that the university
curriculum in the AI domain is well balanced in most technical skills,
including Programming and Machine learning subjects, but have a gap in Data
Science and Maths and Statistics skill categories."
"Sentiment analysis of preservice teachers' reflections using a large
  language model","In this study, the emotion and tone of preservice teachers' reflections were
analyzed using sentiment analysis with LLMs: GPT-4, Gemini, and BERT. We
compared the results to understand how each tool categorizes and describes
individual reflections and multiple reflections as a whole. This study aims to
explore ways to bridge the gaps between qualitative, quantitative, and
computational analyses of reflective practices in teacher education. This study
finds that to effectively integrate LLM analysis into teacher education,
developing an analysis method and result format that are both comprehensive and
relevant for preservice teachers and teacher educators is crucial."
Artificial intelligence for science: The easy and hard problems,"A suite of impressive scientific discoveries have been driven by recent
advances in artificial intelligence. These almost all result from training
flexible algorithms to solve difficult optimization problems specified in
advance by teams of domain scientists and engineers with access to large
amounts of data. Although extremely useful, this kind of problem solving only
corresponds to one part of science - the ""easy problem."" The other part of
scientific research is coming up with the problem itself - the ""hard problem.""
Solving the hard problem is beyond the capacities of current algorithms for
scientific discovery because it requires continual conceptual revision based on
poorly defined constraints. We can make progress on understanding how humans
solve the hard problem by studying the cognitive science of scientists, and
then use the results to design new computational agents that automatically
infer and update their scientific paradigms."
The Artificial Intelligence Act: critical overview,"This article provides a critical overview of the recently approved Artificial
Intelligence Act. It starts by presenting the main structure, objectives, and
approach of Regulation (EU) 2024/1689. A definition of key concepts follows,
and then the material and territorial scope, as well as the timing of
application, are analyzed. Although the Regulation does not explicitly set out
principles, the main ideas of fairness, accountability, transparency, and
equity in AI underly a set of rules of the regulation. This is discussed before
looking at the ill-defined set of forbidden AI practices (manipulation and e
exploitation of vulnerabilities, social scoring, biometric identification and
classification, and predictive policing). It is highlighted that those rules
deal with behaviors rather than AI systems. The qualification and regulation of
high-risk AI systems are tackled, alongside the obligation of transparency for
certain systems, the regulation of general-purpose models, and the rules on
certification, supervision, and sanctions. The text concludes that even if the
overall framework can be deemed adequate and balanced, the approach is so
complex that it risks defeating its own purpose of promoting responsible
innovation within the European Union and beyond its borders."
"Towards Privacy-Preserving Relational Data Synthesis via Probabilistic
  Relational Models","Probabilistic relational models provide a well-established formalism to
combine first-order logic and probabilistic models, thereby allowing to
represent relationships between objects in a relational domain. At the same
time, the field of artificial intelligence requires increasingly large amounts
of relational training data for various machine learning tasks. Collecting
real-world data, however, is often challenging due to privacy concerns, data
protection regulations, high costs, and so on. To mitigate these challenges,
the generation of synthetic data is a promising approach. In this paper, we
solve the problem of generating synthetic relational data via probabilistic
relational models. In particular, we propose a fully-fledged pipeline to go
from relational database to probabilistic relational model, which can then be
used to sample new synthetic relational data points from its underlying
probability distribution. As part of our proposed pipeline, we introduce a
learning algorithm to construct a probabilistic relational model from a given
relational database."
Neuromorphic Spintronics,"Neuromorphic spintronics combines two advanced fields in technology,
neuromorphic computing and spintronics, to create brain-inspired, efficient
computing systems that leverage the unique properties of the electron's spin.
In this book chapter, we first introduce both fields - neuromorphic computing
and spintronics and then make a case for neuromorphic spintronics. We discuss
concrete examples of neuromorphic spintronics, including computing based on
fluctuations, artificial neural networks, and reservoir computing, highlighting
their potential to revolutionize computational efficiency and functionality."
Harnessing Artificial Intelligence for Wildlife Conservation,"The rapid decline in global biodiversity demands innovative conservation
strategies. This paper examines the use of artificial intelligence (AI) in
wildlife conservation, focusing on the Conservation AI platform. Leveraging
machine learning and computer vision, Conservation AI detects and classifies
animals, humans, and poaching-related objects using visual spectrum and thermal
infrared cameras. The platform processes this data with convolutional neural
networks (CNNs) and Transformer architectures to monitor species, including
those which are critically endangered. Real-time detection provides the
immediate responses required for time-critical situations (e.g. poaching),
while non-real-time analysis supports long-term wildlife monitoring and habitat
health assessment. Case studies from Europe, North America, Africa, and
Southeast Asia highlight the platform's success in species identification,
biodiversity monitoring, and poaching prevention. The paper also discusses
challenges related to data quality, model accuracy, and logistical constraints,
while outlining future directions involving technological advancements,
expansion into new geographical regions, and deeper collaboration with local
communities and policymakers. Conservation AI represents a significant step
forward in addressing the urgent challenges of wildlife conservation, offering
a scalable and adaptable solution that can be implemented globally."
"SemAI: Semantic Artificial Intelligence-enhanced DNA storage for
  Internet-of-Things","In the wake of the swift evolution of technologies such as the Internet of
Things (IoT), the global data landscape undergoes an exponential surge,
propelling DNA storage into the spotlight as a prospective medium for
contemporary cloud storage applications. This paper introduces a Semantic
Artificial Intelligence-enhanced DNA storage (SemAI-DNA) paradigm,
distinguishing itself from prevalent deep learning-based methodologies through
two key modifications: 1) embedding a semantic extraction module at the
encoding terminus, facilitating the meticulous encoding and storage of nuanced
semantic information; 2) conceiving a forethoughtful multi-reads filtering
model at the decoding terminus, leveraging the inherent multi-copy propensity
of DNA molecules to bolster system fault tolerance, coupled with a
strategically optimized decoder's architectural framework. Numerical results
demonstrate the SemAI-DNA's efficacy, attaining 2.61 dB Peak Signal-to-Noise
Ratio (PSNR) gain and 0.13 improvement in Structural Similarity Index (SSIM)
over conventional deep learning-based approaches."
"Sparks of Artificial General Intelligence(AGI) in Semiconductor Material
  Science: Early Explorations into the Next Frontier of Generative AI-Assisted
  Electron Micrograph Analysis","Characterizing materials with electron micrographs poses significant
challenges for automated labeling due to the complex nature of nanomaterial
structures. To address this, we introduce a fully automated, end-to-end
pipeline that leverages recent advances in Generative AI. It is designed for
analyzing and understanding the microstructures of semiconductor materials with
effectiveness comparable to that of human experts, contributing to the pursuit
of Artificial General Intelligence (AGI) in nanomaterial identification. Our
approach utilizes Large MultiModal Models (LMMs) such as GPT-4V, alongside
text-to-image models like DALLE-3. We integrate a GPT-4 guided Visual Question
Answering (VQA) method to analyze nanomaterial images, generate synthetic
nanomaterial images via DALLE-3, and employ in-context learning with few-shot
prompting in GPT-4V for accurate nanomaterial identification. Our method
surpasses traditional techniques by enhancing the precision of nanomaterial
identification and optimizing the process for high-throughput screening."
"(Un)certainty of (Un)fairness: Preference-Based Selection of Certainly
  Fair Decision-Makers","Fairness metrics are used to assess discrimination and bias in
decision-making processes across various domains, including machine learning
models and human decision-makers in real-world applications. This involves
calculating the disparities between probabilistic outcomes among social groups,
such as acceptance rates between male and female applicants. However,
traditional fairness metrics do not account for the uncertainty in these
processes and lack of comparability when two decision-makers exhibit the same
disparity. Using Bayesian statistics, we quantify the uncertainty of the
disparity to enhance discrimination assessments. We represent each
decision-maker, whether a machine learning model or a human, by its disparity
and the corresponding uncertainty in that disparity. We define preferences over
decision-makers and utilize brute-force to choose the optimal decision-maker
according to a utility function that ranks decision-makers based on these
preferences. The decision-maker with the highest utility score can be
interpreted as the one for whom we are most certain that it is fair."
"AI Thinking: A framework for rethinking artificial intelligence in
  practice","Artificial intelligence is transforming the way we work with information
across disciplines and practical contexts. A growing range of disciplines are
now involved in studying, developing, and assessing the use of AI in practice,
but these disciplines often employ conflicting understandings of what AI is and
what is involved in its use. New, interdisciplinary approaches are needed to
bridge competing conceptualisations of AI in practice and help shape the future
of AI use. I propose a novel conceptual framework called AI Thinking, which
models key decisions and considerations involved in AI use across disciplinary
perspectives. The AI Thinking model addresses five practice-based competencies
involved in applying AI in context: motivating AI use in information processes,
formulating AI methods, assessing available tools and technologies, selecting
appropriate data, and situating AI in the sociotechnical contexts it is used
in. A hypothetical case study is provided to illustrate the application of AI
Thinking in practice. This article situates AI Thinking in broader
cross-disciplinary discourses of AI, including its connections to ongoing
discussions around AI literacy and AI-driven innovation. AI Thinking can help
to bridge divides between academic disciplines and diverse contexts of AI use,
and to reshape the future of AI in practice."
Brain-Inspired AI with Hyperbolic Geometry,"Artificial neural networks (ANNs) were inspired by the architecture and
functions of the human brain and have revolutionised the field of artificial
intelligence (AI). Inspired by studies on the latent geometry of the brain, in
this perspective paper we posit that an increase in the research and
application of hyperbolic geometry in ANNs and machine learning will lead to
increased accuracy, improved feature space representations and more efficient
models across a range of tasks. We examine the structure and functions of the
human brain, emphasising the correspondence between its scale-free hierarchical
organization and hyperbolic geometry, and reflecting on the central role
hyperbolic geometry plays in facilitating human intelligence. Empirical
evidence indicates that hyperbolic neural networks outperform Euclidean models
for tasks including natural language processing, computer vision and complex
network analysis, requiring fewer parameters and exhibiting better
generalisation. Despite its nascent adoption, hyperbolic geometry holds promise
for improving machine learning models through brain-inspired geometric
representations."
"Enhancing Feature Selection and Interpretability in AI Regression Tasks
  Through Feature Attribution","Research in Explainable Artificial Intelligence (XAI) is increasing, aiming
to make deep learning models more transparent. Most XAI methods focus on
justifying the decisions made by Artificial Intelligence (AI) systems in
security-relevant applications. However, relatively little attention has been
given to using these methods to improve the performance and robustness of deep
learning algorithms. Additionally, much of the existing XAI work primarily
addresses classification problems. In this study, we investigate the potential
of feature attribution methods to filter out uninformative features in input
data for regression problems, thereby improving the accuracy and stability of
predictions. We introduce a feature selection pipeline that combines Integrated
Gradients with k-means clustering to select an optimal set of variables from
the initial data space. To validate the effectiveness of this approach, we
apply it to a real-world industrial problem - blade vibration analysis in the
development process of turbo machinery."
MCGM: Mask Conditional Text-to-Image Generative Model,"Recent advancements in generative models have revolutionized the field of
artificial intelligence, enabling the creation of highly-realistic and detailed
images. In this study, we propose a novel Mask Conditional Text-to-Image
Generative Model (MCGM) that leverages the power of conditional diffusion
models to generate pictures with specific poses. Our model builds upon the
success of the Break-a-scene [1] model in generating new scenes using a single
image with multiple subjects and incorporates a mask embedding injection that
allows the conditioning of the generation process. By introducing this
additional level of control, MCGM offers a flexible and intuitive approach for
generating specific poses for one or more subjects learned from a single image,
empowering users to influence the output based on their requirements. Through
extensive experimentation and evaluation, we demonstrate the effectiveness of
our proposed model in generating high-quality images that meet predefined mask
conditions and improving the current Break-a-scene generative model."
Auction-Based Regulation for Artificial Intelligence,"In an era of ""moving fast and breaking things"", regulators have moved slowly
to pick up the safety, bias, and legal debris left in the wake of broken
Artificial Intelligence (AI) deployment. While there is much-warranted
discussion about how to address the safety, bias, and legal woes of
state-of-the-art AI models, rigorous and realistic mathematical frameworks to
regulate AI are lacking. Our paper addresses this challenge, proposing an
auction-based regulatory mechanism that provably incentivizes devices (i) to
deploy compliant models and (ii) to participate in the regulation process. We
formulate AI regulation as an all-pay auction where enterprises submit models
for approval. The regulator enforces compliance thresholds and further rewards
models exhibiting higher compliance than their peers. We derive Nash Equilibria
demonstrating that rational agents will submit models exceeding the prescribed
compliance threshold. Empirical results show that our regulatory auction boosts
compliance rates by 20% and participation rates by 15% compared to baseline
regulatory mechanisms, outperforming simpler frameworks that merely impose
minimum compliance standards."
"AiBAT: Artificial Intelligence/Instructions for Build, Assembly, and
  Test","Instructions for Build, Assembly, and Test (IBAT) refers to the process used
whenever any operation is conducted on hardware, including tests, assembly, and
maintenance. Currently, the generation of IBAT documents is time-intensive, as
users must manually reference and transfer information from engineering
diagrams and parts lists into IBAT instructions. With advances in machine
learning and computer vision, however, it is possible to have an artificial
intelligence (AI) model perform the partial filling of the IBAT template,
freeing up engineer time for more highly skilled tasks. AiBAT is a novel system
for assisting users in authoring IBATs. It works by first analyzing assembly
drawing documents, extracting information and parsing it, and then filling in
IBAT templates with the extracted information. Such assisted authoring has
potential to save time and reduce cost. This paper presents an overview of the
AiBAT system, including promising preliminary results and discussion on future
work."
"Level of agreement between emotions generated by Artificial Intelligence
  and human evaluation: a methodological proposal","Images are capable of conveying emotions, but emotional experience is highly
subjective. Advances in artificial intelligence have enabled the generation of
images based on emotional descriptions. However, the level of agreement between
the generative images and human emotional responses has not yet been evaluated.
To address this, 20 artistic landscapes were generated using StyleGAN2-ADA.
Four variants evoking positive emotions (contentment, amusement) and negative
emotions (fear, sadness) were created for each image, resulting in 80 pictures.
An online questionnaire was designed using this material, in which 61 observers
classified the generated images. Statistical analyses were performed on the
collected data to determine the level of agreement among participants, between
the observer's responses, and the AI-generated emotions. A generally good level
of agreement was found, with better results for negative emotions. However, the
study confirms the subjectivity inherent in emotional evaluation."
AI in Archival Science -- A Systematic Review,"The rapid expansion of records creates significant challenges in management,
including retention and disposition, appraisal, and organization. Our study
underscores the benefits of integrating artificial intelligence (AI) within the
broad realm of archival science. In this work, we start by performing a
thorough analysis to understand the current use of AI in this area and identify
the techniques employed to address challenges. Subsequently, we document the
results of our review according to specific criteria. Our findings highlight
key AI driven strategies that promise to streamline record-keeping processes
and enhance data retrieval efficiency. We also demonstrate our review process
to ensure transparency regarding our methodology. Furthermore, this review not
only outlines the current state of AI in archival science and records
management but also lays the groundwork for integrating new techniques to
transform archival practices. Our research emphasizes the necessity for
enhanced collaboration between the disciplines of artificial intelligence and
archival science."
"Bayesian Transfer Learning for Artificially Intelligent Geospatial
  Systems: A Predictive Stacking Approach","Building artificially intelligent geospatial systems require rapid delivery
of spatial data analysis at massive scales with minimal human intervention.
Depending upon their intended use, data analysis may also entail model
assessment and uncertainty quantification. This article devises transfer
learning frameworks for deployment in artificially intelligent systems, where a
massive data set is split into smaller data sets that stream into the
analytical framework to propagate learning and assimilate inference for the
entire data set. Specifically, we introduce Bayesian predictive stacking for
multivariate spatial data and demonstrate its effectiveness in rapidly
analyzing massive data sets. Furthermore, we make inference feasible in a
reasonable amount of time, and without excessively demanding hardware settings.
We illustrate the effectiveness of this approach in extensive simulation
experiments and subsequently analyze massive data sets in climate science on
sea surface temperatures and on vegetation index."
"Perceptions of Discriminatory Decisions of Artificial Intelligence:
  Unpacking the Role of Individual Characteristics","This study investigates how personal differences (digital self-efficacy,
technical knowledge, belief in equality, political ideology) and demographic
factors (age, education, and income) are associated with perceptions of
artificial intelligence (AI) outcomes exhibiting gender and racial bias and
with general attitudes towards AI. Analyses of a large-scale experiment dataset
(N = 1,206) indicate that digital self-efficacy and technical knowledge are
positively associated with attitudes toward AI, while liberal ideologies are
negatively associated with outcome trust, higher negative emotion, and greater
skepticism. Furthermore, age and income are closely connected to cognitive gaps
in understanding discriminatory AI outcomes. These findings highlight the
importance of promoting digital literacy skills and enhancing digital
self-efficacy to maintain trust in AI and beliefs in AI usefulness and safety.
The findings also suggest that the disparities in understanding problematic AI
outcomes may be aligned with economic inequalities and generational gaps in
society. Overall, this study sheds light on the socio-technological system in
which complex interactions occur between social hierarchies, divisions, and
machines that reflect and exacerbate the disparities."
Economic Anthropology in the Era of Generative Artificial Intelligence,"This paper explores the intersection of economic anthropology and generative
artificial intelligence (GenAI). It examines how large language models (LLMs)
can simulate human decision-making and the inductive biases present in AI
research. The study introduces two AI models: C.A.L.L.O.N. (Conventionally
Average Late Liberal ONtology) and M.A.U.S.S. (More Accurate Understanding of
Society and its Symbols). The former is trained on standard data, while the
latter is adapted with anthropological knowledge. The research highlights how
anthropological training can enhance LLMs' ability to recognize diverse
economic systems and concepts. The findings suggest that integrating economic
anthropology with AI can provide a more pluralistic understanding of economics
and improve the sustainability of non-market economic systems."
Trustworthy XAI and Application,"Artificial Intelligence (AI) is an important part of our everyday lives. We
use it in self-driving cars and smartphone assistants. People often call it a
""black box"" because its complex systems, especially deep neural networks, are
hard to understand. This complexity raises concerns about accountability, bias,
and fairness, even though AI can be quite accurate. Explainable Artificial
Intelligence (XAI) is important for building trust. It helps ensure that AI
systems work reliably and ethically. This article looks at XAI and its three
main parts: transparency, explainability, and trustworthiness. We will discuss
why these components matter in real-life situations. We will also review recent
studies that show how XAI is used in different fields. Ultimately, gaining
trust in AI systems is crucial for their successful use in society."
Artificial Intelligence in Brazilian News: A Mixed-Methods Analysis,"The current surge in Artificial Intelligence (AI) interest, reflected in
heightened media coverage since 2009, has sparked significant debate on AI's
implications for privacy, social justice, workers' rights, and democracy. The
media plays a crucial role in shaping public perception and acceptance of AI
technologies. However, research into how AI appears in media has primarily
focused on anglophone contexts, leaving a gap in understanding how AI is
represented globally. This study addresses this gap by analyzing 3,560 news
articles from Brazilian media published between July 1, 2023, and February 29,
2024, from 13 popular online news outlets. Using Computational Grounded Theory
(CGT), the study applies Latent Dirichlet Allocation (LDA), BERTopic, and
Named-Entity Recognition to investigate the main topics in AI coverage and the
entities represented. The findings reveal that Brazilian news coverage of AI is
dominated by topics related to applications in the workplace and product
launches, with limited space for societal concerns, which mostly focus on
deepfakes and electoral integrity. The analysis also highlights a significant
presence of industry-related entities, indicating a strong influence of
corporate agendas in the country's news. This study underscores the need for a
more critical and nuanced discussion of AI's societal impacts in Brazilian
media."
"Explaining Bayesian Networks in Natural Language using Factor Arguments.
  Evaluation in the medical domain","In this paper, we propose a model for building natural language explanations
for Bayesian Network Reasoning in terms of factor arguments, which are
argumentation graphs of flowing evidence, relating the observed evidence to a
target variable we want to learn about. We introduce the notion of factor
argument independence to address the outstanding question of defining when
arguments should be presented jointly or separately and present an algorithm
that, starting from the evidence nodes and a target node, produces a list of
all independent factor arguments ordered by their strength. Finally, we
implemented a scheme to build natural language explanations of Bayesian
Reasoning using this approach. Our proposal has been validated in the medical
domain through a human-driven evaluation study where we compare the Bayesian
Network Reasoning explanations obtained using factor arguments with an
alternative explanation method. Evaluation results indicate that our proposed
explanation approach is deemed by users as significantly more useful for
understanding Bayesian Network Reasoning than another existing explanation
method it is compared to."
"Autonomous Building Cyber-Physical Systems Using Decentralized
  Autonomous Organizations, Digital Twins, and Large Language Model","Current autonomous building research primarily focuses on energy efficiency
and automation. While traditional artificial intelligence has advanced
autonomous building research, it often relies on predefined rules and struggles
to adapt to complex, evolving building operations. Moreover, the centralized
organizational structures of facilities management hinder transparency in
decision-making, limiting true building autonomy. Research on decentralized
governance and adaptive building infrastructure, which could overcome these
challenges, remains relatively unexplored. This paper addresses these
limitations by introducing a novel Decentralized Autonomous Building
Cyber-Physical System framework that integrates Decentralized Autonomous
Organizations, Large Language Models, and digital twins to create a smart,
self-managed, operational, and financially autonomous building infrastructure.
This study develops a full-stack decentralized application to facilitate
decentralized governance of building infrastructure. An LLM-based artificial
intelligence assistant is developed to provide intuitive human-building
interaction for blockchain and building operation management-related tasks and
enable autonomous building operation. Six real-world scenarios were tested to
evaluate the autonomous building system's workability, including building
revenue and expense management, AI-assisted facility control, and autonomous
adjustment of building systems. Results indicate that the prototype
successfully executes these operations, confirming the framework's suitability
for developing building infrastructure with decentralized governance and
autonomous operation."
"Standardization Trends on Safety and Trustworthiness Technology for
  Advanced AI","Artificial Intelligence (AI) has rapidly evolved over the past decade and has
advanced in areas such as language comprehension, image and video recognition,
programming, and scientific reasoning. Recent AI technologies based on large
language models and foundation models are approaching or surpassing artificial
general intelligence. These systems demonstrate superior performance in complex
problem solving, natural language processing, and multi-domain tasks, and can
potentially transform fields such as science, industry, healthcare, and
education. However, these advancements have raised concerns regarding the
safety and trustworthiness of advanced AI, including risks related to
uncontrollability, ethical conflicts, long-term socioeconomic impacts, and
safety assurance. Efforts are being expended to develop internationally
agreed-upon standards to ensure the safety and reliability of AI. This study
analyzes international trends in safety and trustworthiness standardization for
advanced AI, identifies key areas for standardization, proposes future
directions and strategies, and draws policy implications. The goal is to
support the safe and trustworthy development of advanced AI and enhance
international competitiveness through effective standardization."
"Explainable Artificial Intelligence for Dependent Features: Additive
  Effects of Collinearity","Explainable Artificial Intelligence (XAI) emerged to reveal the internal
mechanism of machine learning models and how the features affect the prediction
outcome. Collinearity is one of the big issues that XAI methods face when
identifying the most informative features in the model. Current XAI approaches
assume the features in the models are independent and calculate the effect of
each feature toward model prediction independently from the rest of the
features. However, such assumption is not realistic in real life applications.
We propose an Additive Effects of Collinearity (AEC) as a novel XAI method that
aim to considers the collinearity issue when it models the effect of each
feature in the model on the outcome. AEC is based on the idea of dividing
multivariate models into several univariate models in order to examine their
impact on each other and consequently on the outcome. The proposed method is
implemented using simulated and real data to validate its efficiency comparing
with the a state of arts XAI method. The results indicate that AEC is more
robust and stable against the impact of collinearity when it explains AI models
compared with the state of arts XAI method."
Artificial Intelligence for Microbiology and Microbiome Research,"Advancements in artificial intelligence (AI) have transformed many scientific
fields, with microbiology and microbiome research now experiencing significant
breakthroughs through machine learning and deep learning applications. This
review provides a comprehensive overview of AI-driven approaches tailored for
microbiology and microbiome studies, emphasizing both technical advancements
and biological insights. We begin with an introduction to foundational AI
techniques, including primary machine learning paradigms and various deep
learning architectures, and offer guidance on choosing between machine learning
and deep learning methods based on specific research goals. The primary section
on application scenarios spans diverse research areas, from taxonomic
profiling, functional annotation & prediction, microbe-X interactions,
microbial ecology, metabolic modeling, precision nutrition, clinical
microbiology, to prevention & therapeutics. Finally, we discuss challenges
unique to this field, including the balance between interpretability and
complexity, the ""small n, large p"" problem, and the critical need for
standardized benchmarking datasets to validate and compare models. Together,
this review underscores AI's transformative role in microbiology and microbiome
research, paving the way for innovative methodologies and applications that
enhance our understanding of microbial life and its impact on our planet and
our health."
"The Enhancement of Software Delivery Performance through Enterprise
  DevSecOps and Generative Artificial Intelligence in Chinese Technology Firms","This study investigates the impact of integrating DevSecOps and Generative
Artificial Intelligence (GAI) on software delivery performance within
technology firms. Utilizing a qualitative research methodology, the research
involved semi-structured interviews with industry practitioners and analysis of
case studies from organizations that have successfully implemented these
methodologies. The findings reveal significant enhancements in research and
development (R&D) efficiency, improved source code management, and heightened
software quality and security. The integration of GAI facilitated automation of
coding tasks and predictive analytics, while DevSecOps ensured that security
measures were embedded throughout the development lifecycle. Despite the
promising results, the study identifies gaps related to the generalizability of
the findings due to the limited sample size and the qualitative nature of the
research. This paper contributes valuable insights into the practical
implementation of DevSecOps and GAI, highlighting their potential to transform
software delivery processes in technology firms. Future research directions
include quantitative assessments of the impact on specific business outcomes
and comparative studies across different industries."
"Artificial Intelligence in Financial Forecasting: Analyzing the
  Suitability of AI Models for Dollar/TL Exchange Rate Predictions","The development of artificial intelligence has made significant contributions
to the financial sector. One of the main interests of investors is price
predictions. Technical and fundamental analyses, as well as econometric
analyses, are conducted for price predictions; recently, the use of AI-based
methods has become more prevalent. This study examines daily Dollar/TL exchange
rates from January 1, 2020, to October 4, 2024. It has been observed that among
artificial intelligence models, random forest, support vector machines,
k-nearest neighbors, decision trees, and gradient boosting models were not
suitable; however, multilayer perceptron and linear regression models showed
appropriate suitability and despite the sharp increase in Dollar/TL rates in
Turkey as of 2019, the suitability of valid models has been maintained."
Differential Privacy Overview and Fundamental Techniques,"This chapter is meant to be part of the book ""Differential Privacy in
Artificial Intelligence: From Theory to Practice"" and provides an introduction
to Differential Privacy. It starts by illustrating various attempts to protect
data privacy, emphasizing where and why they failed, and providing the key
desiderata of a robust privacy definition. It then defines the key actors,
tasks, and scopes that make up the domain of privacy-preserving data analysis.
Following that, it formalizes the definition of Differential Privacy and its
inherent properties, including composition, post-processing immunity, and group
privacy. The chapter also reviews the basic techniques and mechanisms commonly
used to implement Differential Privacy in its pure and approximate forms."
"Architectural Patterns for Designing Quantum Artificial Intelligence
  Systems","Utilising quantum computing technology to enhance artificial intelligence
systems is expected to improve training and inference times, increase
robustness against noise and adversarial attacks, and reduce the number of
parameters without compromising accuracy. However, moving beyond
proof-of-concept or simulations to develop practical applications of these
systems while ensuring high software quality faces significant challenges due
to the limitations of quantum hardware and the underdeveloped knowledge base in
software engineering for such systems. In this work, we have conducted a
systematic mapping study to identify the challenges and solutions associated
with the software architecture of quantum-enhanced artificial intelligence
systems. The results of the systematic mapping study reveal several
architectural patterns that describe how quantum components can be integrated
into inference engines, as well as middleware patterns that facilitate
communication between classical and quantum components. Each pattern realises a
trade-off between various software quality attributes, such as efficiency,
scalability, trainability, simplicity, portability, and deployability. The
outcomes of this work have been compiled into a catalogue of architectural
patterns."
"COST CA20120 INTERACT Framework of Artificial Intelligence Based Channel
  Modeling","Accurate channel models are the prerequisite for communication-theoretic
investigations as well as system design. Channel modeling generally relies on
statistical and deterministic approaches. However, there are still significant
limits for the traditional modeling methods in terms of accuracy,
generalization ability, and computational complexity. The fundamental reason is
that establishing a quantified and accurate mapping between physical
environment and channel characteristics becomes increasing challenging for
modern communication systems. Here, in the context of COST CA20120 Action, we
evaluate and discuss the feasibility and implementation of using artificial
intelligence (AI) for channel modeling, and explore where the future of this
field lies. Firstly, we present a framework of AI-based channel modeling to
characterize complex wireless channels. Then, we highlight in detail some major
challenges and present the possible solutions: i) estimating the uncertainty of
AI-based channel predictions, ii) integrating prior knowledge of propagation to
improve generalization capabilities, and iii) interpretable AI for channel
modeling. We present and discuss illustrative numerical results to showcase the
capabilities of AI-based channel modeling."
"Artificial Intelligence in Cybersecurity: Building Resilient Cyber
  Diplomacy Frameworks","This paper explores how automation and artificial intelligence (AI) are
transforming U.S. cyber diplomacy. Leveraging these technologies helps the U.S.
manage the complexity and urgency of cyber diplomacy, improving
decision-making, efficiency, and security. As global inter connectivity grows,
cyber diplomacy, managing national interests in the digital space has become
vital. The ability of AI and automation to quickly process vast data volumes
enables timely responses to cyber threats and opportunities. This paper
underscores the strategic integration of these tools to maintain U.S.
competitive advantage and secure national interests. Automation enhances
diplomatic communication and data processing, freeing diplomats to focus on
strategic decisions. AI supports predictive analytics and real time decision
making, offering critical insights and proactive measures during high stakes
engagements. Case studies show AIs effectiveness in monitoring cyber activities
and managing international cyber policy. Challenges such as ethical concerns,
security vulnerabilities, and reliance on technology are also addressed,
emphasizing human oversight and strong governance frameworks. Ensuring proper
ethical guidelines and cybersecurity measures allows the U.S. to harness the
benefits of automation and AI while mitigating risks. By adopting these
technologies, U.S. cyber diplomacy can become more proactive and effective,
navigating the evolving digital landscape with greater agility."
"AI-Driven Agents with Prompts Designed for High Agreeableness Increase
  the Likelihood of Being Mistaken for a Human in the Turing Test","Large Language Models based on transformer algorithms have revolutionized
Artificial Intelligence by enabling verbal interaction with machines akin to
human conversation. These AI agents have surpassed the Turing Test, achieving
confusion rates up to 50%. However, challenges persist, especially with the
advent of robots and the need to humanize machines for improved Human-AI
collaboration. In this experiment, three GPT agents with varying levels of
agreeableness (disagreeable, neutral, agreeable) based on the Big Five
Inventory were tested in a Turing Test. All exceeded a 50% confusion rate, with
the highly agreeable AI agent surpassing 60%. This agent was also recognized as
exhibiting the most human-like traits. Various explanations in the literature
address why these GPT agents were perceived as human, including psychological
frameworks for understanding anthropomorphism. These findings highlight the
importance of personality engineering as an emerging discipline in artificial
intelligence, calling for collaboration with psychology to develop ergonomic
psychological models that enhance system adaptability in collaborative
activities."
"Delegating Responsibilities to Intelligent Autonomous Systems:
  Challenges and Benefits","As AI systems increasingly operate with autonomy and adaptability, the
traditional boundaries of moral responsibility in techno-social systems are
being challenged. This paper explores the evolving discourse on the delegation
of responsibilities to intelligent autonomous agents and the ethical
implications of such practices. Synthesizing recent developments in AI ethics,
including concepts of distributed responsibility and ethical AI by design, the
paper proposes a functionalist perspective as a framework. This perspective
views moral responsibility not as an individual trait but as a role within a
socio-technical system, distributed among human and artificial agents. As an
example of 'AI ethical by design,' we present Basti and Vitiello's
implementation. They suggest that AI can act as artificial moral agents by
learning ethical guidelines and using Deontic Higher-Order Logic to assess
decisions ethically. Motivated by the possible speed and scale beyond human
supervision and ethical implications, the paper argues for 'AI ethical by
design', while acknowledging the distributed, shared, and dynamic nature of
responsibility. This functionalist approach offers a practical framework for
navigating the complexities of AI ethics in a rapidly evolving technological
landscape."
"Towards the Ultimate Programming Language: Trust and Benevolence in the
  Age of Artificial Intelligence","This article explores the evolving role of programming languages in the
context of artificial intelligence. It highlights the need for programming
languages to ensure human understanding while eliminating unnecessary
implementation details and suggests that future programs should be designed to
recognize and actively support user interests. The vision includes a
three-level process: using natural language for requirements, translating it
into a precise system definition language, and finally optimizing the code for
performance. The concept of an ""Ultimate Programming Language"" is introduced,
emphasizing its role in maintaining human control over machines. Trust,
reliability, and benevolence are identified as key elements that will enhance
cooperation between humans and AI systems."
Zonal Architecture Development with evolution of Artificial Intelligence,"This paper explains how traditional centralized architectures are
transitioning to distributed zonal approaches to address challenges in
scalability, reliability, performance, and cost-effectiveness. The role of edge
computing and neural networks in enabling sophisticated sensor fusion and
decision-making capabilities for autonomous vehicles is examined. Additionally,
this paper discusses the impact of zonal architectures on vehicle diagnostics,
power distribution, and smart power management systems. Key design
considerations for implementing effective zonal architectures are presented,
along with an overview of current challenges and future directions. The
objective of this paper is to provide a comprehensive understanding of how
zonal architectures are shaping the future of automotive technology,
particularly in the context of self-driving vehicles and artificial
intelligence integration."
"The Evolution and Future Perspectives of Artificial Intelligence
  Generated Content","Artificial intelligence generated content (AIGC), a rapidly advancing
technology, is transforming content creation across domains, such as text,
images, audio, and video. Its growing potential has attracted more and more
researchers and investors to explore and expand its possibilities. This review
traces AIGC's evolution through four developmental milestones-ranging from
early rule-based systems to modern transfer learning models-within a unified
framework that highlights how each milestone contributes uniquely to content
generation. In particular, the paper employs a common example across all
milestones to illustrate the capabilities and limitations of methods within
each phase, providing a consistent evaluation of AIGC methodologies and their
development. Furthermore, this paper addresses critical challenges associated
with AIGC and proposes actionable strategies to mitigate them. This study aims
to guide researchers and practitioners in selecting and optimizing AIGC models
to enhance the quality and efficiency of content creation across diverse
domains."
Implementing An Artificial Quantum Perceptron,"A Perceptron is a fundamental building block of a neural network. The
flexibility and scalability of perceptron make it ubiquitous in building
intelligent systems. Studies have shown the efficacy of a single neuron in
making intelligent decisions. Here, we examined and compared two perceptrons
with distinct mechanisms, and developed a quantum version of one of those
perceptrons. As a part of this modeling, we implemented the quantum circuit for
an artificial perception, generated a dataset, and simulated the training.
Through these experiments, we show that there is an exponential growth
advantage and test different qubit versions. Our findings show that this
quantum model of an individual perceptron can be used as a pattern classifier.
For the second type of model, we provide an understanding to design and
simulate a spike-dependent quantum perceptron. Our code is available at
https://github.com/ashutosh1919/quantum-perceptron"
"Considerations Influencing Offense-Defense Dynamics From Artificial
  Intelligence","The rapid advancement of artificial intelligence (AI) technologies presents
profound challenges to societal safety. As AI systems become more capable,
accessible, and integrated into critical services, the dual nature of their
potential is increasingly clear. While AI can enhance defensive capabilities in
areas like threat detection, risk assessment, and automated security
operations, it also presents avenues for malicious exploitation and large-scale
societal harm, for example through automated influence operations and cyber
attacks. Understanding the dynamics that shape AI's capacity to both cause harm
and enhance protective measures is essential for informed decision-making
regarding the deployment, use, and integration of advanced AI systems. This
paper builds on recent work on offense-defense dynamics within the realm of AI,
proposing a taxonomy to map and examine the key factors that influence whether
AI systems predominantly pose threats or offer protective benefits to society.
By establishing a shared terminology and conceptual foundation for analyzing
these interactions, this work seeks to facilitate further research and
discourse in this critical area."
"AI4EF: Artificial Intelligence for Energy Efficiency in the Building
  Sector","AI4EF, Artificial Intelligence for Energy Efficiency, is an advanced,
user-centric tool designed to support decision-making in building energy
retrofitting and efficiency optimization. Leveraging machine learning (ML) and
data-driven insights, AI4EF enables stakeholders such as public sector
representatives, energy consultants, and building owners to model, analyze, and
predict energy consumption, retrofit costs, and environmental impacts of
building upgrades. Featuring a modular framework, AI4EF includes customizable
building retrofitting, photovoltaic installation assessment, and predictive
modeling tools that allow users to input building parameters and receive
tailored recommendations for achieving energy savings and carbon reduction
goals. Additionally, the platform incorporates a Training Playground for data
scientists to refine ML models used by said framework. Finally, AI4EF provides
access to the Enershare Data Space to facilitate seamless data sharing and
access within the ecosystem. Its compatibility with open-source identity
management, Keycloak, enhances security and accessibility, making it adaptable
for various regulatory and organizational contexts. This paper presents an
architectural overview of AI4EF, its application in energy efficiency
scenarios, and its potential for advancing sustainable energy practices through
artificial intelligence (AI)."
"Artificial intelligence and cybersecurity in banking sector:
  opportunities and risks","The rapid advancements in artificial intelligence (AI) have presented new
opportunities for enhancing efficiency and economic competitiveness across
various industries, espcially in banking. Machine learning (ML), as a subset of
artificial intelligence, enables systems to adapt and learn from vast datasets,
revolutionizing decision-making processes, fraud detection, and customer
service automation. However, these innovations also introduce new challenges,
particularly in the realm of cybersecurity. Adversarial attacks, such as data
poisoning and evasion attacks, represent critical threats to machine learning
models, exploiting vulnerabilities to manipulate outcomes or compromise
sensitive information. Furthermore, this study highlights the dual-use nature
of AI tools, which can be used by malicious users. To address these challenges,
the paper emphasizes the importance of developing machine learning models with
key characteristics such as security, trust, resilience and robustness. These
features are essential to mitigating risks and ensuring the secure deployment
of AI technologies in banking sectors, where the protection of financial data
is paramount. The findings underscore the urgent need for enhanced
cybersecurity frameworks and continuous improvements in defensive mechanisms.
By exploring both opportunities and risks, this paper aims to guide the
responsible integration of AI in the banking sector, paving the way for
innovation while safeguarding against emerging threats."
"Promoting Cooperation in the Public Goods Game using Artificial
  Intelligent Agents","The tragedy of the commons illustrates a fundamental social dilemma where
individual rational actions lead to collectively undesired outcomes,
threatening the sustainability of shared resources. Strategies to escape this
dilemma, however, are in short supply. In this study, we explore how artificial
intelligence (AI) agents can be leveraged to enhance cooperation in public
goods games, moving beyond traditional regulatory approaches to using AI as
facilitators of cooperation. We investigate three scenarios: (1) Mandatory
Cooperation Policy for AI Agents, where AI agents are institutionally mandated
always to cooperate; (2) Player-Controlled Agent Cooperation Policy, where
players evolve control over AI agents' likelihood to cooperate; and (3) Agents
Mimic Players, where AI agents copy the behavior of players. Using a
computational evolutionary model with a population of agents playing public
goods games, we find that only when AI agents mimic player behavior does the
critical synergy threshold for cooperation decrease, effectively resolving the
dilemma. This suggests that we can leverage AI to promote collective well-being
in societal dilemmas by designing AI agents to mimic human players."
Digital Democracy in the Age of Artificial Intelligence,"This chapter explores the influence of Artificial Intelligence (AI) on
digital democracy, focusing on four main areas: citizenship, participation,
representation, and the public sphere. It traces the evolution from electronic
to virtual and network democracy, underscoring how each stage has broadened
democratic engagement through technology. Focusing on digital citizenship, the
chapter examines how AI can improve online engagement and promote ethical
behaviour while posing privacy risks and fostering identity stereotyping.
Regarding political participation, it highlights AI's dual role in mobilising
civic actions and spreading misinformation. Regarding representation, AI's
involvement in electoral processes can enhance voter registration, e-voting,
and the efficiency of result tabulation but raises concerns regarding privacy
and public trust. Also, AI's predictive capabilities shift the dynamics of
political competition, posing ethical questions about manipulation and the
legitimacy of democracy. Finally, the chapter examines how integrating AI and
digital technologies can facilitate democratic political advocacy and
personalised communication. However, this also comes with higher risks of
misinformation and targeted propaganda."
Key Safety Design Overview in AI-driven Autonomous Vehicles,"With the increasing presence of autonomous SAE level 3 and level 4, which
incorporate artificial intelligence software, along with the complex technical
challenges they present, it is essential to maintain a high level of functional
safety and robust software design. This paper explores the necessary safety
architecture and systematic approach for automotive software and hardware,
including fail soft handling of automotive safety integrity level (ASIL) D
(highest level of safety integrity), integration of artificial intelligence
(AI), and machine learning (ML) in automotive safety architecture. By
addressing the unique challenges presented by increasing AI-based automotive
software, we proposed various techniques, such as mitigation strategies and
safety failure analysis, to ensure the safety and reliability of automotive
software, as well as the role of AI in software reliability throughout the data
lifecycle.
  Index Terms Safety Design, Automotive Software, Performance Evaluation,
Advanced Driver Assistance Systems (ADAS) Applications, Automotive Software
Systems, Electronic Control Units."
"What Can Youth Learn About Artificial Intelligence and Machine Learning
  in One Hour? Examining How Hour of Code Activities Address the Five Big Ideas
  of AI","The prominence of artificial intelligence and machine learning in everyday
life has led to efforts to foster AI literacy for all K-12 students. In this
paper, we review how Hour of Code activities engage with the five big ideas of
AI, in particular with machine learning and societal impact. We found that a
large majority of activities focus on perception and machine learning, with
little attention paid to representation and other topics. A surprising finding
was the increased attention paid to critical aspects of computing. However, we
also observed a limited engagement with hands-on activities. In the discussion,
we address how future introductory activities could be designed to offer a
broader array of topics, including the development of tools to introduce
novices to artificial intelligence and machine learning and the design of more
unplugged and collaborative activities."
"Distributed Collaborative Inference System in Next-Generation Networks
  and Communication","With the rapid advancement of artificial intelligence, generative artificial
intelligence (GAI) has taken a leading role in transforming data processing
methods. However, the high computational demands of GAI present challenges for
devices with limited resources. As we move towards the sixth generation of
mobile networks (6G), the higher data rates and improved energy efficiency of
6G create a need for more efficient data processing in GAI. Traditional GAI,
however, shows its limitations in meeting these demands. To address these
challenges, we introduce a multi-level collaborative inference system designed
for next-generation networks and communication. Our proposed system features a
deployment strategy that assigns models of varying sizes to devices at
different network layers. Then, we design a task offloading strategy to
optimise both efficiency and latency. Furthermore, a modified early exit
mechanism is implemented to enhance the inference process for single models.
Experimental results demonstrate that our system effectively reduces inference
latency while maintaining high-quality output. Specifically, compared to
existing work, our system can reduce inference time by up to 17% without
sacrificing the inference accuracy."
"Artificial Intelligence for Central Dogma-Centric Multi-Omics:
  Challenges and Breakthroughs","With the rapid development of high-throughput sequencing platforms, an
increasing number of omics technologies, such as genomics, metabolomics, and
transcriptomics, are being applied to disease genetics research. However,
biological data often exhibit high dimensionality and significant noise, making
it challenging to effectively distinguish disease subtypes using a single-omics
approach. To address these challenges and better capture the interactions among
DNA, RNA, and proteins described by the central dogma, numerous studies have
leveraged artificial intelligence to develop multi-omics models for disease
research. These AI-driven models have improved the accuracy of disease
prediction and facilitated the identification of genetic loci associated with
diseases, thus advancing precision medicine. This paper reviews the
mathematical definitions of multi-omics, strategies for integrating multi-omics
data, applications of artificial intelligence and deep learning in multi-omics,
the establishment of foundational models, and breakthroughs in multi-omics
technologies, drawing insights from over 130 related articles. It aims to
provide practical guidance for computational biologists to better understand
and effectively utilize AI-based multi-omics machine learning algorithms in the
context of central dogma."
An introduction to reservoir computing,"There is a growing interest in the development of artificial neural networks
that are implemented in a physical system. A major challenge in this context is
that these networks are difficult to train since training here would require a
change of physical parameters rather than simply of coefficients in a computer
program. For this reason, reservoir computing, where one employs
high-dimensional recurrent networks and trains only the final layer, is widely
used in this context. In this chapter, I introduce the basic concepts of
reservoir computing. Moreover, I present some important physical
implementations coming from electronics, photonics, spintronics, mechanics, and
biology. Finally, I provide a brief discussion of quantum reservoir computing."
"Future Research Avenues for Artificial Intelligence in Digital Gaming:
  An Exploratory Report","Video games are a natural and synergistic application domain for artificial
intelligence (AI) systems, offering both the potential to enhance player
experience and immersion, as well as providing valuable benchmarks and virtual
environments to advance AI technologies in general. This report presents a
high-level overview of five promising research pathways for applying
state-of-the-art AI methods, particularly deep learning, to digital gaming
within the context of the current research landscape. The objective of this
work is to outline a curated, non-exhaustive list of encouraging research
directions at the intersection of AI and video games that may serve to inspire
more rigorous and comprehensive research efforts in the future. We discuss (i)
investigating large language models as core engines for game agent modelling,
(ii) using neural cellular automata for procedural game content generation,
(iii) accelerating computationally expensive in-game simulations via deep
surrogate modelling, (iv) leveraging self-supervised learning to obtain useful
video game state embeddings, and (v) training generative models of interactive
worlds using unlabelled video data. We also briefly address current technical
challenges associated with the integration of advanced deep learning systems
into video game development, and indicate key areas where further progress is
likely to be beneficial."
The Role of XAI in Transforming Aeronautics and Aerospace Systems,"Recent advancements in Artificial Intelligence (AI) have transformed
decision-making in aeronautics and aerospace. These advancements in AI have
brought with them the need to understand the reasons behind the predictions
generated by AI systems and models, particularly by professionals in these
sectors. In this context, the emergence of eXplainable Artificial Intelligence
(XAI) has helped bridge the gap between professionals in the aeronautical and
aerospace sectors and the AI systems and models they work with. For this
reason, this paper provides a review of the concept of XAI is carried out
defining the term and the objectives it aims to achieve. Additionally, the
paper discusses the types of models defined within it and the properties these
models must fulfill to be considered transparent, as well as the post-hoc
techniques used to understand AI systems and models after their training.
Finally, various application areas within the aeronautical and aerospace
sectors will be presented, highlighting how XAI is used in these fields to help
professionals understand the functioning of AI systems and models."
"Implications of Artificial Intelligence on Health Data Privacy and
  Confidentiality","The rapid integration of artificial intelligence (AI) in healthcare is
revolutionizing medical diagnostics, personalized medicine, and operational
efficiency. However, alongside these advancements, significant challenges arise
concerning patient data privacy, ethical considerations, and regulatory
compliance. This paper examines the dual impact of AI on healthcare,
highlighting its transformative potential and the critical need for
safeguarding sensitive health information. It explores the role of the Health
Insurance Portability and Accountability Act (HIPAA) as a regulatory framework
for ensuring data privacy and security, emphasizing the importance of robust
safeguards and ethical standards in AI-driven healthcare. Through case studies,
including AI applications in diabetic retinopathy, oncology, and the
controversies surrounding data sharing, this study underscores the ethical and
legal complexities of AI implementation. A balanced approach that fosters
innovation while maintaining patient trust and privacy is imperative. The
findings emphasize the importance of continuous education, transparency, and
adherence to regulatory frameworks to harness AI's full potential responsibly
and ethically in healthcare."
"Multilingual Performance of a Multimodal Artificial Intelligence System
  on Multisubject Physics Concept Inventories","We investigate the multilingual and multimodal performance of a large
language model-based artificial intelligence (AI) system, GPT-4o, using a
diverse set of physics concept inventories spanning multiple languages and
subject categories. The inventories, sourced from the PhysPort website, cover
classical physics topics such as mechanics, electromagnetism, optics, and
thermodynamics, as well as relativity, quantum mechanics, astronomy,
mathematics, and laboratory skills. Unlike previous text-only studies, we
uploaded the inventories as images to reflect what a student would see on
paper, thereby assessing the system's multimodal functionality. Our results
indicate variation in performance across subjects, with laboratory skills
standing out as the weakest. We also observe differences across languages, with
English and European languages showing the strongest performance. Notably, the
relative difficulty of an inventory item is largely independent of the language
of the survey. When comparing AI results to existing literature on student
performance, we find that the AI system outperforms average post-instruction
undergraduate students in all subject categories except laboratory skills.
Furthermore, the AI performs worse on items requiring visual interpretation of
images than on those that are purely text-based."
"OpenAI ChatGPT interprets Radiological Images: GPT-4 as a Medical Doctor
  for a Fast Check-Up","OpenAI released version GPT-4 on March 14, 2023, following the success of
ChatGPT, which was announced in November 2022. In addition to the existing
GPT-3 features, GPT-4 can interpret images. To achieve this, the processing
power and model have been significantly improved. The ability to process and
interpret images goes far beyond the applications and effectiveness of
artificial intelligence. In this study, we first explored the interpretation of
radiological images in healthcare using artificial intelligence (AI). Then, we
experimented with the image interpretation capability of the GPT-4. In this
way, we addressed the question of whether artificial intelligence (AI) can
replace a healthcare professional (e.g., a medical doctor) or whether it can be
used as a decision-support tool that makes decisions easier and more reliable.
Our results showed that ChatGPT is not sufficient and accurate to analyze chest
X-ray images, but it can provide interpretations that can assist medical
doctors or clinicians."
"Generative Artificial Intelligence-Supported Pentesting: A Comparison
  between Claude Opus, GPT-4, and Copilot","The advent of Generative Artificial Intelligence (GenAI) has brought a
significant change to our society. GenAI can be applied across numerous fields,
with particular relevance in cybersecurity. Among the various areas of
application, its use in penetration testing (pentesting) or ethical hacking
processes is of special interest. In this paper, we have analyzed the potential
of leading generic-purpose GenAI tools-Claude Opus, GPT-4 from ChatGPT, and
Copilot-in augmenting the penetration testing process as defined by the
Penetration Testing Execution Standard (PTES). Our analysis involved evaluating
each tool across all PTES phases within a controlled virtualized environment.
The findings reveal that, while these tools cannot fully automate the
pentesting process, they provide substantial support by enhancing efficiency
and effectiveness in specific tasks. Notably, all tools demonstrated utility;
however, Claude Opus consistently outperformed the others in our experimental
scenarios."
"Cyber Shadows: Neutralizing Security Threats with AI and Targeted Policy
  Measures","The digital age, driven by the AI revolution, brings significant
opportunities but also conceals security threats, which we refer to as cyber
shadows. These threats pose risks at individual, organizational, and societal
levels. This paper examines the systemic impact of these cyber threats and
proposes a comprehensive cybersecurity strategy that integrates AI-driven
solutions, such as Intrusion Detection Systems (IDS), with targeted policy
interventions. By combining technological and regulatory measures, we create a
multilevel defense capable of addressing both direct threats and indirect
negative externalities. We emphasize that the synergy between AI-driven
solutions and policy interventions is essential for neutralizing cyber threats
and mitigating their negative impact on the digital economy. Finally, we
underscore the need for continuous adaptation of these strategies, especially
in response to the rapid advancement of autonomous AI-driven attacks, to ensure
the creation of secure and resilient digital ecosystems."
Towards A Litmus Test for Common Sense,"This paper is the second in a planned series aimed at envisioning a path to
safe and beneficial artificial intelligence. Building on the conceptual
insights of ""Common Sense Is All You Need,"" we propose a more formal litmus
test for common sense, adopting an axiomatic approach that combines minimal
prior knowledge (MPK) constraints with diagonal or Godel-style arguments to
create tasks beyond the agent's known concept set. We discuss how this approach
applies to the Abstraction and Reasoning Corpus (ARC), acknowledging
training/test data constraints, physical or virtual embodiment, and large
language models (LLMs). We also integrate observations regarding emergent
deceptive hallucinations, in which more capable AI systems may intentionally
fabricate plausible yet misleading outputs to disguise knowledge gaps. The
overarching theme is that scaling AI without ensuring common sense risks
intensifying such deceptive tendencies, thereby undermining safety and trust.
Aligning with the broader goal of developing beneficial AI without causing
harm, our axiomatic litmus test not only diagnoses whether an AI can handle
truly novel concepts but also provides a stepping stone toward an ethical,
reliable foundation for future safe, beneficial, and aligned artificial
intelligence."
Developing an Ontology for AI Act Fundamental Rights Impact Assessments,"The recently published EU Artificial Intelligence Act (AI Act) is a landmark
regulation that regulates the use of AI technologies. One of its novel
requirements is the obligation to conduct a Fundamental Rights Impact
Assessment (FRIA), where organisations in the role of deployers must assess the
risks of their AI system regarding health, safety, and fundamental rights.
Another novelty in the AI Act is the requirement to create a questionnaire and
an automated tool to support organisations in their FRIA obligations. Such
automated tools will require a machine-readable form of information involved
within the FRIA process, and additionally also require machine-readable
documentation to enable further compliance tools to be created. In this
article, we present our novel representation of the FRIA as an ontology based
on semantic web standards. Our work builds upon the existing state of the art,
notably the Data Privacy Vocabulary (DPV), where similar works have been
established to create tools for GDPR's Data Protection Impact Assessments
(DPIA) and other obligations. Through our ontology, we enable the creation and
management of FRIA, and the use of automated tool in its various steps."
A Basis for Human Responsibility in Artificial Intelligence Computation,"Recent advancements in artificial intelligence have reopened the question
about the boundaries of AI autonomy, particularly in discussions around
artificial general intelligence (AGI) and its potential to act independently
across varied purposes. This paper explores these boundaries through the
analysis of the Alignment Research Center experiment on GPT-4 and introduces
the Start Button Problem, a thought experiment that examines the origins and
limits of AI autonomy. By examining the thought experiment and its
counterarguments will be enlightened how in the need for human activation and
purpose definition lies the AI's inherent dependency on human-initiated
actions, challenging the assumption of AI as an agent. Finally, the paper
addresses the implications of this dependency on human responsibility,
questioning the measure of the extension of human responsibility when using AI
systems."
"A control system framework for counterfactuals: an optimization based
  approach","Counterfactuals are a concept inherited from the field of logic and in
general attain to the existence of causal relations between sentences or
events. In particular, this concept has been introduced also in the context of
interpretability in artificial intelligence, where counterfactuals refer to the
minimum change to the feature values that changes the prediction of a
classification model. The artificial intelligence framework of counterfactuals
is mostly focused on machine learning approaches, typically neglecting the
physics of the variables that determine a change in class. However, a
theoretical formulation of counterfactuals in a control system framework -
i.e., able to account for the mechanisms underlying a change in class - is
lacking. To fill this gap, in this work we propose an original control system,
physics-informed, theoretical foundation for counterfactuals, by means of the
formulation of an optimal control problem. We apply the proposed methodology to
a general glucose-insulin regulation model and results appear promising and
pave the way to the possible integration with artificial intelligence
techniques, with the aim of feeding machine learning models with the physics
knowledge acquired through the system framework."
"Artificial Intelligence for Sustainable Urban Biodiversity: A Framework
  for Monitoring and Conservation","The rapid expansion of urban areas challenges biodiversity conservation,
requiring innovative ecosystem management. This study explores the role of
Artificial Intelligence (AI) in urban biodiversity conservation, its
applications, and a framework for implementation. Key findings show that: (a)
AI enhances species detection and monitoring, achieving over 90% accuracy in
urban wildlife tracking and invasive species management; (b) integrating data
from remote sensing, acoustic monitoring, and citizen science enables
large-scale ecosystem analysis; and (c) AI decision tools improve conservation
planning and resource allocation, increasing prediction accuracy by up to 18.5%
compared to traditional methods. The research presents an AI-Driven Framework
for Urban Biodiversity Management, highlighting AI's impact on monitoring,
conservation strategies, and ecological outcomes. Implementation strategies
include: (a) standardizing data collection and model validation, (b) ensuring
equitable AI access across urban contexts, and (c) developing ethical
guidelines for biodiversity monitoring. The study concludes that integrating AI
in urban biodiversity conservation requires balancing innovation with
ecological wisdom and addressing data quality, socioeconomic disparities, and
ethical concerns."
"Leveraging Social Media Data and Artificial Intelligence for Improving
  Earthquake Response Efforts","The integration of social media and artificial intelligence (AI) into
disaster management, particularly for earthquake response, represents a
profound evolution in emergency management practices. In the digital age,
real-time information sharing has reached unprecedented levels, with social
media platforms emerging as crucial communication channels during crises. This
shift has transformed traditional, centralized emergency services into more
decentralized, participatory models of disaster situational awareness. Our
study includes an experimental analysis of 8,900 social media interactions,
including 2,920 posts and 5,980 replies on X (formerly Twitter), following a
magnitude 5.1 earthquake in Oklahoma on February 2, 2024. The analysis covers
data from the immediate aftermath and extends over the following seven days,
illustrating the critical role of digital platforms in modern disaster
response. The results demonstrate that social media platforms can be
effectively used as real-time situational awareness tools, delivering critical
information to society and authorities during emergencies."
"CrySPAI: A new Crystal Structure Prediction Software Based on Artificial
  Intelligence","Crystal structure predictions based on the combination of first-principles
calculations and machine learning have achieved significant success in
materials science. However, most of these approaches are limited to predicting
specific systems, which hinders their application to unknown or unexplored
domains. In this paper, we present CrySPAI, a crystal structure prediction
package developed using artificial intelligence (AI) to predict energetically
stable crystal structures of inorganic materials given their chemical
compositions. The software consists of three key modules, an evolutionary
optimization algorithm (EOA) that searches for all possible crystal structure
configurations, density functional theory (DFT) that provides the accurate
energy values for these structures, and a deep neural network (DNN) that learns
the relationship between crystal structures and their corresponding energies.
To optimize the process across these modules, a distributed framework is
implemented to parallelize tasks, and an automated workflow has been integrated
into CrySPAI for seamless execution. This paper reports the development and
implementation of AI AI-based CrySPAI Crystal Prediction Software tool and its
unique features."
An Integrated Approach to AI-Generated Content in e-health,"Artificial Intelligence-Generated Content, a subset of Generative Artificial
Intelligence, holds significant potential for advancing the e-health sector by
generating diverse forms of data. In this paper, we propose an end-to-end
class-conditioned framework that addresses the challenge of data scarcity in
health applications by generating synthetic medical images and text data,
evaluating on practical applications such as retinopathy detection, skin
infections and mental health assessments. Our framework integrates Diffusion
and Large Language Models (LLMs) to generate data that closely match real-world
patterns, which is essential for improving downstream task performance and
model robustness in e-health applications. Experimental results demonstrate
that the synthetic images produced by the proposed diffusion model outperform
traditional GAN architectures. Similarly, in the text modality, data generated
by uncensored LLM achieves significantly better alignment with real-world data
than censored models in replicating the authentic tone."
"Explainable Artificial Intelligence for identifying profitability
  predictors in Financial Statements","The interconnected nature of the economic variables influencing a firm's
performance makes the prediction of a company's earning trend a challenging
task. Existing methodologies often rely on simplistic models and financial
ratios failing to capture the complexity of interacting influences. In this
paper, we apply Machine Learning techniques to raw financial statements data
taken from AIDA, a Database comprising Italian listed companies' data from 2013
to 2022.
  We present a comparative study of different models and following the European
AI regulations, we complement our analysis by applying explainability
techniques to the proposed models. In particular, we propose adopting an
eXplainable Artificial Intelligence method based on Game Theory to identify the
most sensitive features and make the result more interpretable."
Progress in Artificial Intelligence and its Determinants,"We study long-run progress in artificial intelligence in a quantitative way.
Many measures, including traditional ones such as patents and publications,
machine learning benchmarks, and a new Aggregate State of the Art in ML (or
ASOTA) Index we have constructed from these, show exponential growth at roughly
constant rates over long periods. Production of patents and publications
doubles every ten years, by contrast with the growth of computing resources
driven by Moore's Law, roughly a doubling every two years. We argue that the
input of AI researchers is also crucial and its contribution can be objectively
estimated. Consequently, we give a simple argument that explains the 5:1
relation between these two rates. We then discuss the application of this
argument to different output measures and compare our analyses with predictions
based on machine learning scaling laws proposed in existing literature. Our
quantitative framework facilitates understanding, predicting, and modulating
the development of these important technologies."
"Towards Transparent and Accurate Diabetes Prediction Using Machine
  Learning and Explainable Artificial Intelligence","Diabetes mellitus (DM) is a global health issue of significance that must be
diagnosed as early as possible and managed well. This study presents a
framework for diabetes prediction using Machine Learning (ML) models,
complemented with eXplainable Artificial Intelligence (XAI) tools, to
investigate both the predictive accuracy and interpretability of the
predictions from ML models. Data Preprocessing is based on the Synthetic
Minority Oversampling Technique (SMOTE) and feature scaling used on the
Diabetes Binary Health Indicators dataset to deal with class imbalance and
variability of clinical features. The ensemble model provided high accuracy,
with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General
Health, Income, and Physical Activity were the most influential predictors
obtained from the model explanations. The results of this study suggest that ML
combined with XAI is a promising means of developing accurate and
computationally transparent tools for use in healthcare systems."
"Review and Recommendations for using Artificial Intelligence in
  Intracoronary Optical Coherence Tomography Analysis","Artificial intelligence (AI) methodologies hold great promise for the rapid
and accurate diagnosis of coronary artery disease (CAD) from intravascular
optical coherent tomography (IVOCT) images. Numerous papers have been published
describing AI-based models for different diagnostic tasks, yet it remains
unclear which models have potential clinical utility and have been properly
validated. This systematic review considered published literature between
January 2015 and February 2023 describing AI-based diagnosis of CAD using
IVOCT. Our search identified 5,576 studies, with 513 included after initial
screening and 35 studies included in the final systematic review after quality
screening. Our findings indicate that most of the identified models are not
currently suitable for clinical use, primarily due to methodological flaws and
underlying biases. To address these issues, we provide recommendations to
improve model quality and research practices to enhance the development of
clinically useful AI products."
"Paper Copilot: The Artificial Intelligence and Machine Learning
  Community Should Adopt a More Transparent and Regulated Peer Review Process","The rapid growth of submissions to top-tier Artificial Intelligence (AI) and
Machine Learning (ML) conferences has prompted many venues to transition from
closed to open review platforms. Some have fully embraced open peer reviews,
allowing public visibility throughout the process, while others adopt hybrid
approaches, such as releasing reviews only after final decisions or keeping
reviews private despite using open peer review systems. In this work, we
analyze the strengths and limitations of these models, highlighting the growing
community interest in transparent peer review. To support this discussion, we
examine insights from Paper Copilot, a website launched two years ago to
aggregate and analyze AI / ML conference data while engaging a global audience.
The site has attracted over 200,000 early-career researchers, particularly
those aged 18-34 from 177 countries, many of whom are actively engaged in the
peer review process. Drawing on our findings, this position paper advocates for
a more transparent, open, and well-regulated peer review aiming to foster
greater community involvement and propel advancements in the field."
Texture Image Synthesis Using Spatial GAN Based on Vision Transformers,"Texture synthesis is a fundamental task in computer vision, whose goal is to
generate visually realistic and structurally coherent textures for a wide range
of applications, from graphics to scientific simulations. While traditional
methods like tiling and patch-based techniques often struggle with complex
textures, recent advancements in deep learning have transformed this field. In
this paper, we propose ViT-SGAN, a new hybrid model that fuses Vision
Transformers (ViTs) with a Spatial Generative Adversarial Network (SGAN) to
address the limitations of previous methods. By incorporating specialized
texture descriptors such as mean-variance (mu, sigma) and textons into the
self-attention mechanism of ViTs, our model achieves superior texture
synthesis. This approach enhances the model's capacity to capture complex
spatial dependencies, leading to improved texture quality that is superior to
state-of-the-art models, especially for regular and irregular textures.
Comparison experiments with metrics such as FID, IS, SSIM, and LPIPS
demonstrate the substantial improvement of ViT-SGAN, which underlines its
efficiency in generating diverse realistic textures."
"Artificial Intelligence and Legal Analysis: Implications for Legal
  Education and the Profession","This article reports the results of a study examining the ability of legal
and non-legal Large Language Models to perform legal analysis using the
Issue-Rule-Application-Conclusion framework. LLMs were tested on legal
reasoning tasks involving rule analysis and analogical reasoning. The results
show that LLMs can conduct basic IRAC analysis, but are limited by brief
responses lacking detail, an inability to commit to answers, false confidence,
and hallucinations. The study compares legal and nonlegal LLMs, identifies
shortcomings, and explores traits that may hinder their ability to think like a
lawyer. It also discusses the implications for legal education and practice,
highlighting the need for critical thinking skills in future lawyers and the
potential pitfalls of overreliance on artificial intelligence AI resulting in a
loss of logic, reasoning, and critical thinking skills."
"Cracking the Code: Enhancing Development finance understanding with
  artificial intelligence","Analyzing development projects is crucial for understanding donors aid
strategies, recipients priorities, and to assess development finance capacity
to adress development issues by on-the-ground actions. In this area, the
Organisation for Economic Co-operation and Developments (OECD) Creditor
Reporting System (CRS) dataset is a reference data source. This dataset
provides a vast collection of project narratives from various sectors
(approximately 5 million projects). While the OECD CRS provides a rich source
of information on development strategies, it falls short in informing project
purposes due to its reporting process based on donors self-declared main
objectives and pre-defined industrial sectors. This research employs a novel
approach that combines Machine Learning (ML) techniques, specifically Natural
Language Processing (NLP), an innovative Python topic modeling technique called
BERTopic, to categorise (cluster) and label development projects based on their
narrative descriptions. By revealing existing yet hidden topics of development
finance, this application of artificial intelligence enables a better
understanding of donor priorities and overall development funding and provides
methods to analyse public and private projects narratives."
"Identifying relevant indicators for monitoring a National Artificial
  Intelligence Strategy","How can a National Artificial Intelligence Strategy be effectively monitored?
To address this question, we propose a methodology consisting of two key
components. First, it involves identifying relevant indicators within national
AI strategies. Second, it assesses the alignment between these indicators and
the strategic actions of a specific government's AI strategy, allowing for a
critical evaluation of its monitoring measures. Moreover, identifying these
indicators helps assess the overall quality of the strategy's structure. A lack
of alignment between strategic actions and the identified indicators may reveal
gaps or blind spots in the strategy. This methodology is demonstrated using the
Brazilian AI strategy as a case study."
Agency in Artificial Intelligence Systems,"There is a general concern that present developments in artificial
intelligence (AI) research will lead to sentient AI systems, and these may pose
an existential threat to humanity. But why cannot sentient AI systems benefit
humanity instead? This paper endeavours to put this question in a tractable
manner. I ask whether a putative AI system will develop an altruistic or a
malicious disposition towards our society, or what would be the nature of its
agency? Given that AI systems are being developed into formidable problem
solvers, we can reasonably expect these systems to preferentially take on
conscious aspects of human problem solving. I identify the relevant phenomenal
aspects of agency in human problem solving. The functional aspects of conscious
agency can be monitored using tools provided by functionalist theories of
consciousness. A recent expert report (Butlin et al. 2023) has identified
functionalist indicators of agency based on these theories. I show how to use
the Integrated Information Theory (IIT) of consciousness, to monitor the
phenomenal nature of this agency. If we are able to monitor the agency of AI
systems as they develop, then we can dissuade them from becoming a menace to
society while encouraging them to be an aid."
A Robust Attack: Displacement Backdoor Attack,"As artificial intelligence becomes more prevalent in our lives, people are
enjoying the convenience it brings, but they are also facing hidden threats,
such as data poisoning and adversarial attacks. These threats can have
disastrous consequences for the application of artificial intelligence,
especially for some applications that take effect immediately, such as
autonomous driving and medical fields. Among these threats, backdoor attacks
have left a deep impression on people with their concealment and simple
deployment, making them a threat that cannot be ignored, however, in the
process of deploying the backdoor model, the backdoor attack often has some
reasons that make it unsatisfactory in real-world applications, such as jitter
and brightness changes. Based on this, we propose a highly robust backdoor
attack that shifts the target sample and combines it with itself to form a
backdoor sample, the Displacement Backdoor Attack(DBA). Experimental results
show that the DBA attack can resist data augmentation that simulates real-world
differences, such as rotation and cropping."
"Unlocking the Potential of Generative AI through Neuro-Symbolic
  Architectures: Benefits and Limitations","Neuro-symbolic artificial intelligence (NSAI) represents a transformative
approach in artificial intelligence (AI) by combining deep learning's ability
to handle large-scale and unstructured data with the structured reasoning of
symbolic methods. By leveraging their complementary strengths, NSAI enhances
generalization, reasoning, and scalability while addressing key challenges such
as transparency and data efficiency. This paper systematically studies diverse
NSAI architectures, highlighting their unique approaches to integrating neural
and symbolic components. It examines the alignment of contemporary AI
techniques such as retrieval-augmented generation, graph neural networks,
reinforcement learning, and multi-agent systems with NSAI paradigms. This study
then evaluates these architectures against comprehensive set of criteria,
including generalization, reasoning capabilities, transferability, and
interpretability, therefore providing a comparative analysis of their
respective strengths and limitations. Notably, the Neuro > Symbolic < Neuro
model consistently outperforms its counterparts across all evaluation metrics.
This result aligns with state-of-the-art research that highlight the efficacy
of such architectures in harnessing advanced technologies like multi-agent
systems."
Bridge the Gaps between Machine Unlearning and AI Regulation,"The ""right to be forgotten"" and the data privacy laws that encode it have
motivated machine unlearning since its earliest days. Now, an inbound wave of
artificial intelligence regulations - like the European Union's Artificial
Intelligence Act (AIA) - potentially offer important new use cases for machine
unlearning. However, this position paper argues, this opportunity will only be
realized if researchers, aided by policymakers, proactively bridge the
(sometimes sizable) gaps between machine unlearning's state of the art and its
potential applications to AI regulation. To demonstrate this point, we use the
AIA as an example. Specifically, we deliver a ""state of the union"" as regards
machine unlearning's current potential for aiding compliance with the AIA. This
starts with a precise cataloging of the potential applications of machine
unlearning to AIA compliance. For each, we flag any legal ambiguities clouding
the potential application and, moreover, flag the technical gaps that exist
between the potential application and the state of the art of machine
unlearning. Finally, we end with a call to action: for both machine learning
researchers and policymakers, to, respectively, solve the open technical and
legal questions that will unlock machine unlearning's potential to assist
compliance with the AIA - and other AI regulation like it."
Time-series attribution maps with regularized contrastive learning,"Gradient-based attribution methods aim to explain decisions of deep learning
models but so far lack identifiability guarantees. Here, we propose a method to
generate attribution maps with identifiability guarantees by developing a
regularized contrastive learning algorithm trained on time-series data plus a
new attribution method called Inverted Neuron Gradient (collectively named
xCEBRA). We show theoretically that xCEBRA has favorable properties for
identifying the Jacobian matrix of the data generating process. Empirically, we
demonstrate robust approximation of zero vs. non-zero entries in the
ground-truth attribution map on synthetic datasets, and significant
improvements across previous attribution methods based on feature ablation,
Shapley values, and other gradient-based methods. Our work constitutes a first
example of identifiable inference of time-series attribution maps and opens
avenues to a better understanding of time-series data, such as for neural
dynamics and decision-processes within neural networks."
"Unlocking the Black Box: Analysing the EU Artificial Intelligence Act's
  Framework for Explainability in AI","The lack of explainability of Artificial Intelligence (AI) is one of the
first obstacles that the industry and regulators must overcome to mitigate the
risks associated with the technology. The need for eXplainable AI (XAI) is
evident in fields where accountability, ethics and fairness are critical, such
as healthcare, credit scoring, policing and the criminal justice system. At the
EU level, the notion of explainability is one of the fundamental principles
that underpin the AI Act, though the exact XAI techniques and requirements are
still to be determined and tested in practice. This paper explores various
approaches and techniques that promise to advance XAI, as well as the
challenges of implementing the principle of explainability in AI governance and
policies. Finally, the paper examines the integration of XAI into EU law,
emphasising the issues of standard setting, oversight, and enforcement."
Robustness and Cybersecurity in the EU Artificial Intelligence Act,"The EU Artificial Intelligence Act (AIA) establishes different legal
principles for different types of AI systems. While prior work has sought to
clarify some of these principles, little attention has been paid to robustness
and cybersecurity. This paper aims to fill this gap. We identify legal
challenges and shortcomings in provisions related to robustness and
cybersecurity for high-risk AI systems (Art. 15 AIA) and general-purpose AI
models (Art. 55 AIA). We show that robustness and cybersecurity demand
resilience against performance disruptions. Furthermore, we assess potential
challenges in implementing these provisions in light of recent advancements in
the machine learning (ML) literature. Our analysis informs efforts to develop
harmonized standards, guidelines by the European Commission, as well as
benchmarks and measurement methodologies under Art. 15(2) AIA. With this, we
seek to bridge the gap between legal terminology and ML research, fostering a
better alignment between research and implementation efforts."
"All You Need for Counterfactual Explainability Is Principled and
  Reliable Estimate of Aleatoric and Epistemic Uncertainty","This position paper argues that, to its detriment, transparency research
overlooks many foundational concepts of artificial intelligence. Here, we focus
on uncertainty quantification -- in the context of ante-hoc interpretability
and counterfactual explainability -- showing how its adoption could address key
challenges in the field. First, we posit that uncertainty and ante-hoc
interpretability offer complementary views of the same underlying idea; second,
we assert that uncertainty provides a principled unifying framework for
counterfactual explainability. Consequently, inherently transparent models can
benefit from human-centred explanatory insights -- like counterfactuals --
which are otherwise missing. At a higher level, integrating artificial
intelligence fundamentals into transparency research promises to yield more
reliable, robust and understandable predictive models."
"Modeling Arbitrarily Applicable Relational Responding with the
  Non-Axiomatic Reasoning System: A Machine Psychology Approach","Arbitrarily Applicable Relational Responding (AARR) is a cornerstone of human
language and reasoning, referring to the learned ability to relate symbols in
flexible, context-dependent ways. In this paper, we present a novel theoretical
approach for modeling AARR within an artificial intelligence framework using
the Non-Axiomatic Reasoning System (NARS). NARS is an adaptive reasoning system
designed for learning under uncertainty. By integrating principles from
Relational Frame Theory - the behavioral psychology account of AARR - with the
reasoning mechanisms of NARS, we conceptually demonstrate how key properties of
AARR (mutual entailment, combinatorial entailment, and transformation of
stimulus functions) can emerge from the inference rules and memory structures
of NARS. Two theoretical experiments illustrate this approach: one modeling
stimulus equivalence and transfer of function, and another modeling complex
relational networks involving opposition frames. In both cases, the system
logically demonstrates the derivation of untrained relations and
context-sensitive transformations of stimulus significance, mirroring
established human cognitive phenomena. These results suggest that AARR - long
considered uniquely human - can be conceptually captured by suitably designed
AI systems, highlighting the value of integrating behavioral science insights
into artificial general intelligence (AGI) research."
"FAIR: Facilitating Artificial Intelligence Resilience in Manufacturing
  Industrial Internet","Artificial intelligence (AI) systems have been increasingly adopted in the
Manufacturing Industrial Internet (MII). Investigating and enabling the AI
resilience is very important to alleviate profound impact of AI system failures
in manufacturing and Industrial Internet of Things (IIoT) operations, leading
to critical decision making. However, there is a wide knowledge gap in defining
the resilience of AI systems and analyzing potential root causes and
corresponding mitigation strategies. In this work, we propose a novel framework
for investigating the resilience of AI performance over time under hazard
factors in data quality, AI pipelines, and the cyber-physical layer. The
proposed method can facilitate effective diagnosis and mitigation strategies to
recover AI performance based on a multimodal multi-head self latent attention
model. The merits of the proposed method are elaborated using an MII testbed of
connected Aerosol Jet Printing (AJP) machines, fog nodes, and Cloud with
inference tasks via AI pipelines."
"Dynamic spillovers and investment strategies across artificial
  intelligence ETFs, artificial intelligence tokens, and green markets","This paper investigates the risk spillovers among AI ETFs, AI tokens, and
green markets using the R2 decomposition method. We reveal several key
insights. First, the overall transmission connectedness index (TCI) closely
aligns with the contemporaneous TCI, while the lagged TCI is significantly
lower. Second, AI ETFs and clean energy act as risk transmitters, whereas AI
tokens and green bond function as risk receivers. Third, AI tokens are
difficult to hedge and provide limited hedging ability compared to AI ETFs and
green assets. However, multivariate portfolios effectively reduce AI tokens
investment risk. Among them, the minimum correlation portfolio outperforms the
minimum variance and minimum connectedness portfolios."
"Enhancing the Product Quality of the Injection Process Using eXplainable
  Artificial Intelligence","The injection molding process is a traditional technique for making products
in various industries such as electronics and automobiles via solidifying
liquid resin into certain molds. Although the process is not related to
creating the main part of engines or semiconductors, this manufacturing
methodology sets the final form of the products. Re-cently, research has
continued to reduce the defect rate of the injection molding process. This
study proposes an optimal injection molding process control system to reduce
the defect rate of injection molding products with XAI (eXplainable Artificial
Intelligence) ap-proaches. Boosting algorithms (XGBoost and LightGBM) are used
as tree-based classifiers for predicting whether each product is normal or
defective. The main features to control the process for improving the product
are extracted by SHapley Additive exPlanations, while the individual
conditional expectation analyzes the optimal control range of these extracted
features. To validate the methodology presented in this work, the actual
injection molding AI manufacturing dataset provided by KAMP (Korea AI
Manufacturing Platform) is employed for the case study. The results reveal that
the defect rate decreases from 1.00% (Original defect rate) to 0.21% with
XGBoost and 0.13% with LightGBM, respectively."
"Applied Machine Learning Methods with Long-Short Term Memory Based
  Recurrent Neural Networks for Multivariate Temperature Prediction","This paper gives an overview on how to develop a dense and deep neural
network for making a time series prediction. First, the history and
cornerstones in Artificial Intelligence and Machine Learning will be presented.
After a short introduction to the theory of Artificial Intelligence and Machine
Learning, the paper will go deeper into the techniques for conducting a time
series prediction with different models of neural networks. For this project,
Python's development environment Jupyter, extended with the TensorFlow package
and deep-learning application Keras is used. The system setup and project
framework are explained in more detail before discussing the time series
prediction. The main part shows an applied example of time series prediction
with weather data. For this work, a deep recurrent neural network with Long
Short-Term Memory cells is used to conduct the time series prediction. The
results and evaluation of the work show that a weather prediction with deep
neural networks can be successful for a short time period. However, there are
some drawbacks and limitations with time series prediction, which will be
discussed towards the end of the paper."
"The StudyChat Dataset: Student Dialogues With ChatGPT in an Artificial
  Intelligence Course","The widespread availability of large language models (LLMs), such as ChatGPT,
has significantly impacted education, raising both opportunities and
challenges. Students can frequently interact with LLM-powered, interactive
learning tools, but their usage patterns need to be analyzed to ensure ethical
usage of these tools. To better understand how students interact with LLMs in
an academic setting, we introduce \textbf{StudyChat}, a publicly available
dataset capturing real-world student interactions with an LLM-powered tutoring
chatbot in a semester-long, university-level artificial intelligence (AI)
course. We deploy a web application that replicates ChatGPT's core
functionalities, and use it to log student interactions with the LLM while
working on programming assignments. We collect 1,197 conversations, which we
annotate using a dialogue act labeling schema inspired by observed interaction
patterns and prior research. Additionally, we analyze these interactions,
highlight behavioral trends, and analyze how specific usage patterns relate to
course outcomes. \textbf{StudyChat} provides a rich resource for the learning
sciences and AI in education communities, enabling further research into the
evolving role of LLMs in education."
Blockchain As a Platform For Artificial Intelligence (AI) Transparency,"As artificial intelligence (AI) systems become increasingly complex and
autonomous, concerns over transparency and accountability have intensified. The
""black box"" problem in AI decision-making limits stakeholders' ability to
understand, trust, and verify outcomes, particularly in high-stakes sectors
such as healthcare, finance, and autonomous systems. Blockchain technology,
with its decentralized, immutable, and transparent characteristics, presents a
potential solution to enhance AI transparency and auditability. This paper
explores the integration of blockchain with AI to improve decision
traceability, data provenance, and model accountability. By leveraging
blockchain as an immutable record-keeping system, AI decision-making can become
more interpretable, fostering trust among users and regulatory compliance.
However, challenges such as scalability, integration complexity, and
computational overhead must be addressed to fully realize this synergy. This
study discusses existing research, proposes a framework for blockchain-enhanced
AI transparency, and highlights practical applications, benefits, and
limitations. The findings suggest that blockchain could be a foundational
technology for ensuring AI systems remain accountable, ethical, and aligned
with regulatory standards."
"The Impact of Artificial Intelligence on Emergency Medicine: A Review of
  Recent Advances","Artificial Intelligence (AI) is revolutionizing emergency medicine by
enhancing diagnostic processes and improving patient outcomes. This article
provides a review of the current applications of AI in emergency imaging
studies, focusing on the last five years of advancements. AI technologies,
particularly machine learning and deep learning, are pivotal in interpreting
complex imaging data, offering rapid, accurate diagnoses and potentially
surpassing traditional diagnostic methods. Studies highlighted within the
article demonstrate AI's capabilities in accurately detecting conditions such
as fractures, pneumothorax, and pulmonary diseases from various imaging
modalities including X-rays, CT scans, and MRIs. Furthermore, AI's ability to
predict clinical outcomes like mechanical ventilation needs illustrates its
potential in crisis resource optimization. Despite these advancements, the
integration of AI into clinical practice presents challenges such as data
privacy, algorithmic bias, and the need for extensive validation across diverse
settings. This review underscores the transformative potential of AI in
emergency settings, advocating for a future where AI and clinical expertise
synergize to elevate patient care standards."
"World Models in Artificial Intelligence: Sensing, Learning, and
  Reasoning Like a Child","World Models help Artificial Intelligence (AI) predict outcomes, reason about
its environment, and guide decision-making. While widely used in reinforcement
learning, they lack the structured, adaptive representations that even young
children intuitively develop. Advancing beyond pattern recognition requires
dynamic, interpretable frameworks inspired by Piaget's cognitive development
theory. We highlight six key research areas -- physics-informed learning,
neurosymbolic learning, continual learning, causal inference, human-in-the-loop
AI, and responsible AI -- as essential for enabling true reasoning in AI. By
integrating statistical learning with advances in these areas, AI can evolve
from pattern recognition to genuine understanding, adaptation and reasoning
capabilities."
ChatGPT or A Silent Everywhere Helper: A Survey of Large Language Models,"Large Language Models (LLMs) have revo lutionized natural language processing
Natural Language Processing (NLP), with Chat Generative Pre-trained Transformer
(ChatGPT) standing out as a notable exampledue to its advanced capabilities and
widespread applications. This survey provides a comprehensive analysis of
ChatGPT, exploring its architecture, training processes, and functionalities.
We examine its integration into various domains across industries such as
customer service, education, healthcare, and entertainment. A comparative
analysis with other LLMs highlights ChatGPT's unique features and performance
metrics. Regarding benchmarks, the paper examines ChatGPT's comparative
performance against other LLMs and discusses potential risks such as
misinformation, bias, and data privacy concerns. Additionally, we offer a
number of figures and tables that outline the backdrop of the discussion, the
main ideas of the article, the numerous LLM models, a thorough list of datasets
used for pre-training, fine-tuning, and evaluation, as well as particular LLM
applications with pertinent references. Finally, we identify future research
directions and technological advancements, underscoring the evolving landscape
of LLMs and their profound impact on artificial intelligence Artificial
Intelligence (AI) and society."
"Risk Management for Distributed Arbitrage Systems: Integrating
  Artificial Intelligence","Effective risk management solutions become absolutely crucial when financial
markets embrace distributed technology and decentralized financing (DeFi). This
study offers a thorough survey and comparative analysis of the integration of
artificial intelligence (AI) in risk management for distributed arbitrage
systems. We examine several modern caching techniques namely in memory caching,
distributed caching, and proxy caching and their functions in enhancing
performance in decentralized settings. Through literature review we examine the
utilization of AI techniques for alleviating risks related to market
volatility, liquidity challenges, operational failures, regulatory compliance,
and security threats. This comparison research evaluates various case studies
from prominent DeFi technologies, emphasizing critical performance metrics like
latency reduction, load balancing, and system resilience. Additionally, we
examine the problems and trade offs associated with these technologies,
emphasizing their effects on consistency, scalability, and fault tolerance. By
meticulously analyzing real world applications, specifically centering on the
Aave platform as our principal case study, we illustrate how the purposeful
amalgamation of AI with contemporary caching methodologies has revolutionized
risk management in distributed arbitrage systems."
"The Role of Artificial Intelligence in Enhancing Insulin Recommendations
  and Therapy Outcomes","The growing worldwide incidence of diabetes requires more effective
approaches for managing blood glucose levels. Insulin delivery systems have
advanced significantly, with artificial intelligence (AI) playing a key role in
improving their precision and adaptability. AI algorithms, particularly those
based on reinforcement learning, allow for personalised insulin dosing by
continuously adapting to an individual's responses. Despite these advancements,
challenges such as data privacy, algorithm transparency, and accessibility
still need to be addressed. Continued progress and validation in AI-driven
insulin delivery systems promise to improve therapy outcomes further, offering
people more effective and individualised management of their diabetes. This
paper presents an overview of current strategies, key challenges, and future
directions."
"Guarding against artificial intelligence--hallucinated citations: the
  case for full-text reference deposit","The tendency of generative artificial intelligence (AI) systems to
""hallucinate"" false information is well-known; AI-generated citations to
non-existent sources have made their way into the reference lists of
peer-reviewed publications. Here, I propose a solution to this problem, taking
inspiration from the Transparency and Openness Promotion (TOP) data sharing
guidelines, the clash of generative AI with the American judiciary, and the
precedent set by submissions of prior art to the United States Patent and
Trademark Office. Journals should require authors to submit the full text of
each cited source along with their manuscripts, thereby preventing authors from
citing any material whose full text they cannot produce. This solution requires
limited additional work on the part of authors or editors while effectively
immunizing journals against hallucinated references."
Body Discovery of Embodied AI,"In the pursuit of realizing artificial general intelligence (AGI), the
importance of embodied artificial intelligence (AI) becomes increasingly
apparent. Following this trend, research integrating robots with AGI has become
prominent. As various kinds of embodiments have been designed, adaptability to
diverse embodiments will become important to AGI. We introduce a new challenge,
termed ""Body Discovery of Embodied AI"", focusing on tasks of recognizing
embodiments and summarizing neural signal functionality. The challenge
encompasses the precise definition of an AI body and the intricate task of
identifying embodiments in dynamic environments, where conventional approaches
often prove inadequate. To address these challenges, we apply causal inference
method and evaluate it by developing a simulator tailored for testing
algorithms with virtual environments. Finally, we validate the efficacy of our
algorithms through empirical testing, demonstrating their robust performance in
various scenarios based on virtual environments."
"Which LIME should I trust? Concepts, Challenges, and Solutions","As neural networks become dominant in essential systems, Explainable
Artificial Intelligence (XAI) plays a crucial role in fostering trust and
detecting potential misbehavior of opaque models. LIME (Local Interpretable
Model-agnostic Explanations) is among the most prominent model-agnostic
approaches, generating explanations by approximating the behavior of black-box
models around specific instances. Despite its popularity, LIME faces challenges
related to fidelity, stability, and applicability to domain-specific problems.
Numerous adaptations and enhancements have been proposed to address these
issues, but the growing number of developments can be overwhelming,
complicating efforts to navigate LIME-related research. To the best of our
knowledge, this is the first survey to comprehensively explore and collect
LIME's foundational concepts and known limitations. We categorize and compare
its various enhancements, offering a structured taxonomy based on intermediate
steps and key issues. Our analysis provides a holistic overview of advancements
in LIME, guiding future research and helping practitioners identify suitable
approaches. Additionally, we provide a continuously updated interactive website
(https://patrick-knab.github.io/which-lime-to-trust/), offering a concise and
accessible overview of the survey."
"Exploring the Societal and Economic Impacts of Artificial Intelligence:
  A Scenario Generation Methodology","This paper explores artificial intelligence's potential societal and economic
impacts (AI) through generating scenarios that assess how AI may influence
various sectors. We categorize and analyze key factors affecting AI's
integration and adoption by applying an Impact-Uncertainty Matrix. A proposed
methodology involves querying academic databases, identifying emerging trends
and topics, and categorizing these into an impact uncertainty framework. The
paper identifies critical areas where AI may bring significant change and
outlines potential future scenarios based on these insights. This research aims
to inform policymakers, industry leaders, and researchers on the strategic
planning required to address the challenges and opportunities AI presents"
"Engineering Artificial Intelligence: Framework, Challenges, and Future
  Direction","Over the past ten years, the application of artificial intelligence (AI) and
machine learning (ML) in engineering domains has gained significant popularity,
showcasing their potential in data-driven contexts. However, the complexity and
diversity of engineering problems often require the development of
domain-specific AI approaches, which are frequently hindered by a lack of
systematic methodologies, scalability, and robustness during the development
process. To address this gap, this paper introduces the ""ABCDE"" as the key
elements of Engineering AI and proposes a unified, systematic engineering AI
ecosystem framework, including eight essential layers, along with attributes,
goals, and applications, to guide the development and deployment of AI
solutions for specific engineering needs. Additionally, key challenges are
examined, and eight future research directions are highlighted. By providing a
comprehensive perspective, this paper aims to advance the strategic
implementation of AI, fostering the development of next-generation engineering
AI solutions."
"My Life in Artificial Intelligence: People, anecdotes, and some lessons
  learnt","In this very personal workography, I relate my 40-year experiences as a
researcher and educator in and around Artificial Intelligence (AI), more
specifically Natural Language Processing. I describe how curiosity, and the
circumstances of the day, led me to work in both industry and academia, and in
various countries, including The Netherlands (Amsterdam, Eindhoven, and
Utrecht), the USA (Stanford), England (Brighton), Scotland (Aberdeen), and
China (Beijing and Harbin). People and anecdotes play a large role in my story;
the history of AI forms its backdrop. I focus on things that might be of
interest to (even) younger colleagues, given the choices they face in their own
work and life at a time when AI is finally emerging from the shadows."
Future-Proof Yourself: An AI Era Survival Guide,"Future-Proof Yourself is a practical guide that helps readers navigate the
fast-changing world of artificial intelligence in everyday life. The book
begins by explaining how computers learn from data in simple, relatable terms,
and gradually introduces the methods used in modern AI. It shows how basic
ideas in machine learning evolve into advanced systems that can recognize
images, understand language, and even make decisions. The guide also reviews
the history of AI and highlights the major breakthroughs that have shaped its
growth. Looking ahead, the book explores emerging trends such as the
integration of AI with digital twins, wearable devices, and virtual
environments. Designed for a general audience, the text avoids heavy technical
jargon and presents complex ideas in clear, straightforward language so that
anyone can gain a solid understanding of the technology that is set to
transform our future."
"Explanation-Driven Interventions for Artificial Intelligence Model
  Customization: Empowering End-Users to Tailor Black-Box AI in Rhinocytology","The integration of Artificial Intelligence (AI) in modern society is
transforming how individuals perform tasks. In high-risk domains, ensuring
human control over AI systems remains a key design challenge. This article
presents a novel End-User Development (EUD) approach for black-box AI models,
enabling users to edit explanations and influence future predictions through
targeted interventions. By combining explainability, user control, and model
adaptability, the proposed method advances Human-Centered AI (HCAI), promoting
a symbiotic relationship between humans and adaptive, user-tailored AI systems."
DBOT: Artificial Intelligence for Systematic Long-Term Investing,"Long-term investing was previously seen as requiring human judgment. With the
advent of generative artificial intelligence (AI) systems, automated systematic
long-term investing is now feasible. In this paper, we present DBOT, a system
whose goal is to reason about valuation like Aswath Damodaran, who is a unique
expert in the investment arena in terms of having published thousands of
valuations on companies in addition to his numerous writings on the topic,
which provide ready training data for an AI system. DBOT can value any publicly
traded company. DBOT can also be back-tested, making its behavior and
performance amenable to scientific inquiry. We compare DBOT to its analytic
parent, Damodaran, and highlight the research challenges involved in raising
its current capability to that of Damodaran's. Finally, we examine the
implications of DBOT-like AI agents for the financial industry, especially how
they will impact the role of human analysts in valuation."
"Generative Artificial Intelligence for Internet of Things Computing: A
  Systematic Survey","The integration of Generative Artificial Intelligence (GenAI) within the
Internet of Things (IoT) is garnering considerable interest. This growing
attention stems from the continuous evolution and widespread adoption they are
both having individually, enough to spontaneously reshape numerous sectors,
including Healthcare, Manufacturing, and Smart Cities. Hence, their increasing
popularity has catalyzed further extensive research for understanding the
potential of the duo GenAI-IoT, how they interplay, and to which extent their
synergy can innovate the state-of-the-art in their individual scenarios.
However, despite the increasing prominence of GenAI for IoT Computing, much of
the existing research remains focused on specific, narrowly scoped
applications. This fragmented approach highlights the need for a more
comprehensive analysis of the potential, challenges, and implications of GenAI
integration within the broader IoT ecosystem. This survey exactly aims to
address this gap by providing a holistic overview of the opportunities, issues,
and considerations arising from the convergence of these mainstream paradigms.
Our contribution is realized through a systematic literature review following
the PRISMA methodology. A comparison framework is presented, and well-defined
research questions are outlined to comprehensively explore the past, present,
and future directions of GenAI integration with IoT Computing, offering
valuable insights for both experts and newcomers."
"AI-guided Antibiotic Discovery Pipeline from Target Selection to
  Compound Identification","Antibiotic resistance presents a growing global health crisis, demanding new
therapeutic strategies that target novel bacterial mechanisms. Recent advances
in protein structure prediction and machine learning-driven molecule generation
offer a promising opportunity to accelerate drug discovery. However, practical
guidance on selecting and integrating these models into real-world pipelines
remains limited. In this study, we develop an end-to-end, artificial
intelligence-guided antibiotic discovery pipeline that spans target
identification to compound realization. We leverage structure-based clustering
across predicted proteomes of multiple pathogens to identify conserved,
essential, and non-human-homologous targets. We then systematically evaluate
six leading 3D-structure-aware generative models$\unicode{x2014}$spanning
diffusion, autoregressive, graph neural network, and language model
architectures$\unicode{x2014}$on their usability, chemical validity, and
biological relevance. Rigorous post-processing filters and commercial analogue
searches reduce over 100 000 generated compounds to a focused, synthesizable
set. Our results highlight DeepBlock and TamGen as top performers across
diverse criteria, while also revealing critical trade-offs between model
complexity, usability, and output quality. This work provides a comparative
benchmark and blueprint for deploying artificial intelligence in early-stage
antibiotic development."
The Chronicles of Foundation AI for Forensics of Multi-Agent Provenance,"Provenance is the chronology of things, resonating with the fundamental
pursuit to uncover origins, trace connections, and situate entities within the
flow of space and time. As artificial intelligence advances towards autonomous
agents capable of interactive collaboration on complex tasks, the provenance of
generated content becomes entangled in the interplay of collective creation,
where contributions are continuously revised, extended or overwritten. In a
multi-agent generative chain, content undergoes successive transformations,
often leaving little, if any, trace of prior contributions. In this study, we
investigates the problem of tracking multi-agent provenance across the temporal
dimension of generation. We propose a chronological system for post hoc
attribution of generative history from content alone, without reliance on
internal memory states or external meta-information. At its core lies the
notion of symbolic chronicles, representing signed and time-stamped records, in
a form analogous to the chain of custody in forensic science. The system
operates through a feedback loop, whereby each generative timestep updates the
chronicle of prior interactions and synchronises it with the synthetic content
in the very act of generation. This research seeks to develop an accountable
form of collaborative artificial intelligence within evolving cyber ecosystems."
"Framework, Standards, Applications and Best practices of Responsible AI
  : A Comprehensive Survey","Responsible Artificial Intelligence (RAI) is a combination of ethics
associated with the usage of artificial intelligence aligned with the common
and standard frameworks. This survey paper extensively discusses the global and
national standards, applications of RAI, current technology and ongoing
projects using RAI, and possible challenges in implementing and designing RAI
in the industries and projects based on AI. Currently, ethical standards and
implementation of RAI are decoupled which caters each industry to follow their
own standards to use AI ethically. Many global firms and government
organizations are taking necessary initiatives to design a common and standard
framework. Social pressure and unethical way of using AI forces the RAI design
rather than implementation."
Giving AI a voice: how does AI think it should be treated?,"With the astounding progress in (generative) artificial intelligence (AI),
there has been significant public discourse regarding regulation and ethics of
the technology. Is it sufficient when humans discuss this with other humans?
Or, given that AI is increasingly becoming a viable source of inspiration for
people (and let alone the hypothetical possibility that the technology may at
some point become ""artificial general intelligence"" and/or develop
consciousness), should AI not join the discourse? There are new questions and
angles that AI brings to the table that we might not have considered before -
so let us make the key subject of this book an active participant. This chapter
therefore includes a brief human-AI conversation on the topic of AI rights and
ethics."
Generative Semantic Communications: Principles and Practices,"Semantic communication leverages artificial intelligence (AI) technologies to
extract semantic information from data for efficient transmission, theraby
significantly reducing communication cost. With the evolution towards
artificial general intelligence (AGI), the increasing demands for AGI services
pose new challenges to semantic communication. In response, we propose a new
paradigm for AGI-driven communications, called generative semantic
communication (GSC), which utilizes advanced AI technologies such as foundation
models and generative models. We first describe the basic concept of GSC and
its difference from existing semantic communications, and then introduce a
general framework of GSC, followed by two case studies to verify the advantages
of GSC in AGI-driven applications. Finally, open challenges and new research
directions are discussed to stimulate this line of research and pave the way
for practical applications."
Better artificial intelligence does not mean better models of biology,"Deep neural networks (DNNs) once showed increasing alignment with primate
perception and neural responses as they improved on vision benchmarks, raising
hopes that advances in AI would yield better models of biological vision.
However, we show across three benchmarks that this alignment is now plateauing
- and in some cases worsening - as DNNs scale to human or superhuman accuracy.
This divergence may reflect the adoption of visual strategies that differ from
those used by primates. These findings challenge the view that progress in
artificial intelligence will naturally translate to neuroscience. We argue that
vision science must chart its own course, developing algorithms grounded in
biological visual systems rather than optimizing for benchmarks based on
internet-scale datasets."
S2Vec: Self-Supervised Geospatial Embeddings,"Scalable general-purpose representations of the built environment are crucial
for geospatial artificial intelligence applications. This paper introduces
S2Vec, a novel self-supervised framework for learning such geospatial
embeddings. S2Vec uses the S2 Geometry library to partition large areas into
discrete S2 cells, rasterizes built environment feature vectors within cells as
images, and applies masked autoencoding on these rasterized images to encode
the feature vectors. This approach yields task-agnostic embeddings that capture
local feature characteristics and broader spatial relationships. We evaluate
S2Vec on three large-scale socioeconomic prediction tasks, showing its
competitive performance against state-of-the-art image-based embeddings. We
also explore the benefits of combining S2Vec embeddings with image-based
embeddings downstream, showing that such multimodal fusion can often improve
performance. Our results highlight how S2Vec can learn effective
general-purpose geospatial representations and how it can complement other data
modalities in geospatial artificial intelligence."
"Sparks: Multi-Agent Artificial Intelligence Model Discovers Protein
  Design Principles","Advances in artificial intelligence (AI) promise autonomous discovery, yet
most systems still resurface knowledge latent in their training data. We
present Sparks, a multi-modal multi-agent AI model that executes the entire
discovery cycle that includes hypothesis generation, experiment design and
iterative refinement to develop generalizable principles and a report without
human intervention. Applied to protein science, Sparks uncovered two previously
unknown phenomena: (i) a length-dependent mechanical crossover whereby
beta-sheet-biased peptides surpass alpha-helical ones in unfolding force beyond
~80 residues, establishing a new design principle for peptide mechanics; and
(ii) a chain-length/secondary-structure stability map revealing unexpectedly
robust beta-sheet-rich architectures and a ""frustration zone"" of high variance
in mixed alpha/beta folds. These findings emerged from fully self-directed
reasoning cycles that combined generative sequence design, high-accuracy
structure prediction and physics-aware property models, with paired
generation-and-reflection agents enforcing self-correction and reproducibility.
The key result is that Sparks can independently conduct rigorous scientific
inquiry and identify previously unknown scientific principles."
"Advancing Multi-Agent Systems Through Model Context Protocol:
  Architecture, Implementation, and Applications","Multi-agent systems represent a significant advancement in artificial
intelligence, enabling complex problem-solving through coordinated specialized
agents. However, these systems face fundamental challenges in context
management, coordination efficiency, and scalable operation. This paper
introduces a comprehensive framework for advancing multi-agent systems through
Model Context Protocol (MCP), addressing these challenges through standardized
context sharing and coordination mechanisms. We extend previous work on AI
agent architectures by developing a unified theoretical foundation, advanced
context management techniques, and scalable coordination patterns. Through
detailed implementation case studies across enterprise knowledge management,
collaborative research, and distributed problem-solving domains, we demonstrate
significant performance improvements compared to traditional approaches. Our
evaluation methodology provides a systematic assessment framework with
benchmark tasks and datasets specifically designed for multi-agent systems. We
identify current limitations, emerging research opportunities, and potential
transformative applications across industries. This work contributes to the
evolution of more capable, collaborative, and context-aware artificial
intelligence systems that can effectively address complex real-world
challenges."
"Theoretical Foundations for Semantic Cognition in Artificial
  Intelligence","This monograph presents a modular cognitive architecture for artificial
intelligence grounded in the formal modeling of belief as structured semantic
state. Belief states are defined as dynamic ensembles of linguistic expressions
embedded within a navigable manifold, where operators enable assimilation,
abstraction, nullification, memory, and introspection. Drawing from philosophy,
cognitive science, and neuroscience, we develop a layered framework that
enables self-regulating epistemic agents capable of reflective, goal-directed
thought. At the core of this framework is the epistemic vacuum: a class of
semantically inert cognitive states that serves as the conceptual origin of
belief space. From this foundation, the Null Tower arises as a generative
structure recursively built through internal representational capacities. The
theoretical constructs are designed to be implementable in both symbolic and
neural systems, including large language models, hybrid agents, and adaptive
memory architectures. This work offers a foundational substrate for
constructing agents that reason, remember, and regulate their beliefs in
structured, interpretable ways."
The Danger Theory and Its Application to Artificial Immune Systems,"Over the last decade, a new idea challenging the classical self-non-self
viewpoint has become popular amongst immunologists. It is called the Danger
Theory. In this conceptual paper, we look at this theory from the perspective
of Artificial Immune System practitioners. An overview of the Danger Theory is
presented with particular emphasis on analogies in the Artificial Immune
Systems world. A number of potential application areas are then used to provide
a framing for a critical assessment of the concept, and its relevance for
Artificial Immune Systems."
An Artificial Immune System as a Recommender System for Web Sites,"Artificial Immune Systems have been used successfully to build recommender
systems for film databases. In this research, an attempt is made to extend this
idea to web site recommendation. A collection of more than 1000 individuals web
profiles (alternatively called preferences / favourites / bookmarks file) will
be used. URLs will be classified using the DMOZ (Directory Mozilla) database of
the Open Directory Project as our ontology. This will then be used as the data
for the Artificial Immune Systems rather than the actual addresses. The first
attempt will involve using a simple classification code number coupled with the
number of pages within that classification code. However, this implementation
does not make use of the hierarchical tree-like structure of DMOZ.
Consideration will then be given to the construction of a similarity measure
for web profiles that makes use of this hierarchical information to build a
better-informed Artificial Immune System."
"Next Challenges in Bringing Artificial Immune Systems to Production in
  Network Security","The human immune system protects the human body against various pathogens
like e.g. biological viruses and bacteria. Artificial immune systems reuse the
architecture, organization, and workflows of the human immune system for
various problems in computer science. In the network security, the artificial
immune system is used to secure a network and its nodes against intrusions like
viruses, worms, and trojans. However, these approaches are far away from
production where they are academic proof-of-concept implementations or use only
a small part to protect against a certain intrusion. This article discusses the
required steps to bring artificial immune systems into production in the
network security domain. It furthermore figures out the challenges and provides
the description and results of the prototype of an artificial immune system,
which is SANA called."
An Immune Inspired Approach to Anomaly Detection,"The immune system provides a rich metaphor for computer security: anomaly
detection that works in nature should work for machines. However, early
artificial immune system approaches for computer security had only limited
success. Arguably, this was due to these artificial systems being based on too
simplistic a view of the immune system. We present here a second generation
artificial immune system for process anomaly detection. It improves on earlier
systems by having different artificial cell types that process information.
Following detailed information about how to build such second generation
systems, we find that communication between cells types is key to performance.
Through realistic testing and validation we show that second generation
artificial immune systems are capable of anomaly detection beyond generic
system policies. The paper concludes with a discussion and outline of the next
steps in this exciting area of computer security."
"Artificial Hormone Reaction Networks: Towards Higher Evolvability in
  Evolutionary Multi-Modular Robotics","The semi-automatic or automatic synthesis of robot controller software is
both desirable and challenging. Synthesis of rather simple behaviors such as
collision avoidance by applying artificial evolution has been shown multiple
times. However, the difficulty of this synthesis increases heavily with
increasing complexity of the task that should be performed by the robot. We try
to tackle this problem of complexity with Artificial Homeostatic Hormone
Systems (AHHS), which provide both intrinsic, homeostatic processes and
(transient) intrinsic, variant behavior. By using AHHS the need for pre-defined
controller topologies or information about the field of application is
minimized. We investigate how the principle design of the controller and the
hormone network size affects the overall performance of the artificial
evolution (i.e., evolvability). This is done by comparing two variants of AHHS
that show different effects when mutated. We evolve a controller for a robot
built from five autonomous, cooperating modules. The desired behavior is a form
of gait resulting in fast locomotion by using the modules' main hinges."
"Quantum control in artificial neurons with superconductor-ionic memory
  inserted in the feedback","To improve artificial intelligence/autonomous systems and help with treating
neurological conditions, there's a requirement for artificial neuron hardware
that mimics biological. We examine experimental artificial neurons with quantum
tunneling memory using 4.2 nm of ionic Hafnium oxide and Niobium metal inserted
in the positive and negative feedback of an oscillator. These neurons have
adaptive spiking behavior and hybrid non-chaotic/chaotic modes. When networked,
they output with strong itinerancy. The superconducting state at 8.1 Kelvin
results in Josephson tunneling with signs that the ionic states are influenced
by quantum coherent control in accordance with quantum master equation
calculations of the expectation values and correlation functions with a
calibrated time dependent Hamiltonian. We experimentally demonstrate a learning
network of 4 artificial neurons, and the modulation of signals."
"Preliminaries to artificial consciousness: a multidimensional heuristic
  approach","The pursuit of artificial consciousness requires conceptual clarity to
navigate its theoretical and empirical challenges. This paper introduces a
composite, multilevel, and multidimensional model of consciousness as a
heuristic framework to guide research in this field. Consciousness is treated
as a complex phenomenon, with distinct constituents and dimensions that can be
operationalized for study and for evaluating their replication. We argue that
this model provides a balanced approach to artificial consciousness research by
avoiding binary thinking (e.g., conscious vs. non-conscious) and offering a
structured basis for testable hypotheses. To illustrate its utility, we focus
on ""awareness"" as a case study, demonstrating how specific dimensions of
consciousness can be pragmatically analyzed and targeted for potential
artificial instantiation. By breaking down the conceptual intricacies of
consciousness and aligning them with practical research goals, this paper lays
the groundwork for a robust strategy to advance the scientific and technical
understanding of artificial consciousness."
Neuromorphic Correlates of Artificial Consciousness,"The concept of neural correlates of consciousness (NCC), which suggests that
specific neural activities are linked to conscious experiences, has gained
widespread acceptance. This acceptance is based on a wealth of evidence from
experimental studies, brain imaging techniques such as fMRI and EEG, and
theoretical frameworks like integrated information theory (IIT) within
neuroscience and the philosophy of mind. This paper explores the potential for
artificial consciousness by merging neuromorphic design and architecture with
brain simulations. It proposes the Neuromorphic Correlates of Artificial
Consciousness (NCAC) as a theoretical framework. While the debate on artificial
consciousness remains contentious due to our incomplete grasp of consciousness,
this work may raise eyebrows and invite criticism. Nevertheless, this
optimistic and forward-thinking approach is fueled by insights from the Human
Brain Project, advancements in brain imaging like EEG and fMRI, and recent
strides in AI and computing, including quantum and neuromorphic designs.
Additionally, this paper outlines how machine learning can play a role in
crafting artificial consciousness, aiming to realise machine consciousness and
awareness in the future."
"Interactive embodied evolution for socially adept Artificial General
  Creatures","We introduce here the concept of Artificial General Creatures (AGC) which
encompasses ""robotic or virtual agents with a wide enough range of capabilities
to ensure their continued survival"". With this in mind, we propose a research
line aimed at incrementally building both the technology and the
trustworthiness of AGC. The core element in this approach is that trust can
only be built over time, through demonstrably mutually beneficial interactions.
  To this end, we advocate starting from unobtrusive, nonthreatening artificial
agents that would explicitly collaborate with humans, similarly to what
domestic animals do. By combining multiple research fields, from Evolutionary
Robotics to Neuroscience, from Ethics to Human-Machine Interaction, we aim at
creating embodied, self-sustaining Artificial General Creatures that would form
social and emotional connections with humans. Although they would not be able
to play competitive online games or generate poems, we argue that creatures
akin to artificial pets would be invaluable stepping stones toward symbiotic
Artificial General Intelligence."
Agnosticism About Artificial Consciousness,"Could an AI have conscious experiences? Any answer to this question should
conform to Evidentialism - that is, it should be based not on intuition, dogma
or speculation but on solid scientific evidence. I argue that such evidence is
hard to come by and that the only justifiable stance on the prospects of
artificial consciousness is agnosticism. In the current debate, the main
division is between biological views that are sceptical of artificial
consciousness and functional views that are sympathetic to it. I argue that
both camps make the same mistake of over-estimating what the evidence tells us.
Scientific insights into consciousness have been achieved through the study of
conscious organisms. Although this has enabled cautious assessments of
consciousness in various creatures, extending this to AI faces serious
obstacles. AI thus presents consciousness researchers with a dilemma: either
reach a verdict on artificial consciousness but violate Evidentialism; or
respect Evidentialism but offer no verdict on the prospects of artificial
consciousness. The dominant trend in the literature has been to take the first
option while purporting to follow the scientific evidence. I argue that if we
truly follow the evidence, we must take the second option and adopt
agnosticism."
"Order to Disorder Transitions in Hybrid Intelligent Systems: a Hatch to
  the Interactions of Nations -Governments","In this study, under general frame of MAny Connected Intelligent Particles
Systems (MACIPS), we reproduce two new simple subsets of such intelligent
complex network, namely hybrid intelligent systems, involved a few prominent
intelligent computing and approximate reasoning methods: self organizing
feature map (SOM), Neuro-Fuzzy Inference System and Rough Set Theory (RST).
Over this, we show how our algorithms can be construed as a linkage of
government-society interaction, where government catches various fashions of
behavior: solid (absolute) or flexible. So, transition of such society, by
changing of connectivity parameters (noise) from order to disorder is inferred.
Add to this, one may find an indirect mapping among financial systems and
eventual market fluctuations with MACIPS."
"Hybrid technique for effective knowledge representation & a comparative
  study","Knowledge representation (KR) and inference mechanism are most desirable
thing to make the system intelligent. System is known to an intelligent if its
intelligence is equivalent to the intelligence of human being for a particular
domain or general. Because of incomplete ambiguous and uncertain information
the task of making intelligent system is very difficult. The objective of this
paper is to present the hybrid KR technique for making the system effective &
Optimistic. The requirement for (effective & optimistic) is because the system
must be able to reply the answer with a confidence of some factor. This paper
also presents the comparison between various hybrid KR techniques with the
proposed one."
"A short note on estimating intelligence from user profiles in the
  context of universal psychometrics: prospects and caveats","There has been an increasing interest in inferring some personality traits
from users and players in social networks and games, respectively. This goes
beyond classical sentiment analysis, and also much further than customer
profiling. The purpose here is to have a characterisation of users in terms of
personality traits, such as openness, conscientiousness, extraversion,
agreeableness, and neuroticism. While this is an incipient area of research, we
ask the question of whether cognitive abilities, and intelligence in
particular, are also measurable from user profiles. However, we pose the
question as broadly as possible in terms of subjects, in the context of
universal psychometrics, including humans, machines and hybrids. Namely, in
this paper we analyse the following question: is it possible to measure the
intelligence of humans and (non-human) bots in a social network or a game just
from their user profiles, i.e., by observation, without the use of interactive
tests, such as IQ tests, the Turing test or other more principled machine
intelligence tests?"
Advice from the Oracle: Really Intelligent Information Retrieval,"What is ""intelligent"" information retrieval? Essentially this is asking what
is intelligence, in this article I will attempt to show some of the aspects of
human intelligence, as related to information retrieval. I will do this by the
device of a semi-imaginary Oracle. Every Observatory has an oracle, someone who
is a distinguished scientist, has great administrative responsibilities, acts
as mentor to a number of less senior people, and as trusted advisor to even the
most accomplished scientists, and knows essentially everyone in the field. In
an appendix I will present a brief summary of the Statistical Factor Space
method for text indexing and retrieval, and indicate how it will be used in the
Astrophysics Data System Abstract Service. 2018 Keywords: Personal Digital
Assistant; Supervised Topic Models"
"Intelligent Identification of Two-Dimensional Structure by
  Machine-Learning Optical Microscopy","Two-dimensional (2D) materials and their heterostructures, with wafer-scale
synthesis methods and fascinating properties, have attracted numerous interest
and triggered revolutions of corresponding device applications. However, facile
methods to realize accurate, intelligent and large-area characterizations of
these 2D structures are still highly desired. Here, we report a successful
application of machine-learning strategy in the optical identification of 2D
structure. The machine-learning optical identification method (MOI method)
endows optical microscopy with intelligent insight into the characteristic
colour information in the optical photograph. Experimental results indicate
that the MOI method enables accurate, intelligent and large-area
characterizations of graphene, molybdenum disulphide (MoS2) and their
heterostructures, including identifications of the thickness, the existence of
impurities, and even the stacking order. Thanks to the convergence of
artificial intelligence and nanoscience, this intelligent identification method
can certainly promote the fundamental research and wafer-scale device
application of 2D structures."
A review of neuro-fuzzy systems based on intelligent control,"The system's ability to adapt and self-organize are two key factors when it
comes to how well the system can survive the changes to the environment and the
plant they work within. Intelligent control improves these two factors in
controllers. Considering the increasing complexity of dynamic systems along
with their need for feedback controls, using more complicated controls has
become necessary and intelligent control can be a suitable response to this
necessity. This paper briefly describes the structure of intelligent control
and provides a review on fuzzy logic and neural networks which are some of the
base methods for intelligent control. The different aspects of these two
methods are then compared together and an example of a combined method is
presented."
"A First Look at Mobile Intelligence: Architecture, Experimentation and
  Challenges","Artificial intelligence (AI) technology makes mobile devices become
intelligent objects which can learn and act automatically. Although AI will
bring great opportunities for mobile applications, little work has focused on
the architecture and the interaction with the cloud. In this article, we
present three existing architectures of mobile intelligence in detail and
introduce its broad application prospects. Furthermore, we conduct a series of
experiments to evaluate the performance of the prevalent commercial
applications and intelligent frameworks. Our results show that there is a big
gap between Quality of Experience (QoE) requirements and the status quo. So
far, we have seen only the tip of the iceberg. We pose issues and challenges to
advance the area of mobile intelligence and hope to pave the way for the
forthcoming."
"Mining Threat Intelligence about Open-Source Projects and Libraries from
  Code Repository Issues and Bug Reports","Open-Source Projects and Libraries are being used in software development
while also bearing multiple security vulnerabilities. This use of third party
ecosystem creates a new kind of attack surface for a product in development. An
intelligent attacker can attack a product by exploiting one of the
vulnerabilities present in linked projects and libraries.
  In this paper, we mine threat intelligence about open source projects and
libraries from bugs and issues reported on public code repositories. We also
track library and project dependencies for installed software on a client
machine. We represent and store this threat intelligence, along with the
software dependencies in a security knowledge graph. Security analysts and
developers can then query and receive alerts from the knowledge graph if any
threat intelligence is found about linked libraries and projects, utilized in
their products."
Label-less Learning for Traffic Control in an Edge Network,"With the development of intelligent applications (e.g., self-driving,
real-time emotion recognition, etc), there are higher requirements for the
cloud intelligence. However, cloud intelligence depends on the multi-modal data
collected by user equipments (UEs). Due to the limited capacity of network
bandwidth, offloading all data generated from the UEs to the remote cloud is
impractical. Thus, in this article, we consider the challenging issue of
achieving a certain level of cloud intelligence while reducing network traffic.
In order to solve this problem, we design a traffic control algorithm based on
label-less learning on the edge cloud, which is dubbed as LLTC. By the use of
the limited computing and storage resources at edge cloud, LLTC evaluates the
value of data, which will be offloaded. Specifically, we first give a statement
of the problem and the system architecture. Then, we design the LLTC algorithm
in detail. Finally, we set up the system testbed. Experimental results show
that the proposed LLTC can guarantee the required cloud intelligence while
minimizing the amount of data transmission."
"Adaptive Intelligent Secondary Control of Microgrids Using a
  Biologically-Inspired Reinforcement Learning","In this paper, a biologically-inspired adaptive intelligent secondary
controller is developed for microgrids to tackle system dynamics uncertainties,
faults, and/or disturbances. The developed adaptive biologically-inspired
controller adopts a novel computational model of emotional learning in
mammalian limbic system. The learning capability of the proposed
biologically-inspired intelligent controller makes it a promising approach to
deal with the power system non-linear and volatile dynamics without increasing
the controller complexity, and maintain the voltage and frequency stabilities
by using an efficient reference tracking mechanism. The performance of the
proposed intelligent secondary controller is validated in terms of the voltage
and frequency absolute errors in the simulated microgrid. Simulation results
highlight the efficiency and robustness of the proposed intelligent controller
under the fault conditions and different system uncertainties compared to other
benchmark controllers."
Multipurpose Intelligent Process Automation via Conversational Assistant,"Intelligent Process Automation (IPA) is an emerging technology with a primary
goal to assist the knowledge worker by taking care of repetitive, routine and
low-cognitive tasks. Conversational agents that can interact with users in a
natural language are potential application for IPA systems. Such intelligent
agents can assist the user by answering specific questions and executing
routine tasks that are ordinarily performed in a natural language (i.e.,
customer support). In this work, we tackle a challenge of implementing an IPA
conversational assistant in a real-world industrial setting with a lack of
structured training data. Our proposed system brings two significant benefits:
First, it reduces repetitive and time-consuming activities and, therefore,
allows workers to focus on more intelligent processes. Second, by interacting
with users, it augments the resources with structured and to some extent
labeled training data. We showcase the usage of the latter by re-implementing
several components of our system with Transfer Learning (TL) methods."
BitTensor: A Peer-to-Peer Intelligence Market,"As with other commodities, markets could help us efficiently produce machine
intelligence. We propose a market where intelligence is priced by other
intelligence systems peer-to-peer across the internet. Peers rank each other by
training neural networks which learn the value of their neighbors. Scores
accumulate on a digital ledger where high ranking peers are monetarily rewarded
with additional weight in the network. However, this form of peer-ranking is
not resistant to collusion, which could disrupt the accuracy of the mechanism.
The solution is a connectivity-based regularization which exponentially rewards
trusted peers, making the system resistant to collusion of up to 50 percent of
the network weight. The result is a collectively run intelligence market which
continual produces newly trained models and pays contributors who create
information theoretic value."
MALOnt: An Ontology for Malware Threat Intelligence,"Malware threat intelligence uncovers deep information about malware, threat
actors, and their tactics, Indicators of Compromise(IoC), and vulnerabilities
in different platforms from scattered threat sources. This collective
information can guide decision making in cyber defense applications utilized by
security operation centers(SoCs). In this paper, we introduce an open-source
malware ontology - MALOnt that allows the structured extraction of information
and knowledge graph generation, especially for threat intelligence. The
knowledge graph that uses MALOnt is instantiated from a corpus comprising
hundreds of annotated malware threat reports. The knowledge graph enables the
analysis, detection, classification, and attribution of cyber threats caused by
malware. We also demonstrate the annotation process using MALOnt on exemplar
threat intelligence reports. A work in progress, this research is part of a
larger effort towards auto-generation of knowledge graphs (KGs)for gathering
malware threat intelligence from heterogeneous online resources."
Blockchained Federated Learning for Threat Defense,"Given the increasing complexity of threats in smart cities, the changing
environment, and the weakness of traditional security systems, which in most
cases fail to detect serious threats such as zero-day attacks, the need for
alternative more active and more effective security methods keeps increasing.
Such approaches are the adoption of intelligent solutions to prevent, detect
and deal with threats or anomalies under the conditions and the operating
parameters of the infrastructure in question. This research paper introduces
the development of an intelligent Threat Defense system, employing Blockchain
Federated Learning, which seeks to fully upgrade the way passive intelligent
systems operate, aiming at implementing an Advanced Adaptive Cooperative
Learning (AACL) mechanism for smart cities networks. The AACL is based on the
most advanced methods of computational intelligence while ensuring privacy and
anonymity for participants and stakeholders. The proposed framework combines
Federated Learning for the distributed and continuously validated learning of
the tracing algorithms. Learning is achieved through encrypted smart contracts
within the blockchain technology, for unambiguous validation and control of the
process. The aim of the proposed Framework is to intelligently classify smart
cities networks traffic derived from Industrial IoT (IIoT) by Deep Content
Inspection (DCI) methods, in order to identify anomalies that are usually due
to Advanced Persistent Threat (APT) attacks."
"Edge Semantic Cognitive Intelligence for 6G Networks: Novel Theoretical
  Models, Enabling Framework, and Typical Applications","Edge intelligence is anticipated to underlay the pathway to connected
intelligence for 6G networks, but the organic confluence of edge computing and
artificial intelligence still needs to be carefully treated. To this end, this
article discusses the concepts of edge intelligence from the semantic cognitive
perspective. Two instructive theoretical models for edge semantic cognitive
intelligence (ESCI) are first established. Afterwards, the ESCI framework
orchestrating deep learning with semantic communication is discussed. Two
representative applications are present to shed light on the prospect of ESCI
in 6G networks. Some open problems are finally listed to elicit the future
research directions of ESCI."
"Toward Multi-Service Edge-Intelligence Paradigm: Temporal-Adaptive
  Prediction for Time-Critical Control over Wireless","Time-critical control applications typically pose stringent connectivity
requirements for communication networks. The imperfections associated with the
wireless medium such as packet losses, synchronization errors, and varying
delays have a detrimental effect on performance of real-time control, often
with safety implications. This paper introduces multi-service edge-intelligence
as a new paradigm for realizing time-critical control over wireless. It
presents the concept of multi-service edge-intelligence which revolves around
tight integration of wireless access, edge-computing and machine learning
techniques, in order to provide stability guarantees under wireless
imperfections. The paper articulates some of the key system design aspects of
multi-service edge-intelligence. It also presents a temporal-adaptive
prediction technique to cope with dynamically changing wireless environments.
It provides performance results in a robotic teleoperation scenario. Finally,
it discusses some open research and design challenges for multi-service
edge-intelligence."
EINCASM: Emergent Intelligence in Neural Cellular Automaton Slime Molds,"This paper presents EINCASM, a prototype system employing a novel framework
for studying emergent intelligence in organisms resembling slime molds. EINCASM
evolves neural cellular automata with NEAT to maximize cell growth constrained
by nutrient and energy costs. These organisms capitalize physically simulated
fluid to transport nutrients and chemical-like signals to orchestrate growth
and adaptation to complex, changing environments. Our framework builds the
foundation for studying how the presence of puzzles, physics, communication,
competition and dynamic open-ended environments contribute to the emergence of
intelligent behavior. We propose preliminary tests for intelligence in such
organisms and suggest future work for more powerful systems employing EINCASM
to better understand intelligence in distributed dynamical systems."
6G Network Business Support System,"6G is the next-generation intelligent and integrated digital information
infrastructure, characterized by ubiquitous interconnection, native
intelligence, multi-dimensional perception, global coverage, green and
low-carbon, native network security, etc. 6G will realize the transition from
serving people and people-things communication to supporting the efficient
connection of intelligent agents, and comprehensively leading the digital,
intelligent and green transformation of the economy and the society. As the
core support system for mobile communication network, 6 6G BSS need to
integrate with new business models brought about by the development of the
next-generation Internet and IT, upgrade from ""network-centric"" to ""business
and service centric"" and ""customer-centric"". 6G OSS and BSS systems need to
strengthen their integration to improve the operational efficiency and benefits
of customers by connecting the digital intelligence support capabilities on
both sides of supply and demand. This paper provides a detailed introduction to
the overall vision, potential key technologies, and functional architecture of
6G BSS systems. It also presents an evolutionary roadmap and technological
prospects for the BSS systems from 5G to 6G."
"Transforming the Output of Generative Pre-trained Transformer: The
  Influence of the PGI Framework on Attention Dynamics","This paper presents a novel approach named Persona-Grouping-Intelligence
(PGI), which has been crafted to tackle the challenges posed by GPT models when
applied to real-world business issues. PGI leverages the inherent capabilities
of the GPT model to comprehend intricate language structures and generate
responses that are contextually relevant. The experiment occurred in a business
scenario where human intelligence was being underutilized due to less optimized
business processes. The primary objective of this approach is to leverage GPT
models to reduce the workload on humans in tasks that are extensive,
monotonous, and repetitive. Instead, the focus is redirected toward
decision-making activities. Remarkably, the experiment yielded an accuracy rate
of 93.81% in validating 4,000 responses generated by the model, underscoring
the effectiveness of the PGI strategies. Effectively addressing the issue of
underutilized human intelligence, this paradigm shift aligns business
environments with dynamic machine intelligence, enabling them to navigate the
intricacies of real-world challenges. This approach facilitates the practical
utilization of these models to tackle actual problems. The methodology offers
an opportunity to reshape the fundamental structure of business processes by
seamlessly integrating human decision-making with adaptable machine
intelligence. Consequently, this optimization enhances operational efficiency
and elevates strategic decision-making across diverse business contexts."
"Social Intelligence Data Infrastructure: Structuring the Present and
  Navigating the Future","As Natural Language Processing (NLP) systems become increasingly integrated
into human social life, these technologies will need to increasingly rely on
social intelligence. Although there are many valuable datasets that benchmark
isolated dimensions of social intelligence, there does not yet exist any body
of work to join these threads into a cohesive subfield in which researchers can
quickly identify research gaps and future directions. Towards this goal, we
build a Social AI Data Infrastructure, which consists of a comprehensive social
AI taxonomy and a data library of 480 NLP datasets. Our infrastructure allows
us to analyze existing dataset efforts, and also evaluate language models'
performance in different social intelligence aspects. Our analyses demonstrate
its utility in enabling a thorough understanding of current data landscape and
providing a holistic perspective on potential directions for future dataset
development. We show there is a need for multifaceted datasets, increased
diversity in language and culture, more long-tailed social situations, and more
interactive data in future social intelligence data efforts."
"Hierarchical Multi-Armed Bandits for the Concurrent Intelligent Tutoring
  of Concepts and Problems of Varying Difficulty Levels","Remote education has proliferated in the twenty-first century, yielding rise
to intelligent tutoring systems. In particular, research has found multi-armed
bandit (MAB) intelligent tutors to have notable abilities in traversing the
exploration-exploitation trade-off landscape for student problem
recommendations. Prior literature, however, contains a significant lack of
open-sourced MAB intelligent tutors, which impedes potential applications of
these educational MAB recommendation systems. In this paper, we combine recent
literature on MAB intelligent tutoring techniques into an open-sourced and
simply deployable hierarchical MAB algorithm, capable of progressing students
concurrently through concepts and problems, determining ideal recommended
problem difficulties, and assessing latent memory decay. We evaluate our
algorithm using simulated groups of 500 students, utilizing Bayesian Knowledge
Tracing to estimate students' content mastery. Results suggest that our
algorithm, when turned difficulty-agnostic, significantly boosts student
success, and that the further addition of problem-difficulty adaptation notably
improves this metric."
"MACeIP: A Multimodal Ambient Context-enriched Intelligence Platform in
  Smart Cities","This paper presents a Multimodal Ambient Context-enriched Intelligence
Platform (MACeIP) for Smart Cities, a comprehensive system designed to enhance
urban management and citizen engagement. Our platform integrates advanced
technologies, including Internet of Things (IoT) sensors, edge and cloud
computing, and Multimodal AI, to create a responsive and intelligent urban
ecosystem. Key components include Interactive Hubs for citizen interaction, an
extensive IoT sensor network, intelligent public asset management, a pedestrian
monitoring system, a City Planning Portal, and a Cloud Computing System. We
demonstrate the prototype of MACeIP in several cities, focusing on Fredericton,
New Brunswick. This work contributes to innovative city development by offering
a scalable, efficient, and user-centric approach to urban intelligence and
management."
"Understanding LLMs' Fluid Intelligence Deficiency: An Analysis of the
  ARC Task","While LLMs have exhibited strong performance on various NLP tasks, it is
noteworthy that most of these tasks rely on utilizing the vast amount of
knowledge encoded in LLMs' parameters, rather than solving new problems without
prior knowledge. In cognitive research, the latter ability is referred to as
fluid intelligence, which is considered to be critical for assessing human
intelligence. Recent research on fluid intelligence assessments has highlighted
significant deficiencies in LLMs' abilities. In this paper, we analyze the
challenges LLMs face in demonstrating fluid intelligence through controlled
experiments, using the most representative ARC task as an example. Our study
revealed three major limitations in existing LLMs: limited ability for skill
composition, unfamiliarity with abstract input formats, and the intrinsic
deficiency of left-to-right decoding. Our data and code can be found in
https://wujunjie1998.github.io/araoc-benchmark.github.io/."
"A Comprehensive LLM-powered Framework for Driving Intelligence
  Evaluation","Evaluation methods for autonomous driving are crucial for algorithm
optimization. However, due to the complexity of driving intelligence, there is
currently no comprehensive evaluation method for the level of autonomous
driving intelligence. In this paper, we propose an evaluation framework for
driving behavior intelligence in complex traffic environments, aiming to fill
this gap. We constructed a natural language evaluation dataset of human
professional drivers and passengers through naturalistic driving experiments
and post-driving behavior evaluation interviews. Based on this dataset, we
developed an LLM-powered driving evaluation framework. The effectiveness of
this framework was validated through simulated experiments in the CARLA urban
traffic simulator and further corroborated by human assessment. Our research
provides valuable insights for evaluating and designing more intelligent,
human-like autonomous driving agents. The implementation details of the
framework and detailed information about the dataset can be found at Github."
Is there a future for AI without representation?,"This paper investigates the prospects of AI without representation in
general, and the proposals of Rodney Brooks in particular. What turns out to be
characteristic of Brooks' proposal is the rejection of central control in
intelligent agents; his systems has as much or as little representation as
traditional AI. The traditional view that representation is necessary for
intelligence presupposes that intelligence requires central control. However,
much of recent cognitive science suggests that we should dispose of the image
of intelligent agents as central representation processors. If this paradigm
shift is achieved, Brooks' proposal for non-centralized cognition without
representation appears promising for full-blown intelligent agents - though not
for conscious agents and thus not for human-like AI."
Designing Ecosystems of Intelligence from First Principles,"This white paper lays out a vision of research and development in the field
of artificial intelligence for the next decade (and beyond). Its denouement is
a cyber-physical ecosystem of natural and synthetic sense-making, in which
humans are integral participants -- what we call ''shared intelligence''. This
vision is premised on active inference, a formulation of adaptive behavior that
can be read as a physics of intelligence, and which inherits from the physics
of self-organization. In this context, we understand intelligence as the
capacity to accumulate evidence for a generative model of one's sensed world --
also known as self-evidencing. Formally, this corresponds to maximizing
(Bayesian) model evidence, via belief updating over several scales: i.e.,
inference, learning, and model selection. Operationally, this self-evidencing
can be realized via (variational) message passing or belief propagation on a
factor graph. Crucially, active inference foregrounds an existential imperative
of intelligent systems; namely, curiosity or the resolution of uncertainty.
This same imperative underwrites belief sharing in ensembles of agents, in
which certain aspects (i.e., factors) of each agent's generative world model
provide a common ground or frame of reference. Active inference plays a
foundational role in this ecology of belief sharing -- leading to a formal
account of collective intelligence that rests on shared narratives and goals.
We also consider the kinds of communication protocols that must be developed to
enable such an ecosystem of intelligences and motivate the development of a
shared hyper-spatial modeling language and transaction protocol, as a first --
and key -- step towards such an ecology."
Parrondo Strategies for Artificial Traders,"On markets with receding prices, artificial noise traders may consider
alternatives to buy-and-hold. By simulating variations of the Parrondo
strategy, using real data from the Swedish stock market, we produce first
indications of a buy-low-sell-random Parrondo variation outperforming
buy-and-hold. Subject to our assumptions, buy-low-sell-random also outperforms
the traditional value and trend investor strategies. We measure the success of
the Parrondo variations not only through their performance compared to other
kinds of strategies, but also relative to varying levels of perfect
information, received through messages within a multi-agent system of
artificial traders."
"Learning and discrimination through STDP in a top-down modulated
  associative memory","This article underlines the learning and discrimination capabilities of a
model of associative memory based on artificial networks of spiking neurons.
Inspired from neuropsychology and neurobiology, the model implements top-down
modulations, as in neocortical layer V pyramidal neurons, with a learning rule
based on synaptic plasticity (STDP), for performing a multimodal association
learning task. A temporal correlation method of analysis proves the ability of
the model to associate specific activity patterns to different samples of
stimulation. Even in the absence of initial learning and with continuously
varying weights, the activity patterns become stable enough for discrimination."
Movie Recommendation Systems Using An Artificial Immune System,"We apply the Artificial Immune System (AIS) technology to the Collaborative
Filtering (CF) technology when we build the movie recommendation system. Two
different affinity measure algorithms of AIS, Kendall tau and Weighted Kappa,
are used to calculate the correlation coefficients for this movie
recommendation system. From the testing we think that Weighted Kappa is more
suitable than Kendall tau for movie problems."
Articulation and Clarification of the Dendritic Cell Algorithm,"The Dendritic Cell algorithm (DCA) is inspired by recent work in innate
immunity. In this paper a formal description of the DCA is given. The DCA is
described in detail, and its use as an anomaly detector is illustrated within
the context of computer security. A port scan detection task is performed to
substantiate the influence of signal selection on the behaviour of the
algorithm. Experimental results provide a comparison of differing input signal
mappings."
Dendritic Cells for Real-Time Anomaly Detection,"Dendritic Cells (DCs) are innate immune system cells which have the power to
activate or suppress the immune system. The behaviour of human of human DCs is
abstracted to form an algorithm suitable for anomaly detection. We test this
algorithm on the real-time problem of port scan detection. Our results show a
significant difference in artificial DC behaviour for an outgoing portscan when
compared to behaviour for normal processes."
"Application of PSO, Artificial Bee Colony and Bacterial Foraging
  Optimization algorithms to economic load dispatch: An analysis","This paper illustrates successful implementation of three evolutionary
algorithms, namely- Particle Swarm Optimization(PSO), Artificial Bee Colony
(ABC) and Bacterial Foraging Optimization (BFO) algorithms to economic load
dispatch problem (ELD). Power output of each generating unit and optimum fuel
cost obtained using all three algorithms have been compared. The results
obtained show that ABC and BFO algorithms converge to optimal fuel cost with
reduced computational time when compared to PSO for the two example problems
considered."
"Motion Planning Of an Autonomous Mobile Robot Using Artificial Neural
  Network","The paper presents the electronic design and motion planning of a robot based
on decision making regarding its straight motion and precise turn using
Artificial Neural Network (ANN). The ANN helps in learning of robot so that it
performs motion autonomously. The weights calculated are implemented in
microcontroller. The performance has been tested to be excellent."
"Obesity Heuristic, New Way On Artificial Immune Systems","There is a need for new metaphors from immunology to flourish the application
areas of Artificial Immune Systems. A metaheuristic called Obesity Heuristic
derived from advances in obesity treatment is proposed. The main forces of the
algorithm are the generation omega-6 and omega-3 fatty acids. The algorithm
works with Just-In-Time philosophy; by starting only when desired. A case study
of data cleaning is provided. With experiments conducted on standard tables,
results show that Obesity Heuristic outperforms other algorithms, with 100%
recall. This is a great improvement over other algorithms"
Artificial Neuron Modelling Based on Wave Shape,"This paper describes a new model for an artificial neural network processing
unit or neuron. It is slightly different to a traditional feedforward network
by the fact that it favours a mechanism of trying to match the wave-like
'shape' of the input with the shape of the output against specific value error
corrections. The expectation is then that a best fit shape can be transposed
into the desired output values more easily. This allows for notions of
reinforcement through resonance and also the construction of synapses."
"Chases and Escapes, and Optimization Problems","We propose a new approach for solving combinatorial optimization problem by
utilizing the mechanism of chases and escapes, which has a long history in
mathematics. In addition to the well-used steepest descent and neighboring
search, we perform a chase and escape game on the ""landscape"" of the cost
function. We have created a concrete algorithm for the Traveling Salesman
Problem. Our preliminary test indicates a possibility that this new fusion of
chases and escapes problem into combinatorial optimization search is fruitful."
Toward Idealized Decision Theory,"This paper motivates the study of decision theory as necessary for aligning
smarter-than-human artificial systems with human interests. We discuss the
shortcomings of two standard formulations of decision theory, and demonstrate
that they cannot be used to describe an idealized decision procedure suitable
for approximation by artificial systems. We then explore the notions of policy
selection and logical counterfactuals, two recent insights into decision theory
that point the way toward promising paths for future research."
Training artificial neural networks to learn a nondeterministic game,"It is well known that artificial neural networks (ANNs) can learn
deterministic automata. Learning nondeterministic automata is another matter.
This is important because much of the world is nondeterministic, taking the
form of unpredictable or probabilistic events that must be acted upon. If ANNs
are to engage such phenomena, then they must be able to learn how to deal with
nondeterminism. In this project the game of Pong poses a nondeterministic
environment. The learner is given an incomplete view of the game state and
underlying deterministic physics, resulting in a nondeterministic game. Three
models were trained and tested on the game: Mona, Elman, and Numenta's NuPIC."
Stoic Ethics for Artificial Agents,"We present a position paper advocating the notion that Stoic philosophy and
ethics can inform the development of ethical A.I. systems. This is in sharp
contrast to most work on building ethical A.I., which has focused on
Utilitarian or Deontological ethical theories. We relate ethical A.I. to
several core Stoic notions, including the dichotomy of control, the four
cardinal virtues, the ideal Sage, Stoic practices, and Stoic perspectives on
emotion or affect. More generally, we put forward an ethical view of A.I. that
focuses more on internal states of the artificial agent rather than on external
actions of the agent. We provide examples relating to near-term A.I. systems as
well as hypothetical superintelligent agents."
Self-Organization and Artificial Life: A Review,"Self-organization has been an important concept within a number of
disciplines, which Artificial Life (ALife) also has heavily utilized since its
inception. The term and its implications, however, are often confusing or
misinterpreted. In this work, we provide a mini-review of self-organization and
its relationship with ALife, aiming at initiating discussions on this important
topic with the interested audience. We first articulate some fundamental
aspects of self-organization, outline its usage, and review its applications to
ALife within its soft, hard, and wet domains. We also provide perspectives for
further research."
Quasi-Dilemmas for Artificial Moral Agents,"In this paper we describe moral quasi-dilemmas (MQDs): situations similar to
moral dilemmas, but in which an agent is unsure whether exploring the plan
space or the world may reveal a course of action that satisfies all moral
requirements. We argue that artificial moral agents (AMAs) should be built to
handle MQDs (in particular, by exploring the plan space rather than immediately
accepting the inevitability of the moral dilemma), and that MQDs may be useful
for evaluating AMA architectures."
Artificial Consciousness and Security,"This paper describes a possible way to improve computer security by
implementing a program which implements the following three features related to
a weak notion of artificial consciousness: (partial) self-monitoring, ability
to compute the truth of quantifier-free propositions and the ability to
communicate with the user. The integrity of the program could be enhanced by
using a trusted computing approach, that is to say a hardware module that is at
the root of a chain of trust. This paper outlines a possible approach but does
not refer to an implementation (which would need further work), but the author
believes that an implementation using current processors, a debugger, a
monitoring program and a trusted processing module is currently possible."
"On the Hardness of Problems Involving Negator Relationships in an
  Artificial Hormone System","The Artificial Hormone System (AHS) is a self-organizing middleware to
allocate tasks in a distributed system. We extended it by so-called negator
hormones to enable conditional task structures. However, this extension
increases the computational complexity of seemingly simple decision problems in
the system: In [1] and [2], we defined the problems Negator-Path and
Negator-Sat and proved their NP-completeness. In this supplementary report to
these papers, we show examples of Negator-Path and Negator-Sat, introduce the
novel problem Negator-Stability and explain why all of these problems involving
negators are hard to solve algorithmically."
Neurocognitive Informatics Manifesto,"Informatics studies all aspects of the structure of natural and artificial
information systems. Theoretical and abstract approaches to information have
made great advances, but human information processing is still unmatched in
many areas, including information management, representation and understanding.
Neurocognitive informatics is a new, emerging field that should help to improve
the matching of artificial and natural systems, and inspire better
computational algorithms to solve problems that are still beyond the reach of
machines. In this position paper examples of neurocognitive inspirations and
promising directions in this area are given."
"Activation Functions in Artificial Neural Networks: A Systematic
  Overview","Activation functions shape the outputs of artificial neurons and, therefore,
are integral parts of neural networks in general and deep learning in
particular. Some activation functions, such as logistic and relu, have been
used for many decades. But with deep learning becoming a mainstream research
topic, new activation functions have mushroomed, leading to confusion in both
theory and practice. This paper provides an analytic yet up-to-date overview of
popular activation functions and their properties, which makes it a timely
resource for anyone who studies or applies neural networks."
"On the use of generative deep neural networks to synthesize artificial
  multichannel EEG signals","Recent promises of generative deep learning lately brought interest to its
potential uses in neural engineering. In this paper we firstly review recently
emerging studies on generating artificial electroencephalography (EEG) signals
with deep neural networks. Subsequently, we present our feasibility experiments
on generating condition-specific multichannel EEG signals using conditional
variational autoencoders. By manipulating real resting-state EEG epochs, we
present an approach to synthetically generate time-series multichannel signals
that show spectro-temporal EEG patterns which are expected to be observed
during distinct motor imagery conditions."
Polarized skylight orientation determination artificial neural network,"This paper proposes an artificial neural network to determine orientation
using polarized skylight. This neural network has specific dilated convolution,
which can extract light intensity information of different polarization
directions. Then, the degree of polarization (DOP) and angle of polarization
(AOP) are directly extracted in the network. In addition, the exponential
function encoding of orientation is designed as the network output, which can
better reflect the insect's encoding of polarization information, and improve
the accuracy of orientation determination. Finally, training and testing were
conducted on a public polarized skylight navigation dataset, and the
experimental results proved the stability and effectiveness of the network."
"NEWRON: A New Generalization of the Artificial Neuron to Enhance the
  Interpretability of Neural Networks","In this work, we formulate NEWRON: a generalization of the McCulloch-Pitts
neuron structure. This new framework aims to explore additional desirable
properties of artificial neurons. We show that some specializations of NEWRON
allow the network to be interpretable with no change in their expressiveness.
By just inspecting the models produced by our NEWRON-based networks, we can
understand the rules governing the task. Extensive experiments show that the
quality of the generated models is better than traditional interpretable models
and in line or better than standard neural networks."
Imagine Networks,"In this paper, we introduce an imagine network that can simulate itself
through artificial association networks. Association, deduction, and memory
networks are learned, and a network is created by combining the discriminator
and reinforcement learning models. This model can learn various datasets or
data samples generated in environments and generate new data samples."
"Simulating reaction time for Eureka effect in visual object recognition
  using artificial neural network","The human brain can recognize objects hidden in even severely degraded images
after observing them for a while, which is known as a type of Eureka effect,
possibly associated with human creativity. A previous psychological study
suggests that the basis of this ""Eureka recognition"" is neural processes of
coincidence of multiple stochastic activities. Here we constructed an
artificial-neural-network-based model that simulated the characteristics of the
human Eureka recognition."
The Role Of Biology In Deep Learning,"Artificial neural networks took a lot of inspiration from their biological
counterparts in becoming our best machine perceptual systems. This work
summarizes some of that history and incorporates modern theoretical
neuroscience into experiments with artificial neural networks from the field of
deep learning. Specifically, iterative magnitude pruning is used to train
sparsely connected networks with 33x fewer weights without loss in performance.
These are used to test and ultimately reject the hypothesis that weight
sparsity alone improves image noise robustness. Recent work mitigated
catastrophic forgetting using weight sparsity, activation sparsity, and active
dendrite modeling. This paper replicates those findings, and extends the method
to train convolutional neural networks on a more challenging continual learning
task. The code has been made publicly available."
Artificial intelligence for artificial materials: moir atom,"Moir\'e engineering in atomically thin van der Waals heterostructures creates
artificial quantum materials with designer properties. We solve the many-body
problem of interacting electrons confined to a moir\'e superlattice potential
minimum (the moir\'e atom) using a 2D fermionic neural network. We show that
strong Coulomb interactions in combination with the anisotropic moir\'e
potential lead to striking ``Wigner molecule"" charge density distributions
observable with scanning tunneling microscopy."
"Using Artificial French Data to Understand the Emergence of Gender Bias
  in Transformer Language Models","Numerous studies have demonstrated the ability of neural language models to
learn various linguistic properties without direct supervision. This work takes
an initial step towards exploring the less researched topic of how neural
models discover linguistic properties of words, such as gender, as well as the
rules governing their usage. We propose to use an artificial corpus generated
by a PCFG based on French to precisely control the gender distribution in the
training data and determine under which conditions a model correctly captures
gender information or, on the contrary, appears gender-biased."
Homogeneous Artificial Neural Network,"The paper proposes an artificial neural network (ANN) being a global
approximator for a special class of functions, which are known as generalized
homogeneous. The homogeneity means a symmetry of a function with respect to a
group of transformations having topological characterization of a dilation. In
this paper, a class of the so-called linear dilations is considered. A
homogeneous universal approximation theorem is proven. Procedures for an
upgrade of an existing ANN to a homogeneous one are developed. Theoretical
results are supported by examples from the various domains (computer science,
systems theory and automatic control)."
"Speculative Exploration on the Concept of Artificial Agents Conducting
  Autonomous Research","This paper engages in a speculative exploration of the concept of an
artificial agent capable of conducting research. Initially, it examines how the
act of research can be conceptually characterized, aiming to provide a starting
point for discussions about what it means to create such agents. The focus then
shifts to the core components of research: question formulation, hypothesis
generation, and hypothesis verification. This discussion includes a
consideration of the potential and challenges associated with enabling machines
to autonomously perform these tasks. Subsequently, this paper briefly considers
the overlapping themes and interconnections that underlie them. Finally, the
paper presents preliminary thoughts on prototyping as an initial step towards
uncovering the challenges involved in developing these research-capable agents."
Artificial Neural Nets and the Representation of Human Concepts,"What do artificial neural networks (ANNs) learn? The machine learning (ML)
community shares the narrative that ANNs must develop abstract human concepts
to perform complex tasks. Some go even further and believe that these concepts
are stored in individual units of the network. Based on current research, I
systematically investigate the assumptions underlying this narrative. I
conclude that ANNs are indeed capable of performing complex prediction tasks,
and that they may learn human and non-human concepts to do so. However,
evidence indicates that ANNs do not represent these concepts in individual
units."
Estimating Continuous Distributions in Bayesian Classifiers,"When modeling a probability distribution with a Bayesian network, we are
faced with the problem of how to handle continuous variables. Most previous
work has either solved the problem by discretizing, or assumed that the data
are generated by a single Gaussian. In this paper we abandon the normality
assumption and instead use statistical methods for nonparametric density
estimation. For a naive Bayesian classifier, we present experimental results on
a variety of natural and artificial domains, comparing two methods of density
estimation: assuming normality and modeling each conditional distribution with
a single Gaussian; and using nonparametric kernel density estimation. We
observe large reductions in error on several natural and artificial data sets,
which suggests that kernel estimation is a useful tool for learning Bayesian
models."
Empirical Study of Artificial Fish Swarm Algorithm,"Artificial fish swarm algorithm (AFSA) is one of the swarm intelligence
optimization algorithms that works based on population and stochastic search.
In order to achieve acceptable result, there are many parameters needs to be
adjusted in AFSA. Among these parameters, visual and step are very significant
in view of the fact that artificial fish basically move based on these
parameters. In standard AFSA, these two parameters remain constant until the
algorithm termination. Large values of these parameters increase the capability
of algorithm in global search, while small values improve the local search
ability of the algorithm. In this paper, we empirically study the performance
of the AFSA and different approaches to balance between local and global
exploration have been tested based on the adaptive modification of visual and
step during algorithm execution. The proposed approaches have been evaluated
based on the four well-known benchmark functions. Experimental results show
considerable positive impact on the performance of AFSA."
"The Information-theoretic and Algorithmic Approach to Human, Animal and
  Artificial Cognition","We survey concepts at the frontier of research connecting artificial, animal
and human cognition to computation and information processing---from the Turing
test to Searle's Chinese Room argument, from Integrated Information Theory to
computational and algorithmic complexity. We start by arguing that passing the
Turing test is a trivial computational problem and that its pragmatic
difficulty sheds light on the computational nature of the human mind more than
it does on the challenge of artificial intelligence. We then review our
proposed algorithmic information-theoretic measures for quantifying and
characterizing cognition in various forms. These are capable of accounting for
known biases in human behavior, thus vindicating a computational algorithmic
view of cognition as first suggested by Turing, but this time rooted in the
concept of algorithmic probability, which in turn is based on computational
universality while being independent of computational model, and which has the
virtue of being predictive and testable as a model theory of cognitive
behavior."
Nipce-Bell or Turing: How to Test Odor Reproduction?,"In a 1950 article in Mind, decades before the existence of anything
resembling an artificial intelligence system, Alan Turing addressed the
question of how to test whether machines can think, or in modern terminology,
whether a computer claimed to exhibit intelligence indeed does so. The current
paper raises the analogous issue for olfaction: how to test the validity of a
system claimed to reproduce arbitrary odors artificially, in a way recognizable
to humans, in face of the unavailability of a general naming method for odors.
Although odor reproduction systems are still far from being viable, the
question of how to test candidates thereof is claimed to be interesting and
nontrivial, and a novel method is proposed. To some extent, the method is
inspired by Turing`s test for AI, in that it involves a human challenger and
the real and artificial entities, yet it is very different: our test is
conditional, requiring from the artificial no more than is required from the
original, and it employs a novel method of immersion that takes advantage of
the availability of near-perfect reproduction methods for sight and sound."
Learning Solving Procedure for Artificial Neural Network,"It is expected that progress toward true artificial intelligence will be
achieved through the emergence of a system that integrates representation
learning and complex reasoning (LeCun et al. 2015). In response to this
prediction, research has been conducted on implementing the symbolic reasoning
of a von Neumann computer in an artificial neural network (Graves et al. 2016;
Graves et al. 2014; Reed et al. 2015). However, these studies have many
limitations in realizing neural-symbolic integration (Jaeger. 2016). Here, we
present a new learning paradigm: a learning solving procedure (LSP) that learns
the procedure for solving complex problems. This is not accomplished merely by
learning input-output data, but by learning algorithms through a solving
procedure that obtains the output as a sequence of tasks for a given input
problem. The LSP neural network system not only learns simple problems of
addition and multiplication, but also the algorithms of complicated problems,
such as complex arithmetic expression, sorting, and Hanoi Tower. To realize
this, the LSP neural network structure consists of a deep neural network and
long short-term memory, which are recursively combined. Through
experimentation, we demonstrate the efficiency and scalability of LSP and its
validity as a mechanism of complex reasoning."
"The Animal-AI Environment: Training and Testing Animal-Like Artificial
  Cognition","Recent advances in artificial intelligence have been strongly driven by the
use of game environments for training and evaluating agents. Games are often
accessible and versatile, with well-defined state-transitions and goals
allowing for intensive training and experimentation. However, agents trained in
a particular environment are usually tested on the same or slightly varied
distributions, and solutions do not necessarily imply any understanding. If we
want AI systems that can model and understand their environment, we need
environments that explicitly test for this. Inspired by the extensive
literature on animal cognition, we present an environment that keeps all the
positive elements of standard gaming environments, but is explicitly designed
for the testing of animal-like artificial cognition."
"Artificial mental phenomena: Psychophysics as a framework to detect
  perception biases in AI models","Detecting biases in artificial intelligence has become difficult because of
the impenetrable nature of deep learning. The central difficulty is in relating
unobservable phenomena deep inside models with observable, outside quantities
that we can measure from inputs and outputs. For example, can we detect
gendered perceptions of occupations (e.g., female librarian, male electrician)
using questions to and answers from a word embedding-based system? Current
techniques for detecting biases are often customized for a task, dataset, or
method, affecting their generalization. In this work, we draw from
Psychophysics in Experimental Psychology---meant to relate quantities from the
real world (i.e., ""Physics"") into subjective measures in the mind (i.e.,
""Psyche"")---to propose an intellectually coherent and generalizable framework
to detect biases in AI. Specifically, we adapt the two-alternative forced
choice task (2AFC) to estimate potential biases and the strength of those
biases in black-box models. We successfully reproduce previously-known biased
perceptions in word embeddings and sentiment analysis predictions. We discuss
how concepts in experimental psychology can be naturally applied to
understanding artificial mental phenomena, and how psychophysics can form a
useful methodological foundation to study fairness in AI."
Improved Binary Artificial Bee Colony Algorithm,"The Artificial Bee Colony (ABC) algorithm is an evolutionary optimization
algorithm based on swarm intelligence and inspired by the honey bees' food
search behavior. Since the ABC algorithm has been developed to achieve optimal
solutions by searching in the continuous search space, modification is required
to apply this method to binary optimization problems. In this paper, we improve
the ABC algorithm to solve binary optimization problems and call it the
improved binary Artificial Bee Colony (ibinABC). The proposed method consists
of an update mechanism based on fitness values and processing different number
of decision variables. Thus, we aim to prevent the ABC algorithm from getting
stuck in a local minimum by increasing its exploration ability. We compare the
ibinABC algorithm with three variants of the ABC and other meta-heuristic
algorithms in the literature. For comparison, we use the wellknown OR-Library
dataset containing 15 problem instances prepared for the uncapacitated facility
location problem. Computational results show that the proposed method is
superior to other methods in terms of convergence speed and robustness. The
source code of the algorithm will be available on GitHub after reviewing
process"
Lenia and Expanded Universe,"We report experimental extensions of Lenia, a continuous cellular automata
family capable of producing lifelike self-organizing autonomous patterns. The
rule of Lenia was generalized into higher dimensions, multiple kernels, and
multiple channels. The final architecture approaches what can be seen as a
recurrent convolutional neural network. Using semi-automatic search e.g.
genetic algorithm, we discovered new phenomena like polyhedral symmetries,
individuality, self-replication, emission, growth by ingestion, and saw the
emergence of ""virtual eukaryotes"" that possess internal division of labor and
type differentiation. We discuss the results in the contexts of biology,
artificial life, and artificial intelligence."
"Recent Advances in Leveraging Human Guidance for Sequential
  Decision-Making Tasks","A longstanding goal of artificial intelligence is to create artificial agents
capable of learning to perform tasks that require sequential decision making.
Importantly, while it is the artificial agent that learns and acts, it is still
up to humans to specify the particular task to be performed. Classical
task-specification approaches typically involve humans providing stationary
reward functions or explicit demonstrations of the desired tasks. However,
there has recently been a great deal of research energy invested in exploring
alternative ways in which humans may guide learning agents that may, e.g., be
more suitable for certain tasks or require less human effort. This survey
provides a high-level overview of five recent machine learning frameworks that
primarily rely on human guidance apart from pre-specified reward functions or
conventional, step-by-step action demonstrations. We review the motivation,
assumptions, and implementation of each framework, and we discuss possible
future research directions."
"Learning offline: memory replay in biological and artificial
  reinforcement learning","Learning to act in an environment to maximise rewards is among the brain's
key functions. This process has often been conceptualised within the framework
of reinforcement learning, which has also gained prominence in machine learning
and artificial intelligence (AI) as a way to optimise decision-making. A common
aspect of both biological and machine reinforcement learning is the
reactivation of previously experienced episodes, referred to as replay. Replay
is important for memory consolidation in biological neural networks, and is key
to stabilising learning in deep neural networks. Here, we review recent
developments concerning the functional roles of replay in the fields of
neuroscience and AI. Complementary progress suggests how replay might support
learning processes, including generalisation and continual learning, affording
opportunities to transfer knowledge across the two fields to advance the
understanding of biological and artificial learning and memory."
Evolutionary Generation of Visual Motion Illusions,"Why do we sometimes perceive static images as if they were moving? Visual
motion illusions enjoy a sustained popularity, yet there is no definitive
answer to the question of why they work. We present a generative model, the
Evolutionary Illusion GENerator (EIGen), that creates new visual motion
illusions. The structure of EIGen supports the hypothesis that illusory motion
might be the result of perceiving the brain's own predictions rather than
perceiving raw visual input from the eyes. The scientific motivation of this
paper is to demonstrate that the perception of illusory motion could be a side
effect of the predictive abilities of the brain. The philosophical motivation
of this paper is to call attention to the untapped potential of ""motivated
failures"", ways for artificial systems to fail as biological systems fail, as a
worthy outlet for Artificial Intelligence and Artificial Life research."
"The Theory of Artificial Immutability: Protecting Algorithmic Groups
  Under Anti-Discrimination Law","Artificial Intelligence (AI) is increasingly used to make important decisions
about people. While issues of AI bias and proxy discrimination are well
explored, less focus has been paid to the harms created by profiling based on
groups that do not map to or correlate with legally protected groups such as
sex or ethnicity. This raises a question: are existing equality laws able to
protect against emergent AI-driven inequality? This article examines the legal
status of algorithmic groups in North American and European non-discrimination
doctrine, law, and jurisprudence and will show that algorithmic groups are not
comparable to traditional protected groups. Nonetheless, these new groups are
worthy of protection. I propose a new theory of harm - ""the theory of
artificial immutability"" - that aims to bring AI groups within the scope of the
law. My theory describes how algorithmic groups act as de facto immutable
characteristics in practice that limit people's autonomy and prevent them from
achieving important goals."
"Biological connectomes as a representation for the architecture of
  artificial neural networks","Grand efforts in neuroscience are working toward mapping the connectomes of
many new species, including the near completion of the Drosophila melanogaster.
It is important to ask whether these models could benefit artificial
intelligence. In this work we ask two fundamental questions: (1) where and when
biological connectomes can provide use in machine learning, (2) which design
principles are necessary for extracting a good representation of the
connectome. Toward this end, we translate the motor circuit of the C. Elegans
nematode into artificial neural networks at varying levels of biophysical
realism and evaluate the outcome of training these networks on motor and
non-motor behavioral tasks. We demonstrate that biophysical realism need not be
upheld to attain the advantages of using biological circuits. We also establish
that, even if the exact wiring diagram is not retained, the architectural
statistics provide a valuable prior. Finally, we show that while the C. Elegans
locomotion circuit provides a powerful inductive bias on locomotion problems,
its structure may hinder performance on tasks unrelated to locomotion such as
visual classification problems."
On Modifying a Neural Network's Perception,"Artificial neural networks have proven to be extremely useful models that
have allowed for multiple recent breakthroughs in the field of Artificial
Intelligence and many others. However, they are typically regarded as black
boxes, given how difficult it is for humans to interpret how these models reach
their results. In this work, we propose a method which allows one to modify
what an artificial neural network is perceiving regarding specific
human-defined concepts, enabling the generation of hypothetical scenarios that
could help understand and even debug the neural network model. Through
empirical evaluation, in a synthetic dataset and in the ImageNet dataset, we
test the proposed method on different models, assessing whether the performed
manipulations are well interpreted by the models, and analyzing how they react
to them."
"Generation of artificial facial drug abuse images using Deep
  De-identified anonymous Dataset augmentation through Genetics Algorithm
  (3DG-GA)","In biomedical research and artificial intelligence, access to large,
well-balanced, and representative datasets is crucial for developing
trustworthy applications that can be used in real-world scenarios. However,
obtaining such datasets can be challenging, as they are often restricted to
hospitals and specialized facilities. To address this issue, the study proposes
to generate highly realistic synthetic faces exhibiting drug abuse traits
through augmentation. The proposed method, called ""3DG-GA"", Deep De-identified
anonymous Dataset Generation, uses Genetics Algorithm as a strategy for
synthetic faces generation. The algorithm includes GAN artificial face
generation, forgery detection, and face recognition. Initially, a dataset of
120 images of actual facial drug abuse is used. By preserving, the drug traits,
the 3DG-GA provides a dataset containing 3000 synthetic facial drug abuse
images. The dataset will be open to the scientific community, which can
reproduce our results and benefit from the generated datasets while avoiding
legal or ethical restrictions."
Brain-inspired learning in artificial neural networks: a review,"Artificial neural networks (ANNs) have emerged as an essential tool in
machine learning, achieving remarkable success across diverse domains,
including image and speech generation, game playing, and robotics. However,
there exist fundamental differences between ANNs' operating mechanisms and
those of the biological brain, particularly concerning learning processes. This
paper presents a comprehensive review of current brain-inspired learning
representations in artificial neural networks. We investigate the integration
of more biologically plausible mechanisms, such as synaptic plasticity, to
enhance these networks' capabilities. Moreover, we delve into the potential
advantages and challenges accompanying this approach. Ultimately, we pinpoint
promising avenues for future research in this rapidly advancing field, which
could bring us closer to understanding the essence of intelligence."
Synaptic Sampling of Neural Networks,"Probabilistic artificial neural networks offer intriguing prospects for
enabling the uncertainty of artificial intelligence methods to be described
explicitly in their function; however, the development of techniques that
quantify uncertainty by well-understood methods such as Monte Carlo sampling
has been limited by the high costs of stochastic sampling on deterministic
computing hardware. Emerging computing systems that are amenable to
hardware-level probabilistic computing, such as those that leverage stochastic
devices, may make probabilistic neural networks more feasible in the
not-too-distant future. This paper describes the scANN technique --
\textit{sampling (by coinflips) artificial neural networks} -- which enables
neural networks to be sampled directly by treating the weights as Bernoulli
coin flips. This method is natively well suited for probabilistic computing
techniques that focus on tunable stochastic devices, nearly matches fully
deterministic performance while also describing the uncertainty of correct and
incorrect neural network outputs."
"Why Machines Can't Be Moral: Turing's Halting Problem and the Moral
  Limits of Artificial Intelligence","In this essay, I argue that explicit ethical machines, whose moral principles
are inferred through a bottom-up approach, are unable to replicate human-like
moral reasoning and cannot be considered moral agents. By utilizing Alan
Turing's theory of computation, I demonstrate that moral reasoning is
computationally intractable by these machines due to the halting problem. I
address the frontiers of machine ethics by formalizing moral problems into
'algorithmic moral questions' and by exploring moral psychology's dual-process
model. While the nature of Turing Machines theoretically allows artificial
agents to engage in recursive moral reasoning, critical limitations are
introduced by the halting problem, which states that it is impossible to
predict with certainty whether a computational process will halt. A thought
experiment involving a military drone illustrates this issue, showing that an
artificial agent might fail to decide between actions due to the halting
problem, which limits the agent's ability to make decisions in all instances,
undermining its moral agency."
Towards Scalable GPU-Accelerated SNN Training via Temporal Fusion,"Drawing on the intricate structures of the brain, Spiking Neural Networks
(SNNs) emerge as a transformative development in artificial intelligence,
closely emulating the complex dynamics of biological neural networks. While
SNNs show promising efficiency on specialized sparse-computational hardware,
their practical training often relies on conventional GPUs. This reliance
frequently leads to extended computation times when contrasted with traditional
Artificial Neural Networks (ANNs), presenting significant hurdles for advancing
SNN research. To navigate this challenge, we present a novel temporal fusion
method, specifically designed to expedite the propagation dynamics of SNNs on
GPU platforms, which serves as an enhancement to the current significant
approaches for handling deep learning tasks with SNNs. This method underwent
thorough validation through extensive experiments in both authentic training
scenarios and idealized conditions, confirming its efficacy and adaptability
for single and multi-GPU systems. Benchmarked against various existing SNN
libraries/implementations, our method achieved accelerations ranging from
$5\times$ to $40\times$ on NVIDIA A100 GPUs. Publicly available experimental
codes can be found at https://github.com/EMI-Group/snn-temporal-fusion."
"A transformer-based deep reinforcement learning approach to spatial
  navigation in a partially observable Morris Water Maze","Navigation is a fundamental cognitive skill extensively studied in
neuroscientific experiments and has lately gained substantial interest in
artificial intelligence research. Recreating the task solved by rodents in the
well-established Morris Water Maze (MWM) experiment, this work applies a
transformer-based architecture using deep reinforcement learning -- an approach
previously unexplored in this context -- to navigate a 2D version of the maze.
Specifically, the agent leverages a decoder-only transformer architecture
serving as a deep Q-network performing effective decision making in the
partially observable environment. We demonstrate that the proposed architecture
enables the agent to efficiently learn spatial navigation strategies,
overcoming challenges associated with a limited field of vision, corresponding
to the visual information available to a rodent in the MWM. Demonstrating the
potential of transformer-based models for enhancing navigation performance in
partially observable environments, this work suggests promising avenues for
future research in artificial agents whose behavior resembles that of
biological agents. Finally, the flexibility of the transformer architecture in
supporting varying input sequence lengths opens opportunities for gaining
increased understanding of the artificial agent's inner representation of the
environment."
"Automated Quality Control System for Canned Tuna Production using
  Artificial Vision","This scientific article presents the implementation of an automated control
system for detecting and classifying faults in tuna metal cans using artificial
vision. The system utilizes a conveyor belt and a camera for visual recognition
triggered by a photoelectric sensor. A robotic arm classifies the metal cans
according to their condition. Industry 4.0 integration is achieved through an
IoT system using Mosquitto, Node-RED, InfluxDB, and Grafana. The YOLOv5 model
is employed to detect faults in the metal can lids and the positioning of the
easy-open ring. Training with GPU on Google Colab enables OCR text detection on
the labels. The results indicate efficient real-time problem identification,
optimization of resources, and delivery of quality products. At the same time,
the vision system contributes to autonomy in quality control tasks, freeing
operators to perform other functions within the company."
"Enhancing Code Annotation Reliability: Generative AI's Role in Comment
  Quality Assessment Models","This paper explores a novel method for enhancing binary classification models
that assess code comment quality, leveraging Generative Artificial Intelligence
to elevate model performance. By integrating 1,437 newly generated code-comment
pairs, labeled as ""Useful"" or ""Not Useful"" and sourced from various GitHub
repositories, into an existing C-language dataset of 9,048 pairs, we
demonstrate substantial model improvements. Using an advanced Large Language
Model, our approach yields a 5.78% precision increase in the Support Vector
Machine (SVM) model, improving from 0.79 to 0.8478, and a 2.17% recall boost in
the Artificial Neural Network (ANN) model, rising from 0.731 to 0.7527. These
results underscore Generative AI's value in advancing code comment
classification models, offering significant potential for enhanced accuracy in
software development and quality control. This study provides a promising
outlook on the integration of generative techniques for refining machine
learning models in practical software engineering settings."
Adapting the Biological SSVEP Response to Artificial Neural Networks,"Neuron importance assessment is crucial for understanding the inner workings
of artificial neural networks (ANNs) and improving their interpretability and
efficiency. This paper introduces a novel approach to neuron significance
assessment inspired by frequency tagging, a technique from neuroscience. By
applying sinusoidal contrast modulation to image inputs and analyzing resulting
neuron activations, this method enables fine-grained analysis of a network's
decision-making processes. Experiments conducted with a convolutional neural
network for image classification reveal notable harmonics and intermodulations
in neuron-specific responses under part-based frequency tagging. These findings
suggest that ANNs exhibit behavior akin to biological brains in tuning to
flickering frequencies, thereby opening avenues for neuron/filter importance
assessment through frequency tagging. The proposed method holds promise for
applications in network pruning, and model interpretability, contributing to
the advancement of explainable artificial intelligence and addressing the lack
of transparency in neural networks. Future research directions include
developing novel loss functions to encourage biologically plausible behavior in
ANNs."
Prolegomena to a Post-Aesthetics of Artificial Imaginations,"The acceleration of the use of generative artificial intelligences (AI),
since 2015 and the turning point operated by Deepdream, tends to obscure a real
analysis of what could be defined as artificial imagination. AIs are either
reduced to simple instruments or thought of according to a form of
techno-theologism. Our research tends to suspend any form of judgment in order
to phenomenally grasp the emergence of these AIs. By taking up the question of
Hegel's aesthetics and of art as the free production of the mind, but by moving
it towards the question of generative AIs and therefore of a post-aesthetics,
this article will show the phenomenal specificity of images generatedby AI."
Decision Support Systems Using Intelligent Paradigms,"Decision-making is a process of choosing among alternative courses of action
for solving complicated problems where multi-criteria objectives are involved.
The past few years have witnessed a growing recognition of Soft Computing (SC)
technologies that underlie the conception, design and utilization of
intelligent systems. In this paper, we present different SC paradigms involving
an artificial neural network trained using the scaled conjugate gradient
algorithm, two different fuzzy inference methods optimised using neural network
learning/evolutionary algorithms and regression trees for developing
intelligent decision support systems. We demonstrate the efficiency of the
different algorithms by developing a decision support system for a Tactical Air
Combat Environment (TACE). Some empirical comparisons between the different
algorithms are also provided."
"A Knowledge Discovery Framework for Learning Task Models from User
  Interactions in Intelligent Tutoring Systems","Domain experts should provide relevant domain knowledge to an Intelligent
Tutoring System (ITS) so that it can guide a learner during problemsolving
learning activities. However, for many ill-defined domains, the domain
knowledge is hard to define explicitly. In previous works, we showed how
sequential pattern mining can be used to extract a partial problem space from
logged user interactions, and how it can support tutoring services during
problem-solving exercises. This article describes an extension of this approach
to extract a problem space that is richer and more adapted for supporting
tutoring services. We combined sequential pattern mining with (1) dimensional
pattern mining (2) time intervals, (3) the automatic clustering of valued
actions and (4) closed sequences mining. Some tutoring services have been
implemented and an experiment has been conducted in a tutoring system."
"Faith in the Algorithm, Part 2: Computational Eudaemonics","Eudaemonics is the study of the nature, causes, and conditions of human
well-being. According to the ethical theory of eudaemonia, reaping satisfaction
and fulfillment from life is not only a desirable end, but a moral
responsibility. However, in modern society, many individuals struggle to meet
this responsibility. Computational mechanisms could better enable individuals
to achieve eudaemonia by yielding practical real-world systems that embody
algorithms that promote human flourishing. This article presents eudaemonic
systems as the evolutionary goal of the present day recommender system."
"Emotional control - conditio sine qua non for advanced artificial
  intelligences?","Humans dispose of two intertwined information processing pathways, cognitive
information processing via neural firing patterns and diffusive volume control
via neuromodulation. The cognitive information processing in the brain is
traditionally considered to be the prime neural correlate of human
intelligence, clinical studies indicate that human emotions intrinsically
correlate with the activation of the neuromodulatory system.
  We examine here the question: Why do humans dispose of the diffusive
emotional control system? Is this a coincidence, a caprice of nature, perhaps a
leftover of our genetic heritage, or a necessary aspect of any advanced
intelligence, being it biological or synthetic? We argue here that emotional
control is necessary to solve the motivational problem, viz the selection of
short-term utility functions, in the context of an environment where
information, computing power and time constitute scarce resources."
"Contribution of Case Based Reasoning (CBR) in the Exploitation of Return
  of Experience. Application to Accident Scenarii in Railroad Transport","The study is from a base of accident scenarii in rail transport (feedback) in
order to develop a tool to share build and sustain knowledge and safety and
secondly to exploit the knowledge stored to prevent the reproduction of
accidents / incidents. This tool should ultimately lead to the proposal of
prevention and protection measures to minimize the risk level of a new
transport system and thus to improve safety. The approach to achieving this
goal largely depends on the use of artificial intelligence techniques and
rarely the use of a method of automatic learning in order to develop a
feasibility model of a software tool based on case based reasoning (CBR) to
exploit stored knowledge in order to create know-how that can help stimulate
domain experts in the task of analysis, evaluation and certification of a new
system."
Expert PC Troubleshooter With Fuzzy-Logic And Self-Learning Support,"Expert systems use human knowledge often stored as rules within the computer
to solve problems that generally would entail human intelligence. Today, with
information systems turning out to be more pervasive and with the myriad
advances in information technologies, automating computer fault diagnosis is
becoming so fundamental that soon every enterprise has to endorse it. This
paper proposes an expert system called Expert PC Troubleshooter for diagnosing
computer problems. The system is composed of a user interface, a rule-base, an
inference engine, and an expert interface. Additionally, the system features a
fuzzy-logic module to troubleshoot POST beep errors, and an intelligent agent
that assists in the knowledge acquisition process. The proposed system is meant
to automate the maintenance, repair, and operations (MRO) process, and free-up
human technicians from manually performing routine, laborious, and
timeconsuming maintenance tasks. As future work, the proposed system is to be
parallelized so as to boost its performance and speed-up its various
operations."
The Anatomy of a Modular System for Media Content Analysis,"Intelligent systems for the annotation of media content are increasingly
being used for the automation of parts of social science research. In this
domain the problem of integrating various Artificial Intelligence (AI)
algorithms into a single intelligent system arises spontaneously. As part of
our ongoing effort in automating media content analysis for the social
sciences, we have built a modular system by combining multiple AI modules into
a flexible framework in which they can cooperate in complex tasks. Our system
combines data gathering, machine translation, topic classification, extraction
and annotation of entities and social networks, as well as many other tasks
that have been perfected over the past years of AI research. Over the last few
years, it has allowed us to realise a series of scientific studies over a vast
range of applications including comparative studies between news outlets and
media content in different countries, modelling of user preferences, and
monitoring public mood. The framework is flexible and allows the design and
implementation of modular agents, where simple modules cooperate in the
annotation of a large dataset without central coordination."
Applications of Algorithmic Probability to the Philosophy of Mind,"This paper presents formulae that can solve various seemingly hopeless
philosophical conundrums. We discuss the simulation argument, teleportation,
mind-uploading, the rationality of utilitarianism, and the ethics of exploiting
artificial general intelligence. Our approach arises from combining the
essential ideas of formalisms such as algorithmic probability, the universal
intelligence measure, space-time-embedded intelligence, and Hutter's observer
localization. We argue that such universal models can yield the ultimate
solutions, but a novel research direction would be required in order to find
computationally efficient approximations thereof."
"Advances in Artificial Intelligence: Are you sure, we are on the right
  track?","Over the past decade, AI has made a remarkable progress. It is agreed that
this is due to the recently revived Deep Learning technology. Deep Learning
enables to process large amounts of data using simplified neuron networks that
simulate the way in which the brain works. However, there is a different point
of view, which posits that the brain is processing information, not data. This
unresolved duality hampered AI progress for years. In this paper, I propose a
notion of Integrated information that hopefully will resolve the problem. I
consider integrated information as a coupling between two separate entities -
physical information (that implies data processing) and semantic information
(that provides physical information interpretation). In this regard,
intelligence becomes a product of information processing. Extending further
this line of thinking, it can be said that information processing does not
require more a human brain for its implementation. Indeed, bacteria and amoebas
exhibit intelligent behavior without any sign of a brain. That dramatically
removes the need for AI systems to emulate the human brain complexity! The
paper tries to explore this shift in AI systems design philosophy."
Only T3-AI can reach human-level intelligence: A variety argument,"The recently introduced theory of practopoiesis offers an account on how
adaptive intelligent systems are organized. According to that theory biological
agents adapt at three levels of organization and this structure applies also to
our brains. This is referred to as tri-traversal theory of the organization of
mind or for short, a T3-structure. To implement a similar T3-organization in an
artificially intelligent agent, it is necessary to have multiple policies, as
usually used as a concept in the theory of reinforcement learning. These
policies have to form a hierarchy. We define adaptive practopoietic systems in
terms of hierarchy of policies and calculate whether the total variety of
behavior required by real-life conditions of an adult human can be
satisfactorily accounted for by a traditional approach to artificial
intelligence based on T2-agents, or whether a T3-agent is needed instead. We
conclude that the complexity of real life can be dealt with appropriately only
by a T3-agent."
The Singularity May Never Be Near,"There is both much optimism and pessimism around artificial intelligence (AI)
today. The optimists are investing millions of dollars, and even in some cases
billions of dollars into AI. The pessimists, on the other hand, predict that AI
will end many things: jobs, warfare, and even the human race. Both the
optimists and the pessimists often appeal to the idea of a technological
singularity, a point in time where machine intelligence starts to run away, and
a new, more intelligent species starts to inhabit the earth. If the optimists
are right, this will be a moment that fundamentally changes our economy and our
society. If the pessimists are right, this will be a moment that also
fundamentally changes our economy and our society. It is therefore very
worthwhile spending some time deciding if either of them might be right."
A Tutorial on Deep Neural Networks for Intelligent Systems,"Developing Intelligent Systems involves artificial intelligence approaches
including artificial neural networks. Here, we present a tutorial of Deep
Neural Networks (DNNs), and some insights about the origin of the term ""deep"";
references to deep learning are also given. Restricted Boltzmann Machines,
which are the core of DNNs, are discussed in detail. An example of a simple
two-layer network, performing unsupervised learning for unlabeled data, is
shown. Deep Belief Networks (DBNs), which are used to build networks with more
than two layers, are also described. Moreover, examples for supervised learning
with DNNs performing simple prediction and classification tasks, are presented
and explained. This tutorial includes two intelligent pattern recognition
applications: hand- written digits (benchmark known as MNIST) and speech
recognition."
Superintelligence cannot be contained: Lessons from Computability Theory,"Superintelligence is a hypothetical agent that possesses intelligence far
surpassing that of the brightest and most gifted human minds. In light of
recent advances in machine intelligence, a number of scientists, philosophers
and technologists have revived the discussion about the potential catastrophic
risks entailed by such an entity. In this article, we trace the origins and
development of the neo-fear of superintelligence, and some of the major
proposals for its containment. We argue that such containment is, in principle,
impossible, due to fundamental limits inherent to computing itself. Assuming
that a superintelligence will contain a program that includes all the programs
that can be executed by a universal Turing machine on input potentially as
complex as the state of the world, strict containment requires simulations of
such a program, something theoretically (and practically) infeasible."
"Virtual Embodiment: A Scalable Long-Term Strategy for Artificial
  Intelligence Research","Meaning has been called the ""holy grail"" of a variety of scientific
disciplines, ranging from linguistics to philosophy, psychology and the
neurosciences. The field of Artifical Intelligence (AI) is very much a part of
that list: the development of sophisticated natural language semantics is a
sine qua non for achieving a level of intelligence comparable to humans.
Embodiment theories in cognitive science hold that human semantic
representation depends on sensori-motor experience; the abundant evidence that
human meaning representation is grounded in the perception of physical reality
leads to the conclusion that meaning must depend on a fusion of multiple
(perceptual) modalities. Despite this, AI research in general, and its
subdisciplines such as computational linguistics and computer vision in
particular, have focused primarily on tasks that involve a single modality.
Here, we propose virtual embodiment as an alternative, long-term strategy for
AI research that is multi-modal in nature and that allows for the kind of
scalability required to develop the field coherently and incrementally, in an
ethically responsible fashion."
"Cooperative Automated Vehicles: a Review of Opportunities and Challenges
  in Socially Intelligent Vehicles Beyond Networking","The connected automated vehicle has been often touted as a technology that
will become pervasive in society in the near future. One can view an automated
vehicle as having Artificial Intelligence (AI) capabilities, being able to
self-drive, sense its surroundings, recognise objects in its vicinity, and
perform reasoning and decision-making.
  Rather than being stand alone, we examine the need for automated vehicles to
cooperate and interact within their socio-cyber-physical environments,
including the problems cooperation will solve, but also the issues and
challenges. We review current work in cooperation for automated vehicles, based
on selected examples from the literature. We conclude noting the need for the
ability to behave cooperatively as a form of social-AI capability for automated
vehicles, beyond sensing the immediate environment and beyond the underlying
networking technology."
Intelligent Drone Swarm for Search and Rescue Operations at Sea,"In recent years, a rising numbers of people arrived in the European Union,
traveling across the Mediterranean Sea or overland through Southeast Europe in
what has been later named as the European migrant crisis. In the last 5 years,
more than 16 thousands people have lost their lives in the Mediterranean sea
during the crossing. The United Nations Secretary General Strategy on New
Technologies is supporting the use of Artificial Intelligence (AI) and Robotics
to accelerate the achievement of the 2030 Sustainable Development Agenda, which
includes safe and regular migration processes among the others. In the same
spirit, the central idea of this project aims at using AI technology for Search
And Rescue (SAR) operations at sea. In particular, we propose an autonomous
fleet of self-organizing intelligent drones that would enable the coverage of a
broader area, speeding-up the search processes and finally increasing the
efficiency and effectiveness of migrants rescue operations."
Scene Recognition Through Visual and Acoustic Cues Using K-Means,"We propose a K-Means based prediction system, nicknamed SERVANT (Scene
Recognition Through Visual and Acoustic Cues), that is capable of recognizing
environmental scenes through analysis of ambient sound and color cues. The
concept and implementation originated within the Learning branch of the
Intelligent Wearable Robotics Project (also known as the Third Arm project) at
the Stanford Artificial Intelligence Lab-Toyota Center (SAIL-TC). The Third Arm
Project focuses on the development and conceptualization of a robotic arm that
can aid users in a whole array of situations: i.e. carrying a cup of coffee,
holding a flashlight. Servant uses a K-Means fit-and-predict architecture to
classify environmental scenes, such as that of a coffee shop or a basketball
gym, using visual and auditory cues. Following such classification, Servant can
recommend contextual actions based on prior training."
"Intelligent architectures for robotics: The merging of cognition and
  emotion","What is the place of emotion in intelligent robots? In the past two decades,
researchers have advocated for the inclusion of some emotion-related components
in the general information processing architecture of autonomous agents, say,
for better communication with humans, or to instill a sense of urgency to
action. The framework advanced here goes beyond these approaches and proposes
that emotion and motivation need to be integrated with all aspects of the
architecture. Thus, cognitive-emotional integration is a key design principle.
Emotion is not an ""add on"" that endows a robot with ""feelings"" (for instance,
reporting or expressing its internal state). It allows the significance of
percepts, plans, and actions to be an integral part of all its computations. It
is hypothesized that a sophisticated artificial intelligence cannot be built
from separate cognitive and emotional modules. A hypothetical test inspired by
the Turing test, called the Dolores test, is proposed to test this assertion."
Federated AI lets a team imagine together: Federated Learning of GANs,"Envisioning a new imaginative idea together is a popular human need.
Imagining together as a team can often lead to breakthrough ideas, but the
collaboration effort can also be challenging, especially when the team members
are separated by time and space. What if there is a AI that can assist the team
to collaboratively envision new ideas?. Is it possible to develop a working
model of such an AI? This paper aims to design such an intelligence. This paper
proposes a approach to design a creative and collaborative intelligence by
employing a form of distributed machine learning approach called Federated
Learning along with fusion on Generative Adversarial Networks, GAN. This
collaborative creative AI presents a new paradigm in AI, one that lets a team
of two or more to come together to imagine and envision ideas that synergies
well with interests of all members of the team. In short, this paper explores
the design of a novel type of AI paradigm, called Federated AI Imagination, one
that lets geographically distributed teams to collaboratively imagine."
"Acceptable Planning: Influencing Individual Behavior to Reduce
  Transportation Energy Expenditure of a City","Our research aims at developing intelligent systems to reduce the
transportation-related energy expenditure of a large city by influencing
individual behavior. We introduce COPTER - an intelligent travel assistant that
evaluates multi-modal travel alternatives to find a plan that is acceptable to
a person given their context and preferences. We propose a formulation for
acceptable planning that brings together ideas from AI, machine learning, and
economics. This formulation has been incorporated in COPTER that produces
acceptable plans in real-time. We adopt a novel empirical evaluation framework
that combines human decision data with a high fidelity multi-modal
transportation simulation to demonstrate a 4\% energy reduction and 20\% delay
reduction in a realistic deployment scenario in Los Angeles, California, USA."
"Toward a Computational Theory of Evidence-Based Reasoning for
  Instructable Cognitive Agents","Evidence-based reasoning is at the core of many problem-solving and
decision-making tasks in a wide variety of domains. Generalizing from the
research and development of cognitive agents in several such domains, this
paper presents progress toward a computational theory for the development of
instructable cognitive agents for evidence-based reasoning tasks. The paper
also illustrates the application of this theory to the development of four
prototype cognitive agents in domains that are critical to the government and
the public sector. Two agents function as cognitive assistants, one in
intelligence analysis, and the other in science education. The other two agents
operate autonomously, one in cybersecurity and the other in intelligence,
surveillance, and reconnaissance. The paper concludes with the directions of
future research on the proposed computational theory."
Towards Intelligent Robotic Process Automation for BPMers,"Robotic Process Automation (RPA) is a fast-emerging automation technology
that sits between the fields of Business Process Management (BPM) and
Artificial Intelligence (AI), and allows organizations to automate high volume
routines. RPA tools are able to capture the execution of such routines
previously performed by a human users on the interface of a computer system,
and then emulate their enactment in place of the user by means of a software
robot. Nowadays, in the BPM domain, only simple, predictable business processes
involving routine work can be automated by RPA tools in situations where there
is no room for interpretation, while more sophisticated work is still left to
human experts. In this paper, starting from an in-depth experimentation of the
RPA tools available on the market, we provide a classification framework to
categorize them on the basis of some key dimensions. Then, based on this
analysis, we derive four research challenges and discuss prospective approaches
necessary to inject intelligence into current RPA technology, in order to
achieve more widespread adoption of RPA in the BPM domain."
"Automated Personalized Feedback Improves Learning Gains in an
  Intelligent Tutoring System","We investigate how automated, data-driven, personalized feedback in a
large-scale intelligent tutoring system (ITS) improves student learning
outcomes. We propose a machine learning approach to generate personalized
feedback, which takes individual needs of students into account. We utilize
state-of-the-art machine learning and natural language processing techniques to
provide the students with personalized hints, Wikipedia-based explanations, and
mathematical hints. Our model is used in Korbit, a large-scale dialogue-based
ITS with thousands of students launched in 2019, and we demonstrate that the
personalized feedback leads to considerable improvement in student learning
outcomes and in the subjective evaluation of the feedback."
"Guru, Partner, or Pencil Sharpener? Understanding Designers' Attitudes
  Towards Intelligent Creativity Support Tools","Creativity Support Tools (CST) aim to enhance human creativity, but the
deeply personal and subjective nature of creativity makes the design of
universal support tools challenging. Individuals develop personal approaches to
creativity, particularly in the context of commercial design where signature
styles and techniques are valuable commodities. Artificial Intelligence (AI)
and Machine Learning (ML) techniques could provide a means of creating
'intelligent' CST which learn and adapt to personal styles of creativity.
Identifying what kind of role such tools could play in the design process
requires a better understanding of designers' attitudes towards working with
AI, and their willingness to include it in their personal creative process.
This paper details the results of a survey of professional designers which
indicates a positive and pragmatic attitude towards collaborating with AI
tools, and a particular opportunity for incorporating them in the research
stages of a design project."
Argumentation-based Agents that Explain their Decisions,"Explainable Artificial Intelligence (XAI) systems, including intelligent
agents, must be able to explain their internal decisions, behaviours and
reasoning that produce their choices to the humans (or other systems) with
which they interact. In this paper, we focus on how an extended model of BDI
(Beliefs-Desires-Intentions) agents can be able to generate explanations about
their reasoning, specifically, about the goals he decides to commit to. Our
proposal is based on argumentation theory, we use arguments to represent the
reasons that lead an agent to make a decision and use argumentation semantics
to determine acceptable arguments (reasons). We propose two types of
explanations: the partial one and the complete one. We apply our proposal to a
scenario of rescue robots."
Pareto-Optimal Bit Allocation for Collaborative Intelligence,"In recent studies, collaborative intelligence (CI) has emerged as a promising
framework for deployment of Artificial Intelligence (AI)-based services on
mobile/edge devices. In CI, the AI model (a deep neural network) is split
between the edge and the cloud, and intermediate features are sent from the
edge sub-model to the cloud sub-model. In this paper, we study bit allocation
for feature coding in multi-stream CI systems. We model task distortion as a
function of rate using convex surfaces similar to those found in
distortion-rate theory. Using such models, we are able to provide closed-form
bit allocation solutions for single-task systems and scalarized multi-task
systems. Moreover, we provide analytical characterization of the full Pareto
set for 2-stream k-task systems, and bounds on the Pareto set for 3-stream
2-task systems. Analytical results are examined on a variety of DNN models from
the literature to demonstrate wide applicability of the results"
"Accelerating the Development of Multimodal, Integrative-AI Systems with
  Platform for Situated Intelligence","We describe Platform for Situated Intelligence, an open-source framework for
multimodal, integrative-AI systems. The framework provides infrastructure,
tools, and components that enable and accelerate the development of
applications that process multimodal streams of data and in which timing is
critical. The framework is particularly well-suited for developing physically
situated interactive systems that perceive and reason about their surroundings
in order to better interact with people, such as social robots, virtual
assistants, smart meeting rooms, etc. In this paper, we provide a brief,
high-level overview of the framework and its main affordances, and discuss its
implications for HRI."
OnRAMP for Regulating AI in Medical Products,"Medical Artificial Intelligence (AI) involves the application of machine
learning algorithms to biomedical datasets in order to improve medical
practices. Products incorporating medical AI require certification before
deployment in most jurisdictions. To date, clear pathways for regulating
medical AI are still under development. Below the level of formal pathways lies
the actual practice of developing a medical AI solution. This Perspective
proposes best practice guidelines for development compatible with the
production of a regulatory package which, regardless of the formal regulatory
path, will form a core component of a certification process. The approach is
predicated on a statistical risk perspective, typical of medical device
regulators, and a deep understanding of machine learning methodologies. These
guidelines will allow all parties to communicate more clearly in the
development of a common Good Machine Learning Practice (GMLP), and thus lead to
the enhanced development of both medical AI products and regulations."
"Explainable AI for System Failures: Generating Explanations that Improve
  Human Assistance in Fault Recovery","With the growing capabilities of intelligent systems, the integration of
artificial intelligence (AI) and robots in everyday life is increasing.
However, when interacting in such complex human environments, the failure of
intelligent systems, such as robots, can be inevitable, requiring recovery
assistance from users. In this work, we develop automated, natural language
explanations for failures encountered during an AI agents' plan execution.
These explanations are developed with a focus of helping non-expert users
understand different point of failures to better provide recovery assistance.
Specifically, we introduce a context-based information type for explanations
that can both help non-expert users understand the underlying cause of a system
failure, and select proper failure recoveries. Additionally, we extend an
existing sequence-to-sequence methodology to automatically generate our
context-based explanations. By doing so, we are able develop a model that can
generalize context-based explanations over both different failure types and
failure scenarios."
"The Less Intelligent the Elements, the More Intelligent the Whole. Or,
  Possibly Not?","We approach the debate on how ``intelligent'' artificial agents should be, by
endowing the preys and predators of the Lotka-Volterra model with behavioural
algorithms characterized by different levels of sophistication. We find that by
endowing both preys and predators with the capability of making predictions
based on linear extrapolation a novel sort of dynamic equilibrium appears,
where both species co-exist while both populations grow indefinitely. While we
confirm that, in general, simple agents favour the emergence of complex
collective behaviour, we also suggest that the capability of individuals to
take first-order derivatives of one other's behaviour may allow the collective
computation of derivatives of any order."
"Artificial Intelligence Driven UAV-NOMA-MEC in Next Generation Wireless
  Networks","Driven by the unprecedented high throughput and low latency requirements in
next-generation wireless networks, this paper introduces an artificial
intelligence (AI) enabled framework in which unmanned aerial vehicles (UAVs)
use non-orthogonal multiple access (NOMA) and mobile edge computing (MEC)
techniques to service terrestrial mobile users (MUs). The proposed framework
enables the terrestrial MUs to offload their computational tasks
simultaneously, intelligently, and flexibly, thus enhancing their connectivity
as well as reducing their transmission latency and their energy consumption. To
this end, the fundamentals of this framework are first introduced. Then, a
number of communication and AI techniques are proposed to improve the quality
of experiences of terrestrial MUs. To this end, federated learning and
reinforcement learning are introduced for intelligent task offloading and
computing resource allocation. For each learning technique, motivations,
challenges, and representative results are introduced. Finally, several key
technical challenges and open research issues of the proposed framework are
summarized."
"""Alexa, Can I Program You?"": Student Perceptions of Conversational
  Artificial Intelligence Before and After Programming Alexa","Growing up in an artificial intelligence-filled world, with Siri and Amazon
Alexa often within arm's - or speech's - reach, could have significant impact
on children. Conversational agents could influence how students
anthropomorphize computer systems or develop a theory of mind. Previous
research has explored how conversational agents are used and perceived by
children within and outside of learning contexts. This study investigates how
middle and high school students' perceptions of Alexa change through
programming their own conversational agents in week-long AI education
workshops. Specifically, we investigate the workshops' influence on student
perceptions of Alexa's intelligence, friendliness, aliveness, safeness,
trustworthiness, human-likeness, and feelings of closeness. We found that
students felt Alexa was more intelligent and felt closer to Alexa after the
workshops. We also found strong correlations between students' perceptions of
Alexa's friendliness and trustworthiness, and safeness and trustworthiness.
Finally, we explored how students tended to more frequently use computer
science-related diction and ideas after the workshops. Based on our findings,
we recommend designers carefully consider personification, transparency,
playfulness and utility when designing CAs for learning contexts."
"Lie-Sensor: A Live Emotion Verifier or a Licensor for Chat Applications
  using Emotional Intelligence","Veracity is an essential key in research and development of innovative
products. Live Emotion analysis and verification nullify deceit made to
complainers on live chat, corroborate messages of both ends in messaging apps
and promote an honest conversation between users. The main concept behind this
emotion artificial intelligent verifier is to license or decline message
accountability by comparing variegated emotions of chat app users recognized
through facial expressions and text prediction. In this paper, a proposed
emotion intelligent live detector acts as an honest arbiter who distributes
facial emotions into labels namely, Happiness, Sadness, Surprise, and Hate.
Further, it separately predicts a label of messages through text
classification. Finally, it compares both labels and declares the message as a
fraud or a bonafide. For emotion detection, we deployed Convolutional Neural
Network (CNN) using a miniXception model and for text prediction, we selected
Support Vector Machine (SVM) natural language processing probability classifier
due to receiving the best accuracy on training dataset after applying Support
Vector Machine (SVM), Random Forest Classifier, Naive Bayes Classifier, and
Logistic regression."
"Eigen-spectrograms: An interpretable feature space for bearing fault
  diagnosis based on artificial intelligence and image processing","The Intelligent Fault Diagnosis of rotating machinery currently proposes some
captivating challenges. Although results achieved by artificial intelligence
and deep learning constantly improve, this field is characterized by several
open issues. Models' interpretation is still buried under the foundations of
data driven science, thus requiring attention to the development of new
opportunities also for machine learning theories. This study proposes a machine
learning diagnosis model, based on intelligent spectrogram recognition, via
image processing. The approach is characterized by the employment of the
eigen-spectrograms and randomized linear algebra in fault diagnosis. Randomized
algebra and eigen-spectrograms enable the construction of a significant feature
space, which nonetheless emerges as a viable device to explore models'
interpretations. The computational efficiency of randomized approaches provides
reading keys of well-established statistical learning theories such as the
Support Vector Machine (SVM). Machine learning applied to spectrogram
recognition shows to be extremely accurate and efficient as compared to state
of the art results."
"The General Theory of General Intelligence: A Pragmatic Patternist
  Perspective","A multi-decade exploration into the theoretical foundations of artificial and
natural general intelligence, which has been expressed in a series of books and
papers and used to guide a series of practical and research-prototype software
systems, is reviewed at a moderate level of detail. The review covers
underlying philosophies (patternist philosophy of mind, foundational
phenomenological and logical ontology), formalizations of the concept of
intelligence, and a proposed high level architecture for AGI systems partly
driven by these formalizations and philosophies. The implementation of specific
cognitive processes such as logical reasoning, program learning, clustering and
attention allocation in the context and language of this high level
architecture is considered, as is the importance of a common (e.g. typed
metagraph based) knowledge representation for enabling ""cognitive synergy""
between the various processes. The specifics of human-like cognitive
architecture are presented as manifestations of these general principles, and
key aspects of machine consciousness and machine ethics are also treated in
this context. Lessons for practical implementation of advanced AGI in
frameworks such as OpenCog Hyperon are briefly considered."
"A Picture is Worth a Collaboration: Accumulating Design Knowledge for
  Computer-Vision-based Hybrid Intelligence Systems","Computer vision (CV) techniques try to mimic human capabilities of visual
perception to support labor-intensive and time-consuming tasks like the
recognition and localization of critical objects. Nowadays, CV increasingly
relies on artificial intelligence (AI) to automatically extract useful
information from images that can be utilized for decision support and business
process automation. However, the focus of extant research is often exclusively
on technical aspects when designing AI-based CV systems while neglecting
socio-technical facets, such as trust, control, and autonomy. For this purpose,
we consider the design of such systems from a hybrid intelligence (HI)
perspective and aim to derive prescriptive design knowledge for CV-based HI
systems. We apply a reflective, practice-inspired design science approach and
accumulate design knowledge from six comprehensive CV projects. As a result, we
identify four design-related mechanisms (i.e., automation, signaling,
modification, and collaboration) that inform our derived meta-requirements and
design principles. This can serve as a basis for further socio-technical
research on CV-based HI systems."
The Role of General Intelligence in Mathematical Reasoning,"Objects are a centerpiece of the mathematical realm and our interaction with
and reasoning about it, just as they are of the physical one (if not more). And
humans' mathematical reasoning must ultimately be grounded in our general
intelligence. Yet in contemporary cognitive science and A.I., the physical and
mathematical domains are customarily explored separately, which allows for
baking in assumptions for what objects are for the system - and missing
potential connections.
  In this paper, I put the issue into its philosophical and cognitive context.
I then describe an abstract theoretical framework for learning object
representations, that makes room for mathematical objects on par with
non-mathematical ones. Finally, I describe a case study that builds on that
view to show how our general ability for integrating different aspects of
objects effects our conception of the natural numbers."
"AIRIS: Artificial Intelligence Enhanced Signal Processing in
  Reconfigurable Intelligent Surface Communications","Reconfigurable intelligent surface (RIS) is an emerging meta-surface that can
provide additional communications links through reflecting the signals, and has
been recognized as a strong candidate of 6G mobile communications systems.
Meanwhile, it has been recently admitted that implementing artificial
intelligence (AI) into RIS communications will extensively benefit the
reconfiguration capacity and enhance the robustness to complicated transmission
environments. Besides the conventional model-driven approaches, AI can also
deal with the existing signal processing problems in a data-driven manner via
digging the inherent characteristic from the real data. Hence, AI is
particularly suitable for the signal processing problems over RIS networks
under unideal scenarios like modeling mismatching, insufficient resource,
hardware impairment, as well as dynamical transmissions. As one of the earliest
survey papers, we will introduce the merging of AI and RIS, called AIRIS, over
various signal processing topics, including environmental sensing, channel
acquisition, beamforming design, and resource scheduling, etc. We will also
discuss the challenges of AIRIS and present some interesting future directions."
Beyond 5G RIS mmWave Systems: Where Communication and Localization Meet,"Upcoming beyond fifth generation (5G) communications systems aim at further
enhancing key performance indicators and fully supporting brand new use cases
by embracing emerging techniques, e.g., reconfigurable intelligent surface
(RIS), integrated communication, localization, and sensing, and mmWave/THz
communications. The wireless intelligence empowered by state-of-the-art
artificial intelligence techniques has been widely considered at the
transceivers, and now the paradigm is deemed to be shifted to the smart control
of radio propagation environment by virtue of RISs. In this article, we argue
that to harness the full potential of RISs, localization and communication must
be tightly coupled. This is in sharp contrast to 5G and earlier generations,
where localization was a minor additional service. To support this, we first
introduce the fundamentals of RIS mmWave channel modeling, followed by RIS
channel state information acquisition and link establishment. Then, we deal
with the connection between localization and communications, from a separate
and joint perspective."
"Intelligent Decision Assistance Versus Automated Decision-Making:
  Enhancing Knowledge Work Through Explainable Artificial Intelligence","While recent advances in AI-based automated decision-making have shown many
benefits for businesses and society, they also come at a cost. It has for long
been known that a high level of automation of decisions can lead to various
drawbacks, such as automation bias and deskilling. In particular, the
deskilling of knowledge workers is a major issue, as they are the same people
who should also train, challenge and evolve AI. To address this issue, we
conceptualize a new class of DSS, namely Intelligent Decision Assistance (IDA)
based on a literature review of two different research streams -- DSS and
automation. IDA supports knowledge workers without influencing them through
automated decision-making. Specifically, we propose to use techniques of
Explainable AI (XAI) while withholding concrete AI recommendations. To test
this conceptualization, we develop hypotheses on the impacts of IDA and provide
first evidence for their validity based on empirical studies in the literature."
Intelligent metaphotonics empowered by machine learning,"In the recent years, we observe a dramatic boost of research in photonics
empowered by the concepts of machine learning and artificial intelligence. The
corresponding photonic systems, to which this new methodology is applied, can
range from traditional optical waveguides to nanoantennas and metasurfaces, and
these novel approaches underpin the fundamental principles of light-matter
interaction developed for a smart design of intelligent photonic devices.
Concepts and approaches of artificial intelligence and machine learning
penetrate rapidly into the fundamental physics of light, and they provide
effective tools for the study of the field of metaphotonics driven by
optically-induced electric and magnetic resonances. Here, we introduce this new
field with its application to metaphotonics and also present a summary of the
basic concepts of machine learning with some specific examples developed and
demonstrated for metasystems and metasurfaces."
"Towards Safe, Explainable, and Regulated Autonomous Driving","There has been recent and growing interest in the development and deployment
of autonomous vehicles, encouraged by the empirical successes of powerful
artificial intelligence techniques (AI), especially in the applications of deep
learning and reinforcement learning. However, as demonstrated by recent traffic
accidents, autonomous driving technology is not fully reliable for safe
deployment. As AI is the main technology behind the intelligent navigation
systems of self-driving vehicles, both the stakeholders and transportation
regulators require their AI-driven software architecture to be safe,
explainable, and regulatory compliant. In this paper, we propose a design
framework that integrates autonomous control, explainable AI (XAI), and
regulatory compliance to address this issue, and then provide an initial
validation of the framework with a critical analysis in a case study. Moreover,
we describe relevant XAI approaches that can help achieve the goals of the
framework."
"IoT-based Route Recommendation for an Intelligent Waste Management
  System","The Internet of Things (IoT) is a paradigm characterized by a network of
embedded sensors and services. These sensors are incorporated to collect
various information, track physical conditions, e.g., waste bins' status, and
exchange data with different centralized platforms. The need for such sensors
is increasing; however, proliferation of technologies comes with various
challenges. For example, how can IoT and its associated data be used to enhance
waste management? In smart cities, an efficient waste management system is
crucial. Artificial Intelligence (AI) and IoT-enabled approaches can empower
cities to manage the waste collection. This work proposes an intelligent
approach to route recommendation in an IoT-enabled waste management system
given spatial constraints. It performs a thorough analysis based on AI-based
methods and compares their corresponding results. Our solution is based on a
multiple-level decision-making process in which bins' status and coordinates
are taken into account to address the routing problem. Such AI-based models can
help engineers design a sustainable infrastructure system."
"A conceptual framework of Intelligent Management Control System for
  Higher Education","The utilization of management control systems in university management poses
a considerable challenge because university's strategic goals are not identical
to those applied in profit-oriented management. A university's management
control system should take into account the processing of management
information for management purposes, allowing for the relationships between
different groups of stakeholders. The specificity of the university operation
assumes conducting long-term scientific research and educational programmes.
Therefore, the controlling approach to university management should considerat
long-term performance measurement as well as management in key areas such as
research, provision of education to students, and interaction with the tertiary
institution's socioeconomic environment.This paper aims to develop a conceptual
framework of the Intelligent Management Control System for Higher Education
(IMCSHE) based on cognitive agents. The main findings are related to developing
the assumption, model, and technological basis including the artificial
intelligence method."
"Bridging the Urban-Rural Connectivity Gap through Intelligent Space,
  Air, and Ground Networks","Connectivity in rural areas is one of the main challenges of communication
networks. To overcome this challenge, a variety of solutions for different
situations are required. Optimizing the current networking paradigms is
therefore mandatory. The high costs of infrastructure and the low revenue of
cell sites in rural areas compared with urban areas are especially unattractive
for telecommunication operators. Therefore, space, air, and ground networks
should all be optimized for achieving connectivity in rural areas. We highlight
the latest works on rural connectivity, discuss the solutions for terrestrial
networks, and study the potential benefits of nonterrestrial networks.
Furthermore, we present an overview of artificial intelligence (AI) techniques
for improving space, air, and ground networks, hence improving connectivity in
rural areas. AI enables intelligent communications and can integrate space,
air, and ground networks for rural connectivity. We discuss the rural
connectivity challenges and highlight the latest projects and research and the
empowerment of networks using AI. Finally, we discuss the potential positive
impacts of providing connectivity to rural communities."
Recent Advances and New Frontiers in Spiking Neural Networks,"In recent years, spiking neural networks (SNNs) have received extensive
attention in brain-inspired intelligence due to their rich spatially-temporal
dynamics, various encoding methods, and event-driven characteristics that
naturally fit the neuromorphic hardware. With the development of SNNs,
brain-inspired intelligence, an emerging research field inspired by brain
science achievements and aiming at artificial general intelligence, is becoming
hot. This paper reviews recent advances and discusses new frontiers in SNNs
from five major research topics, including essential elements (i.e., spiking
neuron models, encoding methods, and topology structures), neuromorphic
datasets, optimization algorithms, software, and hardware frameworks. We hope
our survey can help researchers understand SNNs better and inspire new works to
advance this field."
A Theory of Natural Intelligence,"Introduction: In contrast to current AI technology, natural intelligence --
the kind of autonomous intelligence that is realized in the brains of animals
and humans to attain in their natural environment goals defined by a repertoire
of innate behavioral schemata -- is far superior in terms of learning speed,
generalization capabilities, autonomy and creativity. How are these strengths,
by what means are ideas and imagination produced in natural neural networks?
  Methods: Reviewing the literature, we put forward the argument that both our
natural environment and the brain are of low complexity, that is, require for
their generation very little information and are consequently both highly
structured. We further argue that the structures of brain and natural
environment are closely related.
  Results: We propose that the structural regularity of the brain takes the
form of net fragments (self-organized network patterns) and that these serve as
the powerful inductive bias that enables the brain to learn quickly, generalize
from few examples and bridge the gap between abstractly defined general goals
and concrete situations.
  Conclusions: Our results have important bearings on open problems in
artificial neural network research."
Hybrid Intelligent Testing in Simulation-Based Verification,"Efficient and effective testing for simulation-based hardware verification is
challenging. Using constrained random test generation, several millions of
tests may be required to achieve coverage goals. The vast majority of tests do
not contribute to coverage progress, yet they consume verification resources.
In this paper, we propose a hybrid intelligent testing approach combining two
methods that have previously been treated separately, namely Coverage-Directed
Test Selection and Novelty-Driven Verification. Coverage-Directed Test
Selection learns from coverage feedback to bias testing toward the most
effective tests. Novelty-Driven Verification learns to identify and simulate
stimuli that differ from previous stimuli, thereby reducing the number of
simulations and increasing testing efficiency. We discuss the strengths and
limitations of each method, and we show how our approach addresses each
method's limitations, leading to hardware testing that is both efficient and
effective."
On Avoiding Power-Seeking by Artificial Intelligence,"We do not know how to align a very intelligent AI agent's behavior with human
interests. I investigate whether -- absent a full solution to this AI alignment
problem -- we can build smart AI agents which have limited impact on the world,
and which do not autonomously seek power. In this thesis, I introduce the
attainable utility preservation (AUP) method. I demonstrate that AUP produces
conservative, option-preserving behavior within toy gridworlds and within
complex environments based off of Conway's Game of Life. I formalize the
problem of side effect avoidance, which provides a way to quantify the side
effects an agent had on the world. I also give a formal definition of
power-seeking in the context of AI agents and show that optimal policies tend
to seek power. In particular, most reward functions have optimal policies which
avoid deactivation. This is a problem if we want to deactivate or correct an
intelligent agent after we have deployed it. My theorems suggest that since
most agent goals conflict with ours, the agent would very probably resist
correction. I extend these theorems to show that power-seeking incentives occur
not just for optimal decision-makers, but under a wide range of decision-making
procedures."
"Reconfigurable Intelligent Surface-assisted Classification of
  Modulations using Deep Learning","The fifth generating (5G) of wireless networks will be more adaptive and
heterogeneous. Reconfigurable intelligent surface technology enables the 5G to
work on multistrand waveforms. However, in such a dynamic network, the
identification of specific modulation types is of paramount importance. We
present a RIS-assisted digital classification method based on artificial
intelligence. We train a convolutional neural network to classify digital
modulations. The proposed method operates and learns features directly on the
received signal without feature extraction. The features learned by the
convolutional neural network are presented and analyzed. Furthermore, the
robust features of the received signals at a specific SNR range are studied.
The accuracy of the proposed classification method is found to be remarkable,
particularly for low levels of SNR."
"Actor-Critic Network for O-RAN Resource Allocation: xApp Design,
  Deployment, and Analysis","Open Radio Access Network (O-RAN) has introduced an emerging RAN architecture
that enables openness, intelligence, and automated control. The RAN Intelligent
Controller (RIC) provides the platform to design and deploy RAN controllers.
xApps are the applications which will take this responsibility by leveraging
machine learning (ML) algorithms and acting in near-real time. Despite the
opportunities provided by this new architecture, the progress of practical
artificial intelligence (AI)-based solutions for network control and automation
has been slow. This is mostly because of the lack of an endto-end solution for
designing, deploying, and testing AI-based xApps fully executable in real O-RAN
network. In this paper we introduce an end-to-end O-RAN design and evaluation
procedure and provide a detailed discussion of developing a Reinforcement
Learning (RL) based xApp by using two different RL approaches and considering
the latest released O-RAN architecture and interfaces."
"Distributed Swarm Learning for Internet of Things at the Edge: Where
  Artificial Intelligence Meets Biological Intelligence","With the proliferation of versatile Internet of Things (IoT) services, smart
IoT devices are increasingly deployed at the edge of wireless networks to
perform collaborative machine learning tasks using locally collected data,
giving rise to the edge learning paradigm. Due to device restrictions and
resource constraints, edge learning among massive IoT devices faces major
technical challenges caused by the communication bottleneck, data and device
heterogeneity, non-convex optimization, privacy and security concerns, and
dynamic environments. To overcome these challenges, this article studies a new
framework of distributed swarm learning (DSL) through a holistic integration of
artificial intelligence and biological swarm intelligence. Leveraging efficient
and robust signal processing and communication techniques, DSL contributes to
novel tools for learning and optimization tailored for real-time operations of
large-scale IoT in edge wireless environments, which will benefit a wide range
of edge IoT applications."
General Intelligence Requires Rethinking Exploration,"We are at the cusp of a transition from ""learning from data"" to ""learning
what data to learn from"" as a central focus of artificial intelligence (AI)
research. While the first-order learning problem is not completely solved,
large models under unified architectures, such as transformers, have shifted
the learning bottleneck from how to effectively train our models to how to
effectively acquire and use task-relevant data. This problem, which we frame as
exploration, is a universal aspect of learning in open-ended domains, such as
the real world. Although the study of exploration in AI is largely limited to
the field of reinforcement learning, we argue that exploration is essential to
all learning systems, including supervised learning. We propose the problem of
generalized exploration to conceptually unify exploration-driven learning
between supervised learning and reinforcement learning, allowing us to
highlight key similarities across learning settings and open research
challenges. Importantly, generalized exploration serves as a necessary
objective for maintaining open-ended learning processes, which in continually
learning to discover and solve new problems, provides a promising path to more
general intelligence."
A Review of Intelligent Music Generation Systems,"With the introduction of ChatGPT, the public's perception of AI-generated
content (AIGC) has begun to reshape. Artificial intelligence has significantly
reduced the barrier to entry for non-professionals in creative endeavors,
enhancing the efficiency of content creation. Recent advancements have seen
significant improvements in the quality of symbolic music generation, which is
enabled by the use of modern generative algorithms to extract patterns implicit
in a piece of music based on rule constraints or a musical corpus.
Nevertheless, existing literature reviews tend to present a conventional and
conservative perspective on future development trajectories, with a notable
absence of thorough benchmarking of generative models. This paper provides a
survey and analysis of recent intelligent music generation techniques,
outlining their respective characteristics and discussing existing methods for
evaluation. Additionally, the paper compares the different characteristics of
music generation techniques in the East and West as well as analysing the
field's development prospects."
A Hierarchical Framework for Collaborative Artificial Intelligence,"We propose a hierarchical framework for collaborative intelligent systems.
This framework organizes research challenges based on the nature of the
collaborative activity and the information that must be shared, with each level
building on capabilities provided by lower levels. We review research paradigms
at each level, with a description of classical engineering-based approaches and
modern alternatives based on machine learning, illustrated with a running
example using a hypothetical personal service robot. We discuss cross-cutting
issues that occur at all levels, focusing on the problem of communicating and
sharing comprehension, the role of explanation and the social nature of
collaboration. We conclude with a summary of research challenges and a
discussion of the potential for economic and societal impact provided by
technologies that enhance human abilities and empower people and society
through collaboration with Intelligent Systems."
"Thrill-K Architecture: Towards a Solution to the Problem of Knowledge
  Based Understanding","While end-to-end learning systems are rapidly gaining capabilities and
popularity, the increasing computational demands for deploying such systems,
along with a lack of flexibility, adaptability, explainability, reasoning and
verification capabilities, require new types of architectures. Here we
introduce a classification of hybrid systems which, based on an analysis of
human knowledge and intelligence, combines neural learning with various types
of knowledge and knowledge sources. We present the Thrill-K architecture as a
prototypical solution for integrating instantaneous knowledge, standby
knowledge and external knowledge sources in a framework capable of inference,
learning and intelligent control."
Deep Learning Systems for Advanced Driving Assistance,"Next generation cars embed intelligent assessment of car driving safety
through innovative solutions often based on usage of artificial intelligence.
The safety driving monitoring can be carried out using several methodologies
widely treated in scientific literature. In this context, the author proposes
an innovative approach that uses ad-hoc bio-sensing system suitable to
reconstruct the physio-based attentional status of the car driver. To
reconstruct the car driver physiological status, the author proposed the use of
a bio-sensing probe consisting of a coupled LEDs at Near infrared (NiR)
spectrum with a photodetector. This probe placed over the monitored subject
allows to detect a physiological signal called PhotoPlethysmoGraphy (PPG). The
PPG signal formation is regulated by the change in oxygenated and
non-oxygenated hemoglobin concentration in the monitored subject bloodstream
which will be directly connected to cardiac activity in turn regulated by the
Autonomic Nervous System (ANS) that characterizes the subject's attention
level. This so designed car driver drowsiness monitoring will be combined with
further driving safety assessment based on correlated intelligent driving
scenario understanding."
"Leveraging Deep Reinforcement Learning for Metacognitive Interventions
  across Intelligent Tutoring Systems","This work compares two approaches to provide metacognitive interventions and
their impact on preparing students for future learning across Intelligent
Tutoring Systems (ITSs). In two consecutive semesters, we conducted two
classroom experiments: Exp. 1 used a classic artificial intelligence approach
to classify students into different metacognitive groups and provide static
interventions based on their classified groups. In Exp. 2, we leveraged Deep
Reinforcement Learning (DRL) to provide adaptive interventions that consider
the dynamic changes in the student's metacognitive levels. In both experiments,
students received these interventions that taught how and when to use a
backward-chaining (BC) strategy on a logic tutor that supports a default
forward-chaining strategy. Six weeks later, we trained students on a
probability tutor that only supports BC without interventions. Our results show
that adaptive DRL-based interventions closed the metacognitive skills gap
between students. In contrast, static classifier-based interventions only
benefited a subset of students who knew how to use BC in advance. Additionally,
our DRL agent prepared the experimental students for future learning by
significantly surpassing their control peers on both ITSs."
The Roles of Symbols in Neural-based AI: They are Not What You Think!,"We propose that symbols are first and foremost external communication tools
used between intelligent agents that allow knowledge to be transferred in a
more efficient and effective manner than having to experience the world
directly. But, they are also used internally within an agent through a form of
self-communication to help formulate, describe and justify subsymbolic patterns
of neural activity that truly implement thinking. Symbols, and our languages
that make use of them, not only allow us to explain our thinking to others and
ourselves, but also provide beneficial constraints (inductive bias) on learning
about the world. In this paper we present relevant insights from neuroscience
and cognitive science, about how the human brain represents symbols and the
concepts they refer to, and how today's artificial neural networks can do the
same. We then present a novel neuro-symbolic hypothesis and a plausible
architecture for intelligent agents that combines subsymbolic representations
for symbols and concepts for learning and reasoning. Our hypothesis and
associated architecture imply that symbols will remain critical to the future
of intelligent systems NOT because they are the fundamental building blocks of
thought, but because they are characterizations of subsymbolic processes that
constitute thought."
Self-Aware Trajectory Prediction for Safe Autonomous Driving,"Trajectory prediction is one of the key components of the autonomous driving
software stack. Accurate prediction for the future movement of surrounding
traffic participants is an important prerequisite for ensuring the driving
efficiency and safety of intelligent vehicles. Trajectory prediction algorithms
based on artificial intelligence have been widely studied and applied in recent
years and have achieved remarkable results. However, complex artificial
intelligence models are uncertain and difficult to explain, so they may face
unintended failures when applied in the real world. In this paper, a self-aware
trajectory prediction method is proposed. By introducing a self-awareness
module and a two-stage training process, the original trajectory prediction
module's performance is estimated online, to facilitate the system to deal with
the possible scenario of insufficient prediction function in time, and create
conditions for the realization of safe and reliable autonomous driving.
Comprehensive experiments and analysis are performed, and the proposed method
performed well in terms of self-awareness, memory footprint, and real-time
performance, showing that it may serve as a promising paradigm for safe
autonomous driving."
Codesign of Edge Intelligence and Automated Guided Vehicle Control,"This work presents a harmonic design of autonomous guided vehicle (AGV)
control, edge intelligence, and human input to enable autonomous transportation
in industrial environments. The AGV has the capability to navigate between a
source and destinations and pick/place objects. The human input implicitly
provides preferences of the destination and exact drop point, which are derived
from an artificial intelligence (AI) module at the network edge and shared with
the AGV over a wireless network. The demonstration indicates that the proposed
integrated design of hardware, software, and AI design achieve a technology
readiness level (TRL) of range 4-5"
TestLab: An Intelligent Automated Software Testing Framework,"The prevalence of software systems has become an integral part of modern-day
living. Software usage has increased significantly, leading to its growth in
both size and complexity. Consequently, software development is becoming a more
time-consuming process. In an attempt to accelerate the development cycle, the
testing phase is often neglected, leading to the deployment of flawed systems
that can have significant implications on the users daily activities. This work
presents TestLab, an intelligent automated software testing framework that
attempts to gather a set of testing methods and automate them using Artificial
Intelligence to allow continuous testing of software systems at multiple levels
from different scopes, ranging from developers to end-users. The tool consists
of three modules, each serving a distinct purpose. The first two modules aim to
identify vulnerabilities from different perspectives, while the third module
enhances traditional automated software testing by automatically generating
test cases through source code analysis."
Explaining Autonomous Driving Actions with Visual Question Answering,"The end-to-end learning ability of self-driving vehicles has achieved
significant milestones over the last decade owing to rapid advances in deep
learning and computer vision algorithms. However, as autonomous driving
technology is a safety-critical application of artificial intelligence (AI),
road accidents and established regulatory principles necessitate the need for
the explainability of intelligent action choices for self-driving vehicles. To
facilitate interpretability of decision-making in autonomous driving, we
present a Visual Question Answering (VQA) framework, which explains driving
actions with question-answering-based causal reasoning. To do so, we first
collect driving videos in a simulation environment using reinforcement learning
(RL) and extract consecutive frames from this log data uniformly for five
selected action categories. Further, we manually annotate the extracted frames
using question-answer pairs as justifications for the actions chosen in each
scenario. Finally, we evaluate the correctness of the VQA-predicted answers for
actions on unseen driving scenes. The empirical results suggest that the VQA
mechanism can provide support to interpret real-time decisions of autonomous
vehicles and help enhance overall driving safety."
Mortal Computation: A Foundation for Biomimetic Intelligence,"This review motivates and synthesizes research efforts in
neuroscience-inspired artificial intelligence and biomimetic computing in terms
of mortal computation. Specifically, we characterize the notion of mortality by
recasting ideas in biophysics, cybernetics, and cognitive science in terms of a
theoretical foundation for sentient behavior. We frame the mortal computation
thesis through the Markov blanket formalism and the circular causality entailed
by inference, learning, and selection. The ensuing framework -- underwritten by
the free energy principle -- could prove useful for guiding the construction of
unconventional connectionist computational systems, neuromorphic intelligence,
and chimeric agents, including sentient organoids, which stand to revolutionize
the long-term future of embodied, enactive artificial intelligence and
cognition research."
"Distributed AI in Zero-touch Provisioning for Edge Networks: Challenges
  and Research Directions","Zero-touch network is anticipated to inaugurate the generation of intelligent
and highly flexible resource provisioning strategies where multiple service
providers collaboratively offer computation and storage resources. This
transformation presents substantial challenges to network administration and
service providers regarding sustainability and scalability. This article
combines Distributed Artificial Intelligence (DAI) with Zero-touch Provisioning
(ZTP) for edge networks. This combination helps to manage network devices
seamlessly and intelligently by minimizing human intervention. In addition,
several advantages are also highlighted that come with incorporating
Distributed AI into ZTP in the context of edge networks. Further, we draw
potential research directions to foster novel studies in this field and
overcome the current limitations."
"Evacuation Management Framework towards Smart City-wide Intelligent
  Emergency Interactive Response System","A smart city solution toward future 6G network deployment allows small and
medium sized enterprises (SMEs), industry, and government entities to connect
with the infrastructures and play a crucial role in enhancing emergency
preparedness with advanced sensors. The objective of this work is to propose a
set of coordinated technological solutions to transform an existing emergency
response system into an intelligent interactive system, thereby improving the
public services and the quality of life for residents at home, on road, in
hospitals, transport hubs, etc. In this context, we consider a city wide view
from three different application scenes that are closely related to peoples
daily life, to optimize the actions taken at relevant departments. Therefore,
using artificial intelligence (AI) and machine learning (ML) techniques to
enable the next generation connected vehicle experiences, we specifically focus
on accidents happening in indoor households, urban roads, and at large public
facilities. This smart interactive response system will benefit from advanced
sensor fusion and AI by formulating a real time dynamic model."
On Predictive planning and counterfactual learning in active inference,"Given the rapid advancement of artificial intelligence, understanding the
foundations of intelligent behaviour is increasingly important. Active
inference, regarded as a general theory of behaviour, offers a principled
approach to probing the basis of sophistication in planning and
decision-making. In this paper, we examine two decision-making schemes in
active inference based on 'planning' and 'learning from experience'.
Furthermore, we also introduce a mixed model that navigates the data-complexity
trade-off between these strategies, leveraging the strengths of both to
facilitate balanced decision-making. We evaluate our proposed model in a
challenging grid-world scenario that requires adaptability from the agent.
Additionally, our model provides the opportunity to analyze the evolution of
various parameters, offering valuable insights and contributing to an
explainable framework for intelligent decision-making."
Analyzing the Roles of Language and Vision in Learning from Limited Data,"Does language help make sense of the visual world? How important is it to
actually see the world rather than having it described with words? These basic
questions about the nature of intelligence have been difficult to answer
because we only had one example of an intelligent system -- humans -- and
limited access to cases that isolated language or vision. However, the
development of sophisticated Vision-Language Models (VLMs) by artificial
intelligence researchers offers us new opportunities to explore the
contributions that language and vision make to learning about the world. We
ablate components from the cognitive architecture of these models to identify
their contributions to learning new tasks from limited data. We find that a
language model leveraging all components recovers a majority of a VLM's
performance, despite its lack of visual input, and that language seems to allow
this by providing access to prior knowledge and reasoning."
Visual Knowledge in the Big Model Era: Retrospect and Prospect,"Visual knowledge is a new form of knowledge representation that can
encapsulate visual concepts and their relations in a succinct, comprehensive,
and interpretable manner, with a deep root in cognitive psychology. As the
knowledge about the visual world has been identified as an indispensable
component of human cognition and intelligence, visual knowledge is poised to
have a pivotal role in establishing machine intelligence. With the recent
advance of Artificial Intelligence (AI) techniques, large AI models (or
foundation models) have emerged as a potent tool capable of extracting
versatile patterns from broad data as implicit knowledge, and abstracting them
into an outrageous amount of numeric parameters. To pave the way for creating
visual knowledge empowered AI machines in this coming wave, we present a timely
review that investigates the origins and development of visual knowledge in the
pre-big model era, and accentuates the opportunities and unique role of visual
knowledge in the big model era."
Intelligent Chemical Purification Technique Based on Machine Learning,"We present an innovative of artificial intelligence with column
chromatography, aiming to resolve inefficiencies and standardize data
collection in chemical separation and purification domain. By developing an
automated platform for precise data acquisition and employing advanced machine
learning algorithms, we constructed predictive models to forecast key
separation parameters, thereby enhancing the efficiency and quality of
chromatographic processes. The application of transfer learning allows the
model to adapt across various column specifications, broadening its utility. A
novel metric, separation probability ($S_p$), quantifies the likelihood of
effective compound separation, validated through experimental verification.
This study signifies a significant step forward int the application of AI in
chemical research, offering a scalable solution to traditional chromatography
challenges and providing a foundation for future technological advancements in
chemical analysis and purification."
Making AI Intelligible: Philosophical Foundations,"Can humans and artificial intelligences share concepts and communicate?
'Making AI Intelligible' shows that philosophical work on the metaphysics of
meaning can help answer these questions. Herman Cappelen and Josh Dever use the
externalist tradition in philosophy to create models of how AIs and humans can
understand each other. In doing so, they illustrate ways in which that
philosophical tradition can be improved.
  The questions addressed in the book are not only theoretically interesting,
but the answers have pressing practical implications. Many important decisions
about human life are now influenced by AI. In giving that power to AI, we
presuppose that AIs can track features of the world that we care about (for
example, creditworthiness, recidivism, cancer, and combatants). If AIs can
share our concepts, that will go some way towards justifying this reliance on
AI. This ground-breaking study offers insight into how to take some first steps
towards achieving Interpretable AI."
"Simulation of Neural Responses to Classical Music Using Organoid
  Intelligence Methods","Music is a complex auditory stimulus capable of eliciting significant changes
in brain activity, influencing cognitive processes such as memory, attention,
and emotional regulation. However, the underlying mechanisms of music-induced
cognitive processes remain largely unknown. Organoid intelligence and deep
learning models show promise for simulating and analyzing these neural
responses to classical music, an area significantly unexplored in computational
neuroscience. Hence, we present the PyOrganoid library, an innovative tool that
facilitates the simulation of organoid learning models, integrating
sophisticated machine learning techniques with biologically inspired organoid
simulations. Our study features the development of the Pianoid model, a ""deep
organoid learning"" model that utilizes a Bidirectional LSTM network to predict
EEG responses based on audio features from classical music recordings. This
model demonstrates the feasibility of using computational methods to replicate
complex neural processes, providing valuable insights into music perception and
cognition. Likewise, our findings emphasize the utility of synthetic models in
neuroscience research and highlight the PyOrganoid library's potential as a
versatile tool for advancing studies in neuroscience and artificial
intelligence."
"Mastering the Digital Art of War: Developing Intelligent Combat
  Simulation Agents for Wargaming Using Hierarchical Reinforcement Learning","In today's rapidly evolving military landscape, advancing artificial
intelligence (AI) in support of wargaming becomes essential. Despite
reinforcement learning (RL) showing promise for developing intelligent agents,
conventional RL faces limitations in handling the complexity inherent in combat
simulations. This dissertation proposes a comprehensive approach, including
targeted observation abstractions, multi-model integration, a hybrid AI
framework, and an overarching hierarchical reinforcement learning (HRL)
framework. Our localized observation abstraction using piecewise linear spatial
decay simplifies the RL problem, enhancing computational efficiency and
demonstrating superior efficacy over traditional global observation methods.
Our multi-model framework combines various AI methodologies, optimizing
performance while still enabling the use of diverse, specialized individual
behavior models. Our hybrid AI framework synergizes RL with scripted agents,
leveraging RL for high-level decisions and scripted agents for lower-level
tasks, enhancing adaptability, reliability, and performance. Our HRL
architecture and training framework decomposes complex problems into manageable
subproblems, aligning with military decision-making structures. Although
initial tests did not show improved performance, insights were gained to
improve future iterations. This study underscores AI's potential to
revolutionize wargaming, emphasizing the need for continued research in this
domain."
Passed the Turing Test: Living in Turing Futures,"The world has seen the emergence of machines based on pretrained models,
transformers, also known as generative artificial intelligences for their
ability to produce various types of content, including text, images, audio, and
synthetic data. Without resorting to preprogramming or special tricks, their
intelligence grows as they learn from experience, and to ordinary people, they
can appear human-like in conversation. This means that they can pass the Turing
test, and that we are now living in one of many possible Turing futures where
machines can pass for what they are not. However, the learning machines that
Turing imagined would pass his imitation tests were machines inspired by the
natural development of the low-energy human cortex. They would be raised like
human children and naturally learn the ability to deceive an observer. These
``child machines,'' Turing hoped, would be powerful enough to have an impact on
society and nature."
"Towards Hybrid Intelligence in Journalism: Findings and Lessons Learnt
  from a Collaborative Analysis of Greek Political Rhetoric by ChatGPT and
  Humans","This chapter introduces a research project titled ""Analyzing the Political
Discourse: A Collaboration Between Humans and Artificial Intelligence"", which
was initiated in preparation for Greece's 2023 general elections. The project
focused on the analysis of political leaders' campaign speeches, employing
Artificial Intelligence (AI), in conjunction with an interdisciplinary team
comprising journalists, a political scientist, and data scientists. The chapter
delves into various aspects of political discourse analysis, including
sentiment analysis, polarization, populism, topic detection, and Named Entities
Recognition (NER). This experimental study investigates the capabilities of
large language model (LLMs), and in particular OpenAI's ChatGPT, for analyzing
political speech, evaluates its strengths and weaknesses, and highlights the
essential role of human oversight in using AI in journalism projects and
potentially other societal sectors. The project stands as an innovative example
of human-AI collaboration (known also as ""hybrid intelligence"") within the
realm of digital humanities, offering valuable insights for future initiatives."
"Is artificial intelligence still intelligence? LLMs generalize to novel
  adjective-noun pairs, but don't mimic the full human distribution","Inferences from adjective-noun combinations like ""Is artificial intelligence
still intelligence?"" provide a good test bed for LLMs' understanding of meaning
and compositional generalization capability, since there are many combinations
which are novel to both humans and LLMs but nevertheless elicit convergent
human judgments. We study a range of LLMs and find that the largest models we
tested are able to draw human-like inferences when the inference is determined
by context and can generalize to unseen adjective-noun combinations. We also
propose three methods to evaluate LLMs on these inferences out of context,
where there is a distribution of human-like answers rather than a single
correct answer. We find that LLMs show a human-like distribution on at most
75\% of our dataset, which is promising but still leaves room for improvement."
"Large Language Model-assisted Speech and Pointing Benefits Multiple 3D
  Object Selection in Virtual Reality","Selection of occluded objects is a challenging problem in virtual reality,
even more so if multiple objects are involved. With the advent of new
artificial intelligence technologies, we explore the possibility of leveraging
large language models to assist multi-object selection tasks in virtual reality
via a multimodal speech and raycast interaction technique. We validate the
findings in a comparative user study (n=24), where participants selected target
objects in a virtual reality scene with different levels of scene perplexity.
The performance metrics and user experience metrics are compared against a
mini-map based occluded object selection technique that serves as the baseline.
Results indicate that the introduced technique, AssistVR, outperforms the
baseline technique when there are multiple target objects. Contrary to the
common belief for speech interfaces, AssistVR was able to outperform the
baseline even when the target objects were difficult to reference verbally.
This work demonstrates the viability and interaction potential of an
intelligent multimodal interactive system powered by large laguage models.
Based on the results, we discuss the implications for design of future
intelligent multimodal interactive systems in immersive environments."
"Combining Theory of Mind and Kindness for Self-Supervised Human-AI
  Alignment","As artificial intelligence (AI) becomes deeply integrated into critical
infrastructures and everyday life, ensuring its safe deployment is one of
humanity's most urgent challenges. Current AI models prioritize task
optimization over safety, leading to risks of unintended harm. These risks are
difficult to address due to the competing interests of governments, businesses,
and advocacy groups, all of which have different priorities in the AI race.
Current alignment methods, such as reinforcement learning from human feedback
(RLHF), focus on extrinsic behaviors without instilling a genuine understanding
of human values. These models are vulnerable to manipulation and lack the
social intelligence necessary to infer the mental states and intentions of
others, raising concerns about their ability to safely and responsibly make
important decisions in complex and novel situations. Furthermore, the
divergence between extrinsic and intrinsic motivations in AI introduces the
risk of deceptive or harmful behaviors, particularly as systems become more
autonomous and intelligent. We propose a novel human-inspired approach which
aims to address these various concerns and help align competing objectives."
"Large Language Models and Artificial Intelligence Generated Content
  Technologies Meet Communication Networks","Artificial intelligence generated content (AIGC) technologies, with a
predominance of large language models (LLMs), have demonstrated remarkable
performance improvements in various applications, which have attracted great
interests from both academia and industry. Although some noteworthy
advancements have been made in this area, a comprehensive exploration of the
intricate relationship between AIGC and communication networks remains
relatively limited. To address this issue, this paper conducts an exhaustive
survey from dual standpoints: firstly, it scrutinizes the integration of LLMs
and AIGC technologies within the domain of communication networks; secondly, it
investigates how the communication networks can further bolster the
capabilities of LLMs and AIGC. Additionally, this research explores the
promising applications along with the challenges encountered during the
incorporation of these AI technologies into communication networks. Through
these detailed analyses, our work aims to deepen the understanding of how LLMs
and AIGC can synergize with and enhance the development of advanced intelligent
communication networks, contributing to a more profound comprehension of
next-generation intelligent communication networks."
"In Search of Extraterrestrial Artificial Intelligence Through Dyson
  Sphere-like structures around Primordial Black Holes","Are we alone? It is a compelling question that human beings have confronted
for centuries. The search for extraterrestrial life is a broad range of quests
for finding simple forms of life up to intelligent beings in the Universe. The
plausible assumption is that there is a chance that intelligent life will be
followed by advanced civilization equipped or even dominated by artificial
intelligence (AI). In this work, we categorize advanced civilizations (on an
equal footing, an AI-dominated civilization) on the Kardashev scale. We propose
a new scale known as the space exploration distance to measure civilization
advancement. We propose a relation between this length and the Kardashev scale.
Then, we suggest the idea that advanced civilizations will use primordial black
holes (PBHs) as sources of harvesting energy. We calculate the energy harvested
by calculating the space exploration distance. Finally, we propose an
observational method to detect the possibility of extraterrestrial AI using
Dyson sphere-like structures around PBHs in the Milky Way and other galaxies."
Brain-inspired AI Agent: The Way Towards AGI,"Artificial General Intelligence (AGI), widely regarded as the fundamental
goal of artificial intelligence, represents the realization of cognitive
capabilities that enable the handling of general tasks with human-like
proficiency. Researchers in brain-inspired AI seek inspiration from the
operational mechanisms of the human brain, aiming to replicate its functional
rules in intelligent models. Moreover, with the rapid development of
large-scale models in recent years, the concept of agents has garnered
increasing attention, with researchers widely recognizing it as a necessary
pathway toward achieving AGI. In this article, we propose the concept of a
brain-inspired AI agent and analyze how to extract relatively feasible and
agent-compatible cortical region functionalities and their associated
functional connectivity networks from the complex mechanisms of the human
brain. Implementing these structures within an agent enables it to achieve
basic cognitive intelligence akin to human capabilities. Finally, we explore
the limitations and challenges for realizing brain-inspired agents and discuss
their future development."
"MACI: Multi-Agent Collaborative Intelligence for Adaptive Reasoning and
  Temporal Planning","Artificial intelligence requires deliberate reasoning, temporal awareness,
and effective constraint management, capabilities traditional LLMs often lack
due to their reliance on pattern matching, limited self-verification, and
inconsistent constraint handling. We introduce Multi-Agent Collaborative
Intelligence (MACI), a framework comprising three key components: 1) a
meta-planner (MP) that identifies, formulates, and refines all roles and
constraints of a task (e.g., wedding planning) while generating a dependency
graph, with common-sense augmentation to ensure realistic and practical
constraints; 2) a collection of agents to facilitate planning and address
task-specific requirements; and 3) a run-time monitor that manages plan
adjustments as needed. By decoupling planning from validation, maintaining
minimal agent context, and integrating common-sense reasoning, MACI overcomes
the aforementioned limitations and demonstrates robust performance in two
scheduling problems."
"Enhancing Collective Intelligence in Large Language Models Through
  Emotional Integration","This research investigates the integration of emotional diversity into Large
Language Models (LLMs) to enhance collective intelligence. Inspired by the
human wisdom of crowds phenomenon, where group decisions often outperform
individual judgments, we fine-tuned the DarkIdol-Llama-3.1-8B model using
Google's GoEmotions dataset and Low-Rank Adaptation (LoRA) to simulate
emotionally diverse responses. Evaluating the model on a distance estimation
task between Fargo, ND, and Seattle, WA, across 15,064 unique persona
configurations, we analyzed how emotional states and social attributes
influence decision-making. Our findings demonstrate that emotional integration
shapes response patterns while maintaining acceptable prediction accuracy,
revealing its potential to enhance artificial collective intelligence. This
study provides valuable insights into the interplay of emotional diversity and
decision-making in LLMs, suggesting pathways for creating emotionally aware AI
systems that balance emotional depth with analytical precision."
"General-Purpose Aerial Intelligent Agents Empowered by Large Language
  Models","The emergence of large language models (LLMs) opens new frontiers for
unmanned aerial vehicle (UAVs), yet existing systems remain confined to
predefined tasks due to hardware-software co-design challenges. This paper
presents the first aerial intelligent agent capable of open-world task
execution through tight integration of LLM-based reasoning and robotic
autonomy. Our hardware-software co-designed system addresses two fundamental
limitations: (1) Onboard LLM operation via an edge-optimized computing
platform, achieving 5-6 tokens/sec inference for 14B-parameter models at 220W
peak power; (2) A bidirectional cognitive architecture that synergizes slow
deliberative planning (LLM task planning) with fast reactive control (state
estimation, mapping, obstacle avoidance, and motion planning). Validated
through preliminary results using our prototype, the system demonstrates
reliable task planning and scene understanding in communication-constrained
environments, such as sugarcane monitoring, power grid inspection, mine tunnel
exploration, and biological observation applications. This work establishes a
novel framework for embodied aerial artificial intelligence, bridging the gap
between task planning and robotic autonomy in open environments."
"FedMM-X: A Trustworthy and Interpretable Framework for Federated
  Multi-Modal Learning in Dynamic Environments","As artificial intelligence systems increasingly operate in Real-world
environments, the integration of multi-modal data sources such as vision,
language, and audio presents both unprecedented opportunities and critical
challenges for achieving trustworthy intelligence. In this paper, we propose a
novel framework that unifies federated learning with explainable multi-modal
reasoning to ensure trustworthiness in decentralized, dynamic settings. Our
approach, called FedMM-X (Federated Multi-Modal Explainable Intelligence),
leverages cross-modal consistency checks, client-level interpretability
mechanisms, and dynamic trust calibration to address challenges posed by data
heterogeneity, modality imbalance, and out-of-distribution generalization.
Through rigorous evaluation across federated multi-modal benchmarks involving
vision-language tasks, we demonstrate improved performance in both accuracy and
interpretability while reducing vulnerabilities to adversarial and spurious
correlations. Further, we introduce a novel trust score aggregation method to
quantify global model reliability under dynamic client participation. Our
findings pave the way toward developing robust, interpretable, and socially
responsible AI systems in Real-world environments."
Artificial Immune Systems,"The biological immune system is a robust, complex, adaptive system that
defends the body from foreign pathogens. It is able to categorize all cells (or
molecules) within the body as self-cells or non-self cells. It does this with
the help of a distributed task force that has the intelligence to take action
from a local and also a global perspective using its network of chemical
messengers for communication. There are two major branches of the immune
system. The innate immune system is an unchanging mechanism that detects and
destroys certain invading organisms, whilst the adaptive immune system responds
to previously unknown foreign cells and builds a response to them that can
remain in the body over a long period of time. This remarkable information
processing biological system has caught the attention of computer science in
recent years. A novel computational intelligence technique, inspired by
immunology, has emerged, called Artificial Immune Systems. Several concepts
from the immune have been extracted and applied for solution to real world
science and engineering problems. In this tutorial, we briefly describe the
immune system metaphors that are relevant to existing Artificial Immune Systems
methods. We will then show illustrative real-world problems suitable for
Artificial Immune Systems and give a step-by-step algorithm walkthrough for one
such problem. A comparison of the Artificial Immune Systems to other well-known
algorithms, areas for future work, tips & tricks and a list of resources will
round this tutorial off. It should be noted that as Artificial Immune Systems
is still a young and evolving field, there is not yet a fixed algorithm
template and hence actual implementations might differ somewhat from time to
time and from those examples given here."
Brain-like Functional Organization within Large Language Models,"The human brain has long inspired the pursuit of artificial intelligence
(AI). Recently, neuroimaging studies provide compelling evidence of alignment
between the computational representation of artificial neural networks (ANNs)
and the neural responses of the human brain to stimuli, suggesting that ANNs
may employ brain-like information processing strategies. While such alignment
has been observed across sensory modalities--visual, auditory, and
linguistic--much of the focus has been on the behaviors of artificial neurons
(ANs) at the population level, leaving the functional organization of
individual ANs that facilitates such brain-like processes largely unexplored.
In this study, we bridge this gap by directly coupling sub-groups of artificial
neurons with functional brain networks (FBNs), the foundational organizational
structure of the human brain. Specifically, we extract representative patterns
from temporal responses of ANs in large language models (LLMs), and use them as
fixed regressors to construct voxel-wise encoding models to predict brain
activity recorded by functional magnetic resonance imaging (fMRI). This
framework links the AN sub-groups to FBNs, enabling the delineation of
brain-like functional organization within LLMs. Our findings reveal that LLMs
(BERT and Llama 1-3) exhibit brain-like functional architecture, with
sub-groups of artificial neurons mirroring the organizational patterns of
well-established FBNs. Notably, the brain-like functional organization of LLMs
evolves with the increased sophistication and capability, achieving an improved
balance between the diversity of computational behaviors and the consistency of
functional specializations. This research represents the first exploration of
brain-like functional organization within LLMs, offering novel insights to
inform the development of artificial general intelligence (AGI) with human
brain principles."
"Artificial Intelligence-Assisted Energy and Thermal Comfort Control for
  Sustainable Buildings: An Extended Representation of the Systematic Review","Different factors such as thermal comfort, humidity, air quality, and noise
have significant combined effects on the acceptability and quality of the
activities performed by the building occupants who spend most of their times
indoors. Among the factors cited, thermal comfort, which contributes to the
human well-being because of its connection with the thermoregulation of the
human body. Therefore, the creation of thermally comfortable and energy
efficient environments is of great importance in the design of the buildings
and hence the heating, ventilation and air-conditioning systems. Recent works
have been directed towards more advanced control strategies, based mainly on
artificial intelligence which has the ability to imitate human behavior. This
systematic literature review aims to provide an overview of the intelligent
control strategies inside building and to investigate their ability to balance
thermal comfort and energy efficiency optimization in indoor environments.
Methods. A systematic literature review examined the peer-reviewed research
works using ACM Digital Library, Scopus, Google Scholar, IEEE Xplore (IEOL),
Web of Science, and Science Direct (SDOL), besides other sources from manual
search. With the following string terms: thermal comfort, comfort temperature,
preferred temperature, intelligent control, advanced control, artificial
intelligence, computational intelligence, building, indoors, and built
environment. Inclusion criteria were: English, studies monitoring, mainly,
human thermal comfort in buildings and energy efficiency simultaneously based
on control strategies using the intelligent approaches. Preferred Reporting
Items for Systematic Reviews and Meta-Analysis guidelines were used. Initially,
1,077 articles were yielded, and 120 ultimately met inclusion criteria and were
reviewed."
"Federated Learning in Big Model Era: Domain-Specific Multimodal Large
  Models","Multimodal data, which can comprehensively perceive and recognize the
physical world, has become an essential path towards general artificial
intelligence. However, multimodal large models trained on public datasets often
underperform in specific industrial domains. This paper proposes a multimodal
federated learning framework that enables multiple enterprises to utilize
private domain data to collaboratively train large models for vertical domains,
achieving intelligent services across scenarios. The authors discuss in-depth
the strategic transformation of federated learning in terms of intelligence
foundation and objectives in the era of big model, as well as the new
challenges faced in heterogeneous data, model aggregation, performance and cost
trade-off, data privacy, and incentive mechanism. The paper elaborates a case
study of leading enterprises contributing multimodal data and expert knowledge
to city safety operation management , including distributed deployment and
efficient coordination of the federated learning platform, technical
innovations on data quality improvement based on large model capabilities and
efficient joint fine-tuning approaches. Preliminary experiments show that
enterprises can enhance and accumulate intelligent capabilities through
multimodal model federated learning, thereby jointly creating an smart city
model that provides high-quality intelligent services covering energy
infrastructure safety, residential community security, and urban operation
management. The established federated learning cooperation ecosystem is
expected to further aggregate industry, academia, and research resources,
realize large models in multiple vertical domains, and promote the large-scale
industrial application of artificial intelligence and cutting-edge research on
multimodal federated learning."
"The Interplay of Learning, Analytics, and Artificial Intelligence in
  Education: A Vision for Hybrid Intelligence","This paper presents a multi-dimensional view of AI's role in learning and
education, emphasizing the intricate interplay between AI, analytics, and the
learning processes. Here, I challenge the prevalent narrow conceptualisation of
AI as tools, as exemplified in generative AI tools, and argue for the
importance of alternative conceptualisations of AI for achieving human-AI
hybrid intelligence. I highlight the differences between human intelligence and
artificial information processing, the importance of hybrid human-AI systems to
extend human cognition, and posit that AI can also serve as an instrument for
understanding human learning. Early learning sciences and AI in Education
research (AIED), which saw AI as an analogy for human intelligence, have
diverged from this perspective, prompting a need to rekindle this connection.
The paper presents three unique conceptualisations of AI: the externalization
of human cognition, the internalization of AI models to influence human mental
models, and the extension of human cognition via tightly coupled human-AI
hybrid intelligence systems. Examples from current research and practice are
examined as instances of the three conceptualisations in education,
highlighting the potential value and limitations of each conceptualisation for
education, as well as the perils of overemphasis on externalising human
cognition. The paper concludes with advocacy for a broader approach to AIED
that goes beyond considerations on the design and development of AI, but also
includes educating people about AI and innovating educational systems to remain
relevant in an AI-ubiquitous world."
"EmbodiedCity: A Benchmark Platform for Embodied Agent in Real-world City
  Environment","Embodied artificial intelligence emphasizes the role of an agent's body in
generating human-like behaviors. The recent efforts on EmbodiedAI pay a lot of
attention to building up machine learning models to possess perceiving,
planning, and acting abilities, thereby enabling real-time interaction with the
world. However, most works focus on bounded indoor environments, such as
navigation in a room or manipulating a device, with limited exploration of
embodying the agents in open-world scenarios. That is, embodied intelligence in
the open and outdoor environment is less explored, for which one potential
reason is the lack of high-quality simulators, benchmarks, and datasets. To
address it, in this paper, we construct a benchmark platform for embodied
intelligence evaluation in real-world city environments. Specifically, we first
construct a highly realistic 3D simulation environment based on the real
buildings, roads, and other elements in a real city. In this environment, we
combine historically collected data and simulation algorithms to conduct
simulations of pedestrian and vehicle flows with high fidelity. Further, we
designed a set of evaluation tasks covering different EmbodiedAI abilities.
Moreover, we provide a complete set of input and output interfaces for access,
enabling embodied agents to easily take task requirements and current
environmental observations as input and then make decisions and obtain
performance evaluations. On the one hand, it expands the capability of existing
embodied intelligence to higher levels. On the other hand, it has a higher
practical value in the real world and can support more potential applications
for artificial general intelligence. Based on this platform, we evaluate some
popular large language models for embodied intelligence capabilities of
different dimensions and difficulties."
Quantum Structure in Cognition: Fundamentals and Applications,"Experiments in cognitive science and decision theory show that the ways in
which people combine concepts and make decisions cannot be described by
classical logic and probability theory. This has serious implications for
applied disciplines such as information retrieval, artificial intelligence and
robotics. Inspired by a mathematical formalism that generalizes quantum
mechanics the authors have constructed a contextual framework for both concept
representation and decision making, together with quantum models that are in
strong alignment with experimental data. The results can be interpreted by
assuming the existence in human thought of a double-layered structure, a
'classical logical thought' and a 'quantum conceptual thought', the latter
being responsible of the above paradoxes and nonclassical effects. The presence
of a quantum structure in cognition is relevant, for it shows that quantum
mechanics provides not only a useful modeling tool for experimental data but
also supplies a structural model for human and artificial thought processes.
This approach has strong connections with theories formalizing meaning, such as
semantic analysis, and has also a deep impact on computer science, information
retrieval and artificial intelligence. More specifically, the links with
information retrieval are discussed in this paper."
Adaptive Parallel Iterative Deepening Search,"Many of the artificial intelligence techniques developed to date rely on
heuristic search through large spaces. Unfortunately, the size of these spaces
and the corresponding computational effort reduce the applicability of
otherwise novel and effective algorithms. A number of parallel and distributed
approaches to search have considerably improved the performance of the search
process. Our goal is to develop an architecture that automatically selects
parallel search strategies for optimal performance on a variety of search
problems. In this paper we describe one such architecture realized in the
Eureka system, which combines the benefits of many different approaches to
parallel heuristic search. Through empirical and theoretical analyses we
observe that features of the problem space directly affect the choice of
optimal parallel search strategy. We then employ machine learning techniques to
select the optimal parallel search strategy for a given problem space. When a
new search task is input to the system, Eureka uses features describing the
search space and the chosen architecture to automatically select the
appropriate search strategy. Eureka has been tested on a MIMD parallel
processor, a distributed network of workstations, and a single workstation
using multithreading. Results generated from fifteen puzzle problems, robot arm
motion problems, artificial search spaces, and planning problems indicate that
Eureka outperforms any of the tested strategies used exclusively for all
problem instances and is able to greatly reduce the search time for these
applications."
"Elementos de ingeniera de explotacin de la informacin aplicados
  a la investigacin tributaria fiscal","By introducing elements of information mining to tax analysis, by means of
data mining software and advanced computational concepts of artificial
intelligence, the problem of tax evader's crime against public property has
been addressed. Through an empirical approach from a hypothetical case of use,
induction algorithms, neural networks and bayesian networks are applied to
determine the feasibility of its heuristic application by the tax public
administrator. Different strategies are explored to facilitate the work of
local and regional federal tax inspectors, considering their limited
computational capabilities, but equally effective for those social scientist
committed to handcrafting tax research.
  -----
  Apresentando a introdu\c{c}\~ao de elementos de explora\c{c}\~ao de
informa\c{c}\~oes para an\'alise fiscal, por meio de software de
minera\c{c}\~ao de dados e conceitos avan\c{c}ados computacionais de
intelig\^encia artificial, foi abordado o problema do crime de sonegador fiscal
contra o patrim\^onio p\'ublico. Atrav\'es de uma abordagem emp\'irica a partir
de um caso hipot\'etico de uso, os algoritmos de indu\c{c}\~ao, redes neurais e
redes bayesianas s\~ao aplicados para determinar a viabilidade de sua
aplica\c{c}\~ao heur\'istica pelo administrador p\'ublico tribut\'ario.
Diferentes estrat\'egias s\~ao exploradas para facilitar o trabalho dos
inspectores tribut\'arios federais locais e regionais, tendo em conta as suas
capacidades computacionais limitados, mas igualmente eficaz para aqueles
cientista social comprometido com a investiga\c{c}\~ao fiscal."
A Mathematical Framework for Consciousness in Neural Networks,"This paper presents a novel mathematical framework for bridging the
explanatory gap (Levine, 1983) between consciousness and its physical
correlates. Specifically, we propose that qualia correspond to singularities in
the mathematical representations of neural network topology. Crucially, we do
not claim that qualia are singularities or that singularities ""explain"" why
qualia feel as they do. Instead, we propose that singularities serve as
principled, coordinate-invariant markers of points where attempts at purely
quantitative description of a system's dynamics reach an in-principle limit. By
integrating these formal markers of irreducibility into models of the physical
correlates of consciousness, we establish a framework that recognizes qualia as
phenomena inherently beyond reduction to complexity, computation, or
information. This approach draws on insights from philosophy of mind,
mathematics, cognitive neuroscience, and artificial intelligence (AI). It does
not solve the hard problem of consciousness (Chalmers, 1995), but it advances
the discourse by integrating the irreducible nature of qualia into a rigorous,
physicalist framework. While primarily theoretical, these insights also open
avenues for future AI and artificial consciousness (AC) research, suggesting
that recognizing and harnessing irreducible topological features may be an
important unlock in moving beyond incremental, scale-based improvements and
toward artificial general intelligence (AGI) and AC."
The Hanabi Challenge: A New Frontier for AI Research,"From the early days of computing, games have been important testbeds for
studying how well machines can do sophisticated decision making. In recent
years, machine learning has made dramatic advances with artificial agents
reaching superhuman performance in challenge domains like Go, Atari, and some
variants of poker. As with their predecessors of chess, checkers, and
backgammon, these game domains have driven research by providing sophisticated
yet well-defined challenges for artificial intelligence practitioners. We
continue this tradition by proposing the game of Hanabi as a new challenge
domain with novel problems that arise from its combination of purely
cooperative gameplay with two to five players and imperfect information. In
particular, we argue that Hanabi elevates reasoning about the beliefs and
intentions of other agents to the foreground. We believe developing novel
techniques for such theory of mind reasoning will not only be crucial for
success in Hanabi, but also in broader collaborative efforts, especially those
with human partners. To facilitate future research, we introduce the
open-source Hanabi Learning Environment, propose an experimental framework for
the research community to evaluate algorithmic advances, and assess the
performance of current state-of-the-art techniques."
"Trends in Integration of Vision and Language Research: A Survey of
  Tasks, Datasets, and Methods","Interest in Artificial Intelligence (AI) and its applications has seen
unprecedented growth in the last few years. This success can be partly
attributed to the advancements made in the sub-fields of AI such as machine
learning, computer vision, and natural language processing. Much of the growth
in these fields has been made possible with deep learning, a sub-area of
machine learning that uses artificial neural networks. This has created
significant interest in the integration of vision and language. In this survey,
we focus on ten prominent tasks that integrate language and vision by
discussing their problem formulation, methods, existing datasets, evaluation
measures, and compare the results obtained with corresponding state-of-the-art
methods. Our efforts go beyond earlier surveys which are either task-specific
or concentrate only on one type of visual content, i.e., image or video.
Furthermore, we also provide some potential future directions in this field of
research with an anticipation that this survey stimulates innovative thoughts
and ideas to address the existing challenges and build new applications."
"Extending Machine Language Models toward Human-Level Language
  Understanding","Language is crucial for human intelligence, but what exactly is its role? We
take language to be a part of a system for understanding and communicating
about situations. The human ability to understand and communicate about
situations emerges gradually from experience and depends on domain-general
principles of biological neural networks: connection-based learning,
distributed representation, and context-sensitive, mutual constraint
satisfaction-based processing. Current artificial language processing systems
rely on the same domain general principles, embodied in artificial neural
networks. Indeed, recent progress in this field depends on \emph{query-based
attention}, which extends the ability of these systems to exploit context and
has contributed to remarkable breakthroughs. Nevertheless, most current models
focus exclusively on language-internal tasks, limiting their ability to perform
tasks that depend on understanding situations. These systems also lack memory
for the contents of prior situations outside of a fixed contextual span. We
describe the organization of the brain's distributed understanding system,
which includes a fast learning system that addresses the memory problem. We
sketch a framework for future models of understanding drawing equally on
cognitive neuroscience and artificial intelligence and exploiting query-based
attention. We highlight relevant current directions and consider further
developments needed to fully capture human-level language understanding in a
computational system."
"A Practical Guide to Studying Emergent Communication through Grounded
  Language Games","The question of how an effective and efficient communication system can
emerge in a population of agents that need to solve a particular task attracts
more and more attention from researchers in many fields, including artificial
intelligence, linguistics and statistical physics. A common methodology for
studying this question consists of carrying out multi-agent experiments in
which a population of agents takes part in a series of scripted and
task-oriented communicative interactions, called 'language games'. While each
individual language game is typically played by two agents in the population, a
large series of games allows the population to converge on a shared
communication system. Setting up an experiment in which a rich system for
communicating about the real world emerges is a major enterprise, as it
requires a variety of software components for running multi-agent experiments,
for interacting with sensors and actuators, for conceptualising and
interpreting semantic structures, and for mapping between these semantic
structures and linguistic utterances. The aim of this paper is twofold. On the
one hand, it introduces a high-level robot interface that extends the Babel
software system, presenting for the first time a toolkit that provides flexible
modules for dealing with each subtask involved in running advanced grounded
language game experiments. On the other hand, it provides a practical guide to
using the toolkit for implementing such experiments, taking a grounded colour
naming game experiment as a didactic example."
"Beneficial Perturbation Network for designing general adaptive
  artificial intelligence systems","The human brain is the gold standard of adaptive learning. It not only can
learn and benefit from experience, but also can adapt to new situations. In
contrast, deep neural networks only learn one sophisticated but fixed mapping
from inputs to outputs. This limits their applicability to more dynamic
situations, where input to output mapping may change with different contexts. A
salient example is continual learning - learning new independent tasks
sequentially without forgetting previous tasks. Continual learning of multiple
tasks in artificial neural networks using gradient descent leads to
catastrophic forgetting, whereby a previously learned mapping of an old task is
erased when learning new mappings for new tasks. Here, we propose a new
biologically plausible type of deep neural network with extra, out-of-network,
task-dependent biasing units to accommodate these dynamic situations. This
allows, for the first time, a single network to learn potentially unlimited
parallel input to output mappings, and to switch on the fly between them at
runtime. Biasing units are programmed by leveraging beneficial perturbations
(opposite to well-known adversarial perturbations) for each task. Beneficial
perturbations for a given task bias the network toward that task, essentially
switching the network into a different mode to process that task. This largely
eliminates catastrophic interference between tasks. Our approach is
memory-efficient and parameter-efficient, can accommodate many tasks, and
achieves state-of-the-art performance across different tasks and domains."
"Morality, Machines and the Interpretation Problem: A Value-based,
  Wittgensteinian Approach to Building Moral Agents","We present what we call the Interpretation Problem, whereby any rule in
symbolic form is open to infinite interpretation in ways that we might
disapprove of and argue that any attempt to build morality into machines is
subject to it. We show how the Interpretation Problem in Artificial
Intelligence is an illustration of Wittgenstein's general claim that no rule
can contain the criteria for its own application, and that the risks created by
this problem escalate in proportion to the degree to which to machine is
causally connected to the world, in what we call the Law of Interpretative
Exposure. Using game theory, we attempt to define the structure of normative
spaces and argue that any rule-following within a normative space is guided by
values that are external to that space and which cannot themselves be
represented as rules. In light of this, we categorise the types of mistakes an
artificial moral agent could make into Mistakes of Intention and Instrumental
Mistakes, and we propose ways of building morality into machines by getting
them to interpret the rules we give in accordance with these external values,
through explicit moral reasoning, the Show, not Tell paradigm, the adjustment
of causal power and structure of the agent, and relational values, with the
ultimate aim that the machine develop a virtuous character and that the impact
of the Interpretation Problem is minimised."
"A Deep Generative Artificial Intelligence system to decipher species
  coexistence patterns","1. Deciphering coexistence patterns is a current challenge to understanding
diversity maintenance, especially in rich communities where the complexity of
these patterns is magnified through indirect interactions that prevent their
approximation with classical experimental approaches. 2. We explore
cutting-edge Machine Learning techniques called Generative Artificial
Intelligence (GenAI) to decipher species coexistence patterns in vegetation
patches, training generative adversarial networks (GAN) and variational
AutoEncoders (VAE) that are then used to unravel some of the mechanisms behind
community assemblage. 3. The GAN accurately reproduces the species composition
of real patches as well as the affinity of plant species to different soil
types, and the VAE also reaches a high level of accuracy, above 99%. Using the
artificially generated patches, we found that high order interactions tend to
suppress the positive effects of low order interactions. Finally, by
reconstructing successional trajectories we could identify the pioneer species
with larger potential to generate a high diversity of distinct patches in terms
of species composition. 4. Understanding the complexity of species coexistence
patterns in diverse ecological communities requires new approaches beyond
heuristic rules. Generative Artificial Intelligence can be a powerful tool to
this end as it allows to overcome the inherent dimensionality of this
challenge."
"An Apparatus for the Simulation of Breathing Disorders: Physically
  Meaningful Generation of Surrogate Data","The rapidly increasing prevalence of debilitating breathing disorders, such
as chronic obstructive pulmonary disease (COPD), calls for a meaningful
integration of artificial intelligence (AI) into healthcare. While this
promises improved detection and monitoring of breathing disorders, AI
techniques are almost invariably ""data hungry"" which highlights the importance
of generating physically meaningful surrogate data. Indeed, domain aware
surrogates would enable both an improved understanding of respiratory waveform
changes with different breathing disorders, and enhance the training of machine
learning algorithms. To this end, we introduce an apparatus comprising of PVC
tubes and 3D printed parts as a simple yet effective method of simulating both
obstructive and restrictive respiratory waveforms in healthy subjects.
Independent control over both inspiratory and expiratory resistances allows for
the simulation of obstructive breathing disorders through the whole spectrum of
FEV1/FVC spirometry ratios (used to classify COPD), ranging from healthy values
to values seen in severe chronic obstructive pulmonary disease. Moreover,
waveform characteristics of breathing disorders, such as a change in
inspiratory duty cycle or peak flow are also observed in the waveforms
resulting from use of the artificial breathing disorder simulation apparatus.
Overall, the proposed apparatus provides us with a simple, effective and
physically meaningful way to generate faithful surrogate breathing disorder
waveforms, a prerequisite for the use of artificial intelligence in respiratory
health."
"Artificial Intelligence and Statistical Techniques in Short-Term Load
  Forecasting: A Review","Electrical utilities depend on short-term demand forecasting to proactively
adjust production and distribution in anticipation of major variations. This
systematic review analyzes 240 works published in scholarly journals between
2000 and 2019 that focus on applying Artificial Intelligence (AI), statistical,
and hybrid models to short-term load forecasting (STLF). This work represents
the most comprehensive review of works on this subject to date. A complete
analysis of the literature is conducted to identify the most popular and
accurate techniques as well as existing gaps. The findings show that although
Artificial Neural Networks (ANN) continue to be the most commonly used
standalone technique, researchers have been exceedingly opting for hybrid
combinations of different techniques to leverage the combined advantages of
individual methods. The review demonstrates that it is commonly possible with
these hybrid combinations to achieve prediction accuracy exceeding 99%. The
most successful duration for short-term forecasting has been identified as
prediction for a duration of one day at an hourly interval. The review has
identified a deficiency in access to datasets needed for training of the
models. A significant gap has been identified in researching regions other than
Asia, Europe, North America, and Australia."
"Surrogate Modeling for Physical Systems with Preserved Properties and
  Adjustable Tradeoffs","Determining the proper level of details to develop and solve physical models
is usually difficult when one encounters new engineering problems. Such
difficulty comes from how to balance the time (simulation cost) and accuracy
for the physical model simulation afterwards. We propose a framework for
automatic development of a family of surrogate models of physical systems that
provide flexible cost-accuracy tradeoffs to assist making such determinations.
We present both a model-based and a data-driven strategy to generate surrogate
models. The former starts from a high-fidelity model generated from first
principles and applies a bottom-up model order reduction (MOR) that preserves
stability and convergence while providing a priori error bounds, although the
resulting reduced-order model may lose its interpretability. The latter
generates interpretable surrogate models by fitting artificial constitutive
relations to a presupposed topological structure using experimental or
simulation data. For the latter, we use Tonti diagrams to systematically
produce differential equations from the assumed topological structure using
algebraic topological semantics that are common to various lumped-parameter
models (LPM). The parameter for the constitutive relations are estimated using
standard system identification algorithms. Our framework is compatible with
various spatial discretization schemes for distributed parameter models (DPM),
and can supports solving engineering problems in different domains of physics."
"Deep-Learning-Empowered Inverse Design for Freeform Reconfigurable
  Metasurfaces","The past decade has witnessed the advances of artificial intelligence with
various applications in engineering. Recently, artificial neural network
empowered inverse design for metasurfaces has been developed that can design
on-demand meta-atoms with diverse shapes and high performance, where the design
process based on artificial intelligence is fast and automatic. However, once
the inverse-designed static meta-atom is fabricated, the function of the
metasurface is fixed. Reconfigurable metasurfaces can realize dynamic
functions, while applying artificial intelligence to design practical
reconfigurable meta-atoms inversely has not been reported yet. Here, we present
a deep-learning-empowered inverse design method for freeform reconfigurable
metasurfaces, which can generate on-demand reconfigurable coding meta-atoms at
self-defined frequency bands. To reduce the scale of dataset, a decoupling
method of the reconfigurable meta-atom based on microwave network theory is
proposed at first, which can convert the inverse design process for
reconfigurable coding meta-atoms to the inverse design for static structures. A
convolutional neural network model is trained to predict the responses of
free-shaped meta-atoms, and the genetic algorithm is applied to generate the
optimal structure patterns rapidly. As a demonstration of concept, several
inverse-designed examples are generated with different self-defined spectrum
responses in microwave band, and an inverse-designed wideband reconfigurable
metasurface prototype is fabricated and measured for beam scanning applications
with broad bandwidth. Our work paves the way for the fast and automatic design
process of high-performance reconfigurable metasurfaces."
"Neurosymbolic Artificial Intelligence (NSAI) based Algorithm for
  predicting the Impact Strength of Additive Manufactured Polylactic Acid (PLA)
  Specimens","In this study, we introduce application of Neurosymbolic Artificial
Intelligence (NSAI) for predicting the impact strength of additive manufactured
polylactic acid (PLA) components, representing the first-ever use of NSAI in
the domain of additive manufacturing. The NSAI model amalgamates the advantages
of neural networks and symbolic AI, offering a more robust and accurate
prediction than traditional machine learning techniques. Experimental data was
collected and synthetically augmented to 1000 data points, enhancing the
model's precision. The Neurosymbolic model was developed using a neural network
architecture comprising input, two hidden layers, and an output layer, followed
by a decision tree regressor representing the symbolic component. The model's
performance was benchmarked against a Simple Artificial Neural Network (ANN)
model by assessing mean squared error (MSE) and R-squared (R2) values for both
training and validation datasets. The results reveal that the Neurosymbolic
model surpasses the Simple ANN model, attaining lower MSE and higher R2 values
for both training and validation sets. This innovative application of the
Neurosymbolic approach in estimating the impact strength of additive
manufactured PLA components underscores its potential for optimizing the
additive manufacturing process. Future research could investigate further
refinements to the Neurosymbolic model, extend its application to other
materials and additive manufacturing processes, and incorporate real-time
monitoring and control for enhanced process optimization."
"A philosophical and ontological perspective on Artificial General
  Intelligence and the Metaverse","This paper leverages various philosophical and ontological frameworks to
explore the concept of embodied artificial general intelligence (AGI), its
relationship to human consciousness, and the key role of the metaverse in
facilitating this relationship. Several theoretical frameworks underpin this
exploration, such as embodied cognition, Michael Levin's computational boundary
of a ""Self,"" Donald D. Hoffman's Interface Theory of Perception, and Bernardo
Kastrup's analytical idealism, which lead to considering our perceived outer
reality as a symbolic representation of alternate inner states of being, and
where AGI could embody a different form of consciousness with a larger
computational boundary. The paper further discusses the developmental stages of
AGI, the requirements for the emergence of an embodied AGI, the importance of a
calibrated symbolic interface for AGI, and the key role played by the
metaverse, decentralized systems, open-source blockchain technology, as well as
open-source AI research. It also explores the idea of a feedback loop between
AGI and human users in metaverse spaces as a tool for AGI calibration, as well
as the role of local homeostasis and decentralized governance as preconditions
for achieving a stable embodied AGI. The paper concludes by emphasizing the
importance of achieving a certain degree of harmony in human relations and
recognizing the interconnectedness of humanity at a global level, as key
prerequisites for the emergence of a stable embodied AGI."
"Eternal Sunshine of the Mechanical Mind: The Irreconcilability of
  Machine Learning and the Right to be Forgotten","As we keep rapidly advancing toward an era where artificial intelligence is a
constant and normative experience for most of us, we must also be aware of what
this vision and this progress entail. By first approximating neural connections
and activities in computer circuits and then creating more and more
sophisticated versions of this crude approximation, we are now facing an age to
come where modern deep learning-based artificial intelligence systems can
rightly be called thinking machines, and they are sometimes even lauded for
their emergent behavior and black-box approaches. But as we create more
powerful electronic brains, with billions of neural connections and parameters,
can we guarantee that these mammoths built of artificial neurons will be able
to forget the data that we store in them? If they are at some level like a
brain, can the right to be forgotten still be protected while dealing with
these AIs? The essential gap between machine learning and the RTBF is explored
in this article, with a premonition of far-reaching conclusions if the gap is
not bridged or reconciled any time soon. The core argument is that deep
learning models, due to their structure and size, cannot be expected to forget
or delete a data as it would be expected from a tabular database, and they
should be treated more like a mechanical brain, albeit still in development."
"Discriminant audio properties in deep learning based respiratory
  insufficiency detection in Brazilian Portuguese","This work investigates Artificial Intelligence (AI) systems that detect
respiratory insufficiency (RI) by analyzing speech audios, thus treating speech
as a RI biomarker. Previous works collected RI data (P1) from COVID-19 patients
during the first phase of the pandemic and trained modern AI models, such as
CNNs and Transformers, which achieved $96.5\%$ accuracy, showing the
feasibility of RI detection via AI. Here, we collect RI patient data (P2) with
several causes besides COVID-19, aiming at extending AI-based RI detection. We
also collected control data from hospital patients without RI. We show that the
considered models, when trained on P1, do not generalize to P2, indicating that
COVID-19 RI has features that may not be found in all RI types."
"Consciousness defined: requirements for biological and artificial
  general intelligence","Consciousness is notoriously hard to define with objective terms. An
objective definition of consciousness is critically needed so that we might
accurately understand how consciousness and resultant choice behaviour may
arise in biological or artificial systems. Many theories have integrated
neurobiological and psychological research to explain how consciousness might
arise, but few, if any, outline what is fundamentally required to generate
consciousness. To identify such requirements, I examine current theories of
consciousness and corresponding scientific research to generate a new
definition of consciousness from first principles. Critically, consciousness is
the apparatus that provides the ability to make decisions, but it is not
defined by the decision itself. As such, a definition of consciousness does not
require choice behaviour or an explicit awareness of temporality despite both
being well-characterised outcomes of conscious thought. Rather, requirements
for consciousness include: at least some capability for perception, a memory
for the storage of such perceptual information which in turn provides a
framework for an imagination with which a sense of self can be capable of
making decisions based on possible and desired futures. Thought experiments and
observable neurological phenomena demonstrate that these components are
fundamentally required of consciousness, whereby the loss of any one component
removes the capability for conscious thought. Identifying these requirements
provides a new definition for consciousness by which we can objectively
determine consciousness in any conceivable agent, such as non-human animals and
artificially intelligent systems."
"Psychomatics -- A Multidisciplinary Framework for Understanding
  Artificial Minds","Although LLMs and other artificial intelligence systems demonstrate cognitive
skills similar to humans, like concept learning and language acquisition, the
way they process information fundamentally differs from biological cognition.
To better understand these differences this paper introduces Psychomatics, a
multidisciplinary framework bridging cognitive science, linguistics, and
computer science. It aims to better understand the high-level functioning of
LLMs, focusing specifically on how LLMs acquire, learn, remember, and use
information to produce their outputs. To achieve this goal, Psychomatics will
rely on a comparative methodology, starting from a theory-driven research
question - is the process of language development and use different in humans
and LLMs? - drawing parallels between LLMs and biological systems. Our analysis
shows how LLMs can map and manipulate complex linguistic patterns in their
training data. Moreover, LLMs can follow Grice's Cooperative Principle to
provide relevant and informative responses. However, human cognition draws from
multiple sources of meaning, including experiential, emotional, and imaginative
facets, which transcend mere language processing and are rooted in our social
and developmental trajectories. Moreover, current LLMs lack physical
embodiment, reducing their ability to make sense of the intricate interplay
between perception, action, and cognition that shapes human understanding and
expression. Ultimately, Psychomatics holds the potential to yield
transformative insights into the nature of language, cognition, and
intelligence, both artificial and biological. Moreover, by drawing parallels
between LLMs and human cognitive processes, Psychomatics can inform the
development of more robust and human-like AI systems."
"Learning to Generate and Evaluate Fact-checking Explanations with
  Transformers","In an era increasingly dominated by digital platforms, the spread of
misinformation poses a significant challenge, highlighting the need for
solutions capable of assessing information veracity. Our research contributes
to the field of Explainable Artificial Antelligence (XAI) by developing
transformer-based fact-checking models that contextualise and justify their
decisions by generating human-accessible explanations. Importantly, we also
develop models for automatic evaluation of explanations for fact-checking
verdicts across different dimensions such as \texttt{(self)-contradiction},
\texttt{hallucination}, \texttt{convincingness} and \texttt{overall quality}.
By introducing human-centred evaluation methods and developing specialised
datasets, we emphasise the need for aligning Artificial Intelligence
(AI)-generated explanations with human judgements. This approach not only
advances theoretical knowledge in XAI but also holds practical implications by
enhancing the transparency, reliability and users' trust in AI-driven
fact-checking systems. Furthermore, the development of our metric learning
models is a first step towards potentially increasing efficiency and reducing
reliance on extensive manual assessment. Based on experimental results, our
best performing generative model \textsc{ROUGE-1} score of 47.77, demonstrating
superior performance in generating fact-checking explanations, particularly
when provided with high-quality evidence. Additionally, the best performing
metric learning model showed a moderately strong correlation with human
judgements on objective dimensions such as \texttt{(self)-contradiction and
\texttt{hallucination}, achieving a Matthews Correlation Coefficient (MCC) of
around 0.7.}"
"A Review of Artificial Intelligence Impacting Statistical Process
  Monitoring and Future Directions","It has been 100 years since statistical process control (SPC) or statistical
process monitoring (SPM) was first introduced for production processes and
later applied to service, healthcare, and other industries. The techniques
applied to SPM applications are mostly statistically oriented. Recent advances
in Artificial Intelligence (AI) have reinvigorated the imagination of adopting
AI for SPM applications. This manuscript begins with a concise review of the
historical development of the statistically based SPM methods. Next, this
manuscript explores AI and Machine Learning (ML) algorithms and methods applied
in various SPM applications, addressing quality characteristics of univariate,
multivariate, profile, and image. These AI methods can be classified into the
following categories: classification, pattern recognition, time series
applications, and generative AI. Specifically, different kinds of neural
networks, such as artificial neural networks (ANN), convolutional neural
networks (CNN), recurrent neural networks (RNN), and generative adversarial
networks (GAN), are among the most implemented AI methods impacting SPM.
Finally, this manuscript outlines a couple of future directions that harness
the potential of the Large Multimodal Model (LMM) for advancing SPM research
and applications in complex systems. The ultimate objective is to transform
statistical process monitoring (SPM) into smart process control (SMPC), where
corrective actions are autonomously implemented to either prevent quality
issues or restore process performance."
Provably Bounded-Optimal Agents,"Since its inception, artificial intelligence has relied upon a theoretical
foundation centered around perfect rationality as the desired property of
intelligent systems. We argue, as others have done, that this foundation is
inadequate because it imposes fundamentally unsatisfiable requirements. As a
result, there has arisen a wide gap between theory and practice in AI,
hindering progress in the field. We propose instead a property called bounded
optimality. Roughly speaking, an agent is bounded-optimal if its program is a
solution to the constrained optimization problem presented by its architecture
and the task environment. We show how to construct agents with this property
for a simple class of machine architectures in a broad class of real-time
environments. We illustrate these results using a simple model of an automated
mail sorting facility. We also define a weaker property, asymptotic bounded
optimality (ABO), that generalizes the notion of optimality in classical
complexity theory. We then construct universal ABO programs, i.e., programs
that are ABO no matter what real-time constraints are applied. Universal ABO
programs can be used as building blocks for more complex systems. We conclude
with a discussion of the prospects for bounded optimality as a theoretical
basis for AI, and relate it to similar trends in philosophy, economics, and
game theory."
"POMDPs Make Better Hackers: Accounting for Uncertainty in Penetration
  Testing","Penetration Testing is a methodology for assessing network security, by
generating and executing possible hacking attacks. Doing so automatically
allows for regular and systematic testing. A key question is how to generate
the attacks. This is naturally formulated as planning under uncertainty, i.e.,
under incomplete knowledge about the network configuration. Previous work uses
classical planning, and requires costly pre-processes reducing this uncertainty
by extensive application of scanning methods. By contrast, we herein model the
attack planning problem in terms of partially observable Markov decision
processes (POMDP). This allows to reason about the knowledge available, and to
intelligently employ scanning actions as part of the attack. As one would
expect, this accurate solution does not scale. We devise a method that relies
on POMDPs to find good attacks on individual machines, which are then composed
into an attack on the network as a whole. This decomposition exploits network
structure to the extent possible, making targeted approximations (only) where
needed. Evaluating this method on a suitably adapted industrial test suite, we
demonstrate its effectiveness in both runtime and solution quality."
"Artificial Intelligence Based Cognitive Routing for Cognitive Radio
  Networks","Cognitive radio networks (CRNs) are networks of nodes equipped with cognitive
radios that can optimize performance by adapting to network conditions. While
cognitive radio networks (CRN) are envisioned as intelligent networks,
relatively little research has focused on the network level functionality of
CRNs. Although various routing protocols, incorporating varying degrees of
adaptiveness, have been proposed for CRNs, it is imperative for the long term
success of CRNs that the design of cognitive routing protocols be pursued by
the research community. Cognitive routing protocols are envisioned as routing
protocols that fully and seamless incorporate AI-based techniques into their
design. In this paper, we provide a self-contained tutorial on various AI and
machine-learning techniques that have been, or can be, used for developing
cognitive routing protocols. We also survey the application of various classes
of AI techniques to CRNs in general, and to the problem of routing in
particular. We discuss various decision making techniques and learning
techniques from AI and document their current and potential applications to the
problem of routing in CRNs. We also highlight the various inference, reasoning,
modeling, and learning sub tasks that a cognitive routing protocol must solve.
Finally, open research issues and future directions of work are identified."
The Morphospace of Consciousness,"We construct a complexity-based morphospace to study systems-level properties
of conscious & intelligent systems. The axes of this space label 3 complexity
types: autonomous, cognitive & social. Given recent proposals to synthesize
consciousness, a generic complexity-based conceptualization provides a useful
framework for identifying defining features of conscious & synthetic systems.
Based on current clinical scales of consciousness that measure cognitive
awareness and wakefulness, we take a perspective on how contemporary
artificially intelligent machines & synthetically engineered life forms measure
on these scales. It turns out that awareness & wakefulness can be associated to
computational & autonomous complexity respectively. Subsequently, building on
insights from cognitive robotics, we examine the function that consciousness
serves, & argue the role of consciousness as an evolutionary game-theoretic
strategy. This makes the case for a third type of complexity for describing
consciousness: social complexity. Having identified these complexity types,
allows for a representation of both, biological & synthetic systems in a common
morphospace. A consequence of this classification is a taxonomy of possible
conscious machines. We identify four types of consciousness, based on
embodiment: (i) biological consciousness, (ii) synthetic consciousness, (iii)
group consciousness (resulting from group interactions), & (iv) simulated
consciousness (embodied by virtual agents within a simulated reality). This
taxonomy helps in the investigation of comparative signatures of consciousness
across domains, in order to highlight design principles necessary to engineer
conscious machines. This is particularly relevant in the light of recent
developments at the crossroads of cognitive neuroscience, biomedical
engineering, artificial intelligence & biomimetics."
Active learning machine learns to create new quantum experiments,"How useful can machine learning be in a quantum laboratory? Here we raise the
question of the potential of intelligent machines in the context of scientific
research. A major motivation for the present work is the unknown reachability
of various entanglement classes in quantum experiments. We investigate this
question by using the projective simulation model, a physics-oriented approach
to artificial intelligence. In our approach, the projective simulation system
is challenged to design complex photonic quantum experiments that produce
high-dimensional entangled multiphoton states, which are of high interest in
modern quantum experiments. The artificial intelligence system learns to create
a variety of entangled states, and improves the efficiency of their
realization. In the process, the system autonomously (re)discovers experimental
techniques which are only now becoming standard in modern quantum optical
experiments - a trait which was not explicitly demanded from the system but
emerged through the process of learning. Such features highlight the
possibility that machines could have a significantly more creative role in
future research."
Intelligent Fault Analysis in Electrical Power Grids,"Power grids are one of the most important components of infrastructure in
today's world. Every nation is dependent on the security and stability of its
own power grid to provide electricity to the households and industries. A
malfunction of even a small part of a power grid can cause loss of
productivity, revenue and in some cases even life. Thus, it is imperative to
design a system which can detect the health of the power grid and take
protective measures accordingly even before a serious anomaly takes place. To
achieve this objective, we have set out to create an artificially intelligent
system which can analyze the grid information at any given time and determine
the health of the grid through the usage of sophisticated formal models and
novel machine learning techniques like recurrent neural networks. Our system
simulates grid conditions including stimuli like faults, generator output
fluctuations, load fluctuations using Siemens PSS/E software and this data is
trained using various classifiers like SVM, LSTM and subsequently tested. The
results are excellent with our methods giving very high accuracy for the data.
This model can easily be scaled to handle larger and more complex grid
architectures."
"Cognitive Database: A Step towards Endowing Relational Databases with
  Artificial Intelligence Capabilities","We propose Cognitive Databases, an approach for transparently enabling
Artificial Intelligence (AI) capabilities in relational databases. A novel
aspect of our design is to first view the structured data source as meaningful
unstructured text, and then use the text to build an unsupervised neural
network model using a Natural Language Processing (NLP) technique called word
embedding. This model captures the hidden inter-/intra-column relationships
between database tokens of different types. For each database token, the model
includes a vector that encodes contextual semantic relationships. We seamlessly
integrate the word embedding model into existing SQL query infrastructure and
use it to enable a new class of SQL-based analytics queries called cognitive
intelligence (CI) queries. CI queries use the model vectors to enable complex
queries such as semantic matching, inductive reasoning queries such as
analogies, predictive queries using entities not present in a database, and,
more generally, using knowledge from external sources. We demonstrate unique
capabilities of Cognitive Databases using an Apache Spark based prototype to
execute inductive reasoning CI queries over a multi-modal database containing
text and images. We believe our first-of-a-kind system exemplifies using AI
functionality to endow relational databases with capabilities that were
previously very hard to realize in practice."
There is no Artificial General Intelligence,"The goal of creating Artificial General Intelligence (AGI) -- or in other
words of creating Turing machines (modern computers) that can behave in a way
that mimics human intelligence -- has occupied AI researchers ever since the
idea of AI was first proposed. One common theme in these discussions is the
thesis that the ability of a machine to conduct convincing dialogues with human
beings can serve as at least a sufficient criterion of AGI. We argue that this
very ability should be accepted also as a necessary condition of AGI, and we
provide a description of the nature of human dialogue in particular and of
human language in general against this background. We then argue that it is for
mathematical reasons impossible to program a machine in such a way that it
could master human dialogue behaviour in its full generality. This is (1)
because there are no traditional explicitly designed mathematical models that
could be used as a starting point for creating such programs; and (2) because
even the sorts of automated models generated by using machine learning, which
have been used successfully in areas such as machine translation, cannot be
extended to cope with human dialogue. If this is so, then we can conclude that
a Turing machine also cannot possess AGI, because it fails to fulfil a
necessary condition thereof. At the same time, however, we acknowledge the
potential of Turing machines to master dialogue behaviour in highly restricted
contexts, where what is called ``narrow'' AI can still be of considerable
utility."
"Canada Protocol: an ethical checklist for the use of Artificial
  Intelligence in Suicide Prevention and Mental Health","Introduction: To improve current public health strategies in suicide
prevention and mental health, governments, researchers and private companies
increasingly use information and communication technologies, and more
specifically Artificial Intelligence and Big Data. These technologies are
promising but raise ethical challenges rarely covered by current legal systems.
It is essential to better identify, and prevent potential ethical risks.
Objectives: The Canada Protocol - MHSP is a tool to guide and support
professionals, users, and researchers using AI in mental health and suicide
prevention. Methods: A checklist was constructed based upon ten international
reports on AI and ethics and two guides on mental health and new technologies.
329 recommendations were identified, of which 43 were considered as applicable
to Mental Health and AI. The checklist was validated, using a two round Delphi
Consultation. Results: 16 experts participated in the first round of the Delphi
Consultation and 8 participated in the second round. Of the original 43 items,
38 were retained. They concern five categories: ""Description of the Autonomous
Intelligent System"" (n=8), ""Privacy and Transparency"" (n=8), ""Security"" (n=6),
""Health-Related Risks"" (n=8), ""Biases"" (n=8). The checklist was considered
relevant by most users, and could need versions tailored to each category of
target users."
"From Crystallized Adaptivity to Fluid Adaptivity in Deep Reinforcement
  Learning -- Insights from Biological Systems on Adaptive Flexibility","Recent developments in machine-learning algorithms have led to impressive
performance increases in many traditional application scenarios of artificial
intelligence research. In the area of deep reinforcement learning, deep
learning functional architectures are combined with incremental learning
schemes for sequential tasks that include interaction-based, but often delayed
feedback. Despite their impressive successes, modern machine-learning
approaches, including deep reinforcement learning, still perform weakly when
compared to flexibly adaptive biological systems in certain naturally occurring
scenarios. Such scenarios include transfers to environments different than the
ones in which the training took place or environments that dynamically change,
both of which are often mastered by biological systems through a capability
that we here term ""fluid adaptivity"" to contrast it from the much slower
adaptivity (""crystallized adaptivity"") of the prior learning from which the
behavior emerged. In this article, we derive and discuss research strategies,
based on analyzes of fluid adaptivity in biological systems and its neuronal
modeling, that might aid in equipping future artificially intelligent systems
with capabilities of fluid adaptivity more similar to those seen in some
biologically intelligent systems. A key component of this research strategy is
the dynamization of the problem space itself and the implementation of this
dynamization by suitably designed flexibly interacting modules."
"From Bit To Bedside: A Practical Framework For Artificial Intelligence
  Product Development In Healthcare","Artificial Intelligence (AI) in healthcare holds great potential to expand
access to high-quality medical care, whilst reducing overall systemic costs.
Despite hitting the headlines regularly and many publications of
proofs-of-concept, certified products are failing to breakthrough to the
clinic. AI in healthcare is a multi-party process with deep knowledge required
in multiple individual domains. The lack of understanding of the specific
challenges in the domain is, therefore, the major contributor to the failure to
deliver on the big promises. Thus, we present a decision perspective framework,
for the development of AI-driven biomedical products, from conception to market
launch. Our framework highlights the risks, objectives and key results which
are typically required to proceed through a three-phase process to the market
launch of a validated medical AI product. We focus on issues related to
Clinical validation, Regulatory affairs, Data strategy and Algorithmic
development. The development process we propose for AI in healthcare software
strongly diverges from modern consumer software development processes. We
highlight the key time points to guide founders, investors and key stakeholders
throughout their relevant part of the process. Our framework should be seen as
a template for innovation frameworks, which can be used to coordinate team
communications and responsibilities towards a reasonable product development
roadmap, thus unlocking the potential of AI in medicine."
Shortcut Learning in Deep Neural Networks,"Deep learning has triggered the current rise of artificial intelligence and
is the workhorse of today's machine intelligence. Numerous success stories have
rapidly spread all over science, industry and society, but its limitations have
only recently come into focus. In this perspective we seek to distill how many
of deep learning's problems can be seen as different symptoms of the same
underlying problem: shortcut learning. Shortcuts are decision rules that
perform well on standard benchmarks but fail to transfer to more challenging
testing conditions, such as real-world scenarios. Related issues are known in
Comparative Psychology, Education and Linguistics, suggesting that shortcut
learning may be a common characteristic of learning systems, biological and
artificial alike. Based on these observations, we develop a set of
recommendations for model interpretation and benchmarking, highlighting recent
advances in machine learning to improve robustness and transferability from the
lab to real-world applications."
A Metamodel and Framework for AGI,"Can artificial intelligence systems exhibit superhuman performance, but in
critical ways, lack the intelligence of even a single-celled organism? The
answer is clearly 'yes' for narrow AI systems. Animals, plants, and even
single-celled organisms learn to reliably avoid danger and move towards food.
This is accomplished via a physical knowledge preserving metamodel that
autonomously generates useful models of the world. We posit that preserving the
structure of knowledge is critical for higher intelligences that manage
increasingly higher levels of abstraction, be they human or artificial. This is
the key lesson learned from applying AGI subsystems to complex real-world
problems that require continuous learning and adaptation. In this paper, we
introduce the Deep Fusion Reasoning Engine (DFRE), which implements a
knowledge-preserving metamodel and framework for constructing applied AGI
systems. The DFRE metamodel exhibits some important fundamental knowledge
preserving properties such as clear distinctions between symmetric and
antisymmetric relations, and the ability to create a hierarchical knowledge
representation that clearly delineates between levels of abstraction. The DFRE
metamodel, which incorporates these capabilities, demonstrates how this
approach benefits AGI in specific ways such as managing combinatorial explosion
and enabling cumulative, distributed and federated learning. Our experiments
show that the proposed framework achieves 94% accuracy on average on
unsupervised object detection and recognition. This work is inspired by the
state-of-the-art approaches to AGI, recent AGI-aspiring work, the granular
computing community, as well as Alfred Korzybski's general semantics."
"Receptivity of an AI Cognitive Assistant by the Radiology Community: A
  Report on Data Collected at RSNA","Due to advances in machine learning and artificial intelligence (AI), a new
role is emerging for machines as intelligent assistants to radiologists in
their clinical workflows. But what systematic clinical thought processes are
these machines using? Are they similar enough to those of radiologists to be
trusted as assistants? A live demonstration of such a technology was conducted
at the 2016 Scientific Assembly and Annual Meeting of the Radiological Society
of North America (RSNA). The demonstration was presented in the form of a
question-answering system that took a radiology multiple choice question and a
medical image as inputs. The AI system then demonstrated a cognitive workflow,
involving text analysis, image analysis, and reasoning, to process the question
and generate the most probable answer. A post demonstration survey was made
available to the participants who experienced the demo and tested the question
answering system. Of the reported 54,037 meeting registrants, 2,927 visited the
demonstration booth, 1,991 experienced the demo, and 1,025 completed a
post-demonstration survey. In this paper, the methodology of the survey is
shown and a summary of its results are presented. The results of the survey
show a very high level of receptiveness to cognitive computing technology and
artificial intelligence among radiologists."
"The whole brain architecture approach: Accelerating the development of
  artificial general intelligence by referring to the brain","The vastness of the design space created by the combination of a large number
of computational mechanisms, including machine learning, is an obstacle to
creating an artificial general intelligence (AGI). Brain-inspired AGI
development, in other words, cutting down the design space to look more like a
biological brain, which is an existing model of a general intelligence, is a
promising plan for solving this problem. However, it is difficult for an
individual to design a software program that corresponds to the entire brain
because the neuroscientific data required to understand the architecture of the
brain are extensive and complicated. The whole-brain architecture approach
divides the brain-inspired AGI development process into the task of designing
the brain reference architecture (BRA) -- the flow of information and the
diagram of corresponding components -- and the task of developing each
component using the BRA. This is called BRA-driven development. Another
difficulty lies in the extraction of the operating principles necessary for
reproducing the cognitive-behavioral function of the brain from neuroscience
data. Therefore, this study proposes the Structure-constrained Interface
Decomposition (SCID) method, which is a hypothesis-building method for creating
a hypothetical component diagram consistent with neuroscientific findings. The
application of this approach has begun for building various regions of the
brain. Moving forward, we will examine methods of evaluating the biological
plausibility of brain-inspired software. This evaluation will also be used to
prioritize different computational mechanisms, which should be merged,
associated with the same regions of the brain."
"The Effects of Air Quality on the Spread of the COVID-19 Pandemic in
  Italy: An Artificial Intelligence Approach","The COVID-19 pandemic considerably affects public health systems around the
world. The lack of knowledge about the virus, the extension of this phenomenon,
and the speed of the evolution of the infection are all factors that highlight
the necessity of employing new approaches to study these events. Artificial
intelligence techniques may be useful in analyzing data related to areas
affected by the virus. The aim of this work is to investigate any possible
relationships between air quality and confirmed cases of COVID-19 in Italian
districts. Specifically, we report an analysis of the correlation between daily
COVID-19 cases and environmental factors, such as temperature, relative
humidity, and atmospheric pollutants. Our analysis confirms a significant
association of some environmental parameters with the spread of the virus. This
suggests that machine learning models trained on the environmental parameters
to predict the number of future infected cases may be accurate. Predictive
models may be useful for helping institutions in making decisions for
protecting the population and contrasting the pandemic."
Toward Human-Level Artificial Intelligence,"In this paper, we present our research on programming human-level artificial
intelligence (HLAI), including 1) a definition of HLAI, 2) an environment to
develop and test HLAI, and 3) a cognitive architecture for HLAI. The term AI is
used in a broad meaning, and HLAI is not clearly defined. I claim that the
essence of Human-Level Intelligence to be the capability to learn from others'
experiences via language. The key is that the event described by language has
the same effect as if the agent experiences it firsthand for the update of the
behavior policy. To develop and test models with such a capability, we are
developing a simulated environment called SEDRo. There is a 3D Home, and a
mother character takes care of the baby (the learning agent) and teaches
languages. The environment provides comparable experiences to that of a human
baby from birth to one year. Finally, I propose a cognitive architecture of
HLAI called Modulated Heterarchical Prediction Memory (mHPM). In mHPM, there
are three components: a universal module that learns to predict the next vector
given the sequence of vector signals, a heterarchical network of those modules,
and a reward-based modulation of learning. mHPM models the workings of the
neocortex but the innate auxiliary units such hippocampus, reward system,
instincts, and amygdala play critical roles, too."
"Computational Imaging and Artificial Intelligence: The Next Revolution
  of Mobile Vision","Signal capture stands in the forefront to perceive and understand the
environment and thus imaging plays the pivotal role in mobile vision. Recent
explosive progresses in Artificial Intelligence (AI) have shown great potential
to develop advanced mobile platforms with new imaging devices. Traditional
imaging systems based on the ""capturing images first and processing afterwards""
mechanism cannot meet this unprecedented demand. Differently, Computational
Imaging (CI) systems are designed to capture high-dimensional data in an
encoded manner to provide more information for mobile vision systems.Thanks to
AI, CI can now be used in real systems by integrating deep learning algorithms
into the mobile vision platform to achieve the closed loop of intelligent
acquisition, processing and decision making, thus leading to the next
revolution of mobile vision.Starting from the history of mobile vision using
digital cameras, this work first introduces the advances of CI in diverse
applications and then conducts a comprehensive review of current research
topics combining CI and AI. Motivated by the fact that most existing studies
only loosely connect CI and AI (usually using AI to improve the performance of
CI and only limited works have deeply connected them), in this work, we propose
a framework to deeply integrate CI and AI by using the example of self-driving
vehicles with high-speed communication, edge computing and traffic planning.
Finally, we outlook the future of CI plus AI by investigating new materials,
brain science and new computing techniques to shed light on new directions of
mobile vision systems."
An algorithm for a fairer and better voting system,"The major finding, of this article, is an ensemble method, but more exactly,
a novel, better ranked voting system (and other variations of it), that aims to
solve the problem of finding the best candidate to represent the voters. We
have the source code on GitHub, for making realistic simulations of elections,
based on artificial intelligence for comparing different variations of the
algorithm, and other already known algorithms.
  We have convincing evidence that our algorithm is better than Instant-Runoff
Voting, Preferential Block Voting, Single Transferable Vote, and First Past The
Post (if certain, natural conditions are met, to support the wisdom of the
crowds). By also comparing with the best voter, we demonstrated the wisdom of
the crowds, suggesting that democracy (distributed system) is a better option
than dictatorship (centralized system), if those certain, natural conditions
are met.
  Voting systems are not restricted to politics, they are ensemble methods for
artificial intelligence, but the context of this article is natural
intelligence. It is important to find a system that is fair (e.g. freedom of
expression on the ballot exists), especially when the outcome of the voting
system has social impact: some voting systems have the unfair inevitability to
trend (over time) towards the same two major candidates (Duverger's law)."
The Powerful Use of AI in the Energy Sector: Intelligent Forecasting,"Artificial Intelligence (AI) techniques continue to broaden across
governmental and public sectors, such as power and energy - which serve as
critical infrastructures for most societal operations. However, due to the
requirements of reliability, accountability, and explainability, it is risky to
directly apply AI-based methods to power systems because society cannot afford
cascading failures and large-scale blackouts, which easily cost billions of
dollars. To meet society requirements, this paper proposes a methodology to
develop, deploy, and evaluate AI systems in the energy sector by: (1)
understanding the power system measurements with physics, (2) designing AI
algorithms to forecast the need, (3) developing robust and accountable AI
methods, and (4) creating reliable measures to evaluate the performance of the
AI model. The goal is to provide a high level of confidence to energy utility
users. For illustration purposes, the paper uses power system event forecasting
(PEF) as an example, which carefully analyzes synchrophasor patterns measured
by the Phasor Measurement Units (PMUs). Such a physical understanding leads to
a data-driven framework that reduces the dimensionality with physics and
forecasts the event with high credibility. Specifically, for dimensionality
reduction, machine learning arranges physical information from different
dimensions, resulting inefficient information extraction. For event
forecasting, the supervised learning model fuses the results of different
models to increase the confidence. Finally, comprehensive experiments
demonstrate the high accuracy, efficiency, and reliability as compared to other
state-of-the-art machine learning methods."
NRC-GAMMA: Introducing a Novel Large Gas Meter Image Dataset,"Automatic meter reading technology is not yet widespread. Gas, electricity,
or water accumulation meters reading is mostly done manually on-site either by
an operator or by the homeowner. In some countries, the operator takes a
picture as reading proof to confirm the reading by checking offline with
another operator and/or using it as evidence in case of conflicts or
complaints. The whole process is time-consuming, expensive, and prone to
errors. Automation can optimize and facilitate such labor-intensive and human
error-prone processes. With the recent advances in the fields of artificial
intelligence and computer vision, automatic meter reading systems are becoming
more viable than ever. Motivated by the recent advances in the field of
artificial intelligence and inspired by open-source open-access initiatives in
the research community, we introduce a novel large benchmark dataset of
real-life gas meter images, named the NRC-GAMMA dataset. The data were
collected from an Itron 400A diaphragm gas meter on January 20, 2020, between
00:05 am and 11:59 pm. We employed a systematic approach to label the images,
validate the labellings, and assure the quality of the annotations. The dataset
contains 28,883 images of the entire gas meter along with 57,766 cropped images
of the left and the right dial displays. We hope the NRC-GAMMA dataset helps
the research community to design and implement accurate, innovative,
intelligent, and reproducible automatic gas meter reading solutions."
"Emotions as abstract evaluation criteria in biological and artificial
  intelligences","Biological as well as advanced artificial intelligences (AIs) need to decide
which goals to pursue. We review nature's solution to the time allocation
problem, which is based on a continuously readjusted categorical weighting
mechanism we experience introspectively as emotions. One observes
phylogenetically that the available number of emotional states increases hand
in hand with the cognitive capabilities of animals and that raising levels of
intelligence entail ever larger sets of behavioral options. Our ability to
experience a multitude of potentially conflicting feelings is in this view not
a leftover of a more primitive heritage, but a generic mechanism for
attributing values to behavioral options that can not be specified at birth. In
this view, emotions are essential for understanding the mind.
  For concreteness, we propose and discuss a framework which mimics emotions on
a functional level. Based on time allocation via emotional stationarity (TAES),
emotions are implemented as abstract criteria, such as satisfaction, challenge
and boredom, which serve to evaluate activities that have been carried out. The
resulting timeline of experienced emotions is compared with the `character' of
the agent, which is defined in terms of a preferred distribution of emotional
states. The long-term goal of the agent, to align experience with character, is
achieved by optimizing the frequency for selecting individual tasks. Upon
optimization, the statistics of emotion experience becomes stationary."
"Applications of blockchain and artificial intelligence technologies for
  enabling prosumers in smart grids: A review","Governments' net zero emission target aims at increasing the share of
renewable energy sources as well as influencing the behaviours of consumers to
support the cost-effective balancing of energy supply and demand. These will be
achieved by the advanced information and control infrastructures of smart grids
which allow the interoperability among various stakeholders. Under this
circumstance, increasing number of consumers produce, store, and consume
energy, giving them a new role of prosumers. The integration of prosumers and
accommodation of incurred bidirectional flows of energy and information rely on
two key factors: flexible structures of energy markets and intelligent
operations of power systems. The blockchain and artificial intelligence (AI)
are innovative technologies to fulfil these two factors, by which the
blockchain provides decentralised trading platforms for energy markets and the
AI supports the optimal operational control of power systems. This paper
attempts to address how to incorporate the blockchain and AI in the smart grids
for facilitating prosumers to participate in energy markets. To achieve this
objective, first, this paper reviews how policy designs price carbon emissions
caused by the fossil-fuel based generation so as to facilitate the integration
of prosumers with renewable energy sources. Second, the potential structures of
energy markets with the support of the blockchain technologies are discussed.
Last, how to apply the AI for enhancing the state monitoring and decision
making during the operations of power systems is introduced."
Artificial Intelligence for the Metaverse: A Survey,"Along with the massive growth of the Internet from the 1990s until now,
various innovative technologies have been created to bring users breathtaking
experiences with more virtual interactions in cyberspace. Many virtual
environments with thousands of services and applications, from social networks
to virtual gaming worlds, have been developed with immersive experience and
digital transformation, but most are incoherent instead of being integrated
into a platform. In this context, metaverse, a term formed by combining meta
and universe, has been introduced as a shared virtual world that is fueled by
many emerging technologies, such as fifth-generation networks and beyond,
virtual reality, and artificial intelligence (AI). Among such technologies, AI
has shown the great importance of processing big data to enhance immersive
experience and enable human-like intelligence of virtual agents. In this
survey, we make a beneficial effort to explore the role of AI in the foundation
and development of the metaverse. We first deliver a preliminary of AI,
including machine learning algorithms and deep learning architectures, and its
role in the metaverse. We then convey a comprehensive investigation of AI-based
methods concerning six technical aspects that have potentials for the
metaverse: natural language processing, machine vision, blockchain, networking,
digital twin, and neural interface, and being potential for the metaverse.
Subsequently, several AI-aided applications, such as healthcare, manufacturing,
smart cities, and gaming, are studied to be deployed in the virtual worlds.
Finally, we conclude the key contribution of this survey and open some future
research directions in AI for the metaverse."
"Improving Urban Mobility: using artificial intelligence and new
  technologies to connect supply and demand","As the demand for mobility in our society seems to increase, the various
issues centered on urban mobility are among those that worry most city
inhabitants in this planet. For instance, how to go from A to B in an efficient
(but also less stressful) way? These questions and concerns have not changed
even during the covid-19 pandemic; on the contrary, as the current stand,
people who are avoiding public transportation are only contributing to an
increase in the vehicular traffic. The are of intelligent transportation
systems (ITS) aims at investigating how to employ information and communication
technologies to problems related to transportation. This may mean monitoring
and managing the infrastructure (e.g., traffic roads, traffic signals, etc.).
However, currently, ITS is also targeting the management of demand. In this
panorama, artificial intelligence plays an important role, especially with the
advances in machine learning that translates in the use of computational
vision, connected and autonomous vehicles, agent-based simulation, among
others. In the present work, a survey of several works developed by our group
are discussed in a holistic perspective, i.e., they cover not only the supply
side (as commonly found in ITS works), but also the demand side, and, in an
novel perspective, the integration of both."
"Stochastic Coherence Over Attention Trajectory For Continuous Learning
  In Video Streams","Devising intelligent agents able to live in an environment and learn by
observing the surroundings is a longstanding goal of Artificial Intelligence.
From a bare Machine Learning perspective, challenges arise when the agent is
prevented from leveraging large fully-annotated dataset, but rather the
interactions with supervisory signals are sparsely distributed over space and
time. This paper proposes a novel neural-network-based approach to
progressively and autonomously develop pixel-wise representations in a video
stream. The proposed method is based on a human-like attention mechanism that
allows the agent to learn by observing what is moving in the attended
locations. Spatio-temporal stochastic coherence along the attention trajectory,
paired with a contrastive term, leads to an unsupervised learning criterion
that naturally copes with the considered setting. Differently from most
existing works, the learned representations are used in open-set
class-incremental classification of each frame pixel, relying on few
supervisions. Our experiments leverage 3D virtual environments and they show
that the proposed agents can learn to distinguish objects just by observing the
video stream. Inheriting features from state-of-the art models is not as
powerful as one might expect."
"Acquiring and Modelling Abstract Commonsense Knowledge via
  Conceptualization","Conceptualization, or viewing entities and situations as instances of
abstract concepts in mind and making inferences based on that, is a vital
component in human intelligence for commonsense reasoning. Despite recent
progress in artificial intelligence to acquire and model commonsense attributed
to neural language models and commonsense knowledge graphs (CKGs),
conceptualization is yet to be introduced thoroughly, making current approaches
ineffective to cover knowledge about countless diverse entities and situations
in the real world.
  To address the problem, we thoroughly study the role of conceptualization in
commonsense reasoning, and formulate a framework to replicate human conceptual
induction by acquiring abstract knowledge about events regarding abstract
concepts, as well as higher-level triples or inferences upon them. We then
apply the framework to ATOMIC, a large-scale human-annotated CKG, aided by the
taxonomy Probase. We annotate a dataset on the validity of contextualized
conceptualizations from ATOMIC on both event and triple levels, develop a
series of heuristic rules based on linguistic features, and train a set of
neural models to generate and verify abstract knowledge. Based on these
components, a pipeline to acquire abstract knowledge is built. A large abstract
CKG upon ATOMIC is then induced, ready to be instantiated to infer about unseen
entities or situations. Finally, we empirically show the benefits of augmenting
CKGs with abstract knowledge in downstream tasks like commonsense inference and
zero-shot commonsense QA."
"Stop ordering machine learning algorithms by their explainability! A
  user-centered investigation of performance and explainability","Machine learning algorithms enable advanced decision making in contemporary
intelligent systems. Research indicates that there is a tradeoff between their
model performance and explainability. Machine learning models with higher
performance are often based on more complex algorithms and therefore lack
explainability and vice versa. However, there is little to no empirical
evidence of this tradeoff from an end user perspective. We aim to provide
empirical evidence by conducting two user experiments. Using two distinct
datasets, we first measure the tradeoff for five common classes of machine
learning algorithms. Second, we address the problem of end user perceptions of
explainable artificial intelligence augmentations aimed at increasing the
understanding of the decision logic of high-performing complex models. Our
results diverge from the widespread assumption of a tradeoff curve and indicate
that the tradeoff between model performance and explainability is much less
gradual in the end user's perception. This is a stark contrast to assumed
inherent model interpretability. Further, we found the tradeoff to be
situational for example due to data complexity. Results of our second
experiment show that while explainable artificial intelligence augmentations
can be used to increase explainability, the type of explanation plays an
essential role in end user perception."
"Artificial Intelligence Empowered Multiple Access for Ultra Reliable and
  Low Latency THz Wireless Networks","Terahertz (THz) wireless networks are expected to catalyze the beyond fifth
generation (B5G) era. However, due to the directional nature and the
line-of-sight demand of THz links, as well as the ultra-dense deployment of THz
networks, a number of challenges that the medium access control (MAC) layer
needs to face are created. In more detail, the need of rethinking user
association and resource allocation strategies by incorporating artificial
intelligence (AI) capable of providing ""real-time"" solutions in complex and
frequently changing environments becomes evident. Moreover, to satisfy the
ultra-reliability and low-latency demands of several B5G applications, novel
mobility management approaches are required. Motivated by this, this article
presents a holistic MAC layer approach that enables intelligent user
association and resource allocation, as well as flexible and adaptive mobility
management, while maximizing systems' reliability through blockage
minimization. In more detail, a fast and centralized joint user association,
radio resource allocation, and blockage avoidance by means of a novel
metaheuristic-machine learning framework is documented, that maximizes the THz
networks performance, while minimizing the association latency by approximately
three orders of magnitude. To support, within the access point (AP) coverage
area, mobility management and blockage avoidance, a deep reinforcement learning
(DRL) approach for beam-selection is discussed. Finally, to support user
mobility between coverage areas of neighbor APs, a proactive hand-over
mechanism based on AI-assisted fast channel prediction is~reported."
Adaptive patch foraging in deep reinforcement learning agents,"Patch foraging is one of the most heavily studied behavioral optimization
challenges in biology. However, despite its importance to biological
intelligence, this behavioral optimization problem is understudied in
artificial intelligence research. Patch foraging is especially amenable to
study given that it has a known optimal solution, which may be difficult to
discover given current techniques in deep reinforcement learning. Here, we
investigate deep reinforcement learning agents in an ecological patch foraging
task. For the first time, we show that machine learning agents can learn to
patch forage adaptively in patterns similar to biological foragers, and
approach optimal patch foraging behavior when accounting for temporal
discounting. Finally, we show emergent internal dynamics in these agents that
resemble single-cell recordings from foraging non-human primates, which
complements experimental and theoretical work on the neural mechanisms of
biological foraging. This work suggests that agents interacting in complex
environments with ecologically valid pressures arrive at common solutions,
suggesting the emergence of foundational computations behind adaptive,
intelligent behavior in both biological and artificial agents."
"The Optimal Choice of Hypothesis Is the Weakest, Not the Shortest","If $A$ and $B$ are sets such that $A \subset B$, generalisation may be
understood as the inference from $A$ of a hypothesis sufficient to construct
$B$. One might infer any number of hypotheses from $A$, yet only some of those
may generalise to $B$. How can one know which are likely to generalise? One
strategy is to choose the shortest, equating the ability to compress
information with the ability to generalise (a proxy for intelligence). We
examine this in the context of a mathematical formalism of enactive cognition.
We show that compression is neither necessary nor sufficient to maximise
performance (measured in terms of the probability of a hypothesis
generalising). We formulate a proxy unrelated to length or simplicity, called
weakness. We show that if tasks are uniformly distributed, then there is no
choice of proxy that performs at least as well as weakness maximisation in all
tasks while performing strictly better in at least one. In experiments
comparing maximum weakness and minimum description length in the context of
binary arithmetic, the former generalised at between $1.1$ and $5$ times the
rate of the latter. We argue this demonstrates that weakness is a far better
proxy, and explains why Deepmind's Apperception Engine is able to generalise
effectively."
Emergent Causality and the Foundation of Consciousness,"To make accurate inferences in an interactive setting, an agent must not
confuse passive observation of events with having intervened to cause them. The
$do$ operator formalises interventions so that we may reason about their
effect. Yet there exist pareto optimal mathematical formalisms of general
intelligence in an interactive setting which, presupposing no explicit
representation of intervention, make maximally accurate inferences. We examine
one such formalism. We show that in the absence of a $do$ operator, an
intervention can be represented by a variable. We then argue that variables are
abstractions, and that need to explicitly represent interventions in advance
arises only because we presuppose these sorts of abstractions. The
aforementioned formalism avoids this and so, initial conditions permitting,
representations of relevant causal interventions will emerge through induction.
These emergent abstractions function as representations of one`s self and of
any other object, inasmuch as the interventions of those objects impact the
satisfaction of goals. We argue that this explains how one might reason about
one`s own identity and intent, those of others, of one`s own as perceived by
others and so on. In a narrow sense this describes what it is to be aware, and
is a mechanistic explanation of aspects of consciousness."
"Understanding Policy and Technical Aspects of AI-Enabled Smart Video
  Surveillance to Address Public Safety","Recent advancements in artificial intelligence (AI) have seen the emergence
of smart video surveillance (SVS) in many practical applications, particularly
for building safer and more secure communities in our urban environments.
Cognitive tasks, such as identifying objects, recognizing actions, and
detecting anomalous behaviors, can produce data capable of providing valuable
insights to the community through statistical and analytical tools. However,
artificially intelligent surveillance systems design requires special
considerations for ethical challenges and concerns. The use and storage of
personally identifiable information (PII) commonly pose an increased risk to
personal privacy. To address these issues, this paper identifies the privacy
concerns and requirements needed to address when designing AI-enabled smart
video surveillance. Further, we propose the first end-to-end AI-enabled
privacy-preserving smart video surveillance system that holistically combines
computer vision analytics, statistical data analytics, cloud-native services,
and end-user applications. Finally, we propose quantitative and qualitative
metrics to evaluate intelligent video surveillance systems. The system shows
the 17.8 frame-per-second (FPS) processing in extreme video scenes. However,
considering privacy in designing such a system results in preferring the
pose-based algorithm to the pixel-based one. This choice resulted in dropping
accuracy in both action and anomaly detection tasks. The results drop from
97.48 to 73.72 in anomaly detection and 96 to 83.07 in the action detection
task. On average, the latency of the end-to-end system is 36.1 seconds."
"Opportunities and Challenges to Integrate Artificial Intelligence into
  Manufacturing Systems: Thoughts from a Panel Discussion","Rapid advances in artificial intelligence (AI) have the potential to
significantly increase the productivity, quality, and profitability in future
manufacturing systems. Traditional mass-production will give way to
personalized production, with each item made to order, at the low cost and
high-quality consumers have come to expect. Manufacturing systems will have the
intelligence to be resilient to multiple disruptions, from small-scale machine
breakdowns, to large-scale natural disasters. Products will be made with higher
precision and lower variability. While gains have been made towards the
development of these factories of the future, many challenges remain to fully
realize this vision. To consider the challenges and opportunities associated
with this topic, a panel of experts from Industry, Academia, and Government was
invited to participate in an active discussion at the 2022 Modeling, Estimation
and Control Conference (MECC) held in Jersey City, New Jersey from October 3-
5, 2022. The panel discussion focused on the challenges and opportunities to
more fully integrate AI into manufacturing systems. Three overarching themes
emerged from the panel discussion. First, to be successful, AI will need to
work seamlessly, and in an integrated manner with humans (and vice versa).
Second, significant gaps in the infrastructure needed to enable the full
potential of AI into the manufacturing ecosystem, including sufficient data
availability, storage, and analysis, must be addressed. And finally, improved
coordination between universities, industry, and government agencies can
facilitate greater opportunities to push the field forward. This article
briefly summarizes these three themes, and concludes with a discussion of
promising directions."
"End-User Development for Artificial Intelligence: A Systematic
  Literature Review","In recent years, Artificial Intelligence has become more and more relevant in
our society. Creating AI systems is almost always the prerogative of IT and AI
experts. However, users may need to create intelligent solutions tailored to
their specific needs. In this way, AI systems can be enhanced if new approaches
are devised to allow non-technical users to be directly involved in the
definition and personalization of AI technologies. End-User Development (EUD)
can provide a solution to these problems, allowing people to create, customize,
or adapt AI-based systems to their own needs. This paper presents a systematic
literature review that aims to shed the light on the current landscape of EUD
for AI systems, i.e., how users, even without skills in AI and/or programming,
can customize the AI behavior to their needs. This study also discusses the
current challenges of EUD for AI, the potential benefits, and the future
implications of integrating EUD into the overall AI development process."
"Artificial Neuropsychology: Are Large Language Models Developing
  Executive Functions?","Artificial Intelligence (AI) has been rapidly advancing and has demonstrated
its ability to perform a wide range of cognitive tasks, including language
processing, visual recognition, and decision-making. Part of this progress is
due to LLMs (Large Language Models) like those of the GPT (Generative
Pre-Trained Transformers) family. These models are capable of exhibiting
behavior that can be perceived as intelligent. Most authors in Neuropsychology
consider intelligent behavior to depend on a number of overarching skills, or
Executive Functions (EFs), which rely on the correct functioning of neural
networks in the frontal lobes, and have developed a series of tests to evaluate
them. In this work, we raise the question of whether LLMs are developing
executive functions similar to those of humans as part of their learning, and
we evaluate the planning function and working memory of GPT using the popular
Towers of Hanoi method. Additionally, we introduce a new variant of the
classical method in order to avoid that the solutions are found in the LLM
training data (dataleakeage). Preliminary results show that LLMs generates
near-optimal solutions in Towers of Hanoi related tasks, adheres to task
constraints, and exhibits rapid planning capabilities and efficient working
memory usage, indicating a potential development of executive functions.
However, these abilities are quite limited and worse than well-trained humans
when the tasks are not known and are not part of the training data."
"Vision-Language Models in Remote Sensing: Current Progress and Future
  Trends","The remarkable achievements of ChatGPT and GPT-4 have sparked a wave of
interest and research in the field of large language models for Artificial
General Intelligence (AGI). These models provide intelligent solutions close to
human thinking, enabling us to use general artificial intelligence to solve
problems in various applications. However, in remote sensing (RS), the
scientific literature on the implementation of AGI remains relatively scant.
Existing AI-related research in remote sensing primarily focuses on visual
understanding tasks while neglecting the semantic understanding of the objects
and their relationships. This is where vision-language models excel, as they
enable reasoning about images and their associated textual descriptions,
allowing for a deeper understanding of the underlying semantics.
Vision-language models can go beyond visual recognition of RS images, model
semantic relationships, and generate natural language descriptions of the
image. This makes them better suited for tasks requiring visual and textual
understanding, such as image captioning, and visual question answering. This
paper provides a comprehensive review of the research on vision-language models
in remote sensing, summarizing the latest progress, highlighting challenges,
and identifying potential research opportunities."
"ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health
  Management: A Survey and Roadmaps","Prognostics and health management (PHM) technology plays a critical role in
industrial production and equipment maintenance by identifying and predicting
possible equipment failures and damages, thereby allowing necessary maintenance
measures to be taken to enhance equipment service life and reliability while
reducing production costs and downtime. In recent years, PHM technology based
on artificial intelligence (AI) has made remarkable achievements in the context
of the industrial IoT and big data, and it is widely used in various
industries, such as railway, energy, and aviation, for condition monitoring,
fault prediction, and health management. The emergence of large-scale
foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of
AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved
from a research paradigm of single-modal, single-task, and limited-data to a
multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT
represents a landmark achievement in this research paradigm, offering hope for
general artificial intelligence due to its highly intelligent natural language
understanding ability. However, the PHM field lacks a consensus on how to
respond to this significant change in the AI field, and a systematic review and
roadmap is required to elucidate future development directions. To fill this
gap, this paper systematically expounds on the key components and latest
developments of LSF-Models. Then, we systematically answered how to build the
LSF-Model applicable to PHM tasks and outlined the challenges and future
development roadmaps for this research paradigm."
XInsight: Revealing Model Insights for GNNs with Flow-based Explanations,"Progress in graph neural networks has grown rapidly in recent years, with
many new developments in drug discovery, medical diagnosis, and recommender
systems. While this progress is significant, many networks are `black boxes'
with little understanding of the `what' exactly the network is learning. Many
high-stakes applications, such as drug discovery, require human-intelligible
explanations from the models so that users can recognize errors and discover
new knowledge. Therefore, the development of explainable AI algorithms is
essential for us to reap the benefits of AI.
  We propose an explainability algorithm for GNNs called eXplainable Insight
(XInsight) that generates a distribution of model explanations using GFlowNets.
Since GFlowNets generate objects with probabilities proportional to a reward,
XInsight can generate a diverse set of explanations, compared to previous
methods that only learn the maximum reward sample. We demonstrate XInsight by
generating explanations for GNNs trained on two graph classification tasks:
classifying mutagenic compounds with the MUTAG dataset and classifying acyclic
graphs with a synthetic dataset that we have open-sourced. We show the utility
of XInsight's explanations by analyzing the generated compounds using QSAR
modeling, and we find that XInsight generates compounds that cluster by
lipophilicity, a known correlate of mutagenicity. Our results show that
XInsight generates a distribution of explanations that uncovers the underlying
relationships demonstrated by the model. They also highlight the importance of
generating a diverse set of explanations, as it enables us to discover hidden
relationships in the model and provides valuable guidance for further analysis."
Large Generative AI Models for Telecom: The Next Big Thing?,"The evolution of generative artificial intelligence (GenAI) constitutes a
turning point in reshaping the future of technology in different aspects.
Wireless networks in particular, with the blooming of self-evolving networks,
represent a rich field for exploiting GenAI and reaping several benefits that
can fundamentally change the way how wireless networks are designed and
operated nowadays. To be specific, large GenAI models are envisioned to open up
a new era of autonomous wireless networks, in which multi-modal GenAI models
trained over various Telecom data, can be fine-tuned to perform several
downstream tasks, eliminating the need for building and training dedicated AI
models for each specific task and paving the way for the realization of
artificial general intelligence (AGI)-empowered wireless networks. In this
article, we aim to unfold the opportunities that can be reaped from integrating
large GenAI models into the Telecom domain. In particular, we first highlight
the applications of large GenAI models in future wireless networks, defining
potential use-cases and revealing insights on the associated theoretical and
practical challenges. Furthermore, we unveil how 6G can open up new
opportunities through connecting multiple on-device large GenAI models, and
hence, paves the way to the collective intelligence paradigm. Finally, we put a
forward-looking vision on how large GenAI models will be the key to realize
self-evolving networks."
"A Comprehensive Survey of Artificial Intelligence Techniques for Talent
  Analytics","In today's competitive and fast-evolving business environment, it is a
critical time for organizations to rethink how to make talent-related decisions
in a quantitative manner. Indeed, the recent development of Big Data and
Artificial Intelligence (AI) techniques have revolutionized human resource
management. The availability of large-scale talent and management-related data
provides unparalleled opportunities for business leaders to comprehend
organizational behaviors and gain tangible knowledge from a data science
perspective, which in turn delivers intelligence for real-time decision-making
and effective talent management at work for their organizations. In the last
decade, talent analytics has emerged as a promising field in applied data
science for human resource management, garnering significant attention from AI
communities and inspiring numerous research efforts. To this end, we present an
up-to-date and comprehensive survey on AI technologies used for talent
analytics in the field of human resource management. Specifically, we first
provide the background knowledge of talent analytics and categorize various
pertinent data. Subsequently, we offer a comprehensive taxonomy of relevant
research efforts, categorized based on three distinct application-driven
scenarios: talent management, organization management, and labor market
analysis. In conclusion, we summarize the open challenges and potential
prospects for future research directions in the domain of AI-driven talent
analytics."
"Trustworthy and Synergistic Artificial Intelligence for Software
  Engineering: Vision and Roadmaps","For decades, much software engineering research has been dedicated to
devising automated solutions aimed at enhancing developer productivity and
elevating software quality. The past two decades have witnessed an unparalleled
surge in the development of intelligent solutions tailored for software
engineering tasks. This momentum established the Artificial Intelligence for
Software Engineering (AI4SE) area, which has swiftly become one of the most
active and popular areas within the software engineering field.
  This Future of Software Engineering (FoSE) paper navigates through several
focal points. It commences with a succinct introduction and history of AI4SE.
Thereafter, it underscores the core challenges inherent to AI4SE, particularly
highlighting the need to realize trustworthy and synergistic AI4SE.
Progressing, the paper paints a vision for the potential leaps achievable if
AI4SE's key challenges are surmounted, suggesting a transition towards Software
Engineering 2.0. Two strategic roadmaps are then laid out: one centered on
realizing trustworthy AI4SE, and the other on fostering synergistic AI4SE.
While this paper may not serve as a conclusive guide, its intent is to catalyze
further progress. The ultimate aspiration is to position AI4SE as a linchpin in
redefining the horizons of software engineering, propelling us toward Software
Engineering 2.0."
"Advancing Perception in Artificial Intelligence through Principles of
  Cognitive Science","Although artificial intelligence (AI) has achieved many feats at a rapid
pace, there still exist open problems and fundamental shortcomings related to
performance and resource efficiency. Since AI researchers benchmark a
significant proportion of performance standards through human intelligence,
cognitive sciences-inspired AI is a promising domain of research. Studying
cognitive science can provide a fresh perspective to building fundamental
blocks in AI research, which can lead to improved performance and efficiency.
In this review paper, we focus on the cognitive functions of perception, which
is the process of taking signals from one's surroundings as input, and
processing them to understand the environment. Particularly, we study and
compare its various processes through the lens of both cognitive sciences and
AI. Through this study, we review all current major theories from various
sub-disciplines of cognitive science (specifically neuroscience, psychology and
linguistics), and draw parallels with theories and techniques from current
practices in AI. We, hence, present a detailed collection of methods in AI for
researchers to build AI systems inspired by cognitive science. Further, through
the process of reviewing the state of cognitive-inspired AI, we point out many
gaps in the current state of AI (with respect to the performance of the human
brain), and hence present potential directions for researchers to develop
better perception systems in AI."
"Mobile Traffic Prediction at the Edge Through Distributed and Deep
  Transfer Learning","Traffic prediction represents one of the crucial tasks for smartly optimizing
the mobile network. Recently, Artificial Intelligence (AI) has attracted
attention to solve this problem thanks to its ability in cognizing the state of
the mobile network and make intelligent decisions. Research on this topic has
concentrated on making predictions in a centralized fashion, i.e., by
collecting data from the different network elements and process them in a cloud
center. This translates into inefficiencies due to the large amount of data
transmissions and computations required, leading to high energy consumption. In
this work, we investigate a fully decentralized AI solution for mobile traffic
prediction that allows data to be kept locally, reducing energy consumption
through collaboration among the base station sites. To do so, we propose a
novel prediction framework based on edge computing and Deep Transfer Learning
(DTL) techniques, using datasets obtained at the edge through a large
measurement campaign. Two main Deep Learning architectures are designed based
on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs)
and tested under different training conditions. Simulation results show that
the CNN architectures outperform the RNNs in accuracy and consume less energy.
In both scenarios, DTL contributes to an accuracy enhancement in 85% of the
examined cases compared to their stand-alone counterparts. Additionally, DTL
significantly reduces computational complexity and energy consumption during
training, resulting in a reduction of the energy footprint by 60% for CNNs and
90% for RNNs. Finally, two cutting-edge eXplainable Artificial Intelligence
techniques are employed to interpret the derived learning models."
"Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open
  Challenges and Interdisciplinary Research Directions","As systems based on opaque Artificial Intelligence (AI) continue to flourish
in diverse real-world applications, understanding these black box models has
become paramount. In response, Explainable AI (XAI) has emerged as a field of
research with practical and ethical benefits across various domains. This paper
not only highlights the advancements in XAI and its application in real-world
scenarios but also addresses the ongoing challenges within XAI, emphasizing the
need for broader perspectives and collaborative efforts. We bring together
experts from diverse fields to identify open problems, striving to synchronize
research agendas and accelerate XAI in practical applications. By fostering
collaborative discussion and interdisciplinary cooperation, we aim to propel
XAI forward, contributing to its continued success. Our goal is to put forward
a comprehensive proposal for advancing XAI. To achieve this goal, we present a
manifesto of 27 open problems categorized into nine categories. These
challenges encapsulate the complexities and nuances of XAI and offer a road map
for future research. For each problem, we provide promising research directions
in the hope of harnessing the collective intelligence of interested
stakeholders."
"A Survey of Blockchain, Artificial Intelligence, and Edge Computing for
  Web 3.0","Web 3.0, as the third generation of the World Wide Web, aims to solve
contemporary problems of trust, centralization, and data ownership. Driven by
the latest advances in cutting-edge technologies, Web 3.0 is moving towards a
more open, decentralized, intelligent, and interconnected network. However,
increasingly widespread data breaches have raised awareness of online privacy
and security of personal data. Additionally, since Web 3.0 is a sophisticated
and complex convergence, the technical details behind it are not as clear as
the characteristics it presents. In this survey, we conduct an in-depth
exploration of Web 3.0 from the perspectives of blockchain, artificial
intelligence, and edge computing. Specifically, we begin with summarizing the
evolution of the Internet and providing an overview of these three key
technological factors. Afterward, we provide a thorough analysis of each
technology separately, including its relevance to Web 3.0, key technology
components, and practical applications. We also propose decentralized storage
and computing solutions by exploring the integration of technologies. Finally,
we highlight the key challenges alongside potential research directions.
Through the combination and mutual complementation of multiple technologies,
Web 3.0 is expected to return more control and ownership of data and digital
assets back to users."
"On the Interplay of Artificial Intelligence and Space-Air-Ground
  Integrated Networks: A Survey","Space-Air-Ground Integrated Networks (SAGINs), which incorporate space and
aerial networks with terrestrial wireless systems, are vital enablers of the
emerging sixth-generation (6G) wireless networks. Besides bringing significant
benefits to various applications and services, SAGINs are envisioned to extend
high-speed broadband coverage to remote areas, such as small towns or mining
sites, or areas where terrestrial infrastructure cannot reach, such as
airplanes or maritime use cases. However, due to the limited power and storage
resources, as well as other constraints introduced by the design of terrestrial
networks, SAGINs must be intelligently configured and controlled to satisfy the
envisioned requirements. Meanwhile, Artificial Intelligence (AI) is another
critical enabler of 6G. Due to massive amounts of available data, AI has been
leveraged to address pressing challenges of current and future wireless
networks. By adding AI and facilitating the decision-making and prediction
procedures, SAGINs can effectively adapt to their surrounding environment, thus
enhancing the performance of various metrics. In this work, we aim to
investigate the interplay of AI and SAGINs by providing a holistic overview of
state-of-the-art research in AI-enabled SAGINs. Specifically, we present a
comprehensive overview of some potential applications of AI in SAGINs. We also
cover open issues in employing AI and detail the contributions of SAGINs in the
development of AI. Finally, we highlight some limitations of the existing
research works and outline potential future research directions."
Position: Stop Making Unscientific AGI Performance Claims,"Developments in the field of Artificial Intelligence (AI), and particularly
large language models (LLMs), have created a 'perfect storm' for observing
'sparks' of Artificial General Intelligence (AGI) that are spurious. Like
simpler models, LLMs distill meaningful representations in their latent
embeddings that have been shown to correlate with external variables.
Nonetheless, the correlation of such representations has often been linked to
human-like intelligence in the latter but not the former. We probe models of
varying complexity including random projections, matrix decompositions, deep
autoencoders and transformers: all of them successfully distill information
that can be used to predict latent or external variables and yet none of them
have previously been linked to AGI. We argue and empirically demonstrate that
the finding of meaningful patterns in latent spaces of models cannot be seen as
evidence in favor of AGI. Additionally, we review literature from the social
sciences that shows that humans are prone to seek such patterns and
anthropomorphize. We conclude that both the methodological setup and common
public image of AI are ideal for the misinterpretation that correlations
between model representations and some variables of interest are 'caused' by
the model's understanding of underlying 'ground truth' relationships. We,
therefore, call for the academic community to exercise extra caution, and to be
keenly aware of principles of academic integrity, in interpreting and
communicating about AI research outcomes."
Understanding Biology in the Age of Artificial Intelligence,"Modern life sciences research is increasingly relying on artificial
intelligence approaches to model biological systems, primarily centered around
the use of machine learning (ML) models. Although ML is undeniably useful for
identifying patterns in large, complex data sets, its widespread application in
biological sciences represents a significant deviation from traditional methods
of scientific inquiry. As such, the interplay between these models and
scientific understanding in biology is a topic with important implications for
the future of scientific research, yet it is a subject that has received little
attention. Here, we draw from an epistemological toolkit to contextualize
recent applications of ML in biological sciences under modern philosophical
theories of understanding, identifying general principles that can guide the
design and application of ML systems to model biological phenomena and advance
scientific knowledge. We propose that conceptions of scientific understanding
as information compression, qualitative intelligibility, and dependency
relation modelling provide a useful framework for interpreting ML-mediated
understanding of biological systems. Through a detailed analysis of two key
application areas of ML in modern biological research - protein structure
prediction and single cell RNA-sequencing - we explore how these features have
thus far enabled ML systems to advance scientific understanding of their target
phenomena, how they may guide the development of future ML models, and the key
obstacles that remain in preventing ML from achieving its potential as a tool
for biological discovery. Consideration of the epistemological features of ML
applications in biology will improve the prospects of these methods to solve
important problems and advance scientific understanding of living systems."
A Hybrid Intelligence Method for Argument Mining,"Large-scale survey tools enable the collection of citizen feedback in opinion
corpora. Extracting the key arguments from a large and noisy set of opinions
helps in understanding the opinions quickly and accurately. Fully automated
methods can extract arguments but (1) require large labeled datasets that
induce large annotation costs and (2) work well for known viewpoints, but not
for novel points of view. We propose HyEnA, a hybrid (human + AI) method for
extracting arguments from opinionated texts, combining the speed of automated
processing with the understanding and reasoning capabilities of humans. We
evaluate HyEnA on three citizen feedback corpora. We find that, on the one
hand, HyEnA achieves higher coverage and precision than a state-of-the-art
automated method when compared to a common set of diverse opinions, justifying
the need for human insight. On the other hand, HyEnA requires less human effort
and does not compromise quality compared to (fully manual) expert analysis,
demonstrating the benefit of combining human and artificial intelligence."
"Emotional Intelligence Through Artificial Intelligence : NLP and Deep
  Learning in the Analysis of Healthcare Texts","This manuscript presents a methodical examination of the utilization of
Artificial Intelligence in the assessment of emotions in texts related to
healthcare, with a particular focus on the incorporation of Natural Language
Processing and deep learning technologies. We scrutinize numerous research
studies that employ AI to augment sentiment analysis, categorize emotions, and
forecast patient outcomes based on textual information derived from clinical
narratives, patient feedback on medications, and online health discussions. The
review demonstrates noteworthy progress in the precision of algorithms used for
sentiment classification, the prognostic capabilities of AI models for
neurodegenerative diseases, and the creation of AI-powered systems that offer
support in clinical decision-making. Remarkably, the utilization of AI
applications has exhibited an enhancement in personalized therapy plans by
integrating patient sentiment and contributing to the early identification of
mental health disorders. There persist challenges, which encompass ensuring the
ethical application of AI, safeguarding patient confidentiality, and addressing
potential biases in algorithmic procedures. Nevertheless, the potential of AI
to revolutionize healthcare practices is unmistakable, offering a future where
healthcare is not only more knowledgeable and efficient but also more
empathetic and centered around the needs of patients. This investigation
underscores the transformative influence of AI on healthcare, delivering a
comprehensive comprehension of its role in examining emotional content in
healthcare texts and highlighting the trajectory towards a more compassionate
approach to patient care. The findings advocate for a harmonious synergy
between AI's analytical capabilities and the human aspects of healthcare."
"Topological Representations of Heterogeneous Learning Dynamics of
  Recurrent Spiking Neural Networks","Spiking Neural Networks (SNNs) have become an essential paradigm in
neuroscience and artificial intelligence, providing brain-inspired computation.
Recent advances in literature have studied the network representations of deep
neural networks. However, there has been little work that studies
representations learned by SNNs, especially using unsupervised local learning
methods like spike-timing dependent plasticity (STDP). Recent work by
\cite{barannikov2021representation} has introduced a novel method to compare
topological mappings of learned representations called Representation Topology
Divergence (RTD). Though useful, this method is engineered particularly for
feedforward deep neural networks and cannot be used for recurrent networks like
Recurrent SNNs (RSNNs). This paper introduces a novel methodology to use RTD to
measure the difference between distributed representations of RSNN models with
different learning methods. We propose a novel reformulation of RSNNs using
feedforward autoencoder networks with skip connections to help us compute the
RTD for recurrent networks. Thus, we investigate the learning capabilities of
RSNN trained using STDP and the role of heterogeneity in the synaptic dynamics
in learning such representations. We demonstrate that heterogeneous STDP in
RSNNs yield distinct representations than their homogeneous and surrogate
gradient-based supervised learning counterparts. Our results provide insights
into the potential of heterogeneous SNN models, aiding the development of more
efficient and biologically plausible hybrid artificial intelligence systems."
"Quantum-activated neural reservoirs on-chip open up large hardware
  security models for resilient authentication","Quantum artificial intelligence is a frontier of artificial intelligence
research, pioneering quantum AI-powered circuits to address problems beyond the
reach of deep learning with classical architectures. This work implements a
large-scale quantum-activated recurrent neural network possessing more than 3
trillion hardware nodes/cm$^2$, originating from repeatable atomic-scale
nucleation dynamics in an amorphous material integrated on-chip, controlled
with 0.07 nW electric power per readout channel. Compared to the
best-performing reservoirs currently reported, this implementation increases
the scale of the network by two orders of magnitude and reduces the power
consumption by six, reaching power efficiencies in the range of the human
brain, dissipating 0.2 nW/neuron. When interrogated by a classical input, the
chip implements a large-scale hardware security model, enabling dictionary-free
authentication secure against statistical inference attacks, including AI's
present and future development, even for an adversary with a copy of all the
classical components available. Experimental tests report 99.6% reliability,
100% user authentication accuracy, and an ideal 50% key uniqueness. Due to its
quantum nature, the chip supports a bit density per feature size area three
times higher than the best technology available, with the capacity to store
more than $2^{1104}$ keys in a footprint of 1 cm$^2$. Such a quantum-powered
platform could help counteract the emerging form of warfare led by the
cybercrime industry in breaching authentication to target small to large-scale
facilities, from private users to intelligent energy grids."
Is Complexity an Illusion?,"Simplicity is held by many to be the key to general intelligence. Simpler
models tend to ""generalise"", identifying the cause or generator of data with
greater sample efficiency. The implications of the correlation between
simplicity and generalisation extend far beyond computer science, addressing
questions of physics and even biology. Yet simplicity is a property of form,
while generalisation is of function. In interactive settings, any correlation
between the two depends on interpretation. In theory there could be no
correlation and yet in practice, there is. Previous theoretical work showed
generalisation to be a consequence of ""weak"" constraints implied by function,
not form. Experiments demonstrated choosing weak constraints over simple forms
yielded a 110-500% improvement in generalisation rate. Here we show that all
constraints can take equally simple forms, regardless of weakness. However if
forms are spatially extended, then function is represented using a finite
subset of forms. If function is represented using a finite subset of forms,
then we can force a correlation between simplicity and generalisation by making
weak constraints take simple forms. If function is determined by a goal
directed process that favours versatility (e.g. natural selection), then
efficiency demands weak constraints take simple forms. Complexity has no causal
influence on generalisation, but appears to due to confounding."
Sustainability of Data Center Digital Twins with Reinforcement Learning,"The rapid growth of machine learning (ML) has led to an increased demand for
computational power, resulting in larger data centers (DCs) and higher energy
consumption. To address this issue and reduce carbon emissions, intelligent
design and control of DC components such as IT servers, cabinets, HVAC cooling,
flexible load shifting, and battery energy storage are essential. However, the
complexity of designing and controlling them in tandem presents a significant
challenge. While some individual components like CFD-based design and
Reinforcement Learning (RL) based HVAC control have been researched, there's a
gap in the holistic design and optimization covering all elements
simultaneously. To tackle this, we've developed DCRL-Green, a multi-agent RL
environment that empowers the ML community to design data centers and research,
develop, and refine RL controllers for carbon footprint reduction in DCs. It is
a flexible, modular, scalable, and configurable platform that can handle large
High Performance Computing (HPC) clusters. Furthermore, in its default setup,
DCRL-Green provides a benchmark for evaluating single as well as multi-agent RL
algorithms. It easily allows users to subclass the default implementations and
design their own control approaches, encouraging community development for
sustainable data centers. Open Source Link:
https://github.com/HewlettPackard/dc-rl"
"The Role of Artificial Intelligence and Machine Learning in Software
  Testing","Artificial Intelligence (AI) and Machine Learning (ML) have significantly
impacted various industries, including software development. Software testing,
a crucial part of the software development lifecycle (SDLC), ensures the
quality and reliability of software products. Traditionally, software testing
has been a labor-intensive process requiring significant manual effort.
However, the advent of AI and ML has transformed this landscape by introducing
automation and intelligent decision-making capabilities. AI and ML technologies
enhance the efficiency and effectiveness of software testing by automating
complex tasks such as test case generation, test execution, and result
analysis. These technologies reduce the time required for testing and improve
the accuracy of defect detection, ultimately leading to higher quality
software. AI can predict potential areas of failure by analyzing historical
data and identifying patterns, which allows for more targeted and efficient
testing. This paper explores the role of AI and ML in software testing by
reviewing existing literature, analyzing current tools and techniques, and
presenting case studies that demonstrate the practical benefits of these
technologies. The literature review provides a comprehensive overview of the
advancements in AI and ML applications in software testing, highlighting key
methodologies and findings from various studies. The analysis of current tools
showcases the capabilities of popular AI-driven testing tools such as Eggplant
AI, Test.ai, Selenium, Appvance, Applitools Eyes, Katalon Studio, and Tricentis
Tosca, each offering unique features and advantages. Case studies included in
this paper illustrate real-world applications of AI and ML in software testing,
showing significant improvements in testing efficiency, accuracy, and overall
software quality."
"Online Decision MetaMorphFormer: A Casual Transformer-Based
  Reinforcement Learning Framework of Universal Embodied Intelligence","Interactive artificial intelligence in the motion control field is an
interesting topic, especially when universal knowledge is adaptive to multiple
tasks and universal environments. Despite there being increasing efforts in the
field of Reinforcement Learning (RL) with the aid of transformers, most of them
might be limited by the offline training pipeline, which prohibits exploration
and generalization abilities. To address this limitation, we propose the
framework of Online Decision MetaMorphFormer (ODM) which aims to achieve
self-awareness, environment recognition, and action planning through a unified
model architecture. Motivated by cognitive and behavioral psychology, an ODM
agent is able to learn from others, recognize the world, and practice itself
based on its own experience. ODM can also be applied to any arbitrary agent
with a multi-joint body, located in different environments, and trained with
different types of tasks using large-scale pre-trained datasets. Through the
use of pre-trained datasets, ODM can quickly warm up and learn the necessary
knowledge to perform the desired task, while the target environment continues
to reinforce the universal policy. Extensive online experiments as well as
few-shot and zero-shot environmental tests are used to verify ODM's performance
and generalization ability. The results of our study contribute to the study of
general artificial intelligence in embodied and cognitive fields. Code,
results, and video examples can be found on the website
\url{https://rlodm.github.io/odm/}."
"Real-Time Pedestrian Detection on IoT Edge Devices: A Lightweight Deep
  Learning Approach","Artificial intelligence (AI) has become integral to our everyday lives.
Computer vision has advanced to the point where it can play the safety critical
role of detecting pedestrians at road intersections in intelligent
transportation systems and alert vehicular traffic as to potential collisions.
Centralized computing analyzes camera feeds and generates alerts for nearby
vehicles. However, real-time applications face challenges such as latency,
limited data transfer speeds, and the risk of life loss. Edge servers offer a
potential solution for real-time applications, providing localized computing
and storage resources and lower response times. Unfortunately, edge servers
have limited processing power. Lightweight deep learning (DL) techniques enable
edge servers to utilize compressed deep neural network (DNN) models.
  The research explores implementing a lightweight DL model on Artificial
Intelligence of Things (AIoT) edge devices. An optimized You Only Look Once
(YOLO) based DL model is deployed for real-time pedestrian detection, with
detection events transmitted to the edge server using the Message Queuing
Telemetry Transport (MQTT) protocol. The simulation results demonstrate that
the optimized YOLO model can achieve real-time pedestrian detection, with a
fast inference speed of 147 milliseconds, a frame rate of 2.3 frames per
second, and an accuracy of 78%, representing significant improvements over
baseline models."
"RAB$^2$-DEF: Dynamic and explainable defense against adversarial attacks
  in Federated Learning to fair poor clients","At the same time that artificial intelligence is becoming popular, concern
and the need for regulation is growing, including among other requirements the
data privacy. In this context, Federated Learning is proposed as a solution to
data privacy concerns derived from different source data scenarios due to its
distributed learning. The defense mechanisms proposed in literature are just
focused on defending against adversarial attacks and the performance, leaving
aside other important qualities such as explainability, fairness to poor
quality clients, dynamism in terms of attacks configuration and generality in
terms of being resilient against different kinds of attacks. In this work, we
propose RAB$^2$-DEF, a $\textbf{r}$esilient $\textbf{a}$gainst
$\textbf{b}\text{yzantine}$ and $\textbf{b}$ackdoor attacks which is
$\textbf{d}$ynamic, $\textbf{e}$xplainable and $\textbf{f}$air to poor clients
using local linear explanations. We test the performance of RAB$^2$-DEF in
image datasets and both byzantine and backdoor attacks considering the
state-of-the-art defenses and achieve that RAB$^2$-DEF is a proper defense at
the same time that it boosts the other qualities towards trustworthy artificial
intelligence."
"An Integrated Artificial Intelligence Operating System for Advanced
  Low-Altitude Aviation Applications","This paper introduces a high-performance artificial intelligence operating
system tailored for low-altitude aviation, designed to address key challenges
such as real-time task execution, computational efficiency, and seamless
modular collaboration. Built on a powerful hardware platform and leveraging the
UNIX architecture, the system implements a distributed data processing strategy
that ensures rapid and efficient synchronization across critical modules,
including vision, navigation, and perception. By adopting dynamic resource
management, it optimally allocates computational resources, such as CPU and
GPU, based on task priority and workload, ensuring high performance for
demanding tasks like real-time video processing and AI model inference.
Furthermore, the system features an advanced interrupt handling mechanism that
allows for quick responses to sudden environmental changes, such as obstacle
detection, by prioritizing critical tasks, thus improving safety and mission
success rates. Robust security measures, including data encryption, access
control, and fault tolerance, ensure the system's resilience against external
threats and its ability to recover from potential hardware or software
failures. Complementing these core features are modular components for image
analysis, multi-sensor fusion, dynamic path planning, multi-drone coordination,
and ground station monitoring. Additionally, a low-code development platform
simplifies user customization, making the system adaptable to various
mission-specific needs. This comprehensive approach ensures the system meets
the evolving demands of intelligent aviation, providing a stable, efficient,
and secure environment for complex drone operations."
"SEMANTIC SEE-THROUGH GOGGLES: Wearing Linguistic Virtual Reality in
  (Artificial) Intelligence","When language is utilized as a medium to store and communicate sensory
information, there arises a kind of radical virtual reality, namely ""the
realities that are reduced into the same sentence are virtual/equivalent."" In
the current era, in which artificial intelligence engages in the linguistic
mediation of sensory information, it is imperative to re-examine the various
issues pertaining to this potential VR, particularly in relation to bias and
(dis)communication. Semantic See-through Goggles represent an experimental
framework for glasses through which the view is fully verbalized and
re-depicted into the wearer's view. The participants wear the goggles equipped
with a camera and head-mounted display (HMD). In real-time, the image captured
by the camera is converted by the AI into a single line of text, which is then
transformed into an image and presented to the user's eyes. This process
enables users to perceive and interact with the real physical world through
this redrawn view. We constructed a prototype of these goggles, examined their
fundamental characteristics, and then conducted a qualitative analysis of the
wearer's experience. This project investigates a methodology for subjectively
capturing the situation in which AI serves as a proxy for our perception of the
world. At the same time, It also attempts to appropriate some of the energy of
today's debate over artificial intelligence for a classical inquiry around the
fact that ""intelligence can only see the world under meaning."""
Postsingular Science,"This study presents, for the first time, a conceptual and formal model of
postsingular science (PSS), which analyses and interprets changes in scientific
knowledge driven by accelerating technological progress, singularity, and the
integration of artificial intelligence (AI) into scientific processes. The PSS
model is based on the interplay of six key components: cumulative knowledge,
intelligence, technological synergy, quantum information, social dynamics, and
environmental sustainability. The interaction of these variables is described
through a system of nonlinear differential equations, reflecting the complex
feedback loops and synergetic effects characteristic of the postsingular world.
A differentiation table contrasting postsingular and classical science is also
presented, highlighting the most fundamental differences between contemporary
classical science and future postsingular science. The model emphasizes the
synergy between humans and artificial intelligence, the role of quantum
technologies in accelerating scientific discovery, and the impact of social and
ecological factors that either constrain or stimulate scientific progress. It
is anticipated that new forms of scientific information dissemination will
replace traditional academic publications and that scientific processing will
reach an entirely new level of development following the singularity-driven
acceleration of technological progress and the integration of AI into R&D. This
will herald an era of nonstop, ultrarapid science operating 24/7. The synergy
of humans and artificial intelligence will create a scientific union on the
basis of fundamentally new principles and methods. This research provides an
initial theoretical foundation for further interdisciplinary studies aimed at
developing sustainable strategies and effectively managing scientific progress
in the postsingular era."
"Artificial Intelligence in Spectroscopy: Advancing Chemistry from
  Prediction to Generation and Beyond","The rapid advent of machine learning (ML) and artificial intelligence (AI)
has catalyzed major transformations in chemistry, yet the application of these
methods to spectroscopic and spectrometric data, referred to as Spectroscopy
Machine Learning (SpectraML), remains relatively underexplored. Modern
spectroscopic techniques (MS, NMR, IR, Raman, UV-Vis) generate an ever-growing
volume of high-dimensional data, creating a pressing need for automated and
intelligent analysis beyond traditional expert-based workflows. In this survey,
we provide a unified review of SpectraML, systematically examining
state-of-the-art approaches for both forward tasks (molecule-to-spectrum
prediction) and inverse tasks (spectrum-to-molecule inference). We trace the
historical evolution of ML in spectroscopy, from early pattern recognition to
the latest foundation models capable of advanced reasoning, and offer a
taxonomy of representative neural architectures, including graph-based and
transformer-based methods. Addressing key challenges such as data quality,
multimodal integration, and computational scalability, we highlight emerging
directions such as synthetic data generation, large-scale pretraining, and few-
or zero-shot learning. To foster reproducible research, we also release an
open-source repository containing recent papers and their corresponding curated
datasets (https://github.com/MINE-Lab-ND/SpectrumML_Survey_Papers). Our survey
serves as a roadmap for researchers, guiding progress at the intersection of
spectroscopy and AI."
Brain-Model Evaluations Need the NeuroAI Turing Test,"What makes an artificial system a good model of intelligence? The classical
test proposed by Alan Turing focuses on behavior, requiring that an artificial
agent's behavior be indistinguishable from that of a human. While behavioral
similarity provides a strong starting point, two systems with very different
internal representations can produce the same outputs. Thus, in modeling
biological intelligence, the field of NeuroAI often aims to go beyond
behavioral similarity and achieve representational convergence between a
model's activations and the measured activity of a biological system. This
position paper argues that the standard definition of the Turing Test is
incomplete for NeuroAI, and proposes a stronger framework called the ``NeuroAI
Turing Test'', a benchmark that extends beyond behavior alone and
\emph{additionally} requires models to produce internal neural representations
that are empirically indistinguishable from those of a brain up to measured
individual variability, i.e. the differences between a computational model and
the brain is no more than the difference between one brain and another brain.
While the brain is not necessarily the ceiling of intelligence, it remains the
only universally agreed-upon example, making it a natural reference point for
evaluating computational models. By proposing this framework, we aim to shift
the discourse from loosely defined notions of brain inspiration to a systematic
and testable standard centered on both behavior and internal representations,
providing a clear benchmark for neuroscientific modeling and AI development."
"Expectations vs Reality -- A Secondary Study on AI Adoption in Software
  Testing","In the software industry, artificial intelligence (AI) has been utilized more
and more in software development activities. In some activities, such as
coding, AI has already been an everyday tool, but in software testing
activities AI it has not yet made a significant breakthrough. In this paper,
the objective was to identify what kind of empirical research with industry
context has been conducted on AI in software testing, as well as how AI has
been adopted in software testing practice. To achieve this, we performed a
systematic mapping study of recent (2020 and later) studies on AI adoption in
software testing in the industry, and applied thematic analysis to identify
common themes and categories, such as the real-world use cases and benefits, in
the found papers. The observations suggest that AI is not yet heavily utilized
in software testing, and still relatively few studies on AI adoption in
software testing have been conducted in the industry context to solve
real-world problems. Earlier studies indicated there was a noticeable gap
between the actual use cases and actual benefits versus the expectations, which
we analyzed further. While there were numerous potential use cases for AI in
software testing, such as test case generation, code analysis, and intelligent
test automation, the reported actual implementations and observed benefits were
limited. In addition, the systematic mapping study revealed a potential problem
with false positive search results in online databases when using the search
string ""artificial intelligence""."
"Power Transformer Health Index and Life Span Assessment: A Comprehensive
  Review of Conventional and Machine Learning based Approaches","Power transformers play a critical role within the electrical power system,
making their health assessment and the prediction of their remaining lifespan
paramount for the purpose of ensuring efficient operation and facilitating
effective maintenance planning. This paper undertakes a comprehensive
examination of existent literature, with a primary focus on both conventional
and cutting-edge techniques employed within this domain. The merits and
demerits of recent methodologies and techniques are subjected to meticulous
scrutiny and explication. Furthermore, this paper expounds upon intelligent
fault diagnosis methodologies and delves into the most widely utilized
intelligent algorithms for the assessment of transformer conditions. Diverse
Artificial Intelligence (AI) approaches, including Artificial Neural Networks
(ANN) and Convolutional Neural Network (CNN), Support Vector Machine (SVM),
Random Forest (RF), Genetic Algorithm (GA), and Particle Swarm Optimization
(PSO), are elucidated offering pragmatic solutions for enhancing the
performance of transformer fault diagnosis. The amalgamation of multiple AI
methodologies and the exploration of timeseries analysis further contribute to
the augmentation of diagnostic precision and the early detection of faults in
transformers. By furnishing a comprehensive panorama of AI applications in the
field of transformer fault diagnosis, this study lays the groundwork for future
research endeavors and the progression of this critical area of study."
Meta-Learning Evolutionary Artificial Neural Networks,"In this paper, we present MLEANN (Meta-Learning Evolutionary Artificial
Neural Network), an automatic computational framework for the adaptive
optimization of artificial neural networks wherein the neural network
architecture, activation function, connection weights; learning algorithm and
its parameters are adapted according to the problem. We explored the
performance of MLEANN and conventionally designed artificial neural networks
for function approximation problems. To evaluate the comparative performance,
we used three different well-known chaotic time series. We also present the
state of the art popular neural network learning algorithms and some
experimentation results related to convergence speed and generalization
performance. We explored the performance of backpropagation algorithm;
conjugate gradient algorithm, quasi-Newton algorithm and Levenberg-Marquardt
algorithm for the three chaotic time series. Performances of the different
learning algorithms were evaluated when the activation functions and
architecture were changed. We further present the theoretical background,
algorithm, design strategy and further demonstrate how effective and inevitable
is the proposed MLEANN framework to design a neural network, which is smaller,
faster and with a better generalization performance."
Biological Inspiration for Artificial Immune Systems,"Artificial immune systems (AISs) to date have generally been inspired by
naive biological metaphors. This has limited the effectiveness of these
systems. In this position paper two ways in which AISs could be made more
biologically realistic are discussed. We propose that AISs should draw their
inspiration from organisms which possess only innate immune systems, and that
AISs should employ systemic models of the immune system to structure their
overall design. An outline of plant and invertebrate immune systems is
presented, and a number of contemporary research that more
biologically-realistic AISs could have is also discussed."
Experimenting with Innate Immunity,"In a previous paper the authors argued the case for incorporating ideas from
innate immunity into artificial immune systems (AISs) and presented an outline
for a conceptual framework for such systems. A number of key general properties
observed in the biological innate and adaptive immune systems were highlighted,
and how such properties might be instantiated in artificial systems was
discussed in detail. The next logical step is to take these ideas and build a
software system with which AISs with these properties can be implemented and
experimentally evaluated. This paper reports on the results of that step - the
libtissue system."
"Chemlambda, universality and self-multiplication","We present chemlambda (or the chemical concrete machine), an artificial
chemistry with the following properties: (a) is Turing complete, (b) has a
model of decentralized, distributed computing associated to it, (c) works at
the level of individual (artificial) molecules, subject of reversible, but
otherwise deterministic interactions with a small number of enzymes, (d)
encodes information in the geometrical structure of the molecules and not in
their numbers, (e) all interactions are purely local in space and time. This is
part of a larger project to create computing, artificial chemistry and
artificial life in a distributed context, using topological and graphical
languages."
"Recognition of cDNA microarray image Using Feedforward artificial neural
  network","The complementary DNA (cDNA) sequence is considered to be the magic biometric
technique for personal identification. In this paper, we present a new method
for cDNA recognition based on the artificial neural network (ANN). Microarray
imaging is used for the concurrent identification of thousands of genes. We
have segmented the location of the spots in a cDNA microarray. Thus, a precise
localization and segmenting of a spot are essential to obtain a more accurate
intensity measurement, leading to a more precise expression measurement of a
gene. The segmented cDNA microarray image is resized and it is used as an input
for the proposed artificial neural network. For matching and recognition, we
have trained the artificial neural network. Recognition results are given for
the galleries of cDNA sequences . The numerical results show that, the proposed
matching technique is an effective in the cDNA sequences process. We also
compare our results with previous results and find out that, the proposed
technique is an effective matching performance."
"VHetNets for AI and AI for VHetNets: An Anomaly Detection Case Study for
  Ubiquitous IoT","Vertical heterogenous networks (VHetNets) and artificial intelligence (AI)
play critical roles in 6G and beyond networks. This article presents an
AI-native VHetNets architecture to enable the synergy of VHetNets and AI,
thereby supporting varieties of AI services while facilitating automatic and
intelligent network management. Anomaly detection in Internet of Things (IoT)
is a major AI service required by many fields, including intrusion detection,
state monitoring, device-activity analysis, security supervision and so on.
Conventional anomaly detection technologies mainly consider the anomaly
detection as a standalone service that is independent of any other network
management functionalities, which cannot be used directly in ubiquitous IoT due
to the resource constrained end nodes and decentralized data distribution. In
this article, we develop an AI-native VHetNets-enabled framework to provide the
anomaly detection service for ubiquitous IoT, whose implementation is assisted
by intelligent network management functionalities. We first discuss the
possibilities of VHetNets used for distributed AI model training to provide
anomaly detection service for ubiquitous IoT, i.e., VHetNets for AI. After
that, we study the application of AI approaches in helping provide automatic
and intelligent network management functionalities for VHetNets, i.e., AI for
VHetNets, whose aim is to facilitate the efficient implementation of anomaly
detection service. Finally, a case study is presented to demonstrate the
efficiency and effectiveness of the proposed AI-native VHetNets-enabled anomaly
detection framework."
"An Overview of Artificial Intelligence-based Soft Upper Limb Exoskeleton
  for Rehabilitation: A Descriptive Review","The upper limb robotic exoskeleton is an electromechanical device which use
to recover a patients motor dysfunction in the rehabilitation field. It can
provide repetitive, comprehensive, focused, positive, and precise training to
regain the joints and muscles capability. It has been shown that existing
robotic exoskeletons are generally used rigid motors and mechanical structures.
Soft robotic devices can be a correct substitute for rigid ones. Soft exosuits
are flexible, portable, comfortable, user-friendly, low-cost, and
travel-friendly. Somehow, they need expertise or therapist to assist those
devices. Also, they cannot be adaptable to different patients with
non-identical physical parameters and various rehabilitation needs. For that
reason, nowadays we need intelligent exoskeletons during rehabilitation which
have to learn from patients previous data and act according to it with patients
intention. There also has a big gap between theoretical and practical
applications for using those exoskeletons. Most of the intelligent exoskeletons
are prototype in manner. To solve this problem, the robotic exoskeleton should
be made both criteria as ergonomic and portable. The exoskeletons have to the
power of decision-making to avoid the presence of expertise. In this growing
field, the present trend is to make the exoskeleton intelligent and make it
more reliable to use in clinical practice."
"Explainable, Interpretable & Trustworthy AI for Intelligent Digital
  Twin: Case Study on Remaining Useful Life","Artificial intelligence (AI) and Machine learning (ML) are increasingly used
in energy and engineering systems, but these models must be fair, unbiased, and
explainable. It is critical to have confidence in AI's trustworthiness. ML
techniques have been useful in predicting important parameters and in improving
model performance. However, for these AI techniques to be useful for making
decisions, they need to be audited, accounted for, and easy to understand.
Therefore, the use of explainable AI (XAI) and interpretable machine learning
(IML) is crucial for the accurate prediction of prognostics, such as remaining
useful life (RUL), in a digital twin system, to make it intelligent while
ensuring that the AI model is transparent in its decision-making processes and
that the predictions it generates can be understood and trusted by users. By
using AI that is explainable, interpretable, and trustworthy, intelligent
digital twin systems can make more accurate predictions of RUL, leading to
better maintenance and repair planning, and ultimately, improved system
performance. The objective of this paper is to explain the ideas of XAI and IML
and to justify the important role of AI/ML in the digital twin framework and
components, which requires XAI to understand the prediction better. This paper
explains the importance of XAI and IML in both local and global aspects to
ensure the use of trustworthy AI/ML applications for RUL prediction. We used
the RUL prediction for the XAI and IML studies and leveraged the integrated
Python toolbox for interpretable machine learning~(PiML)."
"Data-driven intelligent computational design for products: Method,
  techniques, and applications","Data-driven intelligent computational design (DICD) is a research hotspot
emerged under the context of fast-developing artificial intelligence. It
emphasizes on utilizing deep learning algorithms to extract and represent the
design features hidden in historical or fabricated design process data, and
then learn the combination and mapping patterns of these design features for
the purposes of design solution retrieval, generation, optimization,
evaluation, etc. Due to its capability of automatically and efficiently
generating design solutions and thus supporting human-in-the-loop intelligent
and innovative design activities, DICD has drawn the attentions from both
academic and industrial fields. However, as an emerging research subject, there
are still many unexplored issues that limit the development and application of
DICD, such as specific dataset building, engineering design related feature
engineering, systematic methods and techniques for DICD implementation in the
entire product design process, etc. In this regard, a systematic and operable
road map for DICD implementation from full-process perspective is established,
including a general workflow for DICD project planning, an overall framework
for DICD project implementation, the computing mechanisms for DICD
implementation, key enabling technologies for detailed DICD implementation, and
three application scenarios of DICD. The road map reveals the common mechanisms
and calculation principles of existing DICD researches, and thus it can provide
systematic guidance for the possible DICD applications that have not been
explored."
Hey Dona! Can you help me with student course registration?,"In this paper, we present a demo of an intelligent personal agent called Hey
Dona (or just Dona) with virtual voice assistance in student course
registration. It is a deployed project in the theme of AI for education. In
this digital age with a myriad of smart devices, users often delegate tasks to
agents. While pointing and clicking supersedes the erstwhile command-typing,
modern devices allow users to speak commands for agents to execute tasks,
enhancing speed and convenience. In line with this progress, Dona is an
intelligent agent catering to student needs by automated, voice-operated course
registration, spanning a multitude of accents, entailing task planning
optimization, with some language translation as needed. Dona accepts voice
input by microphone (Bluetooth, wired microphone), converts human voice to
computer understandable language, performs query processing as per user
commands, connects with the Web to search for answers, models task
dependencies, imbibes quality control, and conveys output by speaking to users
as well as displaying text, thus enabling human-AI interaction by speech cum
text. It is meant to work seamlessly on desktops, smartphones etc. and in
indoor as well as outdoor settings. To the best of our knowledge, Dona is among
the first of its kind as an intelligent personal agent for voice assistance in
student course registration. Due to its ubiquitous access for educational
needs, Dona directly impacts AI for education. It makes a broader impact on
smart city characteristics of smart living and smart people due to its
contributions to providing benefits for new ways of living and assisting 21st
century education, respectively."
"Shapley-based Explainable AI for Clustering Applications in Fault
  Diagnosis and Prognosis","Data-driven artificial intelligence models require explainability in
intelligent manufacturing to streamline adoption and trust in modern industry.
However, recently developed explainable artificial intelligence (XAI)
techniques that estimate feature contributions on a model-agnostic level such
as SHapley Additive exPlanations (SHAP) have not yet been evaluated for
semi-supervised fault diagnosis and prognosis problems characterized by class
imbalance and weakly labeled datasets. This paper explores the potential of
utilizing Shapley values for a new clustering framework compatible with
semi-supervised learning problems, loosening the strict supervision requirement
of current XAI techniques. This broad methodology is validated on two case
studies: a heatmap image dataset obtained from a semiconductor manufacturing
process featuring class imbalance, and a benchmark dataset utilized in the 2021
Prognostics and Health Management (PHM) Data Challenge. Semi-supervised
clustering based on Shapley values significantly improves upon clustering
quality compared to the fully unsupervised case, deriving information-dense and
meaningful clusters that relate to underlying fault diagnosis model
predictions. These clusters can also be characterized by high-precision
decision rules in terms of original feature values, as demonstrated in the
second case study. The rules, limited to 1-2 terms utilizing original feature
scales, describe 12 out of the 16 derived equipment failure clusters with
precision exceeding 0.85, showcasing the promising utility of the explainable
clustering framework for intelligent manufacturing applications."
"Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model
  Challenge of Intelligent Transportation","With the continuous improvement of computing power and deep learning
algorithms in recent years, the foundation model has grown in popularity.
Because of its powerful capabilities and excellent performance, this technology
is being adopted and applied by an increasing number of industries. In the
intelligent transportation industry, artificial intelligence faces the
following typical challenges: few shots, poor generalization, and a lack of
multi-modal techniques. Foundation model technology can significantly alleviate
the aforementioned issues. To address these, we designed the 1st Foundation
Model Challenge, with the goal of increasing the popularity of foundation model
technology in traffic scenarios and promoting the rapid development of the
intelligent transportation industry. The challenge is divided into two tracks:
all-in-one and cross-modal image retrieval. Furthermore, we provide a new
baseline and benchmark for the two tracks, called Open-TransMind. According to
our knowledge, Open-TransMind is the first open-source transportation
foundation model with multi-task and multi-modal capabilities. Simultaneously,
Open-TransMind can achieve state-of-the-art performance on detection,
classification, and segmentation datasets of traffic scenarios. Our source code
is available at https://github.com/Traffic-X/Open-TransMind."
"Multi-Agent Deep Reinforcement Learning for Dynamic Avatar Migration in
  AIoT-enabled Vehicular Metaverses with Trajectory Prediction","Avatars, as promising digital assistants in Vehicular Metaverses, can enable
drivers and passengers to immerse in 3D virtual spaces, serving as a practical
emerging example of Artificial Intelligence of Things (AIoT) in intelligent
vehicular environments. The immersive experience is achieved through seamless
human-avatar interaction, e.g., augmented reality navigation, which requires
intensive resources that are inefficient and impractical to process on
intelligent vehicles locally. Fortunately, offloading avatar tasks to RoadSide
Units (RSUs) or cloud servers for remote execution can effectively reduce
resource consumption. However, the high mobility of vehicles, the dynamic
workload of RSUs, and the heterogeneity of RSUs pose novel challenges to making
avatar migration decisions. To address these challenges, in this paper, we
propose a dynamic migration framework for avatar tasks based on real-time
trajectory prediction and Multi-Agent Deep Reinforcement Learning (MADRL).
Specifically, we propose a model to predict the future trajectories of
intelligent vehicles based on their historical data, indicating the future
workloads of RSUs.Based on the expected workloads of RSUs, we formulate the
avatar task migration problem as a long-term mixed integer programming problem.
To tackle this problem efficiently, the problem is transformed into a Partially
Observable Markov Decision Process (POMDP) and solved by multiple DRL agents
with hybrid continuous and discrete actions in decentralized. Numerical results
demonstrate that our proposed algorithm can effectively reduce the latency of
executing avatar tasks by around 25% without prediction and 30% with prediction
and enhance user immersive experiences in the AIoT-enabled Vehicular Metaverse
(AeVeM)."
"AI-Enabled Unmanned Vehicle-Assisted Reconfigurable Intelligent
  Surfaces: Deployment, Prototyping, Experiments, and Opportunities","The requirement of wireless data demands is increasingly high as the
sixth-generation (6G) technology evolves. Reconfigurable intelligent surface
(RIS) is promisingly deemed to be one of 6G techniques for extending service
coverage, reducing power consumption, and enhancing spectral efficiency. In
this article, we have provided some fundamentals of RIS deployment in theory
and hardware perspectives as well as utilization of artificial intelligence
(AI) and machine learning. We conducted an intelligent deployment of RIS
(i-Dris) prototype, including dual-band auto-guided vehicle (AGV) assisted RISs
associated with an mmWave base station (BS) and a receiver. The RISs are
deployed on the AGV with configured incident/reflection angles. While, both the
mmWave BS and receiver are associated with an edge server monitoring downlink
packets for obtaining system throughput. We have designed a federated
multi-agent reinforcement learning scheme associated with several AGV-RIS
agents and sub-agents per AGV-RIS consisting of the deployment of position,
height, orientation and elevation angles. The experimental results presented
the stationary measurement in different aspects and scenarios. The i-Dris can
reach up to 980 Mbps transmission throughput under a bandwidth of 100 MHz with
comparably low complexity as well as rapid deployment, which outperforms the
other existing works. At last, we highlight some opportunities and future
issues in leveraging RIS-empowered wireless communication networks."
"Exploring Large Language Model based Intelligent Agents: Definitions,
  Methods, and Prospects","Intelligent agents stand out as a potential path toward artificial general
intelligence (AGI). Thus, researchers have dedicated significant effort to
diverse implementations for them. Benefiting from recent progress in large
language models (LLMs), LLM-based agents that use universal natural language as
an interface exhibit robust generalization capabilities across various
applications -- from serving as autonomous general-purpose task assistants to
applications in coding, social, and economic domains, LLM-based agents offer
extensive exploration opportunities. This paper surveys current research to
provide an in-depth overview of LLM-based intelligent agents within
single-agent and multi-agent systems. It covers their definitions, research
frameworks, and foundational components such as their composition, cognitive
and planning methods, tool utilization, and responses to environmental
feedback. We also delve into the mechanisms of deploying LLM-based agents in
multi-agent systems, including multi-role collaboration, message passing, and
strategies to alleviate communication issues between agents. The discussions
also shed light on popular datasets and application scenarios. We conclude by
envisioning prospects for LLM-based agents, considering the evolving landscape
of AI and natural language processing."
"Towards Urban General Intelligence: A Review and Outlook of Urban
  Foundation Models","The integration of machine learning techniques has become a cornerstone in
the development of intelligent urban services, significantly contributing to
the enhancement of urban efficiency, sustainability, and overall livability.
Recent advancements in foundational models, such as ChatGPT, have introduced a
paradigm shift within the fields of machine learning and artificial
intelligence. These models, with their exceptional capacity for contextual
comprehension, problem-solving, and task adaptability, present a transformative
opportunity to reshape the future of smart cities and drive progress toward
Urban General Intelligence (UGI). Despite increasing attention to Urban
Foundation Models (UFMs), this rapidly evolving field faces critical
challenges, including the lack of clear definitions, systematic reviews, and
universalizable solutions. To address these issues, this paper first introduces
the definition and concept of UFMs and highlights the distinctive challenges
involved in their development. Furthermore, we present a data-centric taxonomy
that classifies existing research on UFMs according to the various urban data
modalities and types. In addition, we propose a prospective framework designed
to facilitate the realization of versatile UFMs, aimed at overcoming the
identified challenges and driving further progress in this field. Finally, this
paper explores the wide-ranging applications of UFMs within urban contexts,
illustrating their potential to significantly impact and transform urban
systems. A comprehensive collection of relevant research papers and open-source
resources have been collated and are continuously updated at:
https://github.com/usail-hkust/Awesome-Urban-Foundation-Models."
"Edge Intelligence Optimization for Large Language Model Inference with
  Batching and Quantization","Generative Artificial Intelligence (GAI) is taking the world by storm with
its unparalleled content creation ability. Large Language Models (LLMs) are at
the forefront of this movement. However, the significant resource demands of
LLMs often require cloud hosting, which raises issues regarding privacy,
latency, and usage limitations. Although edge intelligence has long been
utilized to solve these challenges by enabling real-time AI computation on
ubiquitous edge resources close to data sources, most research has focused on
traditional AI models and has left a gap in addressing the unique
characteristics of LLM inference, such as considerable model size,
auto-regressive processes, and self-attention mechanisms. In this paper, we
present an edge intelligence optimization problem tailored for LLM inference.
Specifically, with the deployment of the batching technique and model
quantization on resource-limited edge devices, we formulate an inference model
for transformer decoder-based LLMs. Furthermore, our approach aims to maximize
the inference throughput via batch scheduling and joint allocation of
communication and computation resources, while also considering edge resource
constraints and varying user requirements of latency and accuracy. To address
this NP-hard problem, we develop an optimal Depth-First Tree-Searching
algorithm with online tree-Pruning (DFTSP) that operates within a feasible time
complexity. Simulation results indicate that DFTSP surpasses other batching
benchmarks in throughput across diverse user settings and quantization
techniques, and it reduces time complexity by over 45% compared to the
brute-force searching method."
"Teacher agency in the age of generative AI: towards a framework of
  hybrid intelligence for learning design","Generative AI (genAI) is being used in education for different purposes. From
the teachers' perspective, genAI can support activities such as learning
design. However, there is a need to study the impact of genAI on the teachers'
agency. While GenAI can support certain processes of idea generation and
co-creation, GenAI has the potential to negatively affect professional agency
due to teachers' limited power to (i) act, (ii) affect matters, and (iii) make
decisions or choices, as well as the possibility to (iv) take a stance. Agency
is identified in the learning sciences studies as being one of the factors in
teachers' ability to trust AI. This paper aims to introduce a dual perspective.
First, educational technology, as opposed to other computer-mediated
communication (CMC) tools, has two distinctly different user groups and
different user needs, in the form of learners and teachers, to cater for.
Second, the design of educational technology often prioritises learner agency
and engagement, thereby limiting the opportunities for teachers to influence
the technology and take action. This study aims to analyse the way GenAI is
influencing teachers' agency. After identifying the current limits of GenAI, a
solution based on the combination of human intelligence and artificial
intelligence through a hybrid intelligence approach is proposed. This
combination opens up the discussion of a collaboration between teacher and
genAI being able to open up new practices in learning design in which they HI
support the extension of the teachers' activity."
"SciAgents: Automating scientific discovery through multi-agent
  intelligent graph reasoning","A key challenge in artificial intelligence is the creation of systems capable
of autonomously advancing scientific understanding by exploring novel domains,
identifying complex patterns, and uncovering previously unseen connections in
vast scientific data. In this work, we present SciAgents, an approach that
leverages three core concepts: (1) the use of large-scale ontological knowledge
graphs to organize and interconnect diverse scientific concepts, (2) a suite of
large language models (LLMs) and data retrieval tools, and (3) multi-agent
systems with in-situ learning capabilities. Applied to biologically inspired
materials, SciAgents reveals hidden interdisciplinary relationships that were
previously considered unrelated, achieving a scale, precision, and exploratory
power that surpasses traditional human-driven research methods. The framework
autonomously generates and refines research hypotheses, elucidating underlying
mechanisms, design principles, and unexpected material properties. By
integrating these capabilities in a modular fashion, the intelligent system
yields material discoveries, critique and improve existing hypotheses, retrieve
up-to-date data about existing research, and highlights their strengths and
limitations. Our case studies demonstrate scalable capabilities to combine
generative AI, ontological representations, and multi-agent modeling,
harnessing a `swarm of intelligence' similar to biological systems. This
provides new avenues for materials discovery and accelerates the development of
advanced materials by unlocking Nature's design principles."
Polymetis:Large Language Modeling for Multiple Material Domains,"As the application of large language models in various fields continues to
expand, materials science also ushers in opportunities for AI-driven
innovation. The traditional way of relying on manual search for materials
science-related information is now using artificial intelligence technology as
an auxiliary tool to improve the efficiency of materials science research. To
accelerate researchers' knowledge acquisition and intelligent decision-making
support in materials science research, this paper proposes a large language
model Polymetis model for a variety of materials fields, aiming to provide
highly professional knowledge answers in the field of materials, covering
energy materials, functional materials, alloy materials, physical chemistry,
biology, and other material directions. The model uses a dataset of about 2
million material knowledge instructions, and in the process of building the
dataset, we developed the Intelligent Extraction Large Model (IELM), which is
specially used to extract and form structured knowledge from scientific texts,
avoiding a large number of costs that need to be manually annotated, and
improving efficiency. We inject this data into the GLM4-9B model for learning
to enhance its inference capabilities in a variety of material domains. In
addition, we have introduced enhanced prompt strategies to ensure that the
answers to the model are more organized and comprehensive, providing efficient
and comprehensive intelligent support for the diverse needs of materials
science exploration, and promoting the development of material science."
Towards Anthropomorphic Conversational AI Part I: A Practical Framework,"Large language models (LLMs), due to their advanced natural language
capabilities, have seen significant success in applications where the user
interface is usually a conversational artificial intelligence (AI) agent and
engages the user through multi-round conversations. However, many scenarios
require the agents to exhibit stronger social and conversational intelligence
and demonstrate more human-like (anthropomorphic) reactions. This is an aspect
that foundational LLMs have yet to fully address such that a single call of
foundational models might be insufficient.
  To bridge this gap, we propose a two-stage solution. In this work, we focus
on the first stage, introducing a multi-module framework designed to replicate
the key aspects of human intelligence involved in conversations. This framework
comprises thinking modules for reasoning, resource modules for managing
knowledge and external information, and response modules for generating
contextually appropriate interactions. With all the modules cooperating, the
framework would empower the agents to provide a better human-like conversation
experience. In the second stage of our approach, these conversational data,
after filtering and labeling, can serve as training and testing data for
reinforcement learning, enabling AI to better capture human preferences. This
stage is left for future work.
  In our experiments, volunteers engaged in over 3000 rounds of conversation
with the same AI character powered by a standalone LLM and our framework which
integrates the same LLM. A separate group of evaluators rated the conversation
samples, revealing that our framework significantly enhanced the social and
conversational intelligence, even without fine-tuning the LLM."
"Revolution of Wireless Signal Recognition for 6G: Recent Advances,
  Challenges and Future Directions","Wireless signal recognition (WSR) is a crucial technique for intelligent
communications and spectrum sharing in the next six-generation (6G) wireless
communication networks. It can be utilized to enhance network performance and
efficiency, improve quality of service (QoS), and improve network security and
reliability. Additionally, WSR can be applied for military applications such as
signal interception, signal race, and signal abduction. In the past decades,
great efforts have been made for the research of WSR. Earlier works mainly
focus on model-based methods, including likelihood-based (LB) and feature-based
(FB) methods, which have taken the leading position for many years. With the
emergence of artificial intelligence (AI), intelligent methods including
machine learning-based (ML-based) and deep learning-based (DL-based) methods
have been developed to extract the features of the received signals and perform
the classification. In this work, we provide a comprehensive review of WSR from
the view of applications, main tasks, recent advances, datasets and evaluation
metrics, challenges, and future directions. Specifically, intelligent WSR
methods are introduced from the perspective of model, data, learning and
implementation. Moreover, we analyze the challenges for WSR from the view of
complex, dynamic, and open 6G wireless environments and discuss the future
directions for WSR. This survey is expected to provide a comprehensive overview
of the state-of-the-art WSR techniques and inspire new research directions for
WSR in 6G networks."
"Digital Gene: Learning about the Physical World through Analytic
  Concepts","Reviewing the progress in artificial intelligence over the past decade,
various significant advances (e.g. object detection, image generation, large
language models) have enabled AI systems to produce more semantically
meaningful outputs and achieve widespread adoption in internet scenarios.
Nevertheless, AI systems still struggle when it comes to understanding and
interacting with the physical world. This reveals an important issue: relying
solely on semantic-level concepts learned from internet data (e.g. texts,
images) to understand the physical world is far from sufficient -- machine
intelligence currently lacks an effective way to learn about the physical
world. This research introduces the idea of analytic concept -- representing
the concepts related to the physical world through programs of mathematical
procedures, providing machine intelligence a portal to perceive, reason about,
and interact with the physical world. Except for detailing the design
philosophy and providing guidelines for the application of analytic concepts,
this research also introduce about the infrastructure that has been built
around analytic concepts. I aim for my research to contribute to addressing
these questions: What is a proper abstraction of general concepts in the
physical world for machine intelligence? How to systematically integrate
structured priors with neural networks to constrain AI systems to comply with
physical laws?"
"Modeling the quantum-like dynamics of human reliability ratings in
  Human-AI interactions by interaction dependent Hamiltonians","As our information environments become ever more powered by artificial
intelligence (AI), the phenomenon of trust in a human's interactions with this
intelligence is becoming increasingly pertinent. For example, in the not too
distant future, there will be teams of humans and intelligent robots involved
in dealing with the repercussions of high-risk disaster situations such as
hurricanes, earthquakes, or nuclear accidents. Even in such conditions of high
uncertainty, humans and intelligent machines will need to engage in shared
decision making, and trust is fundamental to the effectiveness of these
interactions. A key challenge in modeling the dynamics of this trust is to
provide a means to incorporate sensitivity to fluctuations in human trust
judgments. In this article, we explore the ability of Quantum Random Walk
models to model the dynamics of trust in human-AI interactions, and to
integrate a sensitivity to fluctuations in participant trust judgments based on
the nature of the interaction with the AI. We found that using empirical
parameters to inform the use of different Hamiltonians can provide a promising
means to model the evolution of trust in Human-AI interactions."
BENCHIP: Benchmarking Intelligence Processors,"The increasing attention on deep learning has tremendously spurred the design
of intelligence processing hardware. The variety of emerging intelligence
processors requires standard benchmarks for fair comparison and system
optimization (in both software and hardware). However, existing benchmarks are
unsuitable for benchmarking intelligence processors due to their non-diversity
and nonrepresentativeness. Also, the lack of a standard benchmarking
methodology further exacerbates this problem. In this paper, we propose
BENCHIP, a benchmark suite and benchmarking methodology for intelligence
processors. The benchmark suite in BENCHIP consists of two sets of benchmarks:
microbenchmarks and macrobenchmarks. The microbenchmarks consist of
single-layer networks. They are mainly designed for bottleneck analysis and
system optimization. The macrobenchmarks contain state-of-the-art industrial
networks, so as to offer a realistic comparison of different platforms. We also
propose a standard benchmarking methodology built upon an industrial software
stack and evaluation metrics that comprehensively reflect the various
characteristics of the evaluated intelligence processors. BENCHIP is utilized
for evaluating various hardware platforms, including CPUs, GPUs, and
accelerators. BENCHIP will be open-sourced soon."
TINKER: A framework for Open source Cyberthreat Intelligence,"Threat intelligence on malware attacks and campaigns is increasingly being
shared with other security experts for a cost or for free. Other security
analysts use this intelligence to inform them of indicators of compromise,
attack techniques, and preventative actions. Security analysts prepare threat
analysis reports after investigating an attack, an emerging cyber threat, or a
recently discovered vulnerability. Collectively known as cyber threat
intelligence (CTI), the reports are typically in an unstructured format and,
therefore, challenging to integrate seamlessly into existing intrusion
detection systems. This paper proposes a framework that uses the aggregated CTI
for analysis and defense at scale. The information is extracted and stored in a
structured format using knowledge graphs such that the semantics of the threat
intelligence can be preserved and shared at scale with other security analysts.
Specifically, we propose the first semi-supervised open-source knowledge
graph-based framework, TINKER, to capture cyber threat information and its
context. Following TINKER, we generate a Cyberthreat Intelligence Knowledge
Graph (CTI-KG) and demonstrate the usage using different use cases."
"Automatic Speaker Independent Dysarthric Speech Intelligibility
  Assessment System","Dysarthria is a condition which hampers the ability of an individual to
control the muscles that play a major role in speech delivery. The loss of fine
control over muscles that assist the movement of lips, vocal chords, tongue and
diaphragm results in abnormal speech delivery. One can assess the severity
level of dysarthria by analyzing the intelligibility of speech spoken by an
individual. Continuous intelligibility assessment helps speech language
pathologists not only study the impact of medication but also allows them to
plan personalized therapy. It helps the clinicians immensely if the
intelligibility assessment system is reliable, automatic, simple for (a)
patients to undergo and (b) clinicians to interpret. Lack of availability of
dysarthric data has resulted in development of speaker dependent automatic
intelligibility assessment systems which requires patients to speak a large
number of utterances. In this paper, we propose (a) a cost minimization
procedure to select an optimal (small) number of utterances that need to be
spoken by the dysarthric patient, (b) four different speaker independent
intelligibility assessment systems which require the patient to speak a small
number of words, and (c) the assessment score is close to the perceptual score
that the Speech Language Pathologist (SLP) can relate to. The need for small
number of utterances to be spoken by the patient and the score being relatable
to the SLP benefits both the dysarthric patient and the clinician from
usability perspective."
"DRL: Deep Reinforcement Learning for Intelligent Robot Control --
  Concept, Literature, and Future","Combination of machine learning (for generating machine intelligence),
computer vision (for better environment perception), and robotic systems (for
controlled environment interaction) motivates this work toward proposing a
vision-based learning framework for intelligent robot control as the ultimate
goal (vision-based learning robot). This work specifically introduces deep
reinforcement learning as the the learning framework, a General-purpose
framework for AI (AGI) meaning application-independent and
platform-independent. In terms of robot control, this framework is proposing
specifically a high-level control architecture independent of the low-level
control, meaning these two required level of control can be developed
separately from each other. In this aspect, the high-level control creates the
required intelligence for the control of the platform using the recorded
low-level controlling data from that same platform generated by a trainer. The
recorded low-level controlling data is simply indicating the successful and
failed experiences or sequences of experiments conducted by a trainer using the
same robotic platform. The sequences of the recorded data are composed of
observation data (input sensor), generated reward (feedback value) and action
data (output controller). For experimental platform and experiments, vision
sensors are used for perception of the environment, different kinematic
controllers create the required motion commands based on the platform
application, deep learning approaches generate the required intelligence, and
finally reinforcement learning techniques incrementally improve the generated
intelligence until the mission is accomplished by the robot."
ACE: Towards Application-Centric Edge-Cloud Collaborative Intelligence,"Intelligent applications based on machine learning are impacting many parts
of our lives. They are required to operate under rigorous practical constraints
in terms of service latency, network bandwidth overheads, and also privacy. Yet
current implementations running in the Cloud are unable to satisfy all these
constraints. The Edge-Cloud Collaborative Intelligence (ECCI) paradigm has
become a popular approach to address such issues, and rapidly increasing
applications are developed and deployed. However, these prototypical
implementations are developer-dependent and scenario-specific without
generality, which cannot be efficiently applied in large-scale or to general
ECC scenarios in practice, due to the lack of supports for infrastructure
management, edge-cloud collaborative service, complex intelligence workload,
and efficient performance optimization. In this article, we systematically
design and construct the first unified platform, ACE, that handles
ever-increasing edge and cloud resources, user-transparent services, and
proliferating intelligence workloads with increasing scale and complexity, to
facilitate cost-efficient and high-performing ECCI application development and
deployment. For verification, we explicitly present the construction process of
an ACE-based intelligent video query application, and demonstrate how to
achieve customizable performance optimization efficiently. Based on our initial
experience, we discuss both the limitations and vision of ACE to shed light on
promising issues to elaborate in the approaching ECCI ecosystem."
"UIILD: A Unified Interpretable Intelligent Learning Diagnosis Framework
  for Intelligent Tutoring Systems","Intelligent learning diagnosis is a critical engine of intelligent tutoring
systems, which aims to estimate learners' current knowledge mastery status and
predict their future learning performance. The significant challenge with
traditional learning diagnosis methods is the inability to balance diagnostic
accuracy and interpretability. Although the existing psychometric-based
learning diagnosis methods provide some domain interpretation through cognitive
parameters, they have insufficient modeling capability with a shallow structure
for large-scale learning data. While the deep learning-based learning diagnosis
methods have improved the accuracy of learning performance prediction, their
inherent black-box properties lead to a lack of interpretability, making their
results untrustworthy for educational applications. To settle the above
problem, the proposed unified interpretable intelligent learning diagnosis
(UIILD) framework, which benefits from the powerful representation learning
ability of deep learning and the interpretability of psychometrics, achieves a
better performance of learning prediction and provides interpretability from
three aspects: cognitive parameters, learner-resource response network, and
weights of self-attention mechanism. Within the proposed framework, this paper
presents a two-channel learning diagnosis mechanism LDM-ID as well as a
three-channel learning diagnosis mechanism LDM-HMI. Experiments on two
real-world datasets and a simulation dataset show that our method has higher
accuracy in predicting learners' performances compared with the
state-of-the-art models, and can provide valuable educational interpretability
for applications such as precise learning resource recommendation and
personalized learning tutoring in intelligent tutoring systems."
"Human and Machine Intelligence in n-Person Games with Partial Knowledge:
  Theory and Computation","In this paper, I formalize intelligence measurement in games by introducing
mechanisms that assign a real number -- interpreted as an intelligence score --
to each player in a game. This score quantifies the ex-post strategic ability
of the players based on empirically observable information, such as the actions
of the players, the game's outcome, strength of the players, and a reference
oracle machine such as a chess-playing artificial intelligence system.
Specifically, I introduce two main concepts: first, the Game Intelligence (GI)
mechanism, which quantifies a player's intelligence in a game by considering
not only the game's outcome but also the ""mistakes"" made during the game
according to the reference machine's intelligence. Second, I define
gamingproofness, a practical and computational concept of strategyproofness. To
illustrate the GI mechanism, I apply it to an extensive dataset comprising over
a billion chess moves, including over a million moves made by top 20
grandmasters in history. Notably, Magnus Carlsen emerges with the highest GI
score among all world championship games included in the dataset. In
machine-vs-machine games, the well-known chess engine Stockfish comes out on
top."
"VSRQ: Quantitative Assessment Method for Safety Risk of Vehicle
  Intelligent Connected System","The field of intelligent connected in modern vehicles continues to expand,
and the functions of vehicles become more and more complex with the development
of the times. This has also led to an increasing number of vehicle
vulnerabilities and many safety issues. Therefore, it is particularly important
to identify high-risk vehicle intelligent connected systems, because it can
inform security personnel which systems are most vulnerable to attacks,
allowing them to conduct more thorough inspections and tests. In this paper, we
develop a new model for vehicle risk assessment by combining I-FAHP with FCA
clustering: VSRQ model. We extract important indicators related to vehicle
safety, use fuzzy cluster analys (FCA) combined with fuzzy analytic hierarchy
process (FAHP) to mine the vulnerable components of the vehicle intelligent
connected system, and conduct priority testing on vulnerable components to
reduce risks and ensure vehicle safety. We evaluate the model on OpenPilot and
experimentally demonstrate the effectiveness of the VSRQ model in identifying
the safety of vehicle intelligent connected systems. The experiment fully
complies with ISO 26262 and ISO/SAE 21434 standards, and our model has a higher
accuracy rate than other models. These results provide a promising new research
direction for predicting the security risks of vehicle intelligent connected
systems and provide typical application tasks for VSRQ. The experimental
results show that the accuracy rate is 94.36%, and the recall rate is 73.43%,
which is at least 14.63% higher than all other known indicators."
"Evolving Testing Scenario Generation Method and Intelligence Evaluation
  Framework for Automated Vehicles","Interaction between the background vehicles (BVs) and automated vehicles
(AVs) in scenario-based testing plays a critical role in evaluating the
intelligence of the AVs. Current testing scenarios typically employ predefined
or scripted BVs, which inadequately reflect the complexity of human-like social
behaviors in real-world driving scenarios, and also lack a systematic metric
for evaluating the comprehensive intelligence of AVs. Therefore, this paper
proposes an evolving scenario generation method that utilizes deep
reinforcement learning (DRL) to create human-like BVs for testing and
intelligence evaluation of AVs. Firstly, a class of driver models with
human-like competitive, cooperative, and mutual driving motivations is
designed. Then, utilizing an improved ""level-k"" training procedure, the three
distinct driver models acquire game-based interactive driving policies. And
these models are assigned to BVs for generating evolving scenarios in which all
BVs can interact continuously and evolve diverse contents. Next, a framework
including safety, driving efficiency, and interaction utility are presented to
evaluate and quantify the intelligence performance of 3 systems under test
(SUTs), indicating the effectiveness of the evolving scenario for intelligence
testing. Finally, the complexity and fidelity of the proposed evolving testing
scenario are validated. The results demonstrate that the proposed evolving
scenario exhibits the highest level of complexity compared to other baseline
scenarios and has more than 85% similarity to naturalistic driving data. This
highlights the potential of the proposed method to facilitate the development
and evaluation of high-level AVs in a realistic and challenging environment."
"An Intelligent Social Learning-based Optimization Strategy for Black-box
  Robotic Control with Reinforcement Learning","Implementing intelligent control of robots is a difficult task, especially
when dealing with complex black-box systems, because of the lack of visibility
and understanding of how these robots work internally. This paper proposes an
Intelligent Social Learning (ISL) algorithm to enable intelligent control of
black-box robotic systems. Inspired by mutual learning among individuals in
human social groups, ISL includes learning, imitation, and self-study styles.
Individuals in the learning style use the Levy flight search strategy to learn
from the best performer and form the closest relationships. In the imitation
style, individuals mimic the best performer with a second-level rapport by
employing a random perturbation strategy. In the self-study style, individuals
learn independently using a normal distribution sampling method while
maintaining a distant relationship with the best performer. Individuals in the
population are regarded as autonomous intelligent agents in each style. Neural
networks perform strategic actions in three styles to interact with the
environment and the robot and iteratively optimize the network policy. Overall,
ISL builds on the principles of intelligent optimization, incorporating ideas
from reinforcement learning, and possesses strong search capabilities, fast
computation speed, fewer hyperparameters, and insensitivity to sparse rewards.
The proposed ISL algorithm is compared with four state-of-the-art methods on
six continuous control benchmark cases in MuJoCo to verify its effectiveness
and advantages. Furthermore, ISL is adopted in the simulation and experimental
grasping tasks of the UR3 robot for validations, and satisfactory solutions are
yielded."
NLP-Based Techniques for Cyber Threat Intelligence,"In the digital era, threat actors employ sophisticated techniques for which,
often, digital traces in the form of textual data are available. Cyber Threat
Intelligence~(CTI) is related to all the solutions inherent to data collection,
processing, and analysis useful to understand a threat actor's targets and
attack behavior. Currently, CTI is assuming an always more crucial role in
identifying and mitigating threats and enabling proactive defense strategies.
In this context, NLP, an artificial intelligence branch, has emerged as a
powerful tool for enhancing threat intelligence capabilities. This survey paper
provides a comprehensive overview of NLP-based techniques applied in the
context of threat intelligence. It begins by describing the foundational
definitions and principles of CTI as a major tool for safeguarding digital
assets. It then undertakes a thorough examination of NLP-based techniques for
CTI data crawling from Web sources, CTI data analysis, Relation Extraction from
cybersecurity data, CTI sharing and collaboration, and security threats of CTI.
Finally, the challenges and limitations of NLP in threat intelligence are
exhaustively examined, including data quality issues and ethical
considerations. This survey draws a complete framework and serves as a valuable
resource for security professionals and researchers seeking to understand the
state-of-the-art NLP-based threat intelligence techniques and their potential
impact on cybersecurity."
Operational Collective Intelligence of Humans and Machines,"We explore the use of aggregative crowdsourced forecasting (ACF) as a
mechanism to help operationalize ``collective intelligence'' of human-machine
teams for coordinated actions. We adopt the definition for Collective
Intelligence as: ``A property of groups that emerges from synergies among
data-information-knowledge, software-hardware, and individuals (those with new
insights as well as recognized authorities) that enables just-in-time knowledge
for better decisions than these three elements acting alone.'' Collective
Intelligence emerges from new ways of connecting humans and AI to enable
decision-advantage, in part by creating and leveraging additional sources of
information that might otherwise not be included. Aggregative crowdsourced
forecasting (ACF) is a recent key advancement towards Collective Intelligence
wherein predictions (X\% probability that Y will happen) and rationales (why I
believe it is this probability that X will happen) are elicited independently
from a diverse crowd, aggregated, and then used to inform higher-level
decision-making. This research asks whether ACF, as a key way to enable
Operational Collective Intelligence, could be brought to bear on operational
scenarios (i.e., sequences of events with defined agents, components, and
interactions) and decision-making, and considers whether such a capability
could provide novel operational capabilities to enable new forms of
decision-advantage."
"A ""User Experience 3.0 (UX 3.0)"" Paradigm Framework: User Experience
  Design for Human-Centered AI Systems","The human-centered artificial intelligence (HCAI) design approach, the
user-centered design (UCD) version in the intelligence era, has been promoted
to address potential negative issues caused by AI technology; user experience
design (UXD) is specifically called out to facilitate the design and
development of human-centered AI systems. Over the last three decades, user
experience (UX) practice can be divided into three stages in terms of
technology platform, user needs, design philosophy, ecosystem, scope, focus,
and methodology of UX practice. UX practice is moving towards the intelligence
era. Still, the existing UX paradigm mainly aims at non-intelligent systems and
lacks a systematic approach to address UX for designing and developing
human-centered AI products and systems. The intelligence era has put forward
new demands on the UX paradigm. This paper proposes a ""UX 3.0"" paradigm
framework and the corresponding UX methodology for UX practice in the
intelligence era. The ""UX 3.0"" paradigm framework includes four categories of
emerging experiences in the intelligence era: ecosystem-based experience,
innovation-enabled experience, AI-enabled experience, and human-AI
interaction-based experience, each compelling us to enhance current UX practice
in terms of design philosophy, scope, focus, and methodology. We believe that
the ""UX 3.0"" paradigm helps enhance existing UX practice and provides
methodological support for the research and applications of UX in developing
human-centered AI systems. Finally, this paper looks forward to future work
implementing the ""UX 3.0"" paradigm."
"InterIntent: Investigating Social Intelligence of LLMs via Intention
  Understanding in an Interactive Game Context","Large language models (LLMs) have demonstrated the potential to mimic human
social intelligence. However, most studies focus on simplistic and static
self-report or performance-based tests, which limits the depth and validity of
the analysis. In this paper, we developed a novel framework, InterIntent, to
assess LLMs' social intelligence by mapping their ability to understand and
manage intentions in a game setting. We focus on four dimensions of social
intelligence: situational awareness, self-regulation, self-awareness, and
theory of mind. Each dimension is linked to a specific game task: intention
selection, intention following, intention summarization, and intention
guessing. Our findings indicate that while LLMs exhibit high proficiency in
selecting intentions, achieving an accuracy of 88%, their ability to infer the
intentions of others is significantly weaker, trailing human performance by
20%. Additionally, game performance correlates with intention understanding,
highlighting the importance of the four components towards success in this
game. These findings underline the crucial role of intention understanding in
evaluating LLMs' social intelligence and highlight the potential of using
social deduction games as a complex testbed to enhance LLM evaluation.
InterIntent contributes a structured approach to bridging the evaluation gap in
social intelligence within multiplayer games."
"AeroVerse: UAV-Agent Benchmark Suite for Simulating, Pre-training,
  Finetuning, and Evaluating Aerospace Embodied World Models","Aerospace embodied intelligence aims to empower unmanned aerial vehicles
(UAVs) and other aerospace platforms to achieve autonomous perception,
cognition, and action, as well as egocentric active interaction with humans and
the environment. The aerospace embodied world model serves as an effective
means to realize the autonomous intelligence of UAVs and represents a necessary
pathway toward aerospace embodied intelligence. However, existing embodied
world models primarily focus on ground-level intelligent agents in indoor
scenarios, while research on UAV intelligent agents remains unexplored. To
address this gap, we construct the first large-scale real-world image-text
pre-training dataset, AerialAgent-Ego10k, featuring urban drones from a
first-person perspective. We also create a virtual image-text-pose alignment
dataset, CyberAgent Ego500k, to facilitate the pre-training of the aerospace
embodied world model. For the first time, we clearly define 5 downstream tasks,
i.e., aerospace embodied scene awareness, spatial reasoning, navigational
exploration, task planning, and motion decision, and construct corresponding
instruction datasets, i.e., SkyAgent-Scene3k, SkyAgent-Reason3k, SkyAgent-Nav3k
and SkyAgent-Plan3k, and SkyAgent-Act3k, for fine-tuning the aerospace
embodiment world model. Simultaneously, we develop SkyAgentEval, the downstream
task evaluation metrics based on GPT-4, to comprehensively, flexibly, and
objectively assess the results, revealing the potential and limitations of
2D/3D visual language models in UAV-agent tasks. Furthermore, we integrate over
10 2D/3D visual-language models, 2 pre-training datasets, 5 finetuning
datasets, more than 10 evaluation metrics, and a simulator into the benchmark
suite, i.e., AeroVerse, which will be released to the community to promote
exploration and development of aerospace embodied intelligence."
"Development and Evaluation Study of Intelligent Cockpit in the Age of
  Large Models","The development of Artificial Intelligence (AI) Large Models has a great
impact on the application development of automotive Intelligent cockpit. The
fusion development of Intelligent Cockpit and Large Models has become a new
growth point of user experience in the industry, which also creates problems
for related scholars, practitioners and users in terms of their understanding
and evaluation of the user experience and the capability characteristics of the
Intelligent Cockpit Large Models (ICLM). This paper aims to analyse the current
situation of Intelligent cockpit, large model and AI Agent, to reveal the key
of application research focuses on the integration of Intelligent Cockpit and
Large Models, and to put forward a necessary limitation for the subsequent
development of an evaluation system for the capability of automotive ICLM and
user experience. The evaluation system, P-CAFE, proposed in this paper mainly
proposes five dimensions of perception, cognition, action, feedback and
evolution as the first-level indicators from the domains of cognitive
architecture, user experience, and capability characteristics of large models,
and many second-level indicators to satisfy the current status of the
application and research focuses are selected. After expert evaluation, the
weights of the indicators were determined, and the indicator system of P-CAFE
was established. Finally, a complete evaluation method was constructed based on
Fuzzy Hierarchical Analysis. It will lay a solid foundation for the application
and evaluation of the automotive ICLM, and provide a reference for the
development and improvement of the future ICLM."
"Continuum-Interaction-Driven Intelligence: Human-Aligned Neural
  Architecture via Crystallized Reasoning and Fluid Generation","Current AI systems based on probabilistic neural networks, such as large
language models (LLMs), have demonstrated remarkable generative capabilities
yet face critical challenges including hallucination, unpredictability, and
misalignment with human decision-making. These issues fundamentally stem from
the over-reliance on randomized (probabilistic) neural networks-oversimplified
models of biological neural networks-while neglecting the role of procedural
reasoning (chain-of-thought) in trustworthy decision-making. Inspired by the
human cognitive duality of fluid intelligence (flexible generation) and
crystallized intelligence (structured knowledge), this study proposes a
dual-channel intelligent architecture that integrates probabilistic generation
(LLMs) with white-box procedural reasoning (chain-of-thought) to construct
interpretable, continuously learnable, and human-aligned AI systems.
Concretely, this work: (1) redefines chain-of-thought as a programmable
crystallized intelligence carrier, enabling dynamic knowledge evolution and
decision verification through multi-turn interaction frameworks; (2) introduces
a task-driven modular network design that explicitly demarcates the functional
boundaries between randomized generation and procedural control to address
trustworthiness in vertical-domain applications; (3) demonstrates that
multi-turn interaction is a necessary condition for intelligence emergence,
with dialogue depth positively correlating with the system's human-alignment
degree. This research not only establishes a new paradigm for trustworthy AI
deployment but also provides theoretical foundations for next-generation
human-AI collaborative systems."
"Learning, Social Intelligence and the Turing Test - why an
  ""out-of-the-box"" Turing Machine will not pass the Turing Test","The Turing Test (TT) checks for human intelligence, rather than any putative
general intelligence. It involves repeated interaction requiring learning in
the form of adaption to the human conversation partner. It is a macro-level
post-hoc test in contrast to the definition of a Turing Machine (TM), which is
a prior micro-level definition. This raises the question of whether learning is
just another computational process, i.e. can be implemented as a TM. Here we
argue that learning or adaption is fundamentally different from computation,
though it does involve processes that can be seen as computations. To
illustrate this difference we compare (a) designing a TM and (b) learning a TM,
defining them for the purpose of the argument. We show that there is a
well-defined sequence of problems which are not effectively designable but are
learnable, in the form of the bounded halting problem. Some characteristics of
human intelligence are reviewed including it's: interactive nature, learning
abilities, imitative tendencies, linguistic ability and context-dependency. A
story that explains some of these is the Social Intelligence Hypothesis. If
this is broadly correct, this points to the necessity of a considerable period
of acculturation (social learning in context) if an artificial intelligence is
to pass the TT. Whilst it is always possible to 'compile' the results of
learning into a TM, this would not be a designed TM and would not be able to
continually adapt (pass future TTs). We conclude three things, namely that: a
purely ""designed"" TM will never pass the TT; that there is no such thing as a
general intelligence since it necessary involves learning; and that
learning/adaption and computation should be clearly distinguished."
"A Whole Brain Probabilistic Generative Model: Toward Realizing Cognitive
  Architectures for Developmental Robots","Building a humanlike integrative artificial cognitive system, that is, an
artificial general intelligence (AGI), is the holy grail of the artificial
intelligence (AI) field. Furthermore, a computational model that enables an
artificial system to achieve cognitive development will be an excellent
reference for brain and cognitive science. This paper describes an approach to
develop a cognitive architecture by integrating elemental cognitive modules to
enable the training of the modules as a whole. This approach is based on two
ideas: (1) brain-inspired AI, learning human brain architecture to build
human-level intelligence, and (2) a probabilistic generative model(PGM)-based
cognitive system to develop a cognitive system for developmental robots by
integrating PGMs. The development framework is called a whole brain PGM
(WB-PGM), which differs fundamentally from existing cognitive architectures in
that it can learn continuously through a system based on sensory-motor
information. In this study, we describe the rationale of WB-PGM, the current
status of PGM-based elemental cognitive modules, their relationship with the
human brain, the approach to the integration of the cognitive modules, and
future challenges. Our findings can serve as a reference for brain studies. As
PGMs describe explicit informational relationships between variables, this
description provides interpretable guidance from computational sciences to
brain science. By providing such information, researchers in neuroscience can
provide feedback to researchers in AI and robotics on what the current models
lack with reference to the brain. Further, it can facilitate collaboration
among researchers in neuro-cognitive sciences as well as AI and robotics."
"Towards deep observation: A systematic survey on artificial intelligence
  techniques to monitor fetus via Ultrasound Images","Developing innovative informatics approaches aimed to enhance fetal
monitoring is a burgeoning field of study in reproductive medicine. Several
reviews have been conducted regarding Artificial intelligence (AI) techniques
to improve pregnancy outcomes. They are limited by focusing on specific data
such as mother's care during pregnancy. This systematic survey aims to explore
how artificial intelligence (AI) can assist with fetal growth monitoring via
Ultrasound (US) image. We used eight medical and computer science bibliographic
databases, including PubMed, Embase, PsycINFO, ScienceDirect, IEEE explore, ACM
Library, Google Scholar, and the Web of Science. We retrieved studies published
between 2010 to 2021. Data extracted from studies were synthesized using a
narrative approach. Out of 1269 retrieved studies, we included 107 distinct
studies from queries that were relevant to the topic in the survey. We found
that 2D ultrasound images were more popular (n=88) than 3D and 4D ultrasound
images (n=19). Classification is the most used method (n=42), followed by
segmentation (n=31), classification integrated with segmentation (n=16) and
other miscellaneous such as object-detection, regression and reinforcement
learning (n=18). The most common areas within the pregnancy domain were the
fetus head (n=43), then fetus body (n=31), fetus heart (n=13), fetus abdomen
(n=10), and lastly the fetus face (n=10). In the most recent studies, deep
learning techniques were primarily used (n=81), followed by machine learning
(n=16), artificial neural network (n=7), and reinforcement learning (n=2). AI
techniques played a crucial role in predicting fetal diseases and identifying
fetus anatomy structures during pregnancy. More research is required to
validate this technology from a physician's perspective, such as pilot studies
and randomized controlled trials on AI and its applications in a hospital
setting."
"Inteligencia Artificial para la conservacin y uso sostenible de la
  biodiversidad, una visin desde Colombia (Artificial Intelligence for
  conservation and sustainable use of biodiversity, a view from Colombia)","The rise of artificial intelligence (AI) and the aggravating biodiversity
crisis have resulted in a research area where AI-based computational methods
are being developed to act as allies in conservation, and the sustainable use
and management of natural resources. While important general guidelines have
been established globally regarding the opportunities and challenges that this
interdisciplinary research offers, it is essential to generate local
reflections from the specific contexts and realities of each region. Hence,
this document aims to analyze the scope of this research area from a
perspective focused on Colombia and the Neotropics. In this paper, we summarize
the main experiences and debates that took place at the Humboldt Institute
between 2023 and 2024 in Colombia. To illustrate the variety of promising
opportunities, we present current uses such as automatic species identification
from images and recordings, species modeling, and in silico bioprospecting,
among others. From the experiences described above, we highlight limitations,
challenges, and opportunities for in order to successfully implementate AI in
conservation efforts and sustainable management of biological resources in the
Neotropics. The result aims to be a guide for researchers, decision makers, and
biodiversity managers, facilitating the understanding of how artificial
intelligence can be effectively integrated into conservation and sustainable
use strategies. Furthermore, it also seeks to open a space for dialogue on the
development of policies that promote the responsible and ethical adoption of AI
in local contexts, ensuring that its benefits are harnessed without
compromising biodiversity or the cultural and ecosystemic values inherent in
Colombia and the Neotropics."
"Deep RTS: A Game Environment for Deep Reinforcement Learning in
  Real-Time Strategy Games","Reinforcement learning (RL) is an area of research that has blossomed
tremendously in recent years and has shown remarkable potential for artificial
intelligence based opponents in computer games. This success is primarily due
to the vast capabilities of convolutional neural networks, that can extract
useful features from noisy and complex data. Games are excellent tools to test
and push the boundaries of novel RL algorithms because they give valuable
insight into how well an algorithm can perform in isolated environments without
the real-life consequences. Real-time strategy games (RTS) is a genre that has
tremendous complexity and challenges the player in short and long-term
planning. There is much research that focuses on applied RL in RTS games, and
novel advances are therefore anticipated in the not too distant future.
However, there are to date few environments for testing RTS AIs. Environments
in the literature are often either overly simplistic, such as microRTS, or
complex and without the possibility for accelerated learning on consumer
hardware like StarCraft II. This paper introduces the Deep RTS game environment
for testing cutting-edge artificial intelligence algorithms for RTS games. Deep
RTS is a high-performance RTS game made specifically for artificial
intelligence research. It supports accelerated learning, meaning that it can
learn at a magnitude of 50 000 times faster compared to existing RTS games.
Deep RTS has a flexible configuration, enabling research in several different
RTS scenarios, including partially observable state-spaces and map complexity.
We show that Deep RTS lives up to our promises by comparing its performance
with microRTS, ELF, and StarCraft II on high-end consumer hardware. Using Deep
RTS, we show that a Deep Q-Network agent beats random-play agents over 70% of
the time. Deep RTS is publicly available at https://github.com/cair/DeepRTS."
"Chatbot Interaction with Artificial Intelligence: Human Data
  Augmentation with T5 and Language Transformer Ensemble for Text
  Classification","In this work, we present the Chatbot Interaction with Artificial Intelligence
(CI-AI) framework as an approach to the training of deep learning chatbots for
task classification. The intelligent system augments human-sourced data via
artificial paraphrasing in order to generate a large set of training data for
further classical, attention, and language transformation-based learning
approaches for Natural Language Processing. Human beings are asked to
paraphrase commands and questions for task identification for further execution
of a machine. The commands and questions are split into training and validation
sets. A total of 483 responses were recorded. Secondly, the training set is
paraphrased by the T5 model in order to augment it with further data. Seven
state-of-the-art transformer-based text classification algorithms (BERT,
DistilBERT, RoBERTa, DistilRoBERTa, XLM, XLM-RoBERTa, and XLNet) are
benchmarked for both sets after fine-tuning on the training data for two
epochs. We find that all models are improved when training data is augmented by
the T5 model, with an average increase of classification accuracy by 4.01%. The
best result was the RoBERTa model trained on T5 augmented data which achieved
98.96% classification accuracy. Finally, we found that an ensemble of the five
best-performing transformer models via Logistic Regression of output label
predictions led to an accuracy of 99.59% on the dataset of human responses. A
highly-performing model allows the intelligent system to interpret human
commands at the social-interaction level through a chatbot-like interface (e.g.
""Robot, can we have a conversation?"") and allows for better accessibility to AI
by non-technical users."
"Applying Deutsch's concept of good explanations to artificial
  intelligence and neuroscience -- an initial exploration","Artificial intelligence has made great strides since the deep learning
revolution, but AI systems still struggle to extrapolate outside of their
training data and adapt to new situations. For inspiration we look to the
domain of science, where scientists have been able to develop theories which
show remarkable ability to extrapolate and sometimes predict the existence of
phenomena which have never been observed before. According to David Deutsch,
this type of extrapolation, which he calls ""reach"", is due to scientific
theories being hard to vary. In this work we investigate Deutsch's hard-to-vary
principle and how it relates to more formalized principles in deep learning
such as the bias-variance trade-off and Occam's razor. We distinguish internal
variability, how much a model/theory can be varied internally while still
yielding the same predictions, with external variability, which is how much a
model must be varied to accurately predict new, out-of-distribution data. We
discuss how to measure internal variability using the size of the Rashomon set
and how to measure external variability using Kolmogorov complexity. We explore
what role hard-to-vary explanations play in intelligence by looking at the
human brain and distinguish two learning systems in the brain. The first system
operates similar to deep learning and likely underlies most of perception and
motor control while the second is a more creative system capable of generating
hard-to-vary explanations of the world. We argue that figuring out how
replicate this second system, which is capable of generating hard-to-vary
explanations, is a key challenge which needs to be solved in order to realize
artificial general intelligence. We make contact with the framework of
Popperian epistemology which rejects induction and asserts that knowledge
generation is an evolutionary process which proceeds through conjecture and
refutation."
"TAU: A Framework for Video-Based Traffic Analytics Leveraging Artificial
  Intelligence and Unmanned Aerial Systems","Smart traffic engineering and intelligent transportation services are in
increasing demand from governmental authorities to optimize traffic performance
and thus reduce energy costs, increase the drivers' safety and comfort, ensure
traffic laws enforcement, and detect traffic violations. In this paper, we
address this challenge, and we leverage the use of Artificial Intelligence (AI)
and Unmanned Aerial Vehicles (UAVs) to develop an AI-integrated video analytics
framework, called TAU (Traffic Analysis from UAVs), for automated traffic
analytics and understanding. Unlike previous works on traffic video analytics,
we propose an automated object detection and tracking pipeline from video
processing to advanced traffic understanding using high-resolution UAV images.
TAU combines six main contributions. First, it proposes a pre-processing
algorithm to adapt the high-resolution UAV image as input to the object
detector without lowering the resolution. This ensures an excellent detection
accuracy from high-quality features, particularly the small size of detected
objects from UAV images. Second, it introduces an algorithm for recalibrating
the vehicle coordinates to ensure that vehicles are uniquely identified and
tracked across the multiple crops of the same frame. Third, it presents a speed
calculation algorithm based on accumulating information from successive frames.
Fourth, TAU counts the number of vehicles per traffic zone based on the Ray
Tracing algorithm. Fifth, TAU has a fully independent algorithm for crossroad
arbitration based on the data gathered from the different zones surrounding it.
Sixth, TAU introduces a set of algorithms for extracting twenty-four types of
insights from the raw data collected. The code is shared here:
https://github.com/bilel-bj/TAU. Video demonstrations are provided here:
https://youtu.be/wXJV0H7LviU and here: https://youtu.be/kGv0gmtVEbI."
Sparks of Artificial General Intelligence: Early experiments with GPT-4,"Artificial intelligence (AI) researchers have been developing and refining
large language models (LLMs) that exhibit remarkable capabilities across a
variety of domains and tasks, challenging our understanding of learning and
cognition. The latest model developed by OpenAI, GPT-4, was trained using an
unprecedented scale of compute and data. In this paper, we report on our
investigation of an early version of GPT-4, when it was still in active
development by OpenAI. We contend that (this early version of) GPT-4 is part of
a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that
exhibit more general intelligence than previous AI models. We discuss the
rising capabilities and implications of these models. We demonstrate that,
beyond its mastery of language, GPT-4 can solve novel and difficult tasks that
span mathematics, coding, vision, medicine, law, psychology and more, without
needing any special prompting. Moreover, in all of these tasks, GPT-4's
performance is strikingly close to human-level performance, and often vastly
surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's
capabilities, we believe that it could reasonably be viewed as an early (yet
still incomplete) version of an artificial general intelligence (AGI) system.
In our exploration of GPT-4, we put special emphasis on discovering its
limitations, and we discuss the challenges ahead for advancing towards deeper
and more comprehensive versions of AGI, including the possible need for
pursuing a new paradigm that moves beyond next-word prediction. We conclude
with reflections on societal influences of the recent technological leap and
future research directions."
"Present and Future of AI in Renewable Energy Domain : A Comprehensive
  Survey","Artificial intelligence (AI) has become a crucial instrument for streamlining
processes in various industries, including electrical power systems, as a
result of recent digitalization. Algorithms for artificial intelligence are
data-driven models that are based on statistical learning theory and are used
as a tool to take use of the data that the power system and its users generate.
Initially, we perform a thorough literature analysis of artificial intelligence
(AI) applications related to renewable energy (RE). Next, we present a thorough
analysis of renewable energy factories and assess their suitability, along with
a list of the most widely used and appropriate AI algorithms. Nine AI-based
strategies are identified here to assist Renewable Energy (RE) in contemporary
power systems. This survey paper comprises an extensive review of the several
AI techniques used for renewable energy as well as a methodical analysis of the
literature for the study of various intelligent system application domains
across different disciplines of renewable energy. This literature review
identifies the performance and outcomes of nine different research methods by
assessing them, and it aims to distill valuable insights into their strengths
and limitations. This study also addressed three main topics: using AI
technology for renewable power generation, utilizing AI for renewable energy
forecasting, and optimizing energy systems. Additionally, it explored AI's
superiority over conventional models in controllability, data handling,
cyberattack prevention, smart grid implementation, robotics- AI's significance
in shaping the future of the energy industry. Furthermore, this article
outlines future directions in the integration of AI for renewable energy."
"A Market-Oriented Programming Environment and its Application to
  Distributed Multicommodity Flow Problems","Market price systems constitute a well-understood class of mechanisms that
under certain conditions provide effective decentralization of decision making
with minimal communication overhead. In a market-oriented programming approach
to distributed problem solving, we derive the activities and resource
allocations for a set of computational agents by computing the competitive
equilibrium of an artificial economy. WALRAS provides basic constructs for
defining computational market structures, and protocols for deriving their
corresponding price equilibria. In a particular realization of this approach
for a form of multicommodity flow problem, we see that careful construction of
the decision process according to economic principles can lead to efficient
distributed resource allocation, and that the behavior of the system can be
meaningfully analyzed in economic terms."
"Learning the Past Tense of English Verbs: The Symbolic Pattern
  Associator vs. Connectionist Models","Learning the past tense of English verbs - a seemingly minor aspect of
language acquisition - has generated heated debates since 1986, and has become
a landmark task for testing the adequacy of cognitive modeling. Several
artificial neural networks (ANNs) have been implemented, and a challenge for
better symbolic models has been posed. In this paper, we present a
general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree
learning algorithm ID3. We conduct extensive head-to-head comparisons on the
generalization ability between ANN models and the SPA under different
representations. We conclude that the SPA generalizes the past tense of unseen
verbs better than ANN models by a wide margin, and we offer insights as to why
this should be the case. We also discuss a new default strategy for
decision-tree learning algorithms."
"Exploring the Decision Forest: An Empirical Investigation of Occam's
  Razor in Decision Tree Induction","We report on a series of experiments in which all decision trees consistent
with the training data are constructed. These experiments were run to gain an
understanding of the properties of the set of consistent decision trees and the
factors that affect the accuracy of individual trees. In particular, we
investigated the relationship between the size of a decision tree consistent
with some training data and the accuracy of the tree on test data. The
experiments were performed on a massively parallel Maspar computer. The results
of the experiments on several artificial and two real world problems indicate
that, for many of the problems investigated, smaller consistent decision trees
are on average less accurate than the average accuracy of slightly larger
trees."
A System for Induction of Oblique Decision Trees,"This article describes a new system for induction of oblique decision trees.
This system, OC1, combines deterministic hill-climbing with two forms of
randomization to find a good oblique split (in the form of a hyperplane) at
each node of a decision tree. Oblique decision tree methods are tuned
especially for domains in which the attributes are numeric, although they can
be adapted to symbolic or mixed symbolic/numeric attributes. We present
extensive empirical studies, using both real and artificial data, that analyze
OC1's ability to construct oblique trees that are smaller and more accurate
than their axis-parallel counterparts. We also examine the benefits of
randomization for the construction of oblique decision trees."
Integrative Windowing,"In this paper we re-investigate windowing for rule learning algorithms. We
show that, contrary to previous results for decision tree learning, windowing
can in fact achieve significant run-time gains in noise-free domains and
explain the different behavior of rule learning algorithms by the fact that
they learn each rule independently. The main contribution of this paper is
integrative windowing, a new type of algorithm that further exploits this
property by integrating good rules into the final theory right after they have
been discovered. Thus it avoids re-learning these rules in subsequent
iterations of the windowing process. Experimental evidence in a variety of
noise-free domains shows that integrative windowing can in fact achieve
substantial run-time gains. Furthermore, we discuss the problem of noise in
windowing and present an algorithm that is able to achieve run-time gains in a
set of experiments in a simple domain with artificial noise."
"Optimization of Evolutionary Neural Networks Using Hybrid Learning
  Algorithms","Evolutionary artificial neural networks (EANNs) refer to a special class of
artificial neural networks (ANNs) in which evolution is another fundamental
form of adaptation in addition to learning. Evolutionary algorithms are used to
adapt the connection weights, network architecture and learning algorithms
according to the problem environment. Even though evolutionary algorithms are
well known as efficient global search algorithms, very often they miss the best
local solutions in the complex solution space. In this paper, we propose a
hybrid meta-heuristic learning approach combining evolutionary learning and
local search methods (using 1st and 2nd order error information) to improve the
learning and faster convergence obtained using a direct evolutionary approach.
The proposed technique is tested on three different chaotic time series and the
test results are compared with some popular neuro-fuzzy systems and a recently
developed cutting angle method of global optimization. Empirical results reveal
that the proposed technique is efficient in spite of the computational
complexity."
"On the Implicit and on the Artificial - Morphogenesis and Emergent
  Aesthetics in Autonomous Collective Systems","Imagine a ""machine"" where there is no pre-commitment to any particular
representational scheme: the desired behaviour is distributed and roughly
specified simultaneously among many parts, but there is minimal specification
of the mechanism required to generate that behaviour, i.e. the global behaviour
evolves from the many relations of multiple simple behaviours. A machine that
lives to and from/with Synergy. An artificial super-organism that avoids
specific constraints and emerges within multiple low-level implicit
bio-inspired mechanisms. KEYWORDS: Complex Science, ArtSBots Project, Swarm
Intelligence, Stigmergy, UnManned Art, Symbiotic Art, Swarm Paintings, Robot
Paintings, Non-Human Art, Painting Emergence and Cooperation, Art and
Complexity, ArtBots: The Robot Talent Show."
"Artificial Immune Systems (AIS) - A New Paradigm for Heuristic Decision
  Making","Over the last few years, more and more heuristic decision making techniques
have been inspired by nature, e.g. evolutionary algorithms, ant colony
optimisation and simulated annealing. More recently, a novel computational
intelligence technique inspired by immunology has emerged, called Artificial
Immune Systems (AIS). This immune system inspired technique has already been
useful in solving some computational problems. In this keynote, we will very
briefly describe the immune system metaphors that are relevant to AIS. We will
then give some illustrative real-world problems suitable for AIS use and show a
step-by-step algorithm walkthrough. A comparison of AIS to other well-known
algorithms and areas for future work will round this keynote off. It should be
noted that as AIS is still a young and evolving field, there is not yet a fixed
algorithm template and hence actual implementations might differ somewhat from
the examples given here."
"The Gn,m Phase Transition is Not Hard for the Hamiltonian Cycle Problem","Using an improved backtrack algorithm with sophisticated pruning techniques,
we revise previous observations correlating a high frequency of hard to solve
Hamiltonian Cycle instances with the Gn,m phase transition between
Hamiltonicity and non-Hamiltonicity. Instead all tested graphs of 100 to 1500
vertices are easily solved. When we artificially restrict the degree sequence
with a bounded maximum degree, although there is some increase in difficulty,
the frequency of hard graphs is still low. When we consider more regular graphs
based on a generalization of knight's tours, we observe frequent instances of
really hard graphs, but on these the average degree is bounded by a constant.
We design a set of graphs with a feature our algorithm is unable to detect and
so are very hard for our algorithm, but in these we can vary the average degree
from O(1) to O(n). We have so far found no class of graphs correlated with the
Gn,m phase transition which asymptotically produces a high frequency of hard
instances."
AntNet: Distributed Stigmergetic Control for Communications Networks,"This paper introduces AntNet, a novel approach to the adaptive learning of
routing tables in communications networks. AntNet is a distributed, mobile
agents based Monte Carlo system that was inspired by recent work on the ant
colony metaphor for solving optimization problems. AntNet's agents concurrently
explore the network and exchange collected information. The communication among
the agents is indirect and asynchronous, mediated by the network itself. This
form of communication is typical of social insects and is called stigmergy. We
compare our algorithm with six state-of-the-art routing algorithms coming from
the telecommunications and machine learning fields. The algorithms' performance
is evaluated over a set of realistic testbeds. We run many experiments over
real and artificial IP datagram networks with increasing number of nodes and
under several paradigmatic spatial and temporal traffic distributions. Results
are very encouraging. AntNet showed superior performance under all the
experimental conditions with respect to its competitors. We analyze the main
characteristics of the algorithm and try to explain the reasons for its
superiority."
"Using Artificial Bee Colony Algorithm for MLP Training on Earthquake
  Time Series Data Prediction","Nowadays, computer scientists have shown the interest in the study of social
insect's behaviour in neural networks area for solving different combinatorial
and statistical problems. Chief among these is the Artificial Bee Colony (ABC)
algorithm. This paper investigates the use of ABC algorithm that simulates the
intelligent foraging behaviour of a honey bee swarm. Multilayer Perceptron
(MLP) trained with the standard back propagation algorithm normally utilises
computationally intensive training algorithms. One of the crucial problems with
the backpropagation (BP) algorithm is that it can sometimes yield the networks
with suboptimal weights because of the presence of many local optima in the
solution space. To overcome ABC algorithm used in this work to train MLP
learning the complex behaviour of earthquake time series data trained by BP,
the performance of MLP-ABC is benchmarked against MLP training with the
standard BP. The experimental result shows that MLP-ABC performance is better
than MLP-BP for time series data."
"Extended Mixture of MLP Experts by Hybrid of Conjugate Gradient Method
  and Modified Cuckoo Search","This paper investigates a new method for improving the learning algorithm of
Mixture of Experts (ME) model using a hybrid of Modified Cuckoo Search (MCS)
and Conjugate Gradient (CG) as a second order optimization technique. The CG
technique is combined with Back-Propagation (BP) algorithm to yield a much more
efficient learning algorithm for ME structure. In addition, the experts and
gating networks in enhanced model are replaced by CG based Multi-Layer
Perceptrons (MLPs) to provide faster and more accurate learning. The CG is
considerably depends on initial weights of connections of Artificial Neural
Network (ANN), so, a metaheuristic algorithm, the so-called Modified Cuckoo
Search is applied in order to select the optimal weights. The performance of
proposed method is compared with Gradient Decent Based ME (GDME) and Conjugate
Gradient Based ME (CGME) in classification and regression problems. The
experimental results show that hybrid MSC and CG based ME (MCS-CGME) has faster
convergence and better performance in utilized benchmark data sets."
"Feature Selection for Generator Excitation Neurocontroller Development
  Using Filter Technique","Essentially, motive behind using control system is to generate suitable
control signal for yielding desired response of a physical process. Control of
synchronous generator has always remained very critical in power system
operation and control. For certain well known reasons power generators are
normally operated well below their steady state stability limit. This raises
demand for efficient and fast controllers. Artificial intelligence has been
reported to give revolutionary outcomes in the field of control engineering.
Artificial Neural Network (ANN), a branch of artificial intelligence has been
used for nonlinear and adaptive control, utilizing its inherent observability.
The overall performance of neurocontroller is dependent upon input features
too. Selecting optimum features to train a neurocontroller optimally is very
critical. Both quality and size of data are of equal importance for better
performance. In this work filter technique is employed to select independent
factors for ANN training."
"A Sampling-Based Approach to Computing Equilibria in Succinct
  Extensive-Form Games","A central task of artificial intelligence is the design of artificial agents
that act towards specified goals in partially observed environments. Since such
environments frequently include interaction over time with other agents with
their own goals, reasoning about such interaction relies on sequential
game-theoretic models such as extensive-form games or some of their succinct
representations such as multi-agent influence diagrams. The current algorithms
for calculating equilibria either work with inefficient representations,
possibly doubly exponential inthe number of time steps, or place strong
assumptions on the game structure. In this paper,we propose a sampling-based
approach, which calculates extensive-form correlated equilibria with small
representations without placing such strong assumptions. Thus, it is practical
in situations where the previous approaches would fail. In addition, our
algorithm allows control over characteristics of the target equilibrium, e.g.,
we can ask for an equilibrium with high social welfare. Our approach is based
on a multiplicativeweight update algorithm analogous to AdaBoost, and Markov
chain Monte Carlo sampling. We prove convergence guarantees and explore the
utility of our approach on several moderately sized multi-player games."
"Improved Local Search in Artificial Bee Colony using Golden Section
  Search","Artificial bee colony (ABC), an optimization algorithm is a recent addition
to the family of population based search algorithm. ABC has taken its
inspiration from the collective intelligent foraging behavior of honey bees. In
this study we have incorporated golden section search mechanism in the
structure of basic ABC to improve the global convergence and prevent to stick
on a local solution. The proposed variant is termed as ILS-ABC. Comparative
numerical results with the state-of-art algorithms show the performance of the
proposal when applied to the set of unconstrained engineering design problems.
The simulated results show that the proposed variant can be successfully
applied to solve real life problems."
Towards the Evolution of Novel Vertical-Axis Wind Turbines,"Renewable and sustainable energy is one of the most important challenges
currently facing mankind. Wind has made an increasing contribution to the
world's energy supply mix, but still remains a long way from reaching its full
potential. In this paper, we investigate the use of artificial evolution to
design vertical-axis wind turbine prototypes that are physically instantiated
and evaluated under approximated wind tunnel conditions. An artificial neural
network is used as a surrogate model to assist learning and found to reduce the
number of fabrications required to reach a higher aerodynamic efficiency,
resulting in an important cost reduction. Unlike in other approaches, such as
computational fluid dynamics simulations, no mathematical formulations are used
and no model assumptions are made."
Estimating Well-Performing Bayesian Networks using Bernoulli Mixtures,"A novel method for estimating Bayesian network (BN) parameters from data is
presented which provides improved performance on test data. Previous research
has shown the value of representing conditional probability distributions
(CPDs) via neural networks(Neal 1992), noisy-OR gates (Neal 1992, Diez 1993)and
decision trees (Friedman and Goldszmidt 1996).The Bernoulli mixture network
(BMN) explicitly represents the CPDs of discrete BN nodes as mixtures of local
distributions,each having a different set of parents.This increases the space
of possible structures which can be considered,enabling the CPDs to have
finer-grained dependencies.The resulting estimation procedure induces a
modelthat is better able to emulate the underlying interactions occurring in
the data than conventional conditional Bernoulli network models.The results for
artificially generated data indicate that overfitting is best reduced by
restricting the complexity of candidate mixture substructures local to each
node. Furthermore, mixtures of very simple substructures can perform almost as
well as more complex ones.The BMN is also applied to data collected from an
online adventure game with an application to keyhole plan recognition. The
results show that the BMN-based model brings a dramatic improvement in
performance over a conventional BN model."
Gaussian Process Networks,"In this paper we address the problem of learning the structure of a Bayesian
network in domains with continuous variables. This task requires a procedure
for comparing different candidate structures. In the Bayesian framework, this
is done by evaluating the {em marginal likelihood/} of the data given a
candidate structure. This term can be computed in closed-form for standard
parametric families (e.g., Gaussians), and can be approximated, at some
computational cost, for some semi-parametric families (e.g., mixtures of
Gaussians).
  We present a new family of continuous variable probabilistic networks that
are based on {em Gaussian Process/} priors. These priors are semi-parametric in
nature and can learn almost arbitrary noisy functional relations. Using these
priors, we can directly compute marginal likelihoods for structure learning.
The resulting method can discover a wide range of functional dependencies in
multivariate data. We develop the Bayesian score of Gaussian Process Networks
and describe how to learn them from data. We present empirical results on
artificial data as well as on real-life domains with non-linear dependencies."
"Optimization of Inter-Subnet Belief Updating in Multiply Sectioned
  Bayesian Networks","Recent developments show that Multiply Sectioned Bayesian Networks (MSBNs)
can be used for diagnosis of natural systems as well as for model-based
diagnosis of artificial systems. They can be applied to single-agent oriented
reasoning systems as well as multi-agent distributed probabilistic reasoning
systems. Belief propagation between a pair of subnets plays a central role in
maintenance of global consistency in a MSBN. This paper studies the operation
UpdateBelief, presented originally with MSBNs, for inter-subnet propagation. We
analyze how the operation achieves its intended functionality, which provides
hints as for how its efficiency can be improved. We then define two new
versions of UpdateBelief that reduce the computation time for inter-subnet
propagation. One of them is optimal in the sense that the minimum amount of
computation for coordinating multi-linkage belief propagation is required. The
optimization problem is solved through the solution of a graph-theoretic
problem: the minimum weight open tour in a tree."
Structured Message Passing,"In this paper, we present structured message passing (SMP), a unifying
framework for approximate inference algorithms that take advantage of
structured representations such as algebraic decision diagrams and sparse hash
tables. These representations can yield significant time and space savings over
the conventional tabular representation when the message has several identical
values (context-specific independence) or zeros (determinism) or both in its
range. Therefore, in order to fully exploit the power of structured
representations, we propose to artificially introduce context-specific
independence and determinism in the messages. This yields a new class of
powerful approximate inference algorithms which includes popular algorithms
such as cluster-graph Belief propagation (BP), expectation propagation and
particle BP as special cases. We show that our new algorithms introduce several
interesting bias-variance trade-offs. We evaluate these trade-offs empirically
and demonstrate that our new algorithms are more accurate and scalable than
state-of-the-art techniques."
Treedy: A Heuristic for Counting and Sampling Subsets,"Consider a collection of weighted subsets of a ground set N. Given a query
subset Q of N, how fast can one (1) find the weighted sum over all subsets of
Q, and (2) sample a subset of Q proportionally to the weights? We present a
tree-based greedy heuristic, Treedy, that for a given positive tolerance d
answers such counting and sampling queries to within a guaranteed relative
error d and total variation distance d, respectively. Experimental results on
artificial instances and in application to Bayesian structure discovery in
Bayesian networks show that approximations yield dramatic savings in running
time compared to exact computation, and that Treedy typically outperforms a
previously proposed sorting-based heuristic."
"ABC-SG: A New Artificial Bee Colony Algorithm-Based Distance of
  Sequential Data Using Sigma Grams","The problem of similarity search is one of the main problems in computer
science. This problem has many applications in text-retrieval, web search,
computational biology, bioinformatics and others. Similarity between two data
objects can be depicted using a similarity measure or a distance metric. There
are numerous distance metrics in the literature, some are used for a particular
data type, and others are more general. In this paper we present a new distance
metric for sequential data which is based on the sum of n-grams. The novelty of
our distance is that these n-grams are weighted using artificial bee colony; a
recent optimization algorithm based on the collective intelligence of a swarm
of bees on their search for nectar. This algorithm has been used in optimizing
a large number of numerical problems. We validate the new distance
experimentally."
"Evolutionary Search in the Space of Rules for Creation of New Two-Player
  Board Games","Games have always been a popular test bed for artificial intelligence
techniques. Game developers are always in constant search for techniques that
can automatically create computer games minimizing the developer's task. In
this work we present an evolutionary strategy based solution towards the
automatic generation of two player board games. To guide the evolutionary
process towards games, which are entertaining, we propose a set of metrics.
These metrics are based upon different theories of entertainment in computer
games. This work also compares the entertainment value of the evolved games
with the existing popular board based games. Further to verify the
entertainment value of the evolved games with the entertainment value of the
human user a human user survey is conducted. In addition to the user survey we
check the learnability of the evolved games using an artificial neural network
based controller. The proposed metrics and the evolutionary process can be
employed for generating new and entertaining board games, provided an initial
search space is given to the evolutionary algorithm."
"A Novel Hybrid Crossover based Artificial Bee Colony Algorithm for
  Optimization Problem","Artificial bee colony (ABC) algorithm has proved its importance in solving a
number of problems including engineering optimization problems. ABC algorithm
is one of the most popular and youngest member of the family of population
based nature inspired meta-heuristic swarm intelligence method. ABC has been
proved its superiority over some other Nature Inspired Algorithms (NIA) when
applied for both benchmark functions and real world problems. The performance
of search process of ABC depends on a random value which tries to balance
exploration and exploitation phase. In order to increase the performance it is
required to balance the exploration of search space and exploitation of optimal
solution of the ABC. This paper outlines a new hybrid of ABC algorithm with
Genetic Algorithm. The proposed method integrates crossover operation from
Genetic Algorithm (GA) with original ABC algorithm. The proposed method is
named as Crossover based ABC (CbABC). The CbABC strengthens the exploitation
phase of ABC as crossover enhances exploration of search space. The CbABC
tested over four standard benchmark functions and a popular continuous
optimization problem."
Memory shapes time perception and intertemporal choices,"There is a consensus that human and non-human subjects experience temporal
distortions in many stages of their perceptual and decision-making systems.
Similarly, intertemporal choice research has shown that decision-makers
undervalue future outcomes relative to immediate ones. Here we combine
techniques from information theory and artificial intelligence to show how both
temporal distortions and intertemporal choice preferences can be explained as a
consequence of the coding efficiency of sensorimotor representation. In
particular, the model implies that interactions that constrain future behavior
are perceived as being both longer in duration and more valuable. Furthermore,
using simulations of artificial agents, we investigate how memory constraints
enforce a renormalization of the perceived timescales. Our results show that
qualitatively different discount functions, such as exponential and hyperbolic
discounting, arise as a consequence of an agent's probabilistic model of the
world."
Bridging LSTM Architecture and the Neural Dynamics during Reading,"Recently, the long short-term memory neural network (LSTM) has attracted wide
interest due to its success in many tasks. LSTM architecture consists of a
memory cell and three gates, which looks similar to the neuronal networks in
the brain. However, there still lacks the evidence of the cognitive
plausibility of LSTM architecture as well as its working mechanism. In this
paper, we study the cognitive plausibility of LSTM by aligning its internal
architecture with the brain activity observed via fMRI when the subjects read a
story. Experiment results show that the artificial memory vector in LSTM can
accurately predict the observed sequential brain activities, indicating the
correlation between LSTM architecture and the cognitive process of story
reading."
DeepMind Lab,"DeepMind Lab is a first-person 3D game platform designed for research and
development of general artificial intelligence and machine learning systems.
DeepMind Lab can be used to study how autonomous artificial agents may learn
complex tasks in large, partially observed, and visually diverse worlds.
DeepMind Lab has a simple and flexible API enabling creative task-designs and
novel AI-designs to be explored and quickly iterated upon. It is powered by a
fast and widely recognised game engine, and tailored for effective use by the
research community."
Intelligent information extraction based on artificial neural network,"Question Answering System (QAS) is used for information retrieval and natural
language processing (NLP) to reduce human effort. There are numerous QAS based
on the user documents present today, but they all are limited to providing
objective answers and process simple questions only. Complex questions cannot
be answered by the existing QAS, as they require interpretation of the current
and old data as well as the question asked by the user. The above limitations
can be overcome by using deep cases and neural network. Hence we propose a
modified QAS in which we create a deep artificial neural network with
associative memory from text documents. The modified QAS processes the contents
of the text document provided to it and find the answer to even complex
questions in the documents."
"Propositional Knowledge Representation and Reasoning in Restricted
  Boltzmann Machines","While knowledge representation and reasoning are considered the keys for
human-level artificial intelligence, connectionist networks have been shown
successful in a broad range of applications due to their capacity for robust
learning and flexible inference under uncertainty. The idea of representing
symbolic knowledge in connectionist networks has been well-received and
attracted much attention from research community as this can establish a
foundation for integration of scalable learning and sound reasoning. In
previous work, there exist a number of approaches that map logical inference
rules with feed-forward propagation of artificial neural networks (ANN).
However, the discriminative structure of an ANN requires the separation of
input/output variables which makes it difficult for general reasoning where any
variables should be inferable. Other approaches address this issue by employing
generative models such as symmetric connectionist networks, however, they are
difficult and convoluted. In this paper we propose a novel method to represent
propositional formulas in restricted Boltzmann machines which is less complex,
especially in the cases of logical implications and Horn clauses. An
integration system is then developed and evaluated in real datasets which shows
promising results."
"Data Fusion on Motion and Magnetic Sensors embedded on Mobile Devices
  for the Identification of Activities of Daily Living","Several types of sensors have been available in off-the-shelf mobile devices,
including motion, magnetic, vision, acoustic, and location sensors. This paper
focuses on the fusion of the data acquired from motion and magnetic sensors,
i.e., accelerometer, gyroscope and magnetometer sensors, for the recognition of
Activities of Daily Living (ADL) using pattern recognition techniques. The
system developed in this study includes data acquisition, data processing, data
fusion, and artificial intelligence methods. Artificial Neural Networks (ANN)
are included in artificial intelligence methods, which are used in this study
for the recognition of ADL. The purpose of this study is the creation of a new
method using ANN for the identification of ADL, comparing three types of ANN,
in order to achieve results with a reliable accuracy. The best accuracy was
obtained with Deep Learning, which, after the application of the L2
regularization and normalization techniques on the sensors data, reports an
accuracy of 89.51%."
A Genetic Programming Framework for 2D Platform AI,"There currently exists a wide range of techniques to model and evolve
artificial players for games. Existing techniques range from black box neural
networks to entirely hand-designed solutions. In this paper, we demonstrate the
feasibility of a genetic programming framework using human controller input to
derive meaningful artificial players which can, later on, be optimised by hand.
The current state of the art in game character design relies heavily on human
designers to manually create and edit scripts and rules for game characters. To
address this manual editing bottleneck, current computational intelligence
techniques approach the issue with fully autonomous character generators,
replacing most of the design process using black box solutions such as neural
networks or the like. Our GP approach to this problem creates character
controllers which can be further authored and developed by a designer it also
offers designers to included their play style without the need to use a
programming language. This keeps the designer in the loop while reducing
repetitive manual labour. Our system also provides insights into how players
express themselves in games and into deriving appropriate models for
representing those insights. We present our framework, supporting findings and
open challenges."
A Dataset and Architecture for Visual Reasoning with a Working Memory,"A vexing problem in artificial intelligence is reasoning about events that
occur in complex, changing visual stimuli such as in video analysis or game
play. Inspired by a rich tradition of visual reasoning and memory in cognitive
psychology and neuroscience, we developed an artificial, configurable visual
question and answer dataset (COG) to parallel experiments in humans and
animals. COG is much simpler than the general problem of video analysis, yet it
addresses many of the problems relating to visual and logical reasoning and
memory -- problems that remain challenging for modern deep learning
architectures. We additionally propose a deep learning architecture that
performs competitively on other diagnostic VQA datasets (i.e. CLEVR) as well as
easy settings of the COG dataset. However, several settings of COG result in
datasets that are progressively more challenging to learn. After training, the
network can zero-shot generalize to many new tasks. Preliminary analyses of the
network architectures trained on COG demonstrate that the network accomplishes
the task in a manner interpretable to humans."
"A unified strategy for implementing curiosity and empowerment driven
  reinforcement learning","Although there are many approaches to implement intrinsically motivated
artificial agents, the combined usage of multiple intrinsic drives remains
still a relatively unexplored research area. Specifically, we hypothesize that
a mechanism capable of quantifying and controlling the evolution of the
information flow between the agent and the environment could be the fundamental
component for implementing a higher degree of autonomy into artificial
intelligent agents. This paper propose a unified strategy for implementing two
semantically orthogonal intrinsic motivations: curiosity and empowerment.
Curiosity reward informs the agent about the relevance of a recent agent
action, whereas empowerment is implemented as the opposite information flow
from the agent to the environment that quantifies the agent's potential of
controlling its own future. We show that an additional homeostatic drive is
derived from the curiosity reward, which generalizes and enhances the
information gain of a classical curious/heterostatic reinforcement learning
agent. We show how a shared internal model by curiosity and empowerment
facilitates a more efficient training of the empowerment function. Finally, we
discuss future directions for further leveraging the interplay between these
two intrinsic rewards."
"Assessing the Contribution of Semantic Congruency to Multisensory
  Integration and Conflict Resolution","The efficient integration of multisensory observations is a key property of
the brain that yields the robust interaction with the environment. However,
artificial multisensory perception remains an open issue especially in
situations of sensory uncertainty and conflicts. In this work, we extend
previous studies on audio-visual (AV) conflict resolution in complex
environments. In particular, we focus on quantitatively assessing the
contribution of semantic congruency during an AV spatial localization task. In
addition to conflicts in the spatial domain (i.e. spatially misaligned
stimuli), we consider gender-specific conflicts with male and female avatars.
Our results suggest that while semantically related stimuli affect the
magnitude of the visual bias (perceptually shifting the location of the sound
towards a semantically congruent visual cue), humans still strongly rely on
environmental statistics to solve AV conflicts. Together with previously
reported results, this work contributes to a better understanding of how
multisensory integration and conflict resolution can be modelled in artificial
agents and robots operating in real-world environments."
"Prediction of Industrial Process Parameters using Artificial
  Intelligence Algorithms","In the present paper, a method of defining the industrial process parameters
for a new product using machine learning algorithms will be presented. The
study will describe how to go from the product characteristics till the
prediction of the suitable machine parameters to produce a good quality of this
product, and this is based on an historical training dataset of similar
products with their respective process parameters. In the first part of our
study, we will focus on the ultrasonic welding process definition, welding
parameters and on how it operate. While in second part, we present the design
and implementation of the prediction models such multiple linear regression,
support vector regression, and we compare them to an artificial neural networks
algorithm. In the following part, we present a new application of Convolutional
Neural Networks (CNN) to the industrial process parameters prediction. In
addition, we will propose the generalization approach of our CNN to any
prediction problem of industrial process parameters. Finally the results of the
four methods will be interpreted and discussed."
"Artificial Intelligence Assists Discovery of Reaction Coordinates and
  Mechanisms from Molecular Dynamics Simulations","Exascale computing holds great opportunities for molecular dynamics (MD)
simulations. However, to take full advantage of the new possibilities, we must
learn how to focus computational power on the discovery of complex molecular
mechanisms, and how to extract them from enormous amounts of data. Both aspects
still rely heavily on human experts, which becomes a serious bottleneck when a
large number of parallel simulations have to be orchestrated to take full
advantage of the available computing power. Here, we use artificial
intelligence (AI) both to guide the sampling and to extract the relevant
mechanistic information. We combine advanced sampling schemes with statistical
inference, artificial neural networks, and deep learning to discover molecular
mechanisms from MD simulations. Our framework adaptively and autonomously
initializes simulations and learns the sampled mechanism, and is thus suitable
for massively parallel computing architectures. We propose practical solutions
to make the neural networks interpretable, as illustrated in applications to
molecular systems."
From Knowledge Map to Mind Map: Artificial Imagination,"Imagination is one of the most important factors which makes an artistic
painting unique and impressive. With the rapid development of Artificial
Intelligence, more and more researchers try to create painting with AI
technology automatically. However, lacking of imagination is still a main
problem for AI painting. In this paper, we propose a novel approach to inject
rich imagination into a special painting art Mind Map creation. We firstly
consider lexical and phonological similarities of seed word, then learn and
inherit original painting style of the author, and finally apply Dadaism and
impossibility of improvisation principles into painting process. We also design
several metrics for imagination evaluation. Experimental results show that our
proposed method can increase imagination of painting and also improve its
overall quality."
"Characterizing the Social Interactions in the Artificial Bee Colony
  Algorithm","Computational swarm intelligence consists of multiple artificial simple
agents exchanging information while exploring a search space. Despite a rich
literature in the field, with works improving old approaches and proposing new
ones, the mechanism by which complex behavior emerges in these systems is still
not well understood. This literature gap hinders the researchers' ability to
deal with known problems in swarms intelligence such as premature convergence,
and the balance of coordination and diversity among agents. Recent advances in
the literature, however, have proposed to study these systems via the network
that emerges from the social interactions within the swarm (i.e., the
interaction network). In our work, we propose a definition of the interaction
network for the Artificial Bee Colony (ABC) algorithm. With our approach, we
captured striking idiosyncrasies of the algorithm. We uncovered the different
patterns of social interactions that emerge from each type of bee, revealing
the importance of the bees variations throughout the iterations of the
algorithm. We found that ABC exhibits a dynamic information flow through the
use of different bees but lacks continuous coordination between the agents."
"The Twin-System Approach as One Generic Solution for XAI: An Overview of
  ANN-CBR Twins for Explaining Deep Learning","The notion of twin systems is proposed to address the eXplainable AI (XAI)
problem, where an uninterpretable black-box system is mapped to a white-box
'twin' that is more interpretable. In this short paper, we overview very recent
work that advances a generic solution to the XAI problem, the so called twin
system approach. The most popular twinning in the literature is that between an
Artificial Neural Networks (ANN ) as a black box and Case Based Reasoning (CBR)
system as a white-box, where the latter acts as an interpretable proxy for the
former. We outline how recent work reviving this idea has applied it to deep
learning methods. Furthermore, we detail the many fruitful directions in which
this work may be taken; such as, determining the most (i) accurate
feature-weighting methods to be used, (ii) appropriate deployments for
explanatory cases, (iii) useful cases of explanatory value to users."
Interval timing in deep reinforcement learning agents,"The measurement of time is central to intelligent behavior. We know that both
animals and artificial agents can successfully use temporal dependencies to
select actions. In artificial agents, little work has directly addressed (1)
which architectural components are necessary for successful development of this
ability, (2) how this timing ability comes to be represented in the units and
actions of the agent, and (3) whether the resulting behavior of the system
converges on solutions similar to those of biology. Here we studied interval
timing abilities in deep reinforcement learning agents trained end-to-end on an
interval reproduction paradigm inspired by experimental literature on
mechanisms of timing. We characterize the strategies developed by recurrent and
feedforward agents, which both succeed at temporal reproduction using distinct
mechanisms, some of which bear specific and intriguing similarities to
biological systems. These findings advance our understanding of how agents come
to represent time, and they highlight the value of experimentally inspired
approaches to characterizing agent abilities."
Project Thyia: A Forever Gameplayer,"The space of Artificial Intelligence entities is dominated by conversational
bots. Some of them fit in our pockets and we take them everywhere we go, or
allow them to be a part of human homes. Siri, Alexa, they are recognised as
present in our world. But a lot of games research is restricted to existing in
the separate realm of software. We enter different worlds when playing games,
but those worlds cease to exist once we quit. Similarly, AI game-players are
run once on a game (or maybe for longer periods of time, in the case of
learning algorithms which need some, still limited, period for training), and
they cease to exist once the game ends. But what if they didn't? What if there
existed artificial game-players that continuously played games, learned from
their experiences and kept getting better? What if they interacted with the
real world and us, humans: live-streaming games, chatting with viewers,
accepting suggestions for strategies or games to play, forming opinions on
popular game titles? In this paper, we introduce the vision behind a new
project called Thyia, which focuses around creating a present, continuous,
`always-on', interactive game-player."
"On Hard Exploration for Reinforcement Learning: a Case Study in
  Pommerman","How to best explore in domains with sparse, delayed, and deceptive rewards is
an important open problem for reinforcement learning (RL). This paper considers
one such domain, the recently-proposed multi-agent benchmark of Pommerman. This
domain is very challenging for RL --- past work has shown that model-free RL
algorithms fail to achieve significant learning without artificially reducing
the environment's complexity. In this paper, we illuminate reasons behind this
failure by providing a thorough analysis on the hardness of random exploration
in Pommerman. While model-free random exploration is typically futile, we
develop a model-based automatic reasoning module that can be used for safer
exploration by pruning actions that will surely lead the agent to death. We
empirically demonstrate that this module can significantly improve learning."
Artificial intelligence empowered multi-AGVs in manufacturing systems,"AGVs are driverless robotic vehicles that picks up and delivers materials.
How to improve the efficiency while preventing deadlocks is the core issue in
designing AGV systems. In this paper, we propose an approach to tackle this
problem.The proposed approach includes a traditional AGV scheduling algorithm,
which aims at solving deadlock problems, and an artificial neural network based
component, which predict future tasks of the AGV system, and make decisions on
whether to send an AGV to the predicted starting location of the upcoming
task,so as to save the time of waiting for an AGV to go to there first when the
upcoming task is created. Simulation results show that the proposed method
significantly improves the efficiency as against traditional method, up to 20%
to 30%."
"Controlled Text Generation for Data Augmentation in Intelligent
  Artificial Agents","Data availability is a bottleneck during early stages of development of new
capabilities for intelligent artificial agents. We investigate the use of text
generation techniques to augment the training data of a popular commercial
artificial agent across categories of functionality, with the goal of faster
development of new functionality. We explore a variety of encoder-decoder
generative models for synthetic training data generation and propose using
conditional variational auto-encoders. Our approach requires only direct
optimization, works well with limited data and significantly outperforms the
previous controlled text generation techniques. Further, the generated data are
used as additional training samples in an extrinsic intent classification task,
leading to improved performance by up to 5\% absolute f-score in low-resource
cases, validating the usefulness of our approach."
"Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating
  Explainable AI Systems","Explainable artificially intelligent (XAI) systems form part of
sociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet,
current XAI systems are rarely evaluated by measuring the performance of
human+AI teams on actual decision-making tasks. We conducted two online
experiments and one in-person think-aloud study to evaluate two currently
common techniques for evaluating XAI systems: (1) using proxy, artificial tasks
such as how well humans predict the AI's decision from the given explanations,
and (2) using subjective measures of trust and preference as predictors of
actual performance. The results of our experiments demonstrate that evaluations
with proxy tasks did not predict the results of the evaluations with the actual
decision-making tasks. Further, the subjective measures on evaluations with
actual decision-making tasks did not predict the objective performance on those
same tasks. Our results suggest that by employing misleading evaluation
methods, our field may be inadvertently slowing its progress toward developing
human+AI teams that can reliably perform better than humans or AIs alone."
The computerization of archaeology: survey on AI techniques,"This paper analyses the application of artificial intelligence techniques to
various areas of archaeology and more specifically: a) The use of software
tools as a creative stimulus for the organization of exhibitions; the use of
humanoid robots and holographic displays as guides that interact and involve
museum visitors; b) The analysis of methods for the classification of fragments
found in archaeological excavations and for the reconstruction of ceramics,
with the recomposition of the parts of text missing from historical documents
and epigraphs; c) The cataloguing and study of human remains to understand the
social and historical context of belonging with the demonstration of the
effectiveness of the AI techniques used; d) The detection of particularly
difficult terrestrial archaeological sites with the analysis of the
architectures of the Artificial Neural Networks most suitable for solving the
problems presented by the site; the design of a study for the exploration of
marine archaeological sites, located at depths that cannot be reached by man,
through the construction of a freely explorable 3D version."
"Should artificial agents ask for help in human-robot collaborative
  problem-solving?","Transferring as fast as possible the functioning of our brain to artificial
intelligence is an ambitious goal that would help advance the state of the art
in AI and robotics. It is in this perspective that we propose to start from
hypotheses derived from an empirical study in a human-robot interaction and to
verify if they are validated in the same way for children as for a basic
reinforcement learning algorithm. Thus, we check whether receiving help from an
expert when solving a simple close-ended task (the Towers of Hano\""i) allows to
accelerate or not the learning of this task, depending on whether the
intervention is canonical or requested by the player. Our experiences have
allowed us to conclude that, whether requested or not, a Q-learning algorithm
benefits in the same way from expert help as children do."
Rinascimento: using event-value functions for playing Splendor,"In the realm of games research, Artificial General Intelligence algorithms
often use score as main reward signal for learning or playing actions. However
this has shown its severe limitations when the point rewards are very rare or
absent until the end of the game. This paper proposes a new approach based on
event logging: the game state triggers an event every time one of its features
changes. These events are processed by an Event-value Function (EF) that
assigns a value to a single action or a sequence. The experiments have shown
that such approach can mitigate the problem of scarce point rewards and improve
the AI performance. Furthermore this represents a step forward in controlling
the strategy adopted by the artificial agent, by describing a much richer and
controllable behavioural space through the EF. Tuned EF are able to neatly
synthesise the relevance of the events in the game. Agents using an EF show
more robust when playing games with several opponents."
Growing Artificial Neural Networks,"Pruning is a legitimate method for reducing the size of a neural network to
fit in low SWaP hardware, but the networks must be trained and pruned offline.
We propose an algorithm, Artificial Neurogenesis (ANG), that grows rather than
prunes the network and enables neural networks to be trained and executed in
low SWaP embedded hardware. ANG accomplishes this by using the training data to
determine critical connections between layers before the actual training takes
place. Our experiments use a modified LeNet-5 as a baseline neural network that
achieves a test accuracy of 98.74% using a total of 61,160 weights. An ANG
grown network achieves a test accuracy of 98.80% with only 21,211 weights."
"Artificial Buildings: Safety, Complexity and a Quantifiable Measure of
  Beauty","A place to live is one of the most crucial necessities for all living
organisms since the advent of life on planet Earth. The nature of homes has
changed considerably over time. At the very early stages, human begins lived in
natural places such as caves. Later on, they started to use their intelligence
to build places with special purposes. Nowadays, modern technologies such as
robotics and artificial intelligence have made their ways into the construction
process and opened up a whole new area of opportunities and concerns that may
be of interest to both technologists and philosophers. In this article, I
review the evolution of buildings from fully natural to fully artificial and
discuss philosophical thoughts that a fully automated construction technology
may raise. I elaborate on the safety concerns of a fully automated
architectural process. Then, I'll borrow Kolmogorov complexity from algorithmic
information theory to define a complexity measure for buildings. The proposed
measure is then used to provide a quantifiable measure of beauty."
An Open-World Simulated Environment for Developmental Robotics,"As the current trend of artificial intelligence is shifting towards
self-supervised learning, conventional norms such as highly curated
domain-specific data, application-specific learning models, extrinsic reward
based learning policies etc. might not provide with the suitable ground for
such developments. In this paper, we introduce SEDRo, a Simulated Environment
for Developmental Robotics which allows a learning agent to have similar
experiences that a human infant goes through from the fetus stage up to 12
months. A series of simulated tests based on developmental psychology will be
used to evaluate the progress of a learning model."
"Toward Machine-Guided, Human-Initiated Explanatory Interactive Learning","Recent work has demonstrated the promise of combining local explanations with
active learning for understanding and supervising black-box models. Here we
show that, under specific conditions, these algorithms may misrepresent the
quality of the model being learned. The reason is that the machine illustrates
its beliefs by predicting and explaining the labels of the query instances: if
the machine is unaware of its own mistakes, it may end up choosing queries on
which it performs artificially well. This biases the ""narrative"" presented by
the machine to the user.We address this narrative bias by introducing
explanatory guided learning, a novel interactive learning strategy in which: i)
the supervisor is in charge of choosing the query instances, while ii) the
machine uses global explanations to illustrate its overall behavior and to
guide the supervisor toward choosing challenging, informative instances. This
strategy retains the key advantages of explanatory interaction while avoiding
narrative bias and compares favorably to active learning in terms of sample
complexity. An initial empirical evaluation with a clustering-based prototype
highlights the promise of our approach."
Analogical and Relational Reasoning with Spiking Neural Networks,"Raven's Progressive Matrices have been widely used for measuring abstract
reasoning and intelligence in humans. However for artificial learning systems,
abstract reasoning remains a challenging problem. In this paper we investigate
how neural networks augmented with biologically inspired spiking modules gain a
significant advantage in solving this problem. To illustrate this, we first
investigate the performance of our networks with supervised learning, then with
unsupervised learning. Experiments on the RAVEN dataset show that the overall
accuracy of our supervised networks surpass human-level performance, while our
unsupervised networks significantly outperform existing unsupervised methods.
Finally, our results from both supervised and unsupervised learning illustrate
that, unlike their non-augmented counterparts, networks with spiking modules
are able to extract and encode temporal features without any explicit
instruction, do not heavily rely on training data, and generalise more readily
to new problems. In summary, the results reported here indicate that artificial
neural networks with spiking modules are well suited to solving abstract
reasoning."
"Testing the Quantitative Spacetime Hypothesis using Artificial Narrative
  Comprehension (I) : Bootstrapping Meaning from Episodic Narrative viewed as a
  Feature Landscape","The problem of extracting important and meaningful parts of a sensory data
stream, without prior training, is studied for symbolic sequences, by using
textual narrative as a test case. This is part of a larger study concerning the
extraction of concepts from spacetime processes, and their knowledge
representations within hybrid symbolic-learning `Artificial Intelligence'. Most
approaches to text analysis make extensive use of the evolved human sense of
language and semantics. In this work, streams are parsed without knowledge of
semantics, using only measurable patterns (size and time) within the changing
stream of symbols -- as an event `landscape'. This is a form of interferometry.
Using lightweight procedures that can be run in just a few seconds on a single
CPU, this work studies the validity of the Semantic Spacetime Hypothesis, for
the extraction of concepts as process invariants. This `semantic preprocessor'
may then act as a front-end for more sophisticated long-term graph-based
learning techniques. The results suggest that what we consider important and
interesting about sensory experience is not solely based on higher reasoning,
but on simple spacetime process cues, and this may be how cognitive processing
is bootstrapped in the beginning."
